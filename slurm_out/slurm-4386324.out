model : lenet
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.4
schedule_freq : 10
seed : 1
n_epochs : 50
batch_size : 64
non_wrapped : False
chunked : False
dense : True
parameter_correction : False
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-19 22:58:43
nonzero elements in E: 444260
elements in E: 444260
fraction nonzero: 1.0
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.08
Batch: 20; loss: 2.29; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.31; acc: 0.16
Batch: 80; loss: 2.33; acc: 0.05
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.31; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.2
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.31; acc: 0.08
Batch: 220; loss: 2.33; acc: 0.05
Batch: 240; loss: 2.32; acc: 0.05
Batch: 260; loss: 2.31; acc: 0.12
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.31; acc: 0.06
Batch: 320; loss: 2.3; acc: 0.11
Batch: 340; loss: 2.3; acc: 0.12
Batch: 360; loss: 2.31; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.31; acc: 0.08
Batch: 420; loss: 2.32; acc: 0.05
Batch: 440; loss: 2.31; acc: 0.09
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.31; acc: 0.06
Batch: 500; loss: 2.31; acc: 0.09
Batch: 520; loss: 2.31; acc: 0.05
Batch: 540; loss: 2.29; acc: 0.12
Batch: 560; loss: 2.31; acc: 0.06
Batch: 580; loss: 2.31; acc: 0.11
Batch: 600; loss: 2.3; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.06
Batch: 640; loss: 2.31; acc: 0.06
Batch: 660; loss: 2.3; acc: 0.17
Batch: 680; loss: 2.31; acc: 0.06
Batch: 700; loss: 2.31; acc: 0.06
Batch: 720; loss: 2.31; acc: 0.05
Batch: 740; loss: 2.3; acc: 0.12
Batch: 760; loss: 2.31; acc: 0.11
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.31; train_accuracy: 0.09 

Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.31; acc: 0.05
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.02
Batch: 140; loss: 2.3; acc: 0.05
Val Epoch over. val_loss: 2.3032065349020017; val_accuracy: 0.07951831210191083 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.08
Batch: 20; loss: 2.31; acc: 0.05
Batch: 40; loss: 2.31; acc: 0.05
Batch: 60; loss: 2.31; acc: 0.02
Batch: 80; loss: 2.28; acc: 0.19
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.12
Batch: 160; loss: 2.31; acc: 0.0
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.31; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.31; acc: 0.08
Batch: 260; loss: 2.29; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.31; acc: 0.06
Batch: 320; loss: 2.3; acc: 0.08
Batch: 340; loss: 2.31; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.31; acc: 0.06
Batch: 400; loss: 2.3; acc: 0.11
Batch: 420; loss: 2.31; acc: 0.09
Batch: 440; loss: 2.31; acc: 0.06
Batch: 460; loss: 2.3; acc: 0.09
Batch: 480; loss: 2.3; acc: 0.08
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.29; acc: 0.12
Batch: 540; loss: 2.3; acc: 0.06
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.09
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.12
Batch: 680; loss: 2.3; acc: 0.12
Batch: 700; loss: 2.29; acc: 0.06
Batch: 720; loss: 2.31; acc: 0.06
Batch: 740; loss: 2.3; acc: 0.06
Batch: 760; loss: 2.29; acc: 0.08
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.09 

Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.16
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.05
Val Epoch over. val_loss: 2.2997797568132925; val_accuracy: 0.0973328025477707 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.31; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.31; acc: 0.09
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.31; acc: 0.14
Batch: 220; loss: 2.29; acc: 0.09
Batch: 240; loss: 2.29; acc: 0.17
Batch: 260; loss: 2.3; acc: 0.03
Batch: 280; loss: 2.28; acc: 0.09
Batch: 300; loss: 2.29; acc: 0.2
Batch: 320; loss: 2.29; acc: 0.14
Batch: 340; loss: 2.29; acc: 0.14
Batch: 360; loss: 2.28; acc: 0.17
Batch: 380; loss: 2.31; acc: 0.09
Batch: 400; loss: 2.29; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.29; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.08
Batch: 480; loss: 2.29; acc: 0.09
Batch: 500; loss: 2.28; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.12
Batch: 540; loss: 2.28; acc: 0.19
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.28; acc: 0.16
Batch: 600; loss: 2.31; acc: 0.08
Batch: 620; loss: 2.32; acc: 0.06
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.16
Batch: 700; loss: 2.3; acc: 0.06
Batch: 720; loss: 2.29; acc: 0.08
Batch: 740; loss: 2.31; acc: 0.08
Batch: 760; loss: 2.27; acc: 0.14
Batch: 780; loss: 2.3; acc: 0.09
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.14
Batch: 20; loss: 2.28; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.08
Batch: 60; loss: 2.27; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.16
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.27; acc: 0.08
Val Epoch over. val_loss: 2.288566073034979; val_accuracy: 0.11176353503184713 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.31; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.05
Batch: 60; loss: 2.3; acc: 0.16
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.09
Batch: 120; loss: 2.27; acc: 0.17
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.25; acc: 0.19
Batch: 180; loss: 2.27; acc: 0.14
Batch: 200; loss: 2.26; acc: 0.17
Batch: 220; loss: 2.23; acc: 0.2
Batch: 240; loss: 2.27; acc: 0.12
Batch: 260; loss: 2.26; acc: 0.16
Batch: 280; loss: 2.26; acc: 0.16
Batch: 300; loss: 2.3; acc: 0.08
Batch: 320; loss: 2.29; acc: 0.12
Batch: 340; loss: 2.3; acc: 0.05
Batch: 360; loss: 2.27; acc: 0.19
Batch: 380; loss: 2.27; acc: 0.19
Batch: 400; loss: 2.29; acc: 0.11
Batch: 420; loss: 2.32; acc: 0.11
Batch: 440; loss: 2.27; acc: 0.09
Batch: 460; loss: 2.29; acc: 0.19
Batch: 480; loss: 2.27; acc: 0.08
Batch: 500; loss: 2.26; acc: 0.16
Batch: 520; loss: 2.23; acc: 0.16
Batch: 540; loss: 2.28; acc: 0.17
Batch: 560; loss: 2.25; acc: 0.16
Batch: 580; loss: 2.29; acc: 0.06
Batch: 600; loss: 2.29; acc: 0.09
Batch: 620; loss: 2.3; acc: 0.17
Batch: 640; loss: 2.32; acc: 0.08
Batch: 660; loss: 2.3; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.17
Batch: 700; loss: 2.26; acc: 0.17
Batch: 720; loss: 2.28; acc: 0.12
Batch: 740; loss: 2.26; acc: 0.16
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.29; acc: 0.06
Train Epoch over. train_loss: 2.28; train_accuracy: 0.12 

Batch: 0; loss: 2.27; acc: 0.16
Batch: 20; loss: 2.26; acc: 0.16
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.26; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.19
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.26; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.12
Val Epoch over. val_loss: 2.2790718488632495; val_accuracy: 0.1259952229299363 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.24; acc: 0.22
Batch: 20; loss: 2.26; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.26; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.28; acc: 0.09
Batch: 120; loss: 2.32; acc: 0.08
Batch: 140; loss: 2.28; acc: 0.14
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.29; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.28; acc: 0.11
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.26; acc: 0.17
Batch: 280; loss: 2.25; acc: 0.09
Batch: 300; loss: 2.26; acc: 0.19
Batch: 320; loss: 2.22; acc: 0.22
Batch: 340; loss: 2.29; acc: 0.09
Batch: 360; loss: 2.26; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.12
Batch: 400; loss: 2.27; acc: 0.16
Batch: 420; loss: 2.31; acc: 0.09
Batch: 440; loss: 2.26; acc: 0.12
Batch: 460; loss: 2.26; acc: 0.12
Batch: 480; loss: 2.24; acc: 0.2
Batch: 500; loss: 2.3; acc: 0.08
Batch: 520; loss: 2.27; acc: 0.09
Batch: 540; loss: 2.28; acc: 0.12
Batch: 560; loss: 2.26; acc: 0.16
Batch: 580; loss: 2.27; acc: 0.12
Batch: 600; loss: 2.27; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.09
Batch: 640; loss: 2.35; acc: 0.0
Batch: 660; loss: 2.28; acc: 0.09
Batch: 680; loss: 2.3; acc: 0.14
Batch: 700; loss: 2.26; acc: 0.12
Batch: 720; loss: 2.27; acc: 0.08
Batch: 740; loss: 2.26; acc: 0.12
Batch: 760; loss: 2.26; acc: 0.09
Batch: 780; loss: 2.3; acc: 0.06
Train Epoch over. train_loss: 2.28; train_accuracy: 0.12 

Batch: 0; loss: 2.27; acc: 0.16
Batch: 20; loss: 2.26; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.26; acc: 0.19
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.26; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.11
Val Epoch over. val_loss: 2.2761433595305034; val_accuracy: 0.12002388535031847 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.24; acc: 0.17
Batch: 20; loss: 2.25; acc: 0.12
Batch: 40; loss: 2.27; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.09
Batch: 80; loss: 2.26; acc: 0.09
Batch: 100; loss: 2.22; acc: 0.16
Batch: 120; loss: 2.26; acc: 0.11
Batch: 140; loss: 2.25; acc: 0.06
Batch: 160; loss: 2.27; acc: 0.06
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.27; acc: 0.14
Batch: 220; loss: 2.24; acc: 0.12
Batch: 240; loss: 2.31; acc: 0.09
Batch: 260; loss: 2.24; acc: 0.19
Batch: 280; loss: 2.29; acc: 0.08
Batch: 300; loss: 2.26; acc: 0.08
Batch: 320; loss: 2.29; acc: 0.11
Batch: 340; loss: 2.28; acc: 0.03
Batch: 360; loss: 2.24; acc: 0.16
Batch: 380; loss: 2.28; acc: 0.08
Batch: 400; loss: 2.26; acc: 0.16
Batch: 420; loss: 2.26; acc: 0.2
Batch: 440; loss: 2.28; acc: 0.05
Batch: 460; loss: 2.28; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.09
Batch: 500; loss: 2.23; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.05
Batch: 540; loss: 2.29; acc: 0.06
Batch: 560; loss: 2.28; acc: 0.08
Batch: 580; loss: 2.26; acc: 0.08
Batch: 600; loss: 2.23; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.09
Batch: 640; loss: 2.29; acc: 0.06
Batch: 660; loss: 2.3; acc: 0.08
Batch: 680; loss: 2.3; acc: 0.08
Batch: 700; loss: 2.25; acc: 0.12
Batch: 720; loss: 2.25; acc: 0.12
Batch: 740; loss: 2.27; acc: 0.11
Batch: 760; loss: 2.22; acc: 0.22
Batch: 780; loss: 2.26; acc: 0.17
Train Epoch over. train_loss: 2.27; train_accuracy: 0.1 

Batch: 0; loss: 2.26; acc: 0.14
Batch: 20; loss: 2.24; acc: 0.11
Batch: 40; loss: 2.27; acc: 0.08
Batch: 60; loss: 2.23; acc: 0.11
Batch: 80; loss: 2.25; acc: 0.16
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.24; acc: 0.09
Val Epoch over. val_loss: 2.2626753081182005; val_accuracy: 0.10380175159235669 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.11
Batch: 20; loss: 2.3; acc: 0.02
Batch: 40; loss: 2.25; acc: 0.06
Batch: 60; loss: 2.28; acc: 0.09
Batch: 80; loss: 2.26; acc: 0.12
Batch: 100; loss: 2.28; acc: 0.08
Batch: 120; loss: 2.26; acc: 0.16
Batch: 140; loss: 2.25; acc: 0.08
Batch: 160; loss: 2.24; acc: 0.08
Batch: 180; loss: 2.27; acc: 0.09
Batch: 200; loss: 2.23; acc: 0.12
Batch: 220; loss: 2.28; acc: 0.06
Batch: 240; loss: 2.27; acc: 0.09
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.22; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.11
Batch: 320; loss: 2.31; acc: 0.06
Batch: 340; loss: 2.23; acc: 0.14
Batch: 360; loss: 2.23; acc: 0.09
Batch: 380; loss: 2.32; acc: 0.05
Batch: 400; loss: 2.23; acc: 0.05
Batch: 420; loss: 2.21; acc: 0.14
Batch: 440; loss: 2.26; acc: 0.11
Batch: 460; loss: 2.2; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.08
Batch: 500; loss: 2.26; acc: 0.11
Batch: 520; loss: 2.32; acc: 0.05
Batch: 540; loss: 2.26; acc: 0.11
Batch: 560; loss: 2.23; acc: 0.12
Batch: 580; loss: 2.21; acc: 0.09
Batch: 600; loss: 2.22; acc: 0.14
Batch: 620; loss: 2.19; acc: 0.11
Batch: 640; loss: 2.23; acc: 0.19
Batch: 660; loss: 2.2; acc: 0.12
Batch: 680; loss: 2.26; acc: 0.16
Batch: 700; loss: 2.24; acc: 0.14
Batch: 720; loss: 2.2; acc: 0.14
Batch: 740; loss: 2.2; acc: 0.19
Batch: 760; loss: 2.18; acc: 0.17
Batch: 780; loss: 2.26; acc: 0.16
Train Epoch over. train_loss: 2.25; train_accuracy: 0.1 

Batch: 0; loss: 2.23; acc: 0.08
Batch: 20; loss: 2.21; acc: 0.14
Batch: 40; loss: 2.15; acc: 0.16
Batch: 60; loss: 2.13; acc: 0.19
Batch: 80; loss: 2.16; acc: 0.2
Batch: 100; loss: 2.24; acc: 0.11
Batch: 120; loss: 2.18; acc: 0.17
Batch: 140; loss: 2.18; acc: 0.2
Val Epoch over. val_loss: 2.198669442705288; val_accuracy: 0.1675955414012739 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 2.22; acc: 0.12
Batch: 20; loss: 2.28; acc: 0.11
Batch: 40; loss: 2.16; acc: 0.2
Batch: 60; loss: 2.23; acc: 0.14
Batch: 80; loss: 2.16; acc: 0.2
Batch: 100; loss: 2.21; acc: 0.19
Batch: 120; loss: 2.2; acc: 0.12
Batch: 140; loss: 2.16; acc: 0.16
Batch: 160; loss: 2.18; acc: 0.2
Batch: 180; loss: 2.13; acc: 0.22
Batch: 200; loss: 2.28; acc: 0.16
Batch: 220; loss: 2.13; acc: 0.23
Batch: 240; loss: 2.14; acc: 0.22
Batch: 260; loss: 2.25; acc: 0.17
Batch: 280; loss: 2.19; acc: 0.19
Batch: 300; loss: 2.24; acc: 0.17
Batch: 320; loss: 2.25; acc: 0.19
Batch: 340; loss: 2.22; acc: 0.2
Batch: 360; loss: 2.3; acc: 0.06
Batch: 380; loss: 2.17; acc: 0.19
Batch: 400; loss: 2.19; acc: 0.22
Batch: 420; loss: 2.22; acc: 0.22
Batch: 440; loss: 2.18; acc: 0.25
Batch: 460; loss: 2.2; acc: 0.23
Batch: 480; loss: 2.25; acc: 0.16
Batch: 500; loss: 2.2; acc: 0.25
Batch: 520; loss: 2.28; acc: 0.12
Batch: 540; loss: 2.15; acc: 0.2
Batch: 560; loss: 2.13; acc: 0.19
Batch: 580; loss: 2.28; acc: 0.16
Batch: 600; loss: 2.14; acc: 0.3
Batch: 620; loss: 2.23; acc: 0.11
Batch: 640; loss: 2.22; acc: 0.22
Batch: 660; loss: 2.23; acc: 0.19
Batch: 680; loss: 2.17; acc: 0.22
Batch: 700; loss: 2.23; acc: 0.17
Batch: 720; loss: 2.2; acc: 0.17
Batch: 740; loss: 2.24; acc: 0.17
Batch: 760; loss: 2.16; acc: 0.12
Batch: 780; loss: 2.17; acc: 0.2
Train Epoch over. train_loss: 2.19; train_accuracy: 0.2 

Batch: 0; loss: 2.21; acc: 0.2
Batch: 20; loss: 2.16; acc: 0.17
Batch: 40; loss: 2.08; acc: 0.25
Batch: 60; loss: 2.08; acc: 0.2
Batch: 80; loss: 2.13; acc: 0.3
Batch: 100; loss: 2.19; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.18; acc: 0.27
Val Epoch over. val_loss: 2.1711584208117927; val_accuracy: 0.22143710191082802 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.22; acc: 0.14
Batch: 20; loss: 2.23; acc: 0.23
Batch: 40; loss: 2.24; acc: 0.17
Batch: 60; loss: 2.2; acc: 0.25
Batch: 80; loss: 2.3; acc: 0.17
Batch: 100; loss: 2.14; acc: 0.23
Batch: 120; loss: 2.16; acc: 0.25
Batch: 140; loss: 2.12; acc: 0.28
Batch: 160; loss: 2.25; acc: 0.16
Batch: 180; loss: 2.21; acc: 0.2
Batch: 200; loss: 2.11; acc: 0.22
Batch: 220; loss: 2.19; acc: 0.17
Batch: 240; loss: 2.05; acc: 0.19
Batch: 260; loss: 2.21; acc: 0.17
Batch: 280; loss: 2.18; acc: 0.14
Batch: 300; loss: 2.16; acc: 0.22
Batch: 320; loss: 2.2; acc: 0.14
Batch: 340; loss: 2.18; acc: 0.22
Batch: 360; loss: 2.12; acc: 0.28
Batch: 380; loss: 2.12; acc: 0.27
Batch: 400; loss: 2.28; acc: 0.19
Batch: 420; loss: 2.2; acc: 0.2
Batch: 440; loss: 2.08; acc: 0.38
Batch: 460; loss: 2.14; acc: 0.17
Batch: 480; loss: 2.11; acc: 0.28
Batch: 500; loss: 2.23; acc: 0.2
Batch: 520; loss: 2.19; acc: 0.2
Batch: 540; loss: 2.14; acc: 0.19
Batch: 560; loss: 2.12; acc: 0.23
Batch: 580; loss: 2.21; acc: 0.12
Batch: 600; loss: 2.22; acc: 0.17
Batch: 620; loss: 2.12; acc: 0.22
Batch: 640; loss: 2.19; acc: 0.25
Batch: 660; loss: 2.27; acc: 0.2
Batch: 680; loss: 2.17; acc: 0.16
Batch: 700; loss: 2.2; acc: 0.2
Batch: 720; loss: 2.15; acc: 0.2
Batch: 740; loss: 2.18; acc: 0.19
Batch: 760; loss: 2.15; acc: 0.23
Batch: 780; loss: 2.13; acc: 0.19
Train Epoch over. train_loss: 2.18; train_accuracy: 0.21 

Batch: 0; loss: 2.19; acc: 0.19
Batch: 20; loss: 2.14; acc: 0.22
Batch: 40; loss: 2.09; acc: 0.27
Batch: 60; loss: 2.09; acc: 0.17
Batch: 80; loss: 2.16; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.22
Val Epoch over. val_loss: 2.1685296441339386; val_accuracy: 0.2148686305732484 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 2.15; acc: 0.25
Batch: 20; loss: 2.07; acc: 0.27
Batch: 40; loss: 2.23; acc: 0.09
Batch: 60; loss: 2.17; acc: 0.2
Batch: 80; loss: 2.09; acc: 0.28
Batch: 100; loss: 2.27; acc: 0.16
Batch: 120; loss: 2.13; acc: 0.22
Batch: 140; loss: 2.16; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.18; acc: 0.11
Batch: 200; loss: 2.18; acc: 0.22
Batch: 220; loss: 2.06; acc: 0.25
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.13; acc: 0.2
Batch: 280; loss: 2.15; acc: 0.23
Batch: 300; loss: 2.18; acc: 0.2
Batch: 320; loss: 2.2; acc: 0.16
Batch: 340; loss: 2.22; acc: 0.2
Batch: 360; loss: 2.32; acc: 0.17
Batch: 380; loss: 2.11; acc: 0.3
Batch: 400; loss: 2.18; acc: 0.22
Batch: 420; loss: 2.19; acc: 0.31
Batch: 440; loss: 2.25; acc: 0.16
Batch: 460; loss: 2.1; acc: 0.22
Batch: 480; loss: 2.13; acc: 0.25
Batch: 500; loss: 2.18; acc: 0.3
Batch: 520; loss: 2.13; acc: 0.23
Batch: 540; loss: 2.11; acc: 0.33
Batch: 560; loss: 2.29; acc: 0.14
Batch: 580; loss: 2.07; acc: 0.22
Batch: 600; loss: 2.14; acc: 0.23
Batch: 620; loss: 2.18; acc: 0.17
Batch: 640; loss: 2.19; acc: 0.22
Batch: 660; loss: 2.18; acc: 0.2
Batch: 680; loss: 2.32; acc: 0.11
Batch: 700; loss: 2.24; acc: 0.2
Batch: 720; loss: 2.26; acc: 0.2
Batch: 740; loss: 2.2; acc: 0.22
Batch: 760; loss: 2.11; acc: 0.22
Batch: 780; loss: 2.19; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.25
Batch: 120; loss: 2.13; acc: 0.23
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.1681541196859566; val_accuracy: 0.2125796178343949 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 2.12; acc: 0.23
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 2.13; acc: 0.22
Batch: 60; loss: 2.15; acc: 0.25
Batch: 80; loss: 2.24; acc: 0.22
Batch: 100; loss: 2.22; acc: 0.22
Batch: 120; loss: 2.25; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.17
Batch: 160; loss: 2.15; acc: 0.16
Batch: 180; loss: 2.08; acc: 0.31
Batch: 200; loss: 2.22; acc: 0.19
Batch: 220; loss: 2.14; acc: 0.19
Batch: 240; loss: 2.26; acc: 0.11
Batch: 260; loss: 2.21; acc: 0.22
Batch: 280; loss: 2.07; acc: 0.31
Batch: 300; loss: 2.27; acc: 0.12
Batch: 320; loss: 2.14; acc: 0.2
Batch: 340; loss: 2.19; acc: 0.2
Batch: 360; loss: 2.22; acc: 0.17
Batch: 380; loss: 2.15; acc: 0.19
Batch: 400; loss: 2.14; acc: 0.22
Batch: 420; loss: 2.25; acc: 0.12
Batch: 440; loss: 2.04; acc: 0.3
Batch: 460; loss: 2.15; acc: 0.14
Batch: 480; loss: 2.18; acc: 0.22
Batch: 500; loss: 2.2; acc: 0.14
Batch: 520; loss: 2.16; acc: 0.2
Batch: 540; loss: 2.11; acc: 0.2
Batch: 560; loss: 2.22; acc: 0.23
Batch: 580; loss: 2.27; acc: 0.16
Batch: 600; loss: 2.2; acc: 0.19
Batch: 620; loss: 2.15; acc: 0.25
Batch: 640; loss: 2.13; acc: 0.23
Batch: 660; loss: 2.19; acc: 0.22
Batch: 680; loss: 2.09; acc: 0.27
Batch: 700; loss: 2.15; acc: 0.16
Batch: 720; loss: 2.15; acc: 0.2
Batch: 740; loss: 2.14; acc: 0.17
Batch: 760; loss: 2.27; acc: 0.17
Batch: 780; loss: 2.22; acc: 0.11
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.28
Batch: 120; loss: 2.14; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1678624213880795; val_accuracy: 0.21277866242038215 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.28
Batch: 20; loss: 2.16; acc: 0.25
Batch: 40; loss: 2.22; acc: 0.17
Batch: 60; loss: 2.07; acc: 0.23
Batch: 80; loss: 2.15; acc: 0.23
Batch: 100; loss: 2.22; acc: 0.12
Batch: 120; loss: 2.2; acc: 0.19
Batch: 140; loss: 2.17; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.25
Batch: 180; loss: 2.16; acc: 0.25
Batch: 200; loss: 2.17; acc: 0.22
Batch: 220; loss: 2.1; acc: 0.25
Batch: 240; loss: 2.24; acc: 0.27
Batch: 260; loss: 2.18; acc: 0.12
Batch: 280; loss: 2.22; acc: 0.17
Batch: 300; loss: 2.14; acc: 0.22
Batch: 320; loss: 2.24; acc: 0.17
Batch: 340; loss: 2.18; acc: 0.12
Batch: 360; loss: 2.24; acc: 0.08
Batch: 380; loss: 2.23; acc: 0.2
Batch: 400; loss: 2.16; acc: 0.2
Batch: 420; loss: 2.15; acc: 0.2
Batch: 440; loss: 2.26; acc: 0.06
Batch: 460; loss: 2.11; acc: 0.23
Batch: 480; loss: 2.19; acc: 0.23
Batch: 500; loss: 2.15; acc: 0.19
Batch: 520; loss: 2.15; acc: 0.2
Batch: 540; loss: 2.21; acc: 0.12
Batch: 560; loss: 2.15; acc: 0.23
Batch: 580; loss: 2.21; acc: 0.14
Batch: 600; loss: 2.23; acc: 0.19
Batch: 620; loss: 2.21; acc: 0.14
Batch: 640; loss: 2.15; acc: 0.14
Batch: 660; loss: 2.13; acc: 0.14
Batch: 680; loss: 2.21; acc: 0.22
Batch: 700; loss: 2.15; acc: 0.25
Batch: 720; loss: 2.17; acc: 0.11
Batch: 740; loss: 2.07; acc: 0.31
Batch: 760; loss: 2.14; acc: 0.23
Batch: 780; loss: 2.11; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.17
Batch: 40; loss: 2.1; acc: 0.23
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.167876756874619; val_accuracy: 0.2085987261146497 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 2.33; acc: 0.06
Batch: 20; loss: 2.15; acc: 0.16
Batch: 40; loss: 2.23; acc: 0.09
Batch: 60; loss: 2.25; acc: 0.17
Batch: 80; loss: 2.24; acc: 0.14
Batch: 100; loss: 2.23; acc: 0.17
Batch: 120; loss: 2.06; acc: 0.23
Batch: 140; loss: 2.13; acc: 0.19
Batch: 160; loss: 2.16; acc: 0.25
Batch: 180; loss: 2.33; acc: 0.14
Batch: 200; loss: 2.15; acc: 0.19
Batch: 220; loss: 2.17; acc: 0.19
Batch: 240; loss: 2.21; acc: 0.23
Batch: 260; loss: 2.16; acc: 0.19
Batch: 280; loss: 2.16; acc: 0.16
Batch: 300; loss: 2.07; acc: 0.23
Batch: 320; loss: 2.14; acc: 0.14
Batch: 340; loss: 2.17; acc: 0.14
Batch: 360; loss: 2.19; acc: 0.25
Batch: 380; loss: 2.11; acc: 0.27
Batch: 400; loss: 2.21; acc: 0.14
Batch: 420; loss: 2.21; acc: 0.16
Batch: 440; loss: 2.2; acc: 0.25
Batch: 460; loss: 2.2; acc: 0.19
Batch: 480; loss: 2.25; acc: 0.2
Batch: 500; loss: 2.17; acc: 0.17
Batch: 520; loss: 2.24; acc: 0.14
Batch: 540; loss: 2.07; acc: 0.27
Batch: 560; loss: 2.21; acc: 0.14
Batch: 580; loss: 2.14; acc: 0.23
Batch: 600; loss: 2.07; acc: 0.3
Batch: 620; loss: 2.16; acc: 0.23
Batch: 640; loss: 2.2; acc: 0.23
Batch: 660; loss: 2.12; acc: 0.2
Batch: 680; loss: 2.16; acc: 0.25
Batch: 700; loss: 2.14; acc: 0.31
Batch: 720; loss: 2.12; acc: 0.22
Batch: 740; loss: 2.24; acc: 0.14
Batch: 760; loss: 2.16; acc: 0.16
Batch: 780; loss: 2.16; acc: 0.3
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.13; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1680583050296565; val_accuracy: 0.21068869426751594 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 2.13; acc: 0.28
Batch: 20; loss: 2.03; acc: 0.23
Batch: 40; loss: 2.16; acc: 0.23
Batch: 60; loss: 2.18; acc: 0.2
Batch: 80; loss: 2.09; acc: 0.27
Batch: 100; loss: 2.11; acc: 0.2
Batch: 120; loss: 2.25; acc: 0.12
Batch: 140; loss: 2.12; acc: 0.2
Batch: 160; loss: 2.14; acc: 0.2
Batch: 180; loss: 2.23; acc: 0.16
Batch: 200; loss: 2.21; acc: 0.17
Batch: 220; loss: 2.26; acc: 0.17
Batch: 240; loss: 2.27; acc: 0.22
Batch: 260; loss: 2.24; acc: 0.23
Batch: 280; loss: 2.13; acc: 0.23
Batch: 300; loss: 2.16; acc: 0.12
Batch: 320; loss: 2.18; acc: 0.17
Batch: 340; loss: 2.15; acc: 0.27
Batch: 360; loss: 2.14; acc: 0.17
Batch: 380; loss: 2.28; acc: 0.11
Batch: 400; loss: 2.13; acc: 0.2
Batch: 420; loss: 2.2; acc: 0.19
Batch: 440; loss: 2.15; acc: 0.09
Batch: 460; loss: 2.24; acc: 0.22
Batch: 480; loss: 2.15; acc: 0.2
Batch: 500; loss: 2.15; acc: 0.27
Batch: 520; loss: 2.24; acc: 0.23
Batch: 540; loss: 2.13; acc: 0.19
Batch: 560; loss: 2.12; acc: 0.17
Batch: 580; loss: 2.22; acc: 0.17
Batch: 600; loss: 2.07; acc: 0.28
Batch: 620; loss: 2.21; acc: 0.17
Batch: 640; loss: 2.19; acc: 0.12
Batch: 660; loss: 2.3; acc: 0.19
Batch: 680; loss: 2.2; acc: 0.22
Batch: 700; loss: 2.16; acc: 0.23
Batch: 720; loss: 2.24; acc: 0.11
Batch: 740; loss: 2.15; acc: 0.22
Batch: 760; loss: 2.11; acc: 0.28
Batch: 780; loss: 2.21; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.22
Val Epoch over. val_loss: 2.168162891059924; val_accuracy: 0.21297770700636942 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 2.18; acc: 0.22
Batch: 20; loss: 2.22; acc: 0.23
Batch: 40; loss: 2.18; acc: 0.23
Batch: 60; loss: 2.23; acc: 0.22
Batch: 80; loss: 2.15; acc: 0.28
Batch: 100; loss: 2.1; acc: 0.28
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.22; acc: 0.17
Batch: 160; loss: 2.23; acc: 0.22
Batch: 180; loss: 2.15; acc: 0.23
Batch: 200; loss: 2.23; acc: 0.19
Batch: 220; loss: 2.12; acc: 0.17
Batch: 240; loss: 2.09; acc: 0.2
Batch: 260; loss: 2.15; acc: 0.28
Batch: 280; loss: 2.16; acc: 0.17
Batch: 300; loss: 2.13; acc: 0.27
Batch: 320; loss: 2.14; acc: 0.17
Batch: 340; loss: 2.19; acc: 0.17
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.18; acc: 0.2
Batch: 400; loss: 2.2; acc: 0.14
Batch: 420; loss: 2.19; acc: 0.25
Batch: 440; loss: 2.15; acc: 0.23
Batch: 460; loss: 2.26; acc: 0.17
Batch: 480; loss: 2.19; acc: 0.19
Batch: 500; loss: 2.18; acc: 0.2
Batch: 520; loss: 2.13; acc: 0.14
Batch: 540; loss: 2.09; acc: 0.23
Batch: 560; loss: 2.2; acc: 0.19
Batch: 580; loss: 2.23; acc: 0.14
Batch: 600; loss: 2.23; acc: 0.17
Batch: 620; loss: 2.26; acc: 0.19
Batch: 640; loss: 2.18; acc: 0.14
Batch: 660; loss: 2.18; acc: 0.19
Batch: 680; loss: 2.12; acc: 0.22
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.14; acc: 0.28
Batch: 740; loss: 2.16; acc: 0.16
Batch: 760; loss: 2.19; acc: 0.3
Batch: 780; loss: 2.29; acc: 0.16
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.16792385213694; val_accuracy: 0.21367436305732485 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 2.21; acc: 0.17
Batch: 20; loss: 2.22; acc: 0.19
Batch: 40; loss: 2.1; acc: 0.16
Batch: 60; loss: 2.12; acc: 0.23
Batch: 80; loss: 2.25; acc: 0.2
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.23; acc: 0.2
Batch: 140; loss: 2.19; acc: 0.19
Batch: 160; loss: 2.21; acc: 0.12
Batch: 180; loss: 2.21; acc: 0.23
Batch: 200; loss: 2.18; acc: 0.22
Batch: 220; loss: 2.12; acc: 0.2
Batch: 240; loss: 2.13; acc: 0.22
Batch: 260; loss: 2.19; acc: 0.19
Batch: 280; loss: 2.12; acc: 0.27
Batch: 300; loss: 2.15; acc: 0.16
Batch: 320; loss: 2.12; acc: 0.19
Batch: 340; loss: 2.12; acc: 0.17
Batch: 360; loss: 2.18; acc: 0.22
Batch: 380; loss: 2.16; acc: 0.17
Batch: 400; loss: 2.26; acc: 0.14
Batch: 420; loss: 2.11; acc: 0.33
Batch: 440; loss: 2.12; acc: 0.33
Batch: 460; loss: 2.15; acc: 0.23
Batch: 480; loss: 2.1; acc: 0.28
Batch: 500; loss: 2.15; acc: 0.22
Batch: 520; loss: 2.12; acc: 0.23
Batch: 540; loss: 2.26; acc: 0.16
Batch: 560; loss: 2.15; acc: 0.2
Batch: 580; loss: 2.16; acc: 0.2
Batch: 600; loss: 2.11; acc: 0.23
Batch: 620; loss: 2.02; acc: 0.27
Batch: 640; loss: 2.14; acc: 0.19
Batch: 660; loss: 2.22; acc: 0.16
Batch: 680; loss: 2.21; acc: 0.2
Batch: 700; loss: 2.18; acc: 0.28
Batch: 720; loss: 2.02; acc: 0.28
Batch: 740; loss: 2.17; acc: 0.23
Batch: 760; loss: 2.14; acc: 0.17
Batch: 780; loss: 2.21; acc: 0.14
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.19
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.19
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1677903271025154; val_accuracy: 0.2086982484076433 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 2.19; acc: 0.19
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.13; acc: 0.22
Batch: 60; loss: 2.14; acc: 0.27
Batch: 80; loss: 2.13; acc: 0.25
Batch: 100; loss: 2.18; acc: 0.16
Batch: 120; loss: 2.18; acc: 0.2
Batch: 140; loss: 2.16; acc: 0.25
Batch: 160; loss: 2.18; acc: 0.23
Batch: 180; loss: 2.13; acc: 0.31
Batch: 200; loss: 2.17; acc: 0.23
Batch: 220; loss: 2.1; acc: 0.17
Batch: 240; loss: 2.19; acc: 0.27
Batch: 260; loss: 2.11; acc: 0.23
Batch: 280; loss: 2.25; acc: 0.17
Batch: 300; loss: 2.12; acc: 0.25
Batch: 320; loss: 2.14; acc: 0.17
Batch: 340; loss: 2.07; acc: 0.25
Batch: 360; loss: 2.17; acc: 0.16
Batch: 380; loss: 2.14; acc: 0.27
Batch: 400; loss: 2.13; acc: 0.2
Batch: 420; loss: 2.15; acc: 0.17
Batch: 440; loss: 2.12; acc: 0.25
Batch: 460; loss: 2.2; acc: 0.19
Batch: 480; loss: 2.15; acc: 0.27
Batch: 500; loss: 2.14; acc: 0.2
Batch: 520; loss: 2.22; acc: 0.12
Batch: 540; loss: 2.13; acc: 0.27
Batch: 560; loss: 2.22; acc: 0.17
Batch: 580; loss: 2.28; acc: 0.17
Batch: 600; loss: 2.34; acc: 0.11
Batch: 620; loss: 2.25; acc: 0.17
Batch: 640; loss: 2.2; acc: 0.19
Batch: 660; loss: 2.13; acc: 0.19
Batch: 680; loss: 2.08; acc: 0.25
Batch: 700; loss: 2.12; acc: 0.2
Batch: 720; loss: 2.2; acc: 0.12
Batch: 740; loss: 2.21; acc: 0.16
Batch: 760; loss: 2.12; acc: 0.23
Batch: 780; loss: 2.18; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.23
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167907203079029; val_accuracy: 0.20939490445859874 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.17; acc: 0.19
Batch: 40; loss: 2.23; acc: 0.16
Batch: 60; loss: 2.19; acc: 0.28
Batch: 80; loss: 2.1; acc: 0.2
Batch: 100; loss: 2.11; acc: 0.22
Batch: 120; loss: 2.22; acc: 0.19
Batch: 140; loss: 2.2; acc: 0.19
Batch: 160; loss: 2.18; acc: 0.2
Batch: 180; loss: 2.15; acc: 0.27
Batch: 200; loss: 2.27; acc: 0.2
Batch: 220; loss: 2.22; acc: 0.14
Batch: 240; loss: 2.23; acc: 0.17
Batch: 260; loss: 2.22; acc: 0.16
Batch: 280; loss: 2.06; acc: 0.33
Batch: 300; loss: 2.24; acc: 0.23
Batch: 320; loss: 2.17; acc: 0.27
Batch: 340; loss: 2.11; acc: 0.23
Batch: 360; loss: 2.12; acc: 0.2
Batch: 380; loss: 2.09; acc: 0.25
Batch: 400; loss: 2.17; acc: 0.19
Batch: 420; loss: 2.24; acc: 0.19
Batch: 440; loss: 2.13; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.14
Batch: 480; loss: 2.14; acc: 0.2
Batch: 500; loss: 2.17; acc: 0.19
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.17; acc: 0.2
Batch: 560; loss: 2.04; acc: 0.38
Batch: 580; loss: 2.34; acc: 0.11
Batch: 600; loss: 2.07; acc: 0.28
Batch: 620; loss: 2.21; acc: 0.17
Batch: 640; loss: 2.22; acc: 0.17
Batch: 660; loss: 2.15; acc: 0.25
Batch: 680; loss: 2.18; acc: 0.2
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.18; acc: 0.17
Batch: 740; loss: 2.16; acc: 0.14
Batch: 760; loss: 2.15; acc: 0.2
Batch: 780; loss: 2.12; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1678360829687424; val_accuracy: 0.20979299363057324 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 2.13; acc: 0.22
Batch: 20; loss: 2.12; acc: 0.3
Batch: 40; loss: 2.21; acc: 0.25
Batch: 60; loss: 2.15; acc: 0.2
Batch: 80; loss: 2.23; acc: 0.14
Batch: 100; loss: 2.2; acc: 0.2
Batch: 120; loss: 2.13; acc: 0.28
Batch: 140; loss: 2.17; acc: 0.25
Batch: 160; loss: 2.18; acc: 0.19
Batch: 180; loss: 2.16; acc: 0.22
Batch: 200; loss: 2.29; acc: 0.14
Batch: 220; loss: 2.25; acc: 0.23
Batch: 240; loss: 2.23; acc: 0.16
Batch: 260; loss: 2.18; acc: 0.2
Batch: 280; loss: 2.08; acc: 0.28
Batch: 300; loss: 2.14; acc: 0.22
Batch: 320; loss: 2.1; acc: 0.25
Batch: 340; loss: 2.25; acc: 0.23
Batch: 360; loss: 2.12; acc: 0.22
Batch: 380; loss: 2.06; acc: 0.2
Batch: 400; loss: 2.2; acc: 0.12
Batch: 420; loss: 2.19; acc: 0.19
Batch: 440; loss: 2.1; acc: 0.33
Batch: 460; loss: 2.09; acc: 0.22
Batch: 480; loss: 2.18; acc: 0.2
Batch: 500; loss: 2.22; acc: 0.2
Batch: 520; loss: 2.23; acc: 0.16
Batch: 540; loss: 2.24; acc: 0.14
Batch: 560; loss: 2.36; acc: 0.16
Batch: 580; loss: 2.18; acc: 0.28
Batch: 600; loss: 2.21; acc: 0.16
Batch: 620; loss: 2.21; acc: 0.22
Batch: 640; loss: 2.2; acc: 0.2
Batch: 660; loss: 2.25; acc: 0.14
Batch: 680; loss: 2.34; acc: 0.11
Batch: 700; loss: 2.12; acc: 0.23
Batch: 720; loss: 2.18; acc: 0.17
Batch: 740; loss: 2.19; acc: 0.17
Batch: 760; loss: 2.18; acc: 0.22
Batch: 780; loss: 2.16; acc: 0.22
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.22
Val Epoch over. val_loss: 2.168004568974683; val_accuracy: 0.21377388535031847 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 2.12; acc: 0.27
Batch: 20; loss: 2.23; acc: 0.19
Batch: 40; loss: 2.21; acc: 0.17
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.24; acc: 0.17
Batch: 100; loss: 2.35; acc: 0.09
Batch: 120; loss: 2.24; acc: 0.2
Batch: 140; loss: 2.22; acc: 0.14
Batch: 160; loss: 2.14; acc: 0.25
Batch: 180; loss: 2.15; acc: 0.27
Batch: 200; loss: 2.18; acc: 0.17
Batch: 220; loss: 2.04; acc: 0.34
Batch: 240; loss: 2.15; acc: 0.2
Batch: 260; loss: 2.25; acc: 0.17
Batch: 280; loss: 2.23; acc: 0.16
Batch: 300; loss: 2.2; acc: 0.22
Batch: 320; loss: 2.17; acc: 0.23
Batch: 340; loss: 2.23; acc: 0.11
Batch: 360; loss: 2.17; acc: 0.2
Batch: 380; loss: 2.18; acc: 0.17
Batch: 400; loss: 2.21; acc: 0.16
Batch: 420; loss: 2.09; acc: 0.3
Batch: 440; loss: 2.18; acc: 0.16
Batch: 460; loss: 2.3; acc: 0.11
Batch: 480; loss: 2.15; acc: 0.27
Batch: 500; loss: 2.13; acc: 0.23
Batch: 520; loss: 2.21; acc: 0.12
Batch: 540; loss: 2.15; acc: 0.25
Batch: 560; loss: 2.16; acc: 0.25
Batch: 580; loss: 2.27; acc: 0.09
Batch: 600; loss: 2.17; acc: 0.17
Batch: 620; loss: 2.13; acc: 0.23
Batch: 640; loss: 2.23; acc: 0.23
Batch: 660; loss: 2.13; acc: 0.2
Batch: 680; loss: 2.21; acc: 0.17
Batch: 700; loss: 2.16; acc: 0.22
Batch: 720; loss: 2.26; acc: 0.12
Batch: 740; loss: 2.22; acc: 0.17
Batch: 760; loss: 2.17; acc: 0.22
Batch: 780; loss: 2.15; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.22
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.22
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167780980183061; val_accuracy: 0.20720541401273884 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.06; acc: 0.31
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.12; acc: 0.17
Batch: 80; loss: 2.22; acc: 0.16
Batch: 100; loss: 2.18; acc: 0.23
Batch: 120; loss: 2.18; acc: 0.28
Batch: 140; loss: 2.19; acc: 0.23
Batch: 160; loss: 2.14; acc: 0.27
Batch: 180; loss: 2.26; acc: 0.16
Batch: 200; loss: 2.08; acc: 0.27
Batch: 220; loss: 2.25; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.16
Batch: 260; loss: 2.1; acc: 0.19
Batch: 280; loss: 2.22; acc: 0.2
Batch: 300; loss: 2.14; acc: 0.19
Batch: 320; loss: 2.19; acc: 0.23
Batch: 340; loss: 2.2; acc: 0.19
Batch: 360; loss: 2.19; acc: 0.17
Batch: 380; loss: 2.16; acc: 0.14
Batch: 400; loss: 2.28; acc: 0.16
Batch: 420; loss: 2.33; acc: 0.09
Batch: 440; loss: 2.16; acc: 0.2
Batch: 460; loss: 2.11; acc: 0.22
Batch: 480; loss: 2.18; acc: 0.22
Batch: 500; loss: 2.13; acc: 0.2
Batch: 520; loss: 2.18; acc: 0.19
Batch: 540; loss: 2.14; acc: 0.22
Batch: 560; loss: 2.15; acc: 0.2
Batch: 580; loss: 2.21; acc: 0.2
Batch: 600; loss: 2.18; acc: 0.22
Batch: 620; loss: 2.13; acc: 0.22
Batch: 640; loss: 2.23; acc: 0.22
Batch: 660; loss: 2.14; acc: 0.22
Batch: 680; loss: 2.22; acc: 0.16
Batch: 700; loss: 2.23; acc: 0.23
Batch: 720; loss: 2.2; acc: 0.17
Batch: 740; loss: 2.23; acc: 0.12
Batch: 760; loss: 2.13; acc: 0.17
Batch: 780; loss: 2.26; acc: 0.12
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167996625991384; val_accuracy: 0.21238057324840764 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.16; acc: 0.25
Batch: 20; loss: 2.19; acc: 0.19
Batch: 40; loss: 2.03; acc: 0.28
Batch: 60; loss: 2.16; acc: 0.25
Batch: 80; loss: 2.19; acc: 0.2
Batch: 100; loss: 2.1; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.22
Batch: 140; loss: 2.21; acc: 0.14
Batch: 160; loss: 2.12; acc: 0.2
Batch: 180; loss: 2.27; acc: 0.16
Batch: 200; loss: 2.06; acc: 0.25
Batch: 220; loss: 2.22; acc: 0.2
Batch: 240; loss: 2.18; acc: 0.23
Batch: 260; loss: 2.2; acc: 0.19
Batch: 280; loss: 2.18; acc: 0.2
Batch: 300; loss: 2.18; acc: 0.23
Batch: 320; loss: 2.19; acc: 0.19
Batch: 340; loss: 2.13; acc: 0.25
Batch: 360; loss: 2.19; acc: 0.27
Batch: 380; loss: 2.22; acc: 0.23
Batch: 400; loss: 2.18; acc: 0.19
Batch: 420; loss: 2.2; acc: 0.23
Batch: 440; loss: 2.19; acc: 0.17
Batch: 460; loss: 2.17; acc: 0.2
Batch: 480; loss: 2.23; acc: 0.25
Batch: 500; loss: 2.18; acc: 0.22
Batch: 520; loss: 2.23; acc: 0.17
Batch: 540; loss: 2.21; acc: 0.16
Batch: 560; loss: 2.26; acc: 0.19
Batch: 580; loss: 2.16; acc: 0.28
Batch: 600; loss: 2.22; acc: 0.2
Batch: 620; loss: 2.25; acc: 0.19
Batch: 640; loss: 2.04; acc: 0.28
Batch: 660; loss: 2.28; acc: 0.16
Batch: 680; loss: 2.16; acc: 0.12
Batch: 700; loss: 2.23; acc: 0.16
Batch: 720; loss: 2.11; acc: 0.22
Batch: 740; loss: 2.1; acc: 0.2
Batch: 760; loss: 2.29; acc: 0.12
Batch: 780; loss: 2.22; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.19
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1678318802718146; val_accuracy: 0.21009156050955413 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.23; acc: 0.17
Batch: 20; loss: 2.12; acc: 0.23
Batch: 40; loss: 2.14; acc: 0.17
Batch: 60; loss: 2.2; acc: 0.19
Batch: 80; loss: 2.18; acc: 0.08
Batch: 100; loss: 2.19; acc: 0.2
Batch: 120; loss: 2.18; acc: 0.17
Batch: 140; loss: 2.21; acc: 0.19
Batch: 160; loss: 2.05; acc: 0.27
Batch: 180; loss: 2.22; acc: 0.11
Batch: 200; loss: 2.17; acc: 0.16
Batch: 220; loss: 2.2; acc: 0.22
Batch: 240; loss: 2.09; acc: 0.23
Batch: 260; loss: 2.12; acc: 0.23
Batch: 280; loss: 2.23; acc: 0.22
Batch: 300; loss: 2.12; acc: 0.22
Batch: 320; loss: 2.21; acc: 0.19
Batch: 340; loss: 2.24; acc: 0.16
Batch: 360; loss: 2.1; acc: 0.25
Batch: 380; loss: 2.06; acc: 0.28
Batch: 400; loss: 2.18; acc: 0.3
Batch: 420; loss: 2.18; acc: 0.12
Batch: 440; loss: 2.12; acc: 0.25
Batch: 460; loss: 2.15; acc: 0.16
Batch: 480; loss: 2.27; acc: 0.14
Batch: 500; loss: 2.1; acc: 0.23
Batch: 520; loss: 2.1; acc: 0.22
Batch: 540; loss: 2.24; acc: 0.11
Batch: 560; loss: 2.23; acc: 0.14
Batch: 580; loss: 2.15; acc: 0.22
Batch: 600; loss: 2.11; acc: 0.12
Batch: 620; loss: 2.15; acc: 0.22
Batch: 640; loss: 2.24; acc: 0.09
Batch: 660; loss: 2.16; acc: 0.19
Batch: 680; loss: 2.12; acc: 0.27
Batch: 700; loss: 2.03; acc: 0.33
Batch: 720; loss: 2.12; acc: 0.25
Batch: 740; loss: 2.08; acc: 0.31
Batch: 760; loss: 2.2; acc: 0.19
Batch: 780; loss: 2.2; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679285187630137; val_accuracy: 0.21347531847133758 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.22; acc: 0.16
Batch: 20; loss: 2.17; acc: 0.2
Batch: 40; loss: 2.19; acc: 0.19
Batch: 60; loss: 2.14; acc: 0.22
Batch: 80; loss: 2.2; acc: 0.16
Batch: 100; loss: 2.16; acc: 0.23
Batch: 120; loss: 2.19; acc: 0.12
Batch: 140; loss: 2.19; acc: 0.28
Batch: 160; loss: 2.1; acc: 0.23
Batch: 180; loss: 2.14; acc: 0.2
Batch: 200; loss: 2.13; acc: 0.23
Batch: 220; loss: 2.02; acc: 0.25
Batch: 240; loss: 2.12; acc: 0.16
Batch: 260; loss: 2.2; acc: 0.2
Batch: 280; loss: 2.13; acc: 0.25
Batch: 300; loss: 2.21; acc: 0.16
Batch: 320; loss: 2.24; acc: 0.17
Batch: 340; loss: 2.21; acc: 0.25
Batch: 360; loss: 2.26; acc: 0.17
Batch: 380; loss: 2.14; acc: 0.19
Batch: 400; loss: 2.14; acc: 0.2
Batch: 420; loss: 2.21; acc: 0.17
Batch: 440; loss: 2.21; acc: 0.17
Batch: 460; loss: 2.24; acc: 0.17
Batch: 480; loss: 2.16; acc: 0.19
Batch: 500; loss: 2.09; acc: 0.23
Batch: 520; loss: 2.25; acc: 0.22
Batch: 540; loss: 2.1; acc: 0.3
Batch: 560; loss: 2.29; acc: 0.12
Batch: 580; loss: 2.22; acc: 0.2
Batch: 600; loss: 2.15; acc: 0.25
Batch: 620; loss: 2.11; acc: 0.25
Batch: 640; loss: 2.22; acc: 0.22
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.18; acc: 0.22
Batch: 700; loss: 2.11; acc: 0.23
Batch: 720; loss: 2.19; acc: 0.25
Batch: 740; loss: 2.14; acc: 0.22
Batch: 760; loss: 2.16; acc: 0.22
Batch: 780; loss: 2.22; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.19; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.22
Val Epoch over. val_loss: 2.167982249502923; val_accuracy: 0.2135748407643312 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.21; acc: 0.17
Batch: 20; loss: 2.17; acc: 0.2
Batch: 40; loss: 2.07; acc: 0.23
Batch: 60; loss: 2.27; acc: 0.16
Batch: 80; loss: 2.16; acc: 0.23
Batch: 100; loss: 2.27; acc: 0.14
Batch: 120; loss: 2.16; acc: 0.2
Batch: 140; loss: 2.18; acc: 0.2
Batch: 160; loss: 2.17; acc: 0.19
Batch: 180; loss: 2.15; acc: 0.19
Batch: 200; loss: 2.05; acc: 0.27
Batch: 220; loss: 2.33; acc: 0.16
Batch: 240; loss: 2.07; acc: 0.33
Batch: 260; loss: 2.21; acc: 0.25
Batch: 280; loss: 2.07; acc: 0.27
Batch: 300; loss: 2.14; acc: 0.3
Batch: 320; loss: 2.29; acc: 0.08
Batch: 340; loss: 2.12; acc: 0.2
Batch: 360; loss: 2.14; acc: 0.27
Batch: 380; loss: 2.05; acc: 0.25
Batch: 400; loss: 2.12; acc: 0.27
Batch: 420; loss: 2.18; acc: 0.17
Batch: 440; loss: 2.16; acc: 0.23
Batch: 460; loss: 2.22; acc: 0.16
Batch: 480; loss: 2.13; acc: 0.19
Batch: 500; loss: 2.16; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.3
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.09; acc: 0.22
Batch: 580; loss: 2.21; acc: 0.19
Batch: 600; loss: 2.18; acc: 0.23
Batch: 620; loss: 2.26; acc: 0.2
Batch: 640; loss: 2.14; acc: 0.22
Batch: 660; loss: 2.06; acc: 0.22
Batch: 680; loss: 2.24; acc: 0.28
Batch: 700; loss: 2.14; acc: 0.16
Batch: 720; loss: 2.19; acc: 0.23
Batch: 740; loss: 2.15; acc: 0.25
Batch: 760; loss: 2.16; acc: 0.23
Batch: 780; loss: 2.12; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679373212680697; val_accuracy: 0.2105891719745223 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.1; acc: 0.25
Batch: 20; loss: 2.24; acc: 0.2
Batch: 40; loss: 2.08; acc: 0.28
Batch: 60; loss: 2.19; acc: 0.2
Batch: 80; loss: 2.17; acc: 0.2
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.15; acc: 0.17
Batch: 140; loss: 2.24; acc: 0.2
Batch: 160; loss: 2.09; acc: 0.25
Batch: 180; loss: 2.14; acc: 0.22
Batch: 200; loss: 2.14; acc: 0.22
Batch: 220; loss: 2.21; acc: 0.12
Batch: 240; loss: 2.2; acc: 0.17
Batch: 260; loss: 2.23; acc: 0.17
Batch: 280; loss: 2.24; acc: 0.22
Batch: 300; loss: 2.13; acc: 0.27
Batch: 320; loss: 2.24; acc: 0.17
Batch: 340; loss: 2.09; acc: 0.3
Batch: 360; loss: 2.16; acc: 0.19
Batch: 380; loss: 2.22; acc: 0.27
Batch: 400; loss: 2.13; acc: 0.2
Batch: 420; loss: 2.08; acc: 0.19
Batch: 440; loss: 2.14; acc: 0.31
Batch: 460; loss: 2.18; acc: 0.27
Batch: 480; loss: 2.21; acc: 0.19
Batch: 500; loss: 2.12; acc: 0.25
Batch: 520; loss: 2.22; acc: 0.22
Batch: 540; loss: 2.16; acc: 0.23
Batch: 560; loss: 2.21; acc: 0.19
Batch: 580; loss: 2.19; acc: 0.16
Batch: 600; loss: 2.19; acc: 0.16
Batch: 620; loss: 2.09; acc: 0.28
Batch: 640; loss: 2.25; acc: 0.19
Batch: 660; loss: 2.25; acc: 0.14
Batch: 680; loss: 2.22; acc: 0.17
Batch: 700; loss: 2.09; acc: 0.3
Batch: 720; loss: 2.14; acc: 0.27
Batch: 740; loss: 2.15; acc: 0.22
Batch: 760; loss: 2.22; acc: 0.25
Batch: 780; loss: 2.18; acc: 0.16
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.17
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.13; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679744614157705; val_accuracy: 0.2105891719745223 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.08; acc: 0.28
Batch: 20; loss: 2.11; acc: 0.28
Batch: 40; loss: 2.1; acc: 0.2
Batch: 60; loss: 2.19; acc: 0.17
Batch: 80; loss: 2.25; acc: 0.12
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.24; acc: 0.17
Batch: 140; loss: 2.13; acc: 0.23
Batch: 160; loss: 2.09; acc: 0.3
Batch: 180; loss: 2.13; acc: 0.22
Batch: 200; loss: 2.18; acc: 0.2
Batch: 220; loss: 2.21; acc: 0.19
Batch: 240; loss: 2.17; acc: 0.2
Batch: 260; loss: 2.17; acc: 0.19
Batch: 280; loss: 2.23; acc: 0.23
Batch: 300; loss: 2.27; acc: 0.16
Batch: 320; loss: 2.1; acc: 0.31
Batch: 340; loss: 2.16; acc: 0.23
Batch: 360; loss: 2.27; acc: 0.19
Batch: 380; loss: 2.16; acc: 0.31
Batch: 400; loss: 2.24; acc: 0.17
Batch: 420; loss: 2.11; acc: 0.23
Batch: 440; loss: 2.2; acc: 0.2
Batch: 460; loss: 2.21; acc: 0.27
Batch: 480; loss: 2.18; acc: 0.23
Batch: 500; loss: 2.12; acc: 0.2
Batch: 520; loss: 2.15; acc: 0.14
Batch: 540; loss: 2.22; acc: 0.14
Batch: 560; loss: 2.28; acc: 0.19
Batch: 580; loss: 2.19; acc: 0.22
Batch: 600; loss: 2.07; acc: 0.23
Batch: 620; loss: 2.11; acc: 0.23
Batch: 640; loss: 2.22; acc: 0.17
Batch: 660; loss: 2.22; acc: 0.2
Batch: 680; loss: 2.08; acc: 0.31
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.15; acc: 0.27
Batch: 740; loss: 2.2; acc: 0.25
Batch: 760; loss: 2.11; acc: 0.22
Batch: 780; loss: 2.21; acc: 0.25
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679678289753617; val_accuracy: 0.21178343949044587 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.26; acc: 0.16
Batch: 20; loss: 2.28; acc: 0.12
Batch: 40; loss: 2.25; acc: 0.14
Batch: 60; loss: 2.05; acc: 0.33
Batch: 80; loss: 2.09; acc: 0.23
Batch: 100; loss: 2.21; acc: 0.28
Batch: 120; loss: 2.24; acc: 0.16
Batch: 140; loss: 2.17; acc: 0.14
Batch: 160; loss: 2.16; acc: 0.22
Batch: 180; loss: 2.1; acc: 0.27
Batch: 200; loss: 2.17; acc: 0.2
Batch: 220; loss: 2.2; acc: 0.2
Batch: 240; loss: 2.17; acc: 0.14
Batch: 260; loss: 2.05; acc: 0.3
Batch: 280; loss: 2.22; acc: 0.2
Batch: 300; loss: 2.16; acc: 0.2
Batch: 320; loss: 2.11; acc: 0.22
Batch: 340; loss: 2.05; acc: 0.3
Batch: 360; loss: 2.14; acc: 0.19
Batch: 380; loss: 2.2; acc: 0.22
Batch: 400; loss: 2.23; acc: 0.14
Batch: 420; loss: 2.17; acc: 0.16
Batch: 440; loss: 2.14; acc: 0.22
Batch: 460; loss: 2.25; acc: 0.11
Batch: 480; loss: 2.12; acc: 0.2
Batch: 500; loss: 2.21; acc: 0.16
Batch: 520; loss: 2.21; acc: 0.2
Batch: 540; loss: 2.13; acc: 0.17
Batch: 560; loss: 2.23; acc: 0.17
Batch: 580; loss: 2.13; acc: 0.31
Batch: 600; loss: 2.13; acc: 0.2
Batch: 620; loss: 2.19; acc: 0.2
Batch: 640; loss: 2.17; acc: 0.22
Batch: 660; loss: 2.12; acc: 0.22
Batch: 680; loss: 2.3; acc: 0.11
Batch: 700; loss: 2.25; acc: 0.2
Batch: 720; loss: 2.04; acc: 0.31
Batch: 740; loss: 2.1; acc: 0.22
Batch: 760; loss: 2.25; acc: 0.19
Batch: 780; loss: 2.22; acc: 0.16
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679193047201557; val_accuracy: 0.21098726114649682 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.1; acc: 0.22
Batch: 40; loss: 2.16; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.2
Batch: 80; loss: 2.21; acc: 0.14
Batch: 100; loss: 2.14; acc: 0.25
Batch: 120; loss: 2.15; acc: 0.19
Batch: 140; loss: 2.23; acc: 0.17
Batch: 160; loss: 2.17; acc: 0.19
Batch: 180; loss: 2.18; acc: 0.23
Batch: 200; loss: 2.16; acc: 0.19
Batch: 220; loss: 2.1; acc: 0.31
Batch: 240; loss: 2.13; acc: 0.22
Batch: 260; loss: 2.19; acc: 0.17
Batch: 280; loss: 2.16; acc: 0.22
Batch: 300; loss: 2.2; acc: 0.17
Batch: 320; loss: 2.26; acc: 0.11
Batch: 340; loss: 2.12; acc: 0.19
Batch: 360; loss: 2.21; acc: 0.16
Batch: 380; loss: 2.1; acc: 0.28
Batch: 400; loss: 2.21; acc: 0.19
Batch: 420; loss: 2.28; acc: 0.16
Batch: 440; loss: 2.11; acc: 0.2
Batch: 460; loss: 2.15; acc: 0.19
Batch: 480; loss: 2.15; acc: 0.17
Batch: 500; loss: 2.13; acc: 0.19
Batch: 520; loss: 2.13; acc: 0.17
Batch: 540; loss: 2.13; acc: 0.19
Batch: 560; loss: 2.16; acc: 0.27
Batch: 580; loss: 2.08; acc: 0.28
Batch: 600; loss: 2.09; acc: 0.3
Batch: 620; loss: 2.12; acc: 0.28
Batch: 640; loss: 2.16; acc: 0.19
Batch: 660; loss: 2.17; acc: 0.19
Batch: 680; loss: 2.17; acc: 0.27
Batch: 700; loss: 2.16; acc: 0.2
Batch: 720; loss: 2.08; acc: 0.22
Batch: 740; loss: 2.21; acc: 0.12
Batch: 760; loss: 2.26; acc: 0.12
Batch: 780; loss: 2.15; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.22
Val Epoch over. val_loss: 2.167952195853944; val_accuracy: 0.2118829617834395 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.16; acc: 0.28
Batch: 20; loss: 2.21; acc: 0.25
Batch: 40; loss: 2.1; acc: 0.33
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.25; acc: 0.08
Batch: 120; loss: 2.17; acc: 0.16
Batch: 140; loss: 2.26; acc: 0.16
Batch: 160; loss: 2.22; acc: 0.12
Batch: 180; loss: 2.22; acc: 0.19
Batch: 200; loss: 2.18; acc: 0.23
Batch: 220; loss: 2.13; acc: 0.25
Batch: 240; loss: 2.17; acc: 0.16
Batch: 260; loss: 2.21; acc: 0.17
Batch: 280; loss: 2.2; acc: 0.19
Batch: 300; loss: 2.16; acc: 0.22
Batch: 320; loss: 2.21; acc: 0.2
Batch: 340; loss: 2.25; acc: 0.14
Batch: 360; loss: 2.15; acc: 0.19
Batch: 380; loss: 2.07; acc: 0.27
Batch: 400; loss: 2.2; acc: 0.2
Batch: 420; loss: 2.22; acc: 0.22
Batch: 440; loss: 2.11; acc: 0.27
Batch: 460; loss: 2.25; acc: 0.14
Batch: 480; loss: 2.19; acc: 0.23
Batch: 500; loss: 2.2; acc: 0.19
Batch: 520; loss: 2.19; acc: 0.2
Batch: 540; loss: 2.15; acc: 0.25
Batch: 560; loss: 2.15; acc: 0.17
Batch: 580; loss: 2.15; acc: 0.2
Batch: 600; loss: 2.13; acc: 0.28
Batch: 620; loss: 2.13; acc: 0.25
Batch: 640; loss: 2.16; acc: 0.2
Batch: 660; loss: 2.24; acc: 0.12
Batch: 680; loss: 2.01; acc: 0.31
Batch: 700; loss: 2.13; acc: 0.16
Batch: 720; loss: 2.23; acc: 0.25
Batch: 740; loss: 2.19; acc: 0.19
Batch: 760; loss: 2.25; acc: 0.2
Batch: 780; loss: 2.12; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167905108943866; val_accuracy: 0.21088773885350318 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.24; acc: 0.14
Batch: 20; loss: 2.15; acc: 0.22
Batch: 40; loss: 2.18; acc: 0.11
Batch: 60; loss: 2.14; acc: 0.2
Batch: 80; loss: 2.21; acc: 0.16
Batch: 100; loss: 2.16; acc: 0.2
Batch: 120; loss: 2.2; acc: 0.14
Batch: 140; loss: 2.11; acc: 0.22
Batch: 160; loss: 2.19; acc: 0.16
Batch: 180; loss: 2.2; acc: 0.17
Batch: 200; loss: 2.15; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.27
Batch: 240; loss: 2.19; acc: 0.2
Batch: 260; loss: 2.19; acc: 0.16
Batch: 280; loss: 2.1; acc: 0.19
Batch: 300; loss: 2.21; acc: 0.22
Batch: 320; loss: 2.18; acc: 0.2
Batch: 340; loss: 2.23; acc: 0.22
Batch: 360; loss: 2.15; acc: 0.2
Batch: 380; loss: 2.22; acc: 0.2
Batch: 400; loss: 2.14; acc: 0.17
Batch: 420; loss: 2.1; acc: 0.27
Batch: 440; loss: 2.16; acc: 0.22
Batch: 460; loss: 2.14; acc: 0.25
Batch: 480; loss: 2.21; acc: 0.2
Batch: 500; loss: 2.21; acc: 0.14
Batch: 520; loss: 2.18; acc: 0.2
Batch: 540; loss: 2.16; acc: 0.25
Batch: 560; loss: 2.14; acc: 0.19
Batch: 580; loss: 2.15; acc: 0.14
Batch: 600; loss: 2.21; acc: 0.23
Batch: 620; loss: 2.15; acc: 0.19
Batch: 640; loss: 2.14; acc: 0.25
Batch: 660; loss: 2.17; acc: 0.14
Batch: 680; loss: 2.2; acc: 0.17
Batch: 700; loss: 2.08; acc: 0.27
Batch: 720; loss: 2.29; acc: 0.14
Batch: 740; loss: 2.16; acc: 0.23
Batch: 760; loss: 2.22; acc: 0.16
Batch: 780; loss: 2.15; acc: 0.14
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679234838789436; val_accuracy: 0.21138535031847133 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.25; acc: 0.16
Batch: 20; loss: 2.14; acc: 0.23
Batch: 40; loss: 2.09; acc: 0.22
Batch: 60; loss: 2.12; acc: 0.31
Batch: 80; loss: 2.17; acc: 0.16
Batch: 100; loss: 2.13; acc: 0.2
Batch: 120; loss: 2.22; acc: 0.17
Batch: 140; loss: 2.19; acc: 0.25
Batch: 160; loss: 2.24; acc: 0.17
Batch: 180; loss: 2.17; acc: 0.2
Batch: 200; loss: 2.15; acc: 0.22
Batch: 220; loss: 2.04; acc: 0.31
Batch: 240; loss: 2.13; acc: 0.22
Batch: 260; loss: 2.11; acc: 0.3
Batch: 280; loss: 2.23; acc: 0.19
Batch: 300; loss: 2.15; acc: 0.22
Batch: 320; loss: 2.2; acc: 0.16
Batch: 340; loss: 2.2; acc: 0.19
Batch: 360; loss: 2.28; acc: 0.12
Batch: 380; loss: 2.2; acc: 0.19
Batch: 400; loss: 2.23; acc: 0.19
Batch: 420; loss: 2.11; acc: 0.25
Batch: 440; loss: 2.14; acc: 0.22
Batch: 460; loss: 2.1; acc: 0.2
Batch: 480; loss: 2.21; acc: 0.2
Batch: 500; loss: 2.24; acc: 0.16
Batch: 520; loss: 2.15; acc: 0.25
Batch: 540; loss: 2.22; acc: 0.2
Batch: 560; loss: 2.17; acc: 0.3
Batch: 580; loss: 2.15; acc: 0.23
Batch: 600; loss: 2.09; acc: 0.23
Batch: 620; loss: 2.28; acc: 0.16
Batch: 640; loss: 2.12; acc: 0.22
Batch: 660; loss: 2.11; acc: 0.31
Batch: 680; loss: 2.21; acc: 0.23
Batch: 700; loss: 2.24; acc: 0.14
Batch: 720; loss: 2.2; acc: 0.27
Batch: 740; loss: 2.16; acc: 0.2
Batch: 760; loss: 2.16; acc: 0.17
Batch: 780; loss: 2.13; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167940626478499; val_accuracy: 0.21228105095541402 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.26; acc: 0.06
Batch: 20; loss: 2.17; acc: 0.19
Batch: 40; loss: 2.23; acc: 0.2
Batch: 60; loss: 2.17; acc: 0.16
Batch: 80; loss: 2.12; acc: 0.22
Batch: 100; loss: 2.1; acc: 0.25
Batch: 120; loss: 2.05; acc: 0.33
Batch: 140; loss: 2.18; acc: 0.23
Batch: 160; loss: 2.09; acc: 0.25
Batch: 180; loss: 2.2; acc: 0.17
Batch: 200; loss: 2.14; acc: 0.23
Batch: 220; loss: 2.12; acc: 0.27
Batch: 240; loss: 2.23; acc: 0.12
Batch: 260; loss: 2.19; acc: 0.22
Batch: 280; loss: 2.12; acc: 0.25
Batch: 300; loss: 2.22; acc: 0.19
Batch: 320; loss: 2.19; acc: 0.22
Batch: 340; loss: 2.08; acc: 0.28
Batch: 360; loss: 2.2; acc: 0.25
Batch: 380; loss: 2.19; acc: 0.22
Batch: 400; loss: 2.12; acc: 0.22
Batch: 420; loss: 2.2; acc: 0.12
Batch: 440; loss: 2.26; acc: 0.16
Batch: 460; loss: 2.02; acc: 0.36
Batch: 480; loss: 2.2; acc: 0.19
Batch: 500; loss: 2.13; acc: 0.25
Batch: 520; loss: 2.16; acc: 0.12
Batch: 540; loss: 2.24; acc: 0.16
Batch: 560; loss: 2.16; acc: 0.23
Batch: 580; loss: 2.12; acc: 0.17
Batch: 600; loss: 2.24; acc: 0.11
Batch: 620; loss: 2.13; acc: 0.27
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.28; acc: 0.14
Batch: 680; loss: 2.11; acc: 0.28
Batch: 700; loss: 2.2; acc: 0.16
Batch: 720; loss: 2.23; acc: 0.14
Batch: 740; loss: 2.23; acc: 0.12
Batch: 760; loss: 2.24; acc: 0.19
Batch: 780; loss: 2.2; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.21 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679025425273144; val_accuracy: 0.21088773885350318 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.08; acc: 0.25
Batch: 20; loss: 2.22; acc: 0.22
Batch: 40; loss: 2.09; acc: 0.2
Batch: 60; loss: 2.15; acc: 0.19
Batch: 80; loss: 2.1; acc: 0.23
Batch: 100; loss: 2.13; acc: 0.2
Batch: 120; loss: 2.25; acc: 0.2
Batch: 140; loss: 2.14; acc: 0.2
Batch: 160; loss: 2.13; acc: 0.16
Batch: 180; loss: 2.24; acc: 0.09
Batch: 200; loss: 2.08; acc: 0.2
Batch: 220; loss: 2.2; acc: 0.12
Batch: 240; loss: 2.21; acc: 0.11
Batch: 260; loss: 2.19; acc: 0.19
Batch: 280; loss: 2.26; acc: 0.09
Batch: 300; loss: 2.2; acc: 0.23
Batch: 320; loss: 2.2; acc: 0.11
Batch: 340; loss: 2.2; acc: 0.22
Batch: 360; loss: 2.19; acc: 0.16
Batch: 380; loss: 2.24; acc: 0.19
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.19; acc: 0.09
Batch: 440; loss: 2.23; acc: 0.23
Batch: 460; loss: 2.34; acc: 0.08
Batch: 480; loss: 2.17; acc: 0.19
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.21; acc: 0.19
Batch: 540; loss: 2.11; acc: 0.3
Batch: 560; loss: 2.16; acc: 0.28
Batch: 580; loss: 2.26; acc: 0.16
Batch: 600; loss: 2.12; acc: 0.23
Batch: 620; loss: 2.18; acc: 0.14
Batch: 640; loss: 2.09; acc: 0.23
Batch: 660; loss: 2.21; acc: 0.16
Batch: 680; loss: 2.13; acc: 0.23
Batch: 700; loss: 2.19; acc: 0.25
Batch: 720; loss: 2.21; acc: 0.14
Batch: 740; loss: 2.24; acc: 0.14
Batch: 760; loss: 2.15; acc: 0.17
Batch: 780; loss: 2.23; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167919987326215; val_accuracy: 0.2115843949044586 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.16
Batch: 20; loss: 2.15; acc: 0.2
Batch: 40; loss: 2.13; acc: 0.17
Batch: 60; loss: 2.13; acc: 0.22
Batch: 80; loss: 2.22; acc: 0.2
Batch: 100; loss: 2.13; acc: 0.22
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.18; acc: 0.28
Batch: 160; loss: 2.21; acc: 0.17
Batch: 180; loss: 2.17; acc: 0.28
Batch: 200; loss: 2.2; acc: 0.22
Batch: 220; loss: 2.27; acc: 0.17
Batch: 240; loss: 2.21; acc: 0.23
Batch: 260; loss: 2.14; acc: 0.17
Batch: 280; loss: 2.18; acc: 0.19
Batch: 300; loss: 2.14; acc: 0.25
Batch: 320; loss: 2.15; acc: 0.28
Batch: 340; loss: 2.19; acc: 0.19
Batch: 360; loss: 2.21; acc: 0.19
Batch: 380; loss: 2.22; acc: 0.19
Batch: 400; loss: 2.19; acc: 0.23
Batch: 420; loss: 2.18; acc: 0.17
Batch: 440; loss: 2.15; acc: 0.25
Batch: 460; loss: 2.25; acc: 0.19
Batch: 480; loss: 2.2; acc: 0.17
Batch: 500; loss: 2.2; acc: 0.22
Batch: 520; loss: 2.12; acc: 0.23
Batch: 540; loss: 2.08; acc: 0.27
Batch: 560; loss: 2.05; acc: 0.27
Batch: 580; loss: 2.15; acc: 0.25
Batch: 600; loss: 2.21; acc: 0.2
Batch: 620; loss: 2.15; acc: 0.17
Batch: 640; loss: 2.18; acc: 0.28
Batch: 660; loss: 2.09; acc: 0.27
Batch: 680; loss: 2.26; acc: 0.2
Batch: 700; loss: 2.21; acc: 0.2
Batch: 720; loss: 2.28; acc: 0.16
Batch: 740; loss: 2.12; acc: 0.19
Batch: 760; loss: 2.16; acc: 0.19
Batch: 780; loss: 2.18; acc: 0.23
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679255529573767; val_accuracy: 0.21178343949044587 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.05; acc: 0.33
Batch: 20; loss: 2.25; acc: 0.16
Batch: 40; loss: 2.17; acc: 0.17
Batch: 60; loss: 2.16; acc: 0.2
Batch: 80; loss: 2.22; acc: 0.12
Batch: 100; loss: 2.23; acc: 0.17
Batch: 120; loss: 2.21; acc: 0.22
Batch: 140; loss: 2.09; acc: 0.19
Batch: 160; loss: 2.17; acc: 0.28
Batch: 180; loss: 2.21; acc: 0.19
Batch: 200; loss: 2.26; acc: 0.11
Batch: 220; loss: 2.2; acc: 0.2
Batch: 240; loss: 2.13; acc: 0.23
Batch: 260; loss: 2.1; acc: 0.3
Batch: 280; loss: 2.15; acc: 0.16
Batch: 300; loss: 2.17; acc: 0.22
Batch: 320; loss: 2.27; acc: 0.12
Batch: 340; loss: 2.14; acc: 0.22
Batch: 360; loss: 2.03; acc: 0.28
Batch: 380; loss: 2.13; acc: 0.14
Batch: 400; loss: 2.2; acc: 0.19
Batch: 420; loss: 2.21; acc: 0.25
Batch: 440; loss: 2.08; acc: 0.23
Batch: 460; loss: 2.11; acc: 0.25
Batch: 480; loss: 2.24; acc: 0.17
Batch: 500; loss: 2.14; acc: 0.23
Batch: 520; loss: 2.29; acc: 0.16
Batch: 540; loss: 2.25; acc: 0.16
Batch: 560; loss: 2.2; acc: 0.2
Batch: 580; loss: 2.21; acc: 0.16
Batch: 600; loss: 2.18; acc: 0.17
Batch: 620; loss: 2.24; acc: 0.16
Batch: 640; loss: 2.23; acc: 0.14
Batch: 660; loss: 2.13; acc: 0.25
Batch: 680; loss: 2.15; acc: 0.2
Batch: 700; loss: 2.12; acc: 0.23
Batch: 720; loss: 2.28; acc: 0.17
Batch: 740; loss: 2.18; acc: 0.19
Batch: 760; loss: 2.1; acc: 0.25
Batch: 780; loss: 2.18; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167941024348994; val_accuracy: 0.21228105095541402 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.15; acc: 0.17
Batch: 20; loss: 2.12; acc: 0.19
Batch: 40; loss: 2.27; acc: 0.17
Batch: 60; loss: 2.18; acc: 0.14
Batch: 80; loss: 2.21; acc: 0.14
Batch: 100; loss: 2.11; acc: 0.31
Batch: 120; loss: 2.24; acc: 0.12
Batch: 140; loss: 2.24; acc: 0.09
Batch: 160; loss: 2.15; acc: 0.17
Batch: 180; loss: 2.19; acc: 0.27
Batch: 200; loss: 2.27; acc: 0.2
Batch: 220; loss: 2.23; acc: 0.14
Batch: 240; loss: 2.16; acc: 0.22
Batch: 260; loss: 2.18; acc: 0.2
Batch: 280; loss: 2.16; acc: 0.2
Batch: 300; loss: 2.36; acc: 0.14
Batch: 320; loss: 2.07; acc: 0.34
Batch: 340; loss: 2.16; acc: 0.23
Batch: 360; loss: 2.22; acc: 0.19
Batch: 380; loss: 2.26; acc: 0.14
Batch: 400; loss: 2.13; acc: 0.19
Batch: 420; loss: 2.2; acc: 0.09
Batch: 440; loss: 2.27; acc: 0.23
Batch: 460; loss: 2.15; acc: 0.23
Batch: 480; loss: 2.16; acc: 0.28
Batch: 500; loss: 2.21; acc: 0.16
Batch: 520; loss: 2.11; acc: 0.23
Batch: 540; loss: 2.21; acc: 0.19
Batch: 560; loss: 2.15; acc: 0.2
Batch: 580; loss: 2.24; acc: 0.14
Batch: 600; loss: 2.2; acc: 0.27
Batch: 620; loss: 2.19; acc: 0.23
Batch: 640; loss: 2.25; acc: 0.23
Batch: 660; loss: 2.18; acc: 0.17
Batch: 680; loss: 2.23; acc: 0.14
Batch: 700; loss: 2.16; acc: 0.2
Batch: 720; loss: 2.13; acc: 0.25
Batch: 740; loss: 2.15; acc: 0.25
Batch: 760; loss: 2.17; acc: 0.25
Batch: 780; loss: 2.16; acc: 0.27
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679379788173994; val_accuracy: 0.21208200636942676 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.22; acc: 0.17
Batch: 20; loss: 2.23; acc: 0.14
Batch: 40; loss: 2.21; acc: 0.19
Batch: 60; loss: 2.18; acc: 0.2
Batch: 80; loss: 2.16; acc: 0.2
Batch: 100; loss: 2.17; acc: 0.22
Batch: 120; loss: 2.16; acc: 0.2
Batch: 140; loss: 2.19; acc: 0.2
Batch: 160; loss: 2.09; acc: 0.2
Batch: 180; loss: 2.28; acc: 0.2
Batch: 200; loss: 2.28; acc: 0.14
Batch: 220; loss: 2.18; acc: 0.27
Batch: 240; loss: 2.22; acc: 0.19
Batch: 260; loss: 2.2; acc: 0.19
Batch: 280; loss: 2.22; acc: 0.12
Batch: 300; loss: 2.17; acc: 0.28
Batch: 320; loss: 2.18; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.14
Batch: 360; loss: 2.12; acc: 0.25
Batch: 380; loss: 2.13; acc: 0.2
Batch: 400; loss: 2.16; acc: 0.23
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.15; acc: 0.17
Batch: 460; loss: 2.28; acc: 0.2
Batch: 480; loss: 2.17; acc: 0.19
Batch: 500; loss: 2.13; acc: 0.23
Batch: 520; loss: 2.04; acc: 0.27
Batch: 540; loss: 2.12; acc: 0.3
Batch: 560; loss: 2.23; acc: 0.19
Batch: 580; loss: 2.21; acc: 0.23
Batch: 600; loss: 2.11; acc: 0.16
Batch: 620; loss: 2.19; acc: 0.23
Batch: 640; loss: 2.23; acc: 0.16
Batch: 660; loss: 2.27; acc: 0.19
Batch: 680; loss: 2.07; acc: 0.34
Batch: 700; loss: 2.19; acc: 0.17
Batch: 720; loss: 2.18; acc: 0.25
Batch: 740; loss: 2.2; acc: 0.25
Batch: 760; loss: 2.15; acc: 0.19
Batch: 780; loss: 2.21; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167935103367848; val_accuracy: 0.21088773885350318 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.14; acc: 0.28
Batch: 20; loss: 2.16; acc: 0.22
Batch: 40; loss: 2.2; acc: 0.22
Batch: 60; loss: 2.09; acc: 0.28
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.2; acc: 0.17
Batch: 120; loss: 2.11; acc: 0.2
Batch: 140; loss: 2.21; acc: 0.17
Batch: 160; loss: 2.21; acc: 0.2
Batch: 180; loss: 2.17; acc: 0.16
Batch: 200; loss: 2.22; acc: 0.16
Batch: 220; loss: 2.17; acc: 0.19
Batch: 240; loss: 2.22; acc: 0.17
Batch: 260; loss: 2.15; acc: 0.33
Batch: 280; loss: 2.12; acc: 0.27
Batch: 300; loss: 2.12; acc: 0.25
Batch: 320; loss: 2.11; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.14
Batch: 380; loss: 2.2; acc: 0.19
Batch: 400; loss: 2.17; acc: 0.2
Batch: 420; loss: 2.23; acc: 0.2
Batch: 440; loss: 2.28; acc: 0.17
Batch: 460; loss: 2.17; acc: 0.16
Batch: 480; loss: 2.28; acc: 0.14
Batch: 500; loss: 2.13; acc: 0.27
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.17; acc: 0.22
Batch: 560; loss: 2.13; acc: 0.23
Batch: 580; loss: 2.13; acc: 0.27
Batch: 600; loss: 2.26; acc: 0.2
Batch: 620; loss: 2.14; acc: 0.16
Batch: 640; loss: 2.1; acc: 0.27
Batch: 660; loss: 2.17; acc: 0.25
Batch: 680; loss: 2.07; acc: 0.25
Batch: 700; loss: 2.17; acc: 0.22
Batch: 720; loss: 2.14; acc: 0.25
Batch: 740; loss: 2.16; acc: 0.2
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.09; acc: 0.23
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167929127717474; val_accuracy: 0.21148487261146498 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.19; acc: 0.28
Batch: 20; loss: 2.19; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.2
Batch: 80; loss: 2.28; acc: 0.17
Batch: 100; loss: 2.08; acc: 0.25
Batch: 120; loss: 2.12; acc: 0.22
Batch: 140; loss: 2.14; acc: 0.2
Batch: 160; loss: 2.16; acc: 0.22
Batch: 180; loss: 2.2; acc: 0.19
Batch: 200; loss: 2.14; acc: 0.22
Batch: 220; loss: 2.25; acc: 0.17
Batch: 240; loss: 2.2; acc: 0.22
Batch: 260; loss: 2.21; acc: 0.19
Batch: 280; loss: 2.19; acc: 0.17
Batch: 300; loss: 2.12; acc: 0.17
Batch: 320; loss: 2.18; acc: 0.25
Batch: 340; loss: 2.23; acc: 0.19
Batch: 360; loss: 2.22; acc: 0.17
Batch: 380; loss: 2.0; acc: 0.28
Batch: 400; loss: 2.14; acc: 0.17
Batch: 420; loss: 2.15; acc: 0.23
Batch: 440; loss: 2.09; acc: 0.3
Batch: 460; loss: 2.16; acc: 0.25
Batch: 480; loss: 2.27; acc: 0.12
Batch: 500; loss: 2.23; acc: 0.19
Batch: 520; loss: 2.18; acc: 0.22
Batch: 540; loss: 2.13; acc: 0.25
Batch: 560; loss: 2.1; acc: 0.23
Batch: 580; loss: 2.13; acc: 0.25
Batch: 600; loss: 2.18; acc: 0.22
Batch: 620; loss: 2.14; acc: 0.22
Batch: 640; loss: 2.17; acc: 0.17
Batch: 660; loss: 2.07; acc: 0.23
Batch: 680; loss: 2.1; acc: 0.22
Batch: 700; loss: 2.19; acc: 0.23
Batch: 720; loss: 2.22; acc: 0.2
Batch: 740; loss: 2.07; acc: 0.3
Batch: 760; loss: 2.09; acc: 0.25
Batch: 780; loss: 2.17; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167939978800002; val_accuracy: 0.21218152866242038 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.09; acc: 0.28
Batch: 20; loss: 2.25; acc: 0.17
Batch: 40; loss: 2.22; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.1; acc: 0.31
Batch: 100; loss: 2.12; acc: 0.19
Batch: 120; loss: 2.31; acc: 0.09
Batch: 140; loss: 2.18; acc: 0.17
Batch: 160; loss: 2.13; acc: 0.3
Batch: 180; loss: 2.16; acc: 0.12
Batch: 200; loss: 2.16; acc: 0.2
Batch: 220; loss: 2.12; acc: 0.2
Batch: 240; loss: 2.2; acc: 0.16
Batch: 260; loss: 2.2; acc: 0.17
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.16; acc: 0.19
Batch: 320; loss: 2.19; acc: 0.12
Batch: 340; loss: 2.23; acc: 0.22
Batch: 360; loss: 2.1; acc: 0.28
Batch: 380; loss: 2.18; acc: 0.2
Batch: 400; loss: 2.09; acc: 0.3
Batch: 420; loss: 2.27; acc: 0.12
Batch: 440; loss: 2.11; acc: 0.3
Batch: 460; loss: 2.15; acc: 0.22
Batch: 480; loss: 2.09; acc: 0.17
Batch: 500; loss: 2.15; acc: 0.28
Batch: 520; loss: 2.29; acc: 0.2
Batch: 540; loss: 2.1; acc: 0.28
Batch: 560; loss: 2.13; acc: 0.22
Batch: 580; loss: 2.15; acc: 0.2
Batch: 600; loss: 2.15; acc: 0.23
Batch: 620; loss: 2.18; acc: 0.11
Batch: 640; loss: 2.13; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.16
Batch: 680; loss: 2.23; acc: 0.17
Batch: 700; loss: 2.16; acc: 0.19
Batch: 720; loss: 2.13; acc: 0.23
Batch: 740; loss: 2.19; acc: 0.12
Batch: 760; loss: 2.27; acc: 0.14
Batch: 780; loss: 2.14; acc: 0.25
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.16792907380754; val_accuracy: 0.21218152866242038 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.13; acc: 0.33
Batch: 20; loss: 2.05; acc: 0.27
Batch: 40; loss: 2.2; acc: 0.23
Batch: 60; loss: 2.1; acc: 0.22
Batch: 80; loss: 2.24; acc: 0.2
Batch: 100; loss: 2.21; acc: 0.19
Batch: 120; loss: 2.19; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.17
Batch: 160; loss: 2.15; acc: 0.23
Batch: 180; loss: 2.23; acc: 0.2
Batch: 200; loss: 2.21; acc: 0.22
Batch: 220; loss: 2.32; acc: 0.16
Batch: 240; loss: 2.18; acc: 0.22
Batch: 260; loss: 2.19; acc: 0.17
Batch: 280; loss: 2.22; acc: 0.17
Batch: 300; loss: 2.12; acc: 0.28
Batch: 320; loss: 2.13; acc: 0.16
Batch: 340; loss: 2.23; acc: 0.17
Batch: 360; loss: 2.17; acc: 0.17
Batch: 380; loss: 2.18; acc: 0.2
Batch: 400; loss: 2.11; acc: 0.22
Batch: 420; loss: 2.14; acc: 0.23
Batch: 440; loss: 2.21; acc: 0.25
Batch: 460; loss: 2.15; acc: 0.17
Batch: 480; loss: 2.25; acc: 0.16
Batch: 500; loss: 2.04; acc: 0.23
Batch: 520; loss: 2.1; acc: 0.3
Batch: 540; loss: 2.27; acc: 0.2
Batch: 560; loss: 2.31; acc: 0.11
Batch: 580; loss: 2.17; acc: 0.22
Batch: 600; loss: 2.21; acc: 0.23
Batch: 620; loss: 2.21; acc: 0.2
Batch: 640; loss: 2.13; acc: 0.23
Batch: 660; loss: 2.11; acc: 0.28
Batch: 680; loss: 2.21; acc: 0.19
Batch: 700; loss: 2.04; acc: 0.38
Batch: 720; loss: 2.2; acc: 0.2
Batch: 740; loss: 2.13; acc: 0.27
Batch: 760; loss: 2.16; acc: 0.22
Batch: 780; loss: 2.06; acc: 0.28
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679311147920646; val_accuracy: 0.2115843949044586 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.06; acc: 0.19
Batch: 20; loss: 2.19; acc: 0.14
Batch: 40; loss: 2.13; acc: 0.22
Batch: 60; loss: 2.1; acc: 0.25
Batch: 80; loss: 2.16; acc: 0.27
Batch: 100; loss: 2.16; acc: 0.25
Batch: 120; loss: 2.15; acc: 0.23
Batch: 140; loss: 2.07; acc: 0.27
Batch: 160; loss: 2.16; acc: 0.19
Batch: 180; loss: 2.27; acc: 0.19
Batch: 200; loss: 2.16; acc: 0.22
Batch: 220; loss: 2.1; acc: 0.27
Batch: 240; loss: 2.16; acc: 0.19
Batch: 260; loss: 2.17; acc: 0.23
Batch: 280; loss: 2.16; acc: 0.2
Batch: 300; loss: 2.26; acc: 0.14
Batch: 320; loss: 2.27; acc: 0.12
Batch: 340; loss: 2.12; acc: 0.23
Batch: 360; loss: 2.17; acc: 0.14
Batch: 380; loss: 2.18; acc: 0.27
Batch: 400; loss: 2.19; acc: 0.25
Batch: 420; loss: 2.22; acc: 0.25
Batch: 440; loss: 2.03; acc: 0.39
Batch: 460; loss: 2.22; acc: 0.14
Batch: 480; loss: 2.11; acc: 0.28
Batch: 500; loss: 2.13; acc: 0.22
Batch: 520; loss: 2.19; acc: 0.12
Batch: 540; loss: 2.2; acc: 0.22
Batch: 560; loss: 2.27; acc: 0.12
Batch: 580; loss: 2.19; acc: 0.17
Batch: 600; loss: 2.21; acc: 0.16
Batch: 620; loss: 2.18; acc: 0.25
Batch: 640; loss: 2.27; acc: 0.17
Batch: 660; loss: 2.09; acc: 0.2
Batch: 680; loss: 2.23; acc: 0.14
Batch: 700; loss: 2.14; acc: 0.22
Batch: 720; loss: 2.24; acc: 0.17
Batch: 740; loss: 2.12; acc: 0.22
Batch: 760; loss: 2.16; acc: 0.23
Batch: 780; loss: 2.14; acc: 0.23
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167934775352478; val_accuracy: 0.21178343949044587 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.3; acc: 0.17
Batch: 20; loss: 2.09; acc: 0.22
Batch: 40; loss: 2.23; acc: 0.16
Batch: 60; loss: 2.16; acc: 0.16
Batch: 80; loss: 2.15; acc: 0.25
Batch: 100; loss: 2.15; acc: 0.2
Batch: 120; loss: 2.19; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.14
Batch: 160; loss: 2.19; acc: 0.14
Batch: 180; loss: 2.2; acc: 0.16
Batch: 200; loss: 2.2; acc: 0.23
Batch: 220; loss: 2.23; acc: 0.16
Batch: 240; loss: 2.19; acc: 0.22
Batch: 260; loss: 2.32; acc: 0.12
Batch: 280; loss: 2.15; acc: 0.25
Batch: 300; loss: 2.16; acc: 0.28
Batch: 320; loss: 2.2; acc: 0.22
Batch: 340; loss: 2.15; acc: 0.2
Batch: 360; loss: 2.05; acc: 0.25
Batch: 380; loss: 2.16; acc: 0.23
Batch: 400; loss: 2.29; acc: 0.16
Batch: 420; loss: 2.16; acc: 0.22
Batch: 440; loss: 2.15; acc: 0.28
Batch: 460; loss: 2.11; acc: 0.28
Batch: 480; loss: 2.17; acc: 0.25
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.21; acc: 0.16
Batch: 540; loss: 2.11; acc: 0.25
Batch: 560; loss: 2.06; acc: 0.3
Batch: 580; loss: 2.24; acc: 0.17
Batch: 600; loss: 2.2; acc: 0.22
Batch: 620; loss: 2.16; acc: 0.23
Batch: 640; loss: 2.2; acc: 0.16
Batch: 660; loss: 2.17; acc: 0.2
Batch: 680; loss: 2.08; acc: 0.27
Batch: 700; loss: 2.24; acc: 0.2
Batch: 720; loss: 2.12; acc: 0.27
Batch: 740; loss: 2.15; acc: 0.2
Batch: 760; loss: 2.27; acc: 0.16
Batch: 780; loss: 2.21; acc: 0.14
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679462604461963; val_accuracy: 0.2119824840764331 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.17; acc: 0.23
Batch: 20; loss: 2.15; acc: 0.22
Batch: 40; loss: 2.1; acc: 0.25
Batch: 60; loss: 2.1; acc: 0.2
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.19; acc: 0.22
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.27; acc: 0.14
Batch: 160; loss: 2.17; acc: 0.12
Batch: 180; loss: 2.1; acc: 0.23
Batch: 200; loss: 2.14; acc: 0.2
Batch: 220; loss: 2.02; acc: 0.31
Batch: 240; loss: 2.24; acc: 0.2
Batch: 260; loss: 2.14; acc: 0.2
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.16; acc: 0.2
Batch: 320; loss: 2.02; acc: 0.33
Batch: 340; loss: 2.21; acc: 0.17
Batch: 360; loss: 2.15; acc: 0.25
Batch: 380; loss: 2.09; acc: 0.31
Batch: 400; loss: 2.11; acc: 0.28
Batch: 420; loss: 2.26; acc: 0.17
Batch: 440; loss: 2.19; acc: 0.2
Batch: 460; loss: 2.12; acc: 0.17
Batch: 480; loss: 2.14; acc: 0.19
Batch: 500; loss: 2.18; acc: 0.14
Batch: 520; loss: 2.08; acc: 0.36
Batch: 540; loss: 2.2; acc: 0.17
Batch: 560; loss: 2.14; acc: 0.28
Batch: 580; loss: 2.17; acc: 0.28
Batch: 600; loss: 2.17; acc: 0.23
Batch: 620; loss: 2.19; acc: 0.22
Batch: 640; loss: 2.13; acc: 0.3
Batch: 660; loss: 2.2; acc: 0.12
Batch: 680; loss: 2.11; acc: 0.27
Batch: 700; loss: 2.12; acc: 0.23
Batch: 720; loss: 2.2; acc: 0.14
Batch: 740; loss: 2.13; acc: 0.23
Batch: 760; loss: 2.19; acc: 0.22
Batch: 780; loss: 2.2; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679348171136943; val_accuracy: 0.21168391719745222 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.12; acc: 0.19
Batch: 20; loss: 2.1; acc: 0.22
Batch: 40; loss: 2.23; acc: 0.22
Batch: 60; loss: 2.17; acc: 0.2
Batch: 80; loss: 2.26; acc: 0.14
Batch: 100; loss: 2.24; acc: 0.19
Batch: 120; loss: 2.17; acc: 0.19
Batch: 140; loss: 2.24; acc: 0.12
Batch: 160; loss: 2.17; acc: 0.22
Batch: 180; loss: 2.13; acc: 0.25
Batch: 200; loss: 2.05; acc: 0.27
Batch: 220; loss: 2.15; acc: 0.22
Batch: 240; loss: 2.15; acc: 0.22
Batch: 260; loss: 2.19; acc: 0.14
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.17; acc: 0.2
Batch: 320; loss: 2.17; acc: 0.22
Batch: 340; loss: 2.08; acc: 0.28
Batch: 360; loss: 2.25; acc: 0.14
Batch: 380; loss: 2.1; acc: 0.27
Batch: 400; loss: 2.2; acc: 0.19
Batch: 420; loss: 2.1; acc: 0.23
Batch: 440; loss: 2.24; acc: 0.09
Batch: 460; loss: 2.15; acc: 0.19
Batch: 480; loss: 2.18; acc: 0.14
Batch: 500; loss: 2.13; acc: 0.22
Batch: 520; loss: 2.1; acc: 0.3
Batch: 540; loss: 2.24; acc: 0.19
Batch: 560; loss: 2.14; acc: 0.17
Batch: 580; loss: 2.13; acc: 0.23
Batch: 600; loss: 2.2; acc: 0.17
Batch: 620; loss: 2.23; acc: 0.16
Batch: 640; loss: 2.14; acc: 0.17
Batch: 660; loss: 2.15; acc: 0.17
Batch: 680; loss: 2.31; acc: 0.17
Batch: 700; loss: 2.21; acc: 0.2
Batch: 720; loss: 2.17; acc: 0.16
Batch: 740; loss: 2.24; acc: 0.17
Batch: 760; loss: 2.17; acc: 0.16
Batch: 780; loss: 2.16; acc: 0.17
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167934798131323; val_accuracy: 0.21148487261146498 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.22; acc: 0.14
Batch: 20; loss: 2.14; acc: 0.25
Batch: 40; loss: 2.11; acc: 0.28
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.16; acc: 0.17
Batch: 100; loss: 2.25; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.22
Batch: 140; loss: 2.2; acc: 0.2
Batch: 160; loss: 2.17; acc: 0.27
Batch: 180; loss: 2.12; acc: 0.23
Batch: 200; loss: 2.2; acc: 0.12
Batch: 220; loss: 2.27; acc: 0.12
Batch: 240; loss: 2.27; acc: 0.25
Batch: 260; loss: 2.23; acc: 0.2
Batch: 280; loss: 2.24; acc: 0.2
Batch: 300; loss: 2.08; acc: 0.33
Batch: 320; loss: 2.19; acc: 0.12
Batch: 340; loss: 2.13; acc: 0.22
Batch: 360; loss: 2.27; acc: 0.16
Batch: 380; loss: 2.17; acc: 0.22
Batch: 400; loss: 2.26; acc: 0.17
Batch: 420; loss: 2.16; acc: 0.19
Batch: 440; loss: 2.2; acc: 0.2
Batch: 460; loss: 2.1; acc: 0.25
Batch: 480; loss: 2.24; acc: 0.17
Batch: 500; loss: 2.18; acc: 0.14
Batch: 520; loss: 2.24; acc: 0.11
Batch: 540; loss: 2.22; acc: 0.25
Batch: 560; loss: 2.17; acc: 0.22
Batch: 580; loss: 2.15; acc: 0.25
Batch: 600; loss: 2.17; acc: 0.19
Batch: 620; loss: 2.16; acc: 0.22
Batch: 640; loss: 2.17; acc: 0.12
Batch: 660; loss: 2.08; acc: 0.22
Batch: 680; loss: 2.13; acc: 0.19
Batch: 700; loss: 2.11; acc: 0.25
Batch: 720; loss: 2.12; acc: 0.16
Batch: 740; loss: 2.1; acc: 0.17
Batch: 760; loss: 2.14; acc: 0.22
Batch: 780; loss: 2.21; acc: 0.2
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679386834430088; val_accuracy: 0.21168391719745222 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.29; acc: 0.17
Batch: 20; loss: 2.18; acc: 0.19
Batch: 40; loss: 2.15; acc: 0.27
Batch: 60; loss: 2.21; acc: 0.19
Batch: 80; loss: 2.18; acc: 0.16
Batch: 100; loss: 2.18; acc: 0.16
Batch: 120; loss: 2.12; acc: 0.22
Batch: 140; loss: 2.11; acc: 0.27
Batch: 160; loss: 2.23; acc: 0.14
Batch: 180; loss: 2.22; acc: 0.19
Batch: 200; loss: 2.26; acc: 0.2
Batch: 220; loss: 2.16; acc: 0.23
Batch: 240; loss: 2.16; acc: 0.16
Batch: 260; loss: 2.17; acc: 0.2
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.16; acc: 0.19
Batch: 320; loss: 2.2; acc: 0.16
Batch: 340; loss: 2.25; acc: 0.11
Batch: 360; loss: 2.15; acc: 0.25
Batch: 380; loss: 2.19; acc: 0.19
Batch: 400; loss: 2.2; acc: 0.19
Batch: 420; loss: 2.11; acc: 0.3
Batch: 440; loss: 2.22; acc: 0.19
Batch: 460; loss: 2.16; acc: 0.22
Batch: 480; loss: 2.14; acc: 0.23
Batch: 500; loss: 2.11; acc: 0.3
Batch: 520; loss: 2.23; acc: 0.22
Batch: 540; loss: 2.22; acc: 0.12
Batch: 560; loss: 2.14; acc: 0.14
Batch: 580; loss: 2.19; acc: 0.23
Batch: 600; loss: 2.2; acc: 0.19
Batch: 620; loss: 2.17; acc: 0.17
Batch: 640; loss: 2.22; acc: 0.19
Batch: 660; loss: 2.16; acc: 0.19
Batch: 680; loss: 2.2; acc: 0.14
Batch: 700; loss: 2.1; acc: 0.23
Batch: 720; loss: 2.11; acc: 0.22
Batch: 740; loss: 2.09; acc: 0.22
Batch: 760; loss: 2.16; acc: 0.25
Batch: 780; loss: 2.15; acc: 0.19
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.1679429316976266; val_accuracy: 0.21178343949044587 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.14; acc: 0.2
Batch: 20; loss: 2.18; acc: 0.19
Batch: 40; loss: 2.22; acc: 0.22
Batch: 60; loss: 2.24; acc: 0.12
Batch: 80; loss: 2.11; acc: 0.19
Batch: 100; loss: 2.18; acc: 0.22
Batch: 120; loss: 2.2; acc: 0.19
Batch: 140; loss: 2.14; acc: 0.22
Batch: 160; loss: 2.14; acc: 0.16
Batch: 180; loss: 2.06; acc: 0.25
Batch: 200; loss: 2.13; acc: 0.2
Batch: 220; loss: 2.06; acc: 0.31
Batch: 240; loss: 2.26; acc: 0.09
Batch: 260; loss: 2.17; acc: 0.23
Batch: 280; loss: 2.29; acc: 0.14
Batch: 300; loss: 2.16; acc: 0.27
Batch: 320; loss: 2.19; acc: 0.25
Batch: 340; loss: 2.14; acc: 0.27
Batch: 360; loss: 2.15; acc: 0.22
Batch: 380; loss: 2.2; acc: 0.19
Batch: 400; loss: 2.12; acc: 0.17
Batch: 420; loss: 2.18; acc: 0.2
Batch: 440; loss: 2.2; acc: 0.22
Batch: 460; loss: 2.18; acc: 0.16
Batch: 480; loss: 2.26; acc: 0.22
Batch: 500; loss: 2.23; acc: 0.12
Batch: 520; loss: 2.08; acc: 0.2
Batch: 540; loss: 2.15; acc: 0.28
Batch: 560; loss: 2.21; acc: 0.23
Batch: 580; loss: 2.26; acc: 0.11
Batch: 600; loss: 2.2; acc: 0.08
Batch: 620; loss: 2.18; acc: 0.25
Batch: 640; loss: 2.31; acc: 0.17
Batch: 660; loss: 2.22; acc: 0.14
Batch: 680; loss: 2.19; acc: 0.12
Batch: 700; loss: 2.19; acc: 0.27
Batch: 720; loss: 2.09; acc: 0.22
Batch: 740; loss: 2.32; acc: 0.2
Batch: 760; loss: 2.23; acc: 0.19
Batch: 780; loss: 2.18; acc: 0.25
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167938292406167; val_accuracy: 0.21148487261146498 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.1; acc: 0.28
Batch: 20; loss: 2.16; acc: 0.22
Batch: 40; loss: 2.24; acc: 0.14
Batch: 60; loss: 2.18; acc: 0.22
Batch: 80; loss: 2.21; acc: 0.22
Batch: 100; loss: 2.18; acc: 0.17
Batch: 120; loss: 2.25; acc: 0.22
Batch: 140; loss: 2.23; acc: 0.2
Batch: 160; loss: 2.16; acc: 0.23
Batch: 180; loss: 2.2; acc: 0.23
Batch: 200; loss: 2.19; acc: 0.19
Batch: 220; loss: 2.17; acc: 0.22
Batch: 240; loss: 2.19; acc: 0.19
Batch: 260; loss: 2.03; acc: 0.31
Batch: 280; loss: 2.24; acc: 0.22
Batch: 300; loss: 2.06; acc: 0.22
Batch: 320; loss: 2.19; acc: 0.27
Batch: 340; loss: 2.11; acc: 0.27
Batch: 360; loss: 2.12; acc: 0.25
Batch: 380; loss: 2.19; acc: 0.27
Batch: 400; loss: 2.27; acc: 0.12
Batch: 420; loss: 2.16; acc: 0.25
Batch: 440; loss: 2.28; acc: 0.14
Batch: 460; loss: 2.12; acc: 0.2
Batch: 480; loss: 2.12; acc: 0.22
Batch: 500; loss: 2.08; acc: 0.2
Batch: 520; loss: 2.31; acc: 0.12
Batch: 540; loss: 2.05; acc: 0.28
Batch: 560; loss: 2.27; acc: 0.12
Batch: 580; loss: 2.13; acc: 0.23
Batch: 600; loss: 2.1; acc: 0.31
Batch: 620; loss: 2.25; acc: 0.16
Batch: 640; loss: 2.22; acc: 0.2
Batch: 660; loss: 2.16; acc: 0.23
Batch: 680; loss: 2.07; acc: 0.23
Batch: 700; loss: 2.18; acc: 0.23
Batch: 720; loss: 2.22; acc: 0.11
Batch: 740; loss: 2.15; acc: 0.12
Batch: 760; loss: 2.11; acc: 0.2
Batch: 780; loss: 2.08; acc: 0.3
Train Epoch over. train_loss: 2.17; train_accuracy: 0.2 

Batch: 0; loss: 2.18; acc: 0.2
Batch: 20; loss: 2.13; acc: 0.2
Batch: 40; loss: 2.1; acc: 0.27
Batch: 60; loss: 2.1; acc: 0.17
Batch: 80; loss: 2.17; acc: 0.23
Batch: 100; loss: 2.15; acc: 0.27
Batch: 120; loss: 2.14; acc: 0.2
Batch: 140; loss: 2.2; acc: 0.2
Val Epoch over. val_loss: 2.167940373633318; val_accuracy: 0.2112858280254777 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_10_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 1110650
elements in E: 1110650
fraction nonzero: 1.0
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.31; acc: 0.05
Batch: 60; loss: 2.31; acc: 0.05
Batch: 80; loss: 2.31; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.31; acc: 0.06
Batch: 180; loss: 2.31; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.31; acc: 0.06
Batch: 260; loss: 2.31; acc: 0.06
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.14
Batch: 320; loss: 2.3; acc: 0.09
Batch: 340; loss: 2.31; acc: 0.05
Batch: 360; loss: 2.32; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.31; acc: 0.06
Batch: 420; loss: 2.29; acc: 0.09
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.3; acc: 0.09
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.29; acc: 0.16
Batch: 520; loss: 2.31; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.19
Batch: 560; loss: 2.32; acc: 0.03
Batch: 580; loss: 2.31; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.09
Batch: 620; loss: 2.3; acc: 0.19
Batch: 640; loss: 2.3; acc: 0.09
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.16
Batch: 700; loss: 2.29; acc: 0.17
Batch: 720; loss: 2.3; acc: 0.09
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.29; acc: 0.11
Batch: 780; loss: 2.29; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.05
Batch: 140; loss: 2.29; acc: 0.14
Val Epoch over. val_loss: 2.2958420188563644; val_accuracy: 0.10947452229299363 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.08
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.28; acc: 0.17
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.31; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.16
Batch: 200; loss: 2.29; acc: 0.16
Batch: 220; loss: 2.3; acc: 0.17
Batch: 240; loss: 2.29; acc: 0.19
Batch: 260; loss: 2.29; acc: 0.09
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.29; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.14
Batch: 340; loss: 2.28; acc: 0.2
Batch: 360; loss: 2.31; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.14
Batch: 400; loss: 2.29; acc: 0.17
Batch: 420; loss: 2.29; acc: 0.19
Batch: 440; loss: 2.3; acc: 0.19
Batch: 460; loss: 2.3; acc: 0.19
Batch: 480; loss: 2.29; acc: 0.22
Batch: 500; loss: 2.28; acc: 0.27
Batch: 520; loss: 2.28; acc: 0.23
Batch: 540; loss: 2.28; acc: 0.17
Batch: 560; loss: 2.28; acc: 0.27
Batch: 580; loss: 2.28; acc: 0.38
Batch: 600; loss: 2.28; acc: 0.3
Batch: 620; loss: 2.29; acc: 0.31
Batch: 640; loss: 2.3; acc: 0.17
Batch: 660; loss: 2.28; acc: 0.19
Batch: 680; loss: 2.28; acc: 0.3
Batch: 700; loss: 2.28; acc: 0.23
Batch: 720; loss: 2.29; acc: 0.33
Batch: 740; loss: 2.29; acc: 0.31
Batch: 760; loss: 2.29; acc: 0.25
Batch: 780; loss: 2.29; acc: 0.28
Train Epoch over. train_loss: 2.29; train_accuracy: 0.18 

Batch: 0; loss: 2.28; acc: 0.22
Batch: 20; loss: 2.29; acc: 0.33
Batch: 40; loss: 2.28; acc: 0.31
Batch: 60; loss: 2.29; acc: 0.3
Batch: 80; loss: 2.28; acc: 0.31
Batch: 100; loss: 2.3; acc: 0.27
Batch: 120; loss: 2.29; acc: 0.31
Batch: 140; loss: 2.28; acc: 0.25
Val Epoch over. val_loss: 2.2866212243487123; val_accuracy: 0.25278662420382164 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.25
Batch: 20; loss: 2.28; acc: 0.28
Batch: 40; loss: 2.28; acc: 0.31
Batch: 60; loss: 2.3; acc: 0.17
Batch: 80; loss: 2.29; acc: 0.22
Batch: 100; loss: 2.28; acc: 0.28
Batch: 120; loss: 2.28; acc: 0.23
Batch: 140; loss: 2.28; acc: 0.31
Batch: 160; loss: 2.29; acc: 0.22
Batch: 180; loss: 2.28; acc: 0.28
Batch: 200; loss: 2.29; acc: 0.19
Batch: 220; loss: 2.28; acc: 0.3
Batch: 240; loss: 2.28; acc: 0.34
Batch: 260; loss: 2.28; acc: 0.33
Batch: 280; loss: 2.3; acc: 0.17
Batch: 300; loss: 2.28; acc: 0.17
Batch: 320; loss: 2.28; acc: 0.22
Batch: 340; loss: 2.28; acc: 0.3
Batch: 360; loss: 2.28; acc: 0.27
Batch: 380; loss: 2.28; acc: 0.25
Batch: 400; loss: 2.28; acc: 0.16
Batch: 420; loss: 2.27; acc: 0.33
Batch: 440; loss: 2.28; acc: 0.33
Batch: 460; loss: 2.28; acc: 0.22
Batch: 480; loss: 2.28; acc: 0.19
Batch: 500; loss: 2.27; acc: 0.3
Batch: 520; loss: 2.27; acc: 0.31
Batch: 540; loss: 2.29; acc: 0.17
Batch: 560; loss: 2.28; acc: 0.19
Batch: 580; loss: 2.27; acc: 0.31
Batch: 600; loss: 2.27; acc: 0.25
Batch: 620; loss: 2.28; acc: 0.14
Batch: 640; loss: 2.27; acc: 0.23
Batch: 660; loss: 2.27; acc: 0.25
Batch: 680; loss: 2.26; acc: 0.31
Batch: 700; loss: 2.26; acc: 0.25
Batch: 720; loss: 2.27; acc: 0.25
Batch: 740; loss: 2.25; acc: 0.42
Batch: 760; loss: 2.27; acc: 0.28
Batch: 780; loss: 2.26; acc: 0.3
Train Epoch over. train_loss: 2.28; train_accuracy: 0.26 

Batch: 0; loss: 2.26; acc: 0.28
Batch: 20; loss: 2.27; acc: 0.28
Batch: 40; loss: 2.25; acc: 0.44
Batch: 60; loss: 2.26; acc: 0.36
Batch: 80; loss: 2.26; acc: 0.31
Batch: 100; loss: 2.27; acc: 0.28
Batch: 120; loss: 2.26; acc: 0.38
Batch: 140; loss: 2.25; acc: 0.3
Val Epoch over. val_loss: 2.2617720570533897; val_accuracy: 0.2864251592356688 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.23
Batch: 20; loss: 2.26; acc: 0.22
Batch: 40; loss: 2.27; acc: 0.23
Batch: 60; loss: 2.28; acc: 0.17
Batch: 80; loss: 2.26; acc: 0.31
Batch: 100; loss: 2.26; acc: 0.3
Batch: 120; loss: 2.26; acc: 0.31
Batch: 140; loss: 2.27; acc: 0.23
Batch: 160; loss: 2.24; acc: 0.33
Batch: 180; loss: 2.24; acc: 0.31
Batch: 200; loss: 2.27; acc: 0.27
Batch: 220; loss: 2.23; acc: 0.31
Batch: 240; loss: 2.24; acc: 0.2
Batch: 260; loss: 2.26; acc: 0.16
Batch: 280; loss: 2.23; acc: 0.36
Batch: 300; loss: 2.25; acc: 0.28
Batch: 320; loss: 2.22; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.3
Batch: 360; loss: 2.2; acc: 0.28
Batch: 380; loss: 2.26; acc: 0.09
Batch: 400; loss: 2.21; acc: 0.25
Batch: 420; loss: 2.2; acc: 0.3
Batch: 440; loss: 2.2; acc: 0.3
Batch: 460; loss: 2.17; acc: 0.33
Batch: 480; loss: 2.19; acc: 0.19
Batch: 500; loss: 2.22; acc: 0.19
Batch: 520; loss: 2.21; acc: 0.22
Batch: 540; loss: 2.16; acc: 0.23
Batch: 560; loss: 2.17; acc: 0.3
Batch: 580; loss: 2.16; acc: 0.28
Batch: 600; loss: 2.14; acc: 0.25
Batch: 620; loss: 2.17; acc: 0.19
Batch: 640; loss: 2.1; acc: 0.33
Batch: 660; loss: 2.15; acc: 0.23
Batch: 680; loss: 2.11; acc: 0.36
Batch: 700; loss: 2.07; acc: 0.33
Batch: 720; loss: 2.05; acc: 0.34
Batch: 740; loss: 2.07; acc: 0.3
Batch: 760; loss: 2.03; acc: 0.33
Batch: 780; loss: 2.1; acc: 0.31
Train Epoch over. train_loss: 2.2; train_accuracy: 0.26 

Batch: 0; loss: 2.13; acc: 0.23
Batch: 20; loss: 2.1; acc: 0.27
Batch: 40; loss: 1.97; acc: 0.34
Batch: 60; loss: 2.01; acc: 0.3
Batch: 80; loss: 2.0; acc: 0.27
Batch: 100; loss: 2.07; acc: 0.27
Batch: 120; loss: 2.05; acc: 0.27
Batch: 140; loss: 2.0; acc: 0.33
Val Epoch over. val_loss: 2.063163115720081; val_accuracy: 0.288515127388535 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.06; acc: 0.42
Batch: 20; loss: 2.03; acc: 0.36
Batch: 40; loss: 2.11; acc: 0.28
Batch: 60; loss: 1.94; acc: 0.39
Batch: 80; loss: 2.13; acc: 0.2
Batch: 100; loss: 1.96; acc: 0.3
Batch: 120; loss: 1.92; acc: 0.36
Batch: 140; loss: 2.0; acc: 0.31
Batch: 160; loss: 2.0; acc: 0.38
Batch: 180; loss: 1.94; acc: 0.33
Batch: 200; loss: 1.92; acc: 0.3
Batch: 220; loss: 2.0; acc: 0.27
Batch: 240; loss: 1.94; acc: 0.25
Batch: 260; loss: 1.89; acc: 0.36
Batch: 280; loss: 1.81; acc: 0.36
Batch: 300; loss: 1.95; acc: 0.33
Batch: 320; loss: 1.93; acc: 0.38
Batch: 340; loss: 1.96; acc: 0.3
Batch: 360; loss: 1.91; acc: 0.41
Batch: 380; loss: 1.97; acc: 0.27
Batch: 400; loss: 1.8; acc: 0.47
Batch: 420; loss: 2.03; acc: 0.33
Batch: 440; loss: 1.99; acc: 0.25
Batch: 460; loss: 1.91; acc: 0.28
Batch: 480; loss: 1.82; acc: 0.44
Batch: 500; loss: 1.83; acc: 0.34
Batch: 520; loss: 1.76; acc: 0.41
Batch: 540; loss: 1.83; acc: 0.41
Batch: 560; loss: 1.76; acc: 0.42
Batch: 580; loss: 1.95; acc: 0.31
Batch: 600; loss: 1.93; acc: 0.3
Batch: 620; loss: 1.73; acc: 0.42
Batch: 640; loss: 1.9; acc: 0.3
Batch: 660; loss: 1.87; acc: 0.36
Batch: 680; loss: 1.94; acc: 0.27
Batch: 700; loss: 1.84; acc: 0.39
Batch: 720; loss: 1.87; acc: 0.39
Batch: 740; loss: 1.78; acc: 0.38
Batch: 760; loss: 1.93; acc: 0.33
Batch: 780; loss: 2.01; acc: 0.27
Train Epoch over. train_loss: 1.91; train_accuracy: 0.34 

Batch: 0; loss: 1.96; acc: 0.33
Batch: 20; loss: 2.0; acc: 0.22
Batch: 40; loss: 1.76; acc: 0.38
Batch: 60; loss: 1.68; acc: 0.5
Batch: 80; loss: 1.75; acc: 0.47
Batch: 100; loss: 2.02; acc: 0.23
Batch: 120; loss: 1.74; acc: 0.52
Batch: 140; loss: 1.8; acc: 0.38
Val Epoch over. val_loss: 1.8499529490804976; val_accuracy: 0.36634156050955413 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.8; acc: 0.31
Batch: 20; loss: 1.92; acc: 0.31
Batch: 40; loss: 1.75; acc: 0.42
Batch: 60; loss: 1.74; acc: 0.47
Batch: 80; loss: 1.8; acc: 0.36
Batch: 100; loss: 1.85; acc: 0.34
Batch: 120; loss: 1.81; acc: 0.41
Batch: 140; loss: 1.75; acc: 0.41
Batch: 160; loss: 1.86; acc: 0.36
Batch: 180; loss: 1.83; acc: 0.33
Batch: 200; loss: 1.75; acc: 0.41
Batch: 220; loss: 1.92; acc: 0.33
Batch: 240; loss: 1.77; acc: 0.42
Batch: 260; loss: 1.78; acc: 0.36
Batch: 280; loss: 1.86; acc: 0.33
Batch: 300; loss: 1.98; acc: 0.33
Batch: 320; loss: 1.8; acc: 0.36
Batch: 340; loss: 1.8; acc: 0.39
Batch: 360; loss: 1.84; acc: 0.33
Batch: 380; loss: 2.01; acc: 0.23
Batch: 400; loss: 1.77; acc: 0.36
Batch: 420; loss: 1.86; acc: 0.38
Batch: 440; loss: 1.77; acc: 0.39
Batch: 460; loss: 1.94; acc: 0.36
Batch: 480; loss: 1.99; acc: 0.31
Batch: 500; loss: 1.85; acc: 0.34
Batch: 520; loss: 1.8; acc: 0.39
Batch: 540; loss: 1.85; acc: 0.38
Batch: 560; loss: 1.81; acc: 0.39
Batch: 580; loss: 2.0; acc: 0.28
Batch: 600; loss: 1.77; acc: 0.41
Batch: 620; loss: 1.68; acc: 0.48
Batch: 640; loss: 1.67; acc: 0.42
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.86; acc: 0.38
Batch: 700; loss: 1.75; acc: 0.42
Batch: 720; loss: 2.18; acc: 0.27
Batch: 740; loss: 1.74; acc: 0.33
Batch: 760; loss: 1.71; acc: 0.47
Batch: 780; loss: 1.77; acc: 0.41
Train Epoch over. train_loss: 1.85; train_accuracy: 0.36 

Batch: 0; loss: 1.95; acc: 0.3
Batch: 20; loss: 1.9; acc: 0.31
Batch: 40; loss: 1.73; acc: 0.41
Batch: 60; loss: 1.68; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.42
Batch: 100; loss: 1.9; acc: 0.3
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.75; acc: 0.36
Val Epoch over. val_loss: 1.8191548525148136; val_accuracy: 0.37101910828025475 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.54; acc: 0.45
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.87; acc: 0.41
Batch: 60; loss: 1.8; acc: 0.36
Batch: 80; loss: 1.89; acc: 0.39
Batch: 100; loss: 1.75; acc: 0.39
Batch: 120; loss: 1.66; acc: 0.47
Batch: 140; loss: 1.96; acc: 0.31
Batch: 160; loss: 1.8; acc: 0.39
Batch: 180; loss: 1.82; acc: 0.44
Batch: 200; loss: 1.61; acc: 0.52
Batch: 220; loss: 1.94; acc: 0.28
Batch: 240; loss: 1.75; acc: 0.38
Batch: 260; loss: 1.86; acc: 0.41
Batch: 280; loss: 1.85; acc: 0.33
Batch: 300; loss: 1.89; acc: 0.31
Batch: 320; loss: 1.72; acc: 0.36
Batch: 340; loss: 1.67; acc: 0.45
Batch: 360; loss: 1.9; acc: 0.38
Batch: 380; loss: 1.86; acc: 0.36
Batch: 400; loss: 2.21; acc: 0.27
Batch: 420; loss: 1.94; acc: 0.27
Batch: 440; loss: 1.78; acc: 0.44
Batch: 460; loss: 1.87; acc: 0.39
Batch: 480; loss: 1.77; acc: 0.39
Batch: 500; loss: 1.93; acc: 0.39
Batch: 520; loss: 1.95; acc: 0.31
Batch: 540; loss: 1.8; acc: 0.45
Batch: 560; loss: 1.91; acc: 0.28
Batch: 580; loss: 1.74; acc: 0.39
Batch: 600; loss: 1.96; acc: 0.36
Batch: 620; loss: 1.95; acc: 0.38
Batch: 640; loss: 1.84; acc: 0.39
Batch: 660; loss: 1.98; acc: 0.28
Batch: 680; loss: 1.75; acc: 0.42
Batch: 700; loss: 1.94; acc: 0.33
Batch: 720; loss: 1.67; acc: 0.44
Batch: 740; loss: 1.62; acc: 0.55
Batch: 760; loss: 1.88; acc: 0.44
Batch: 780; loss: 1.71; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.33
Batch: 20; loss: 1.9; acc: 0.33
Batch: 40; loss: 1.72; acc: 0.41
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.41
Batch: 100; loss: 1.86; acc: 0.3
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.76; acc: 0.36
Val Epoch over. val_loss: 1.8167152025137738; val_accuracy: 0.36415207006369427 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.83; acc: 0.38
Batch: 20; loss: 1.83; acc: 0.38
Batch: 40; loss: 1.96; acc: 0.28
Batch: 60; loss: 1.74; acc: 0.41
Batch: 80; loss: 1.89; acc: 0.3
Batch: 100; loss: 1.66; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.42
Batch: 140; loss: 2.11; acc: 0.2
Batch: 160; loss: 1.92; acc: 0.31
Batch: 180; loss: 1.79; acc: 0.39
Batch: 200; loss: 1.86; acc: 0.31
Batch: 220; loss: 1.73; acc: 0.44
Batch: 240; loss: 1.98; acc: 0.3
Batch: 260; loss: 1.77; acc: 0.39
Batch: 280; loss: 1.81; acc: 0.36
Batch: 300; loss: 1.68; acc: 0.38
Batch: 320; loss: 1.67; acc: 0.45
Batch: 340; loss: 2.04; acc: 0.22
Batch: 360; loss: 1.76; acc: 0.41
Batch: 380; loss: 1.82; acc: 0.36
Batch: 400; loss: 1.95; acc: 0.28
Batch: 420; loss: 1.88; acc: 0.34
Batch: 440; loss: 2.06; acc: 0.31
Batch: 460; loss: 1.91; acc: 0.33
Batch: 480; loss: 1.8; acc: 0.41
Batch: 500; loss: 1.55; acc: 0.48
Batch: 520; loss: 1.83; acc: 0.33
Batch: 540; loss: 2.08; acc: 0.2
Batch: 560; loss: 1.8; acc: 0.36
Batch: 580; loss: 1.72; acc: 0.44
Batch: 600; loss: 1.93; acc: 0.41
Batch: 620; loss: 1.81; acc: 0.44
Batch: 640; loss: 1.88; acc: 0.34
Batch: 660; loss: 1.68; acc: 0.44
Batch: 680; loss: 1.76; acc: 0.33
Batch: 700; loss: 1.84; acc: 0.3
Batch: 720; loss: 1.77; acc: 0.36
Batch: 740; loss: 1.77; acc: 0.38
Batch: 760; loss: 1.92; acc: 0.25
Batch: 780; loss: 1.7; acc: 0.39
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.33
Batch: 20; loss: 1.86; acc: 0.33
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.69; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.86; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8106382842276507; val_accuracy: 0.3734076433121019 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.73; acc: 0.47
Batch: 20; loss: 1.85; acc: 0.27
Batch: 40; loss: 1.89; acc: 0.38
Batch: 60; loss: 1.88; acc: 0.38
Batch: 80; loss: 1.85; acc: 0.38
Batch: 100; loss: 1.71; acc: 0.39
Batch: 120; loss: 1.83; acc: 0.38
Batch: 140; loss: 1.76; acc: 0.41
Batch: 160; loss: 1.71; acc: 0.42
Batch: 180; loss: 1.89; acc: 0.36
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.83; acc: 0.34
Batch: 240; loss: 1.81; acc: 0.36
Batch: 260; loss: 1.99; acc: 0.27
Batch: 280; loss: 1.93; acc: 0.31
Batch: 300; loss: 1.88; acc: 0.33
Batch: 320; loss: 1.9; acc: 0.31
Batch: 340; loss: 1.69; acc: 0.48
Batch: 360; loss: 1.76; acc: 0.41
Batch: 380; loss: 1.74; acc: 0.44
Batch: 400; loss: 1.98; acc: 0.27
Batch: 420; loss: 1.75; acc: 0.45
Batch: 440; loss: 1.71; acc: 0.45
Batch: 460; loss: 1.87; acc: 0.36
Batch: 480; loss: 1.7; acc: 0.42
Batch: 500; loss: 1.89; acc: 0.3
Batch: 520; loss: 1.87; acc: 0.34
Batch: 540; loss: 1.88; acc: 0.34
Batch: 560; loss: 1.96; acc: 0.38
Batch: 580; loss: 1.88; acc: 0.31
Batch: 600; loss: 1.81; acc: 0.38
Batch: 620; loss: 1.78; acc: 0.38
Batch: 640; loss: 1.82; acc: 0.3
Batch: 660; loss: 1.72; acc: 0.39
Batch: 680; loss: 1.7; acc: 0.39
Batch: 700; loss: 1.98; acc: 0.27
Batch: 720; loss: 1.9; acc: 0.34
Batch: 740; loss: 1.78; acc: 0.36
Batch: 760; loss: 1.87; acc: 0.36
Batch: 780; loss: 1.79; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.94; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.42
Batch: 100; loss: 1.84; acc: 0.31
Batch: 120; loss: 1.76; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.807561792385806; val_accuracy: 0.37191480891719747 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.9; acc: 0.39
Batch: 20; loss: 1.8; acc: 0.3
Batch: 40; loss: 2.0; acc: 0.25
Batch: 60; loss: 1.81; acc: 0.47
Batch: 80; loss: 1.73; acc: 0.41
Batch: 100; loss: 1.78; acc: 0.38
Batch: 120; loss: 1.78; acc: 0.34
Batch: 140; loss: 1.89; acc: 0.33
Batch: 160; loss: 1.67; acc: 0.41
Batch: 180; loss: 1.77; acc: 0.38
Batch: 200; loss: 1.66; acc: 0.47
Batch: 220; loss: 1.84; acc: 0.31
Batch: 240; loss: 1.91; acc: 0.33
Batch: 260; loss: 1.99; acc: 0.22
Batch: 280; loss: 1.94; acc: 0.3
Batch: 300; loss: 1.77; acc: 0.39
Batch: 320; loss: 1.87; acc: 0.36
Batch: 340; loss: 1.98; acc: 0.31
Batch: 360; loss: 1.93; acc: 0.31
Batch: 380; loss: 1.86; acc: 0.39
Batch: 400; loss: 1.79; acc: 0.38
Batch: 420; loss: 1.88; acc: 0.33
Batch: 440; loss: 1.68; acc: 0.42
Batch: 460; loss: 1.78; acc: 0.39
Batch: 480; loss: 1.94; acc: 0.3
Batch: 500; loss: 1.79; acc: 0.34
Batch: 520; loss: 1.79; acc: 0.34
Batch: 540; loss: 2.01; acc: 0.28
Batch: 560; loss: 1.89; acc: 0.34
Batch: 580; loss: 1.94; acc: 0.33
Batch: 600; loss: 1.67; acc: 0.39
Batch: 620; loss: 1.78; acc: 0.34
Batch: 640; loss: 1.72; acc: 0.44
Batch: 660; loss: 1.83; acc: 0.38
Batch: 680; loss: 1.94; acc: 0.38
Batch: 700; loss: 2.13; acc: 0.2
Batch: 720; loss: 1.9; acc: 0.33
Batch: 740; loss: 1.95; acc: 0.28
Batch: 760; loss: 1.82; acc: 0.36
Batch: 780; loss: 1.74; acc: 0.39
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.33
Batch: 20; loss: 1.83; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.82; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.38
Batch: 140; loss: 1.74; acc: 0.36
Val Epoch over. val_loss: 1.8115243471352158; val_accuracy: 0.36206210191082805 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 1.74; acc: 0.44
Batch: 40; loss: 1.97; acc: 0.34
Batch: 60; loss: 1.91; acc: 0.36
Batch: 80; loss: 1.81; acc: 0.36
Batch: 100; loss: 1.91; acc: 0.28
Batch: 120; loss: 1.84; acc: 0.36
Batch: 140; loss: 1.64; acc: 0.41
Batch: 160; loss: 1.72; acc: 0.5
Batch: 180; loss: 1.84; acc: 0.34
Batch: 200; loss: 1.83; acc: 0.33
Batch: 220; loss: 1.77; acc: 0.36
Batch: 240; loss: 1.85; acc: 0.36
Batch: 260; loss: 1.78; acc: 0.45
Batch: 280; loss: 1.82; acc: 0.3
Batch: 300; loss: 1.85; acc: 0.34
Batch: 320; loss: 1.64; acc: 0.39
Batch: 340; loss: 1.96; acc: 0.36
Batch: 360; loss: 1.86; acc: 0.33
Batch: 380; loss: 1.87; acc: 0.31
Batch: 400; loss: 1.71; acc: 0.45
Batch: 420; loss: 1.82; acc: 0.38
Batch: 440; loss: 1.83; acc: 0.44
Batch: 460; loss: 1.73; acc: 0.42
Batch: 480; loss: 1.94; acc: 0.33
Batch: 500; loss: 1.87; acc: 0.36
Batch: 520; loss: 1.86; acc: 0.41
Batch: 540; loss: 1.79; acc: 0.41
Batch: 560; loss: 1.83; acc: 0.31
Batch: 580; loss: 1.86; acc: 0.33
Batch: 600; loss: 1.89; acc: 0.34
Batch: 620; loss: 1.84; acc: 0.33
Batch: 640; loss: 1.94; acc: 0.22
Batch: 660; loss: 1.86; acc: 0.31
Batch: 680; loss: 1.93; acc: 0.22
Batch: 700; loss: 1.72; acc: 0.45
Batch: 720; loss: 1.83; acc: 0.34
Batch: 740; loss: 1.78; acc: 0.42
Batch: 760; loss: 1.88; acc: 0.3
Batch: 780; loss: 2.04; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.44
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8083103302937404; val_accuracy: 0.3681329617834395 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.98; acc: 0.34
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.84; acc: 0.36
Batch: 60; loss: 1.88; acc: 0.38
Batch: 80; loss: 1.9; acc: 0.33
Batch: 100; loss: 1.66; acc: 0.39
Batch: 120; loss: 1.58; acc: 0.53
Batch: 140; loss: 1.83; acc: 0.33
Batch: 160; loss: 1.68; acc: 0.47
Batch: 180; loss: 1.73; acc: 0.41
Batch: 200; loss: 1.65; acc: 0.44
Batch: 220; loss: 1.93; acc: 0.3
Batch: 240; loss: 1.76; acc: 0.36
Batch: 260; loss: 1.7; acc: 0.41
Batch: 280; loss: 1.89; acc: 0.31
Batch: 300; loss: 1.91; acc: 0.31
Batch: 320; loss: 1.78; acc: 0.36
Batch: 340; loss: 1.71; acc: 0.38
Batch: 360; loss: 1.71; acc: 0.44
Batch: 380; loss: 1.71; acc: 0.41
Batch: 400; loss: 1.8; acc: 0.45
Batch: 420; loss: 1.76; acc: 0.41
Batch: 440; loss: 1.68; acc: 0.41
Batch: 460; loss: 1.87; acc: 0.38
Batch: 480; loss: 1.9; acc: 0.33
Batch: 500; loss: 1.9; acc: 0.25
Batch: 520; loss: 1.73; acc: 0.38
Batch: 540; loss: 2.0; acc: 0.27
Batch: 560; loss: 1.86; acc: 0.34
Batch: 580; loss: 1.71; acc: 0.41
Batch: 600; loss: 1.68; acc: 0.44
Batch: 620; loss: 1.89; acc: 0.3
Batch: 640; loss: 2.02; acc: 0.27
Batch: 660; loss: 1.91; acc: 0.33
Batch: 680; loss: 1.75; acc: 0.36
Batch: 700; loss: 1.87; acc: 0.36
Batch: 720; loss: 1.98; acc: 0.28
Batch: 740; loss: 2.09; acc: 0.2
Batch: 760; loss: 1.91; acc: 0.28
Batch: 780; loss: 2.0; acc: 0.3
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.36
Batch: 20; loss: 1.87; acc: 0.36
Batch: 40; loss: 1.72; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.67; acc: 0.44
Batch: 100; loss: 1.85; acc: 0.3
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.809661593406823; val_accuracy: 0.3674363057324841 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.8; acc: 0.33
Batch: 20; loss: 1.83; acc: 0.3
Batch: 40; loss: 1.96; acc: 0.23
Batch: 60; loss: 1.95; acc: 0.27
Batch: 80; loss: 1.79; acc: 0.33
Batch: 100; loss: 1.95; acc: 0.23
Batch: 120; loss: 1.89; acc: 0.31
Batch: 140; loss: 1.83; acc: 0.34
Batch: 160; loss: 1.81; acc: 0.39
Batch: 180; loss: 1.75; acc: 0.44
Batch: 200; loss: 1.9; acc: 0.38
Batch: 220; loss: 1.86; acc: 0.33
Batch: 240; loss: 1.84; acc: 0.39
Batch: 260; loss: 1.82; acc: 0.34
Batch: 280; loss: 1.83; acc: 0.33
Batch: 300; loss: 1.74; acc: 0.39
Batch: 320; loss: 1.98; acc: 0.28
Batch: 340; loss: 1.83; acc: 0.36
Batch: 360; loss: 1.83; acc: 0.36
Batch: 380; loss: 1.77; acc: 0.44
Batch: 400; loss: 1.91; acc: 0.36
Batch: 420; loss: 1.71; acc: 0.44
Batch: 440; loss: 1.9; acc: 0.36
Batch: 460; loss: 1.85; acc: 0.38
Batch: 480; loss: 1.82; acc: 0.41
Batch: 500; loss: 1.85; acc: 0.33
Batch: 520; loss: 1.8; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.39
Batch: 560; loss: 1.89; acc: 0.41
Batch: 580; loss: 1.88; acc: 0.34
Batch: 600; loss: 1.87; acc: 0.36
Batch: 620; loss: 1.85; acc: 0.39
Batch: 640; loss: 1.74; acc: 0.41
Batch: 660; loss: 1.78; acc: 0.39
Batch: 680; loss: 1.92; acc: 0.33
Batch: 700; loss: 1.98; acc: 0.31
Batch: 720; loss: 1.92; acc: 0.36
Batch: 740; loss: 1.6; acc: 0.45
Batch: 760; loss: 1.89; acc: 0.28
Batch: 780; loss: 1.73; acc: 0.38
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8071105935770995; val_accuracy: 0.3697253184713376 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.82; acc: 0.36
Batch: 20; loss: 1.86; acc: 0.28
Batch: 40; loss: 1.73; acc: 0.34
Batch: 60; loss: 1.93; acc: 0.34
Batch: 80; loss: 1.79; acc: 0.42
Batch: 100; loss: 1.72; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.25
Batch: 140; loss: 1.82; acc: 0.33
Batch: 160; loss: 1.74; acc: 0.39
Batch: 180; loss: 1.79; acc: 0.34
Batch: 200; loss: 1.76; acc: 0.42
Batch: 220; loss: 1.67; acc: 0.38
Batch: 240; loss: 1.93; acc: 0.28
Batch: 260; loss: 1.72; acc: 0.44
Batch: 280; loss: 1.78; acc: 0.39
Batch: 300; loss: 1.8; acc: 0.36
Batch: 320; loss: 1.92; acc: 0.28
Batch: 340; loss: 1.8; acc: 0.39
Batch: 360; loss: 1.79; acc: 0.31
Batch: 380; loss: 1.98; acc: 0.27
Batch: 400; loss: 1.91; acc: 0.33
Batch: 420; loss: 1.83; acc: 0.38
Batch: 440; loss: 2.0; acc: 0.17
Batch: 460; loss: 1.88; acc: 0.36
Batch: 480; loss: 1.79; acc: 0.36
Batch: 500; loss: 1.8; acc: 0.25
Batch: 520; loss: 1.83; acc: 0.42
Batch: 540; loss: 1.9; acc: 0.36
Batch: 560; loss: 1.86; acc: 0.31
Batch: 580; loss: 1.97; acc: 0.27
Batch: 600; loss: 1.89; acc: 0.33
Batch: 620; loss: 1.68; acc: 0.41
Batch: 640; loss: 2.03; acc: 0.25
Batch: 660; loss: 1.83; acc: 0.39
Batch: 680; loss: 1.82; acc: 0.36
Batch: 700; loss: 1.81; acc: 0.38
Batch: 720; loss: 1.78; acc: 0.36
Batch: 740; loss: 1.87; acc: 0.27
Batch: 760; loss: 1.73; acc: 0.48
Batch: 780; loss: 1.89; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.808796468813708; val_accuracy: 0.3682324840764331 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.78; acc: 0.39
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.78; acc: 0.33
Batch: 60; loss: 1.74; acc: 0.38
Batch: 80; loss: 1.92; acc: 0.36
Batch: 100; loss: 1.86; acc: 0.38
Batch: 120; loss: 1.84; acc: 0.34
Batch: 140; loss: 1.83; acc: 0.38
Batch: 160; loss: 1.81; acc: 0.34
Batch: 180; loss: 1.86; acc: 0.39
Batch: 200; loss: 1.99; acc: 0.22
Batch: 220; loss: 1.67; acc: 0.47
Batch: 240; loss: 1.88; acc: 0.39
Batch: 260; loss: 1.69; acc: 0.47
Batch: 280; loss: 1.78; acc: 0.42
Batch: 300; loss: 1.94; acc: 0.36
Batch: 320; loss: 1.77; acc: 0.41
Batch: 340; loss: 1.93; acc: 0.34
Batch: 360; loss: 1.75; acc: 0.42
Batch: 380; loss: 1.81; acc: 0.33
Batch: 400; loss: 1.97; acc: 0.31
Batch: 420; loss: 1.73; acc: 0.45
Batch: 440; loss: 1.76; acc: 0.33
Batch: 460; loss: 1.8; acc: 0.33
Batch: 480; loss: 1.84; acc: 0.39
Batch: 500; loss: 2.04; acc: 0.27
Batch: 520; loss: 1.68; acc: 0.47
Batch: 540; loss: 1.58; acc: 0.52
Batch: 560; loss: 1.94; acc: 0.34
Batch: 580; loss: 1.67; acc: 0.47
Batch: 600; loss: 1.94; acc: 0.25
Batch: 620; loss: 1.73; acc: 0.42
Batch: 640; loss: 1.86; acc: 0.34
Batch: 660; loss: 1.68; acc: 0.41
Batch: 680; loss: 1.67; acc: 0.48
Batch: 700; loss: 1.72; acc: 0.42
Batch: 720; loss: 1.7; acc: 0.44
Batch: 740; loss: 1.96; acc: 0.33
Batch: 760; loss: 1.89; acc: 0.36
Batch: 780; loss: 1.68; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.3
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.41
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.74; acc: 0.36
Val Epoch over. val_loss: 1.8090854101120286; val_accuracy: 0.36733678343949044 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.83; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.36
Batch: 40; loss: 1.84; acc: 0.34
Batch: 60; loss: 1.81; acc: 0.42
Batch: 80; loss: 1.76; acc: 0.44
Batch: 100; loss: 1.94; acc: 0.3
Batch: 120; loss: 1.81; acc: 0.3
Batch: 140; loss: 1.87; acc: 0.31
Batch: 160; loss: 1.98; acc: 0.3
Batch: 180; loss: 1.87; acc: 0.33
Batch: 200; loss: 1.89; acc: 0.31
Batch: 220; loss: 1.85; acc: 0.31
Batch: 240; loss: 1.97; acc: 0.38
Batch: 260; loss: 1.87; acc: 0.27
Batch: 280; loss: 1.85; acc: 0.41
Batch: 300; loss: 1.72; acc: 0.38
Batch: 320; loss: 1.79; acc: 0.33
Batch: 340; loss: 1.81; acc: 0.44
Batch: 360; loss: 1.63; acc: 0.5
Batch: 380; loss: 1.72; acc: 0.41
Batch: 400; loss: 1.87; acc: 0.36
Batch: 420; loss: 1.9; acc: 0.33
Batch: 440; loss: 2.08; acc: 0.28
Batch: 460; loss: 1.9; acc: 0.3
Batch: 480; loss: 1.98; acc: 0.36
Batch: 500; loss: 1.88; acc: 0.3
Batch: 520; loss: 1.79; acc: 0.39
Batch: 540; loss: 1.78; acc: 0.45
Batch: 560; loss: 1.91; acc: 0.34
Batch: 580; loss: 1.93; acc: 0.33
Batch: 600; loss: 1.84; acc: 0.34
Batch: 620; loss: 1.68; acc: 0.42
Batch: 640; loss: 1.81; acc: 0.42
Batch: 660; loss: 1.68; acc: 0.36
Batch: 680; loss: 1.72; acc: 0.47
Batch: 700; loss: 2.0; acc: 0.3
Batch: 720; loss: 1.85; acc: 0.33
Batch: 740; loss: 1.83; acc: 0.41
Batch: 760; loss: 1.88; acc: 0.33
Batch: 780; loss: 1.85; acc: 0.42
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.3
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.47
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8089314759916562; val_accuracy: 0.36833200636942676 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.85; acc: 0.33
Batch: 20; loss: 1.85; acc: 0.41
Batch: 40; loss: 1.76; acc: 0.38
Batch: 60; loss: 1.75; acc: 0.38
Batch: 80; loss: 1.75; acc: 0.38
Batch: 100; loss: 1.76; acc: 0.39
Batch: 120; loss: 1.9; acc: 0.28
Batch: 140; loss: 1.92; acc: 0.25
Batch: 160; loss: 1.63; acc: 0.5
Batch: 180; loss: 1.68; acc: 0.47
Batch: 200; loss: 1.9; acc: 0.33
Batch: 220; loss: 1.68; acc: 0.44
Batch: 240; loss: 1.82; acc: 0.36
Batch: 260; loss: 1.61; acc: 0.5
Batch: 280; loss: 1.79; acc: 0.31
Batch: 300; loss: 1.76; acc: 0.42
Batch: 320; loss: 1.82; acc: 0.31
Batch: 340; loss: 1.78; acc: 0.38
Batch: 360; loss: 2.03; acc: 0.36
Batch: 380; loss: 1.94; acc: 0.34
Batch: 400; loss: 2.08; acc: 0.27
Batch: 420; loss: 1.85; acc: 0.33
Batch: 440; loss: 1.76; acc: 0.44
Batch: 460; loss: 1.78; acc: 0.34
Batch: 480; loss: 1.81; acc: 0.34
Batch: 500; loss: 1.75; acc: 0.36
Batch: 520; loss: 1.93; acc: 0.27
Batch: 540; loss: 1.91; acc: 0.36
Batch: 560; loss: 2.0; acc: 0.27
Batch: 580; loss: 1.82; acc: 0.33
Batch: 600; loss: 2.0; acc: 0.25
Batch: 620; loss: 1.88; acc: 0.34
Batch: 640; loss: 1.98; acc: 0.23
Batch: 660; loss: 1.79; acc: 0.48
Batch: 680; loss: 1.91; acc: 0.3
Batch: 700; loss: 1.63; acc: 0.48
Batch: 720; loss: 1.85; acc: 0.38
Batch: 740; loss: 1.71; acc: 0.39
Batch: 760; loss: 1.87; acc: 0.38
Batch: 780; loss: 1.74; acc: 0.48
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.806819017525691; val_accuracy: 0.3692277070063694 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 1.8; acc: 0.38
Batch: 40; loss: 1.82; acc: 0.41
Batch: 60; loss: 1.77; acc: 0.39
Batch: 80; loss: 1.9; acc: 0.3
Batch: 100; loss: 1.8; acc: 0.42
Batch: 120; loss: 1.97; acc: 0.33
Batch: 140; loss: 1.97; acc: 0.31
Batch: 160; loss: 1.77; acc: 0.42
Batch: 180; loss: 1.66; acc: 0.45
Batch: 200; loss: 1.82; acc: 0.31
Batch: 220; loss: 1.82; acc: 0.38
Batch: 240; loss: 1.86; acc: 0.38
Batch: 260; loss: 1.94; acc: 0.28
Batch: 280; loss: 1.82; acc: 0.36
Batch: 300; loss: 1.77; acc: 0.36
Batch: 320; loss: 1.86; acc: 0.34
Batch: 340; loss: 1.74; acc: 0.38
Batch: 360; loss: 1.9; acc: 0.34
Batch: 380; loss: 1.81; acc: 0.34
Batch: 400; loss: 1.8; acc: 0.34
Batch: 420; loss: 1.97; acc: 0.31
Batch: 440; loss: 1.8; acc: 0.38
Batch: 460; loss: 1.84; acc: 0.31
Batch: 480; loss: 1.92; acc: 0.33
Batch: 500; loss: 1.82; acc: 0.33
Batch: 520; loss: 1.77; acc: 0.42
Batch: 540; loss: 1.71; acc: 0.44
Batch: 560; loss: 1.73; acc: 0.33
Batch: 580; loss: 1.71; acc: 0.44
Batch: 600; loss: 1.7; acc: 0.38
Batch: 620; loss: 1.8; acc: 0.31
Batch: 640; loss: 1.74; acc: 0.36
Batch: 660; loss: 1.73; acc: 0.45
Batch: 680; loss: 1.81; acc: 0.41
Batch: 700; loss: 2.07; acc: 0.25
Batch: 720; loss: 1.86; acc: 0.42
Batch: 740; loss: 2.03; acc: 0.17
Batch: 760; loss: 1.92; acc: 0.2
Batch: 780; loss: 1.77; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.3
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8084292001784987; val_accuracy: 0.36863057324840764 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.91; acc: 0.3
Batch: 20; loss: 1.77; acc: 0.39
Batch: 40; loss: 1.71; acc: 0.42
Batch: 60; loss: 1.71; acc: 0.39
Batch: 80; loss: 1.77; acc: 0.39
Batch: 100; loss: 1.81; acc: 0.41
Batch: 120; loss: 1.76; acc: 0.47
Batch: 140; loss: 1.82; acc: 0.28
Batch: 160; loss: 1.84; acc: 0.39
Batch: 180; loss: 2.04; acc: 0.25
Batch: 200; loss: 2.19; acc: 0.25
Batch: 220; loss: 1.77; acc: 0.34
Batch: 240; loss: 1.98; acc: 0.2
Batch: 260; loss: 1.77; acc: 0.38
Batch: 280; loss: 1.75; acc: 0.38
Batch: 300; loss: 1.79; acc: 0.42
Batch: 320; loss: 1.86; acc: 0.31
Batch: 340; loss: 1.83; acc: 0.39
Batch: 360; loss: 1.83; acc: 0.31
Batch: 380; loss: 1.84; acc: 0.33
Batch: 400; loss: 1.88; acc: 0.36
Batch: 420; loss: 1.72; acc: 0.41
Batch: 440; loss: 1.85; acc: 0.36
Batch: 460; loss: 1.75; acc: 0.38
Batch: 480; loss: 1.82; acc: 0.41
Batch: 500; loss: 1.71; acc: 0.47
Batch: 520; loss: 1.89; acc: 0.31
Batch: 540; loss: 2.02; acc: 0.23
Batch: 560; loss: 1.77; acc: 0.45
Batch: 580; loss: 1.8; acc: 0.38
Batch: 600; loss: 1.84; acc: 0.41
Batch: 620; loss: 1.89; acc: 0.34
Batch: 640; loss: 2.06; acc: 0.3
Batch: 660; loss: 1.81; acc: 0.38
Batch: 680; loss: 1.79; acc: 0.38
Batch: 700; loss: 1.86; acc: 0.34
Batch: 720; loss: 1.8; acc: 0.36
Batch: 740; loss: 1.81; acc: 0.38
Batch: 760; loss: 1.94; acc: 0.33
Batch: 780; loss: 1.76; acc: 0.39
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.47
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.808746749428427; val_accuracy: 0.367734872611465 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.71; acc: 0.45
Batch: 20; loss: 1.61; acc: 0.52
Batch: 40; loss: 1.79; acc: 0.36
Batch: 60; loss: 1.78; acc: 0.31
Batch: 80; loss: 1.88; acc: 0.33
Batch: 100; loss: 1.75; acc: 0.42
Batch: 120; loss: 1.9; acc: 0.31
Batch: 140; loss: 1.85; acc: 0.27
Batch: 160; loss: 1.75; acc: 0.34
Batch: 180; loss: 1.69; acc: 0.44
Batch: 200; loss: 1.89; acc: 0.33
Batch: 220; loss: 1.91; acc: 0.34
Batch: 240; loss: 1.59; acc: 0.38
Batch: 260; loss: 1.87; acc: 0.36
Batch: 280; loss: 1.99; acc: 0.34
Batch: 300; loss: 1.79; acc: 0.34
Batch: 320; loss: 1.84; acc: 0.31
Batch: 340; loss: 1.82; acc: 0.36
Batch: 360; loss: 1.74; acc: 0.36
Batch: 380; loss: 1.75; acc: 0.44
Batch: 400; loss: 1.83; acc: 0.34
Batch: 420; loss: 1.9; acc: 0.3
Batch: 440; loss: 1.93; acc: 0.25
Batch: 460; loss: 1.77; acc: 0.39
Batch: 480; loss: 1.91; acc: 0.31
Batch: 500; loss: 2.03; acc: 0.27
Batch: 520; loss: 1.99; acc: 0.28
Batch: 540; loss: 1.74; acc: 0.38
Batch: 560; loss: 1.74; acc: 0.34
Batch: 580; loss: 1.95; acc: 0.31
Batch: 600; loss: 1.92; acc: 0.36
Batch: 620; loss: 1.91; acc: 0.25
Batch: 640; loss: 1.73; acc: 0.39
Batch: 660; loss: 1.9; acc: 0.38
Batch: 680; loss: 1.82; acc: 0.44
Batch: 700; loss: 1.75; acc: 0.39
Batch: 720; loss: 1.81; acc: 0.33
Batch: 740; loss: 1.78; acc: 0.44
Batch: 760; loss: 1.81; acc: 0.39
Batch: 780; loss: 1.82; acc: 0.45
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.3
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.47
Batch: 100; loss: 1.83; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.807833222826575; val_accuracy: 0.3684315286624204 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.75; acc: 0.38
Batch: 20; loss: 1.77; acc: 0.44
Batch: 40; loss: 1.87; acc: 0.34
Batch: 60; loss: 1.86; acc: 0.34
Batch: 80; loss: 1.8; acc: 0.33
Batch: 100; loss: 1.85; acc: 0.3
Batch: 120; loss: 1.91; acc: 0.28
Batch: 140; loss: 1.74; acc: 0.39
Batch: 160; loss: 1.84; acc: 0.38
Batch: 180; loss: 1.82; acc: 0.41
Batch: 200; loss: 1.83; acc: 0.38
Batch: 220; loss: 1.87; acc: 0.38
Batch: 240; loss: 2.04; acc: 0.3
Batch: 260; loss: 1.74; acc: 0.44
Batch: 280; loss: 1.89; acc: 0.3
Batch: 300; loss: 1.63; acc: 0.44
Batch: 320; loss: 1.81; acc: 0.31
Batch: 340; loss: 1.76; acc: 0.41
Batch: 360; loss: 1.98; acc: 0.31
Batch: 380; loss: 1.85; acc: 0.39
Batch: 400; loss: 1.84; acc: 0.33
Batch: 420; loss: 1.85; acc: 0.38
Batch: 440; loss: 1.86; acc: 0.34
Batch: 460; loss: 1.92; acc: 0.41
Batch: 480; loss: 1.78; acc: 0.42
Batch: 500; loss: 2.0; acc: 0.34
Batch: 520; loss: 1.82; acc: 0.33
Batch: 540; loss: 1.76; acc: 0.38
Batch: 560; loss: 1.94; acc: 0.3
Batch: 580; loss: 1.99; acc: 0.36
Batch: 600; loss: 1.86; acc: 0.3
Batch: 620; loss: 1.86; acc: 0.27
Batch: 640; loss: 1.69; acc: 0.38
Batch: 660; loss: 1.87; acc: 0.39
Batch: 680; loss: 1.86; acc: 0.34
Batch: 700; loss: 1.84; acc: 0.34
Batch: 720; loss: 1.87; acc: 0.36
Batch: 740; loss: 1.85; acc: 0.31
Batch: 760; loss: 1.81; acc: 0.31
Batch: 780; loss: 1.86; acc: 0.44
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.34
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8075200706530528; val_accuracy: 0.3698248407643312 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.81; acc: 0.42
Batch: 20; loss: 1.68; acc: 0.44
Batch: 40; loss: 1.83; acc: 0.33
Batch: 60; loss: 1.9; acc: 0.34
Batch: 80; loss: 1.99; acc: 0.33
Batch: 100; loss: 1.79; acc: 0.3
Batch: 120; loss: 1.82; acc: 0.31
Batch: 140; loss: 1.88; acc: 0.38
Batch: 160; loss: 1.9; acc: 0.3
Batch: 180; loss: 1.75; acc: 0.45
Batch: 200; loss: 1.68; acc: 0.41
Batch: 220; loss: 1.71; acc: 0.41
Batch: 240; loss: 1.76; acc: 0.39
Batch: 260; loss: 1.76; acc: 0.36
Batch: 280; loss: 1.9; acc: 0.28
Batch: 300; loss: 1.88; acc: 0.36
Batch: 320; loss: 1.78; acc: 0.39
Batch: 340; loss: 1.96; acc: 0.34
Batch: 360; loss: 1.81; acc: 0.45
Batch: 380; loss: 1.6; acc: 0.44
Batch: 400; loss: 1.77; acc: 0.39
Batch: 420; loss: 1.75; acc: 0.41
Batch: 440; loss: 1.77; acc: 0.38
Batch: 460; loss: 1.87; acc: 0.38
Batch: 480; loss: 1.95; acc: 0.34
Batch: 500; loss: 1.82; acc: 0.36
Batch: 520; loss: 2.03; acc: 0.31
Batch: 540; loss: 1.76; acc: 0.47
Batch: 560; loss: 1.66; acc: 0.44
Batch: 580; loss: 1.71; acc: 0.36
Batch: 600; loss: 1.73; acc: 0.38
Batch: 620; loss: 1.76; acc: 0.38
Batch: 640; loss: 1.74; acc: 0.41
Batch: 660; loss: 1.82; acc: 0.39
Batch: 680; loss: 1.71; acc: 0.39
Batch: 700; loss: 1.99; acc: 0.33
Batch: 720; loss: 1.89; acc: 0.31
Batch: 740; loss: 1.98; acc: 0.28
Batch: 760; loss: 1.84; acc: 0.39
Batch: 780; loss: 1.75; acc: 0.36
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.47
Batch: 100; loss: 1.83; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8074591372423112; val_accuracy: 0.3690286624203822 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.08; acc: 0.31
Batch: 20; loss: 1.99; acc: 0.33
Batch: 40; loss: 1.97; acc: 0.31
Batch: 60; loss: 1.84; acc: 0.38
Batch: 80; loss: 1.86; acc: 0.31
Batch: 100; loss: 1.77; acc: 0.33
Batch: 120; loss: 1.9; acc: 0.36
Batch: 140; loss: 1.79; acc: 0.36
Batch: 160; loss: 1.83; acc: 0.38
Batch: 180; loss: 1.88; acc: 0.38
Batch: 200; loss: 1.95; acc: 0.28
Batch: 220; loss: 2.0; acc: 0.3
Batch: 240; loss: 1.82; acc: 0.45
Batch: 260; loss: 1.95; acc: 0.38
Batch: 280; loss: 1.78; acc: 0.39
Batch: 300; loss: 1.76; acc: 0.39
Batch: 320; loss: 1.95; acc: 0.27
Batch: 340; loss: 1.91; acc: 0.3
Batch: 360; loss: 1.68; acc: 0.47
Batch: 380; loss: 1.8; acc: 0.38
Batch: 400; loss: 1.82; acc: 0.34
Batch: 420; loss: 1.9; acc: 0.39
Batch: 440; loss: 2.0; acc: 0.23
Batch: 460; loss: 1.83; acc: 0.33
Batch: 480; loss: 1.76; acc: 0.34
Batch: 500; loss: 1.82; acc: 0.33
Batch: 520; loss: 1.88; acc: 0.36
Batch: 540; loss: 1.8; acc: 0.36
Batch: 560; loss: 1.74; acc: 0.36
Batch: 580; loss: 1.93; acc: 0.27
Batch: 600; loss: 1.7; acc: 0.47
Batch: 620; loss: 1.89; acc: 0.34
Batch: 640; loss: 1.85; acc: 0.33
Batch: 660; loss: 1.97; acc: 0.3
Batch: 680; loss: 1.91; acc: 0.25
Batch: 700; loss: 2.0; acc: 0.3
Batch: 720; loss: 1.9; acc: 0.39
Batch: 740; loss: 1.69; acc: 0.42
Batch: 760; loss: 1.85; acc: 0.33
Batch: 780; loss: 1.89; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807776089686497; val_accuracy: 0.36932722929936307 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.86; acc: 0.39
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.84; acc: 0.44
Batch: 60; loss: 1.88; acc: 0.31
Batch: 80; loss: 1.86; acc: 0.38
Batch: 100; loss: 1.94; acc: 0.28
Batch: 120; loss: 2.05; acc: 0.28
Batch: 140; loss: 1.83; acc: 0.31
Batch: 160; loss: 1.7; acc: 0.48
Batch: 180; loss: 1.76; acc: 0.34
Batch: 200; loss: 1.87; acc: 0.27
Batch: 220; loss: 1.82; acc: 0.34
Batch: 240; loss: 1.94; acc: 0.31
Batch: 260; loss: 1.79; acc: 0.38
Batch: 280; loss: 1.82; acc: 0.33
Batch: 300; loss: 1.81; acc: 0.33
Batch: 320; loss: 1.77; acc: 0.36
Batch: 340; loss: 1.93; acc: 0.3
Batch: 360; loss: 1.88; acc: 0.34
Batch: 380; loss: 1.82; acc: 0.33
Batch: 400; loss: 1.87; acc: 0.31
Batch: 420; loss: 1.78; acc: 0.42
Batch: 440; loss: 2.0; acc: 0.31
Batch: 460; loss: 1.88; acc: 0.27
Batch: 480; loss: 1.89; acc: 0.36
Batch: 500; loss: 1.79; acc: 0.36
Batch: 520; loss: 1.79; acc: 0.38
Batch: 540; loss: 1.71; acc: 0.44
Batch: 560; loss: 1.83; acc: 0.31
Batch: 580; loss: 1.73; acc: 0.42
Batch: 600; loss: 1.79; acc: 0.42
Batch: 620; loss: 1.95; acc: 0.28
Batch: 640; loss: 1.88; acc: 0.41
Batch: 660; loss: 1.77; acc: 0.39
Batch: 680; loss: 1.82; acc: 0.36
Batch: 700; loss: 1.84; acc: 0.34
Batch: 720; loss: 1.88; acc: 0.33
Batch: 740; loss: 1.73; acc: 0.44
Batch: 760; loss: 1.64; acc: 0.5
Batch: 780; loss: 1.92; acc: 0.34
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.34
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8077229390478438; val_accuracy: 0.3701234076433121 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.92; acc: 0.22
Batch: 20; loss: 1.98; acc: 0.31
Batch: 40; loss: 1.92; acc: 0.34
Batch: 60; loss: 1.72; acc: 0.38
Batch: 80; loss: 1.69; acc: 0.45
Batch: 100; loss: 1.85; acc: 0.34
Batch: 120; loss: 1.81; acc: 0.44
Batch: 140; loss: 1.84; acc: 0.31
Batch: 160; loss: 1.85; acc: 0.3
Batch: 180; loss: 1.96; acc: 0.3
Batch: 200; loss: 1.97; acc: 0.27
Batch: 220; loss: 1.72; acc: 0.44
Batch: 240; loss: 1.87; acc: 0.27
Batch: 260; loss: 1.81; acc: 0.3
Batch: 280; loss: 1.86; acc: 0.41
Batch: 300; loss: 1.96; acc: 0.36
Batch: 320; loss: 1.71; acc: 0.41
Batch: 340; loss: 1.69; acc: 0.42
Batch: 360; loss: 1.92; acc: 0.31
Batch: 380; loss: 1.74; acc: 0.39
Batch: 400; loss: 1.79; acc: 0.41
Batch: 420; loss: 2.01; acc: 0.3
Batch: 440; loss: 1.99; acc: 0.25
Batch: 460; loss: 1.87; acc: 0.42
Batch: 480; loss: 1.97; acc: 0.28
Batch: 500; loss: 1.61; acc: 0.44
Batch: 520; loss: 1.81; acc: 0.36
Batch: 540; loss: 2.0; acc: 0.34
Batch: 560; loss: 1.83; acc: 0.34
Batch: 580; loss: 1.83; acc: 0.36
Batch: 600; loss: 1.73; acc: 0.34
Batch: 620; loss: 1.77; acc: 0.41
Batch: 640; loss: 1.81; acc: 0.45
Batch: 660; loss: 1.74; acc: 0.38
Batch: 680; loss: 1.92; acc: 0.31
Batch: 700; loss: 1.76; acc: 0.42
Batch: 720; loss: 1.81; acc: 0.3
Batch: 740; loss: 1.84; acc: 0.31
Batch: 760; loss: 1.89; acc: 0.33
Batch: 780; loss: 2.05; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.44
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8075116607034283; val_accuracy: 0.3708200636942675 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.78; acc: 0.42
Batch: 20; loss: 1.93; acc: 0.27
Batch: 40; loss: 1.78; acc: 0.39
Batch: 60; loss: 1.81; acc: 0.36
Batch: 80; loss: 1.69; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.3
Batch: 120; loss: 2.01; acc: 0.3
Batch: 140; loss: 1.78; acc: 0.39
Batch: 160; loss: 1.91; acc: 0.38
Batch: 180; loss: 1.77; acc: 0.41
Batch: 200; loss: 1.83; acc: 0.42
Batch: 220; loss: 1.77; acc: 0.42
Batch: 240; loss: 1.9; acc: 0.31
Batch: 260; loss: 1.78; acc: 0.33
Batch: 280; loss: 1.94; acc: 0.34
Batch: 300; loss: 1.95; acc: 0.28
Batch: 320; loss: 1.91; acc: 0.31
Batch: 340; loss: 1.78; acc: 0.39
Batch: 360; loss: 1.95; acc: 0.34
Batch: 380; loss: 1.56; acc: 0.48
Batch: 400; loss: 1.75; acc: 0.41
Batch: 420; loss: 1.83; acc: 0.36
Batch: 440; loss: 1.96; acc: 0.33
Batch: 460; loss: 1.77; acc: 0.33
Batch: 480; loss: 1.87; acc: 0.34
Batch: 500; loss: 1.78; acc: 0.38
Batch: 520; loss: 2.0; acc: 0.27
Batch: 540; loss: 1.88; acc: 0.45
Batch: 560; loss: 2.01; acc: 0.25
Batch: 580; loss: 1.85; acc: 0.39
Batch: 600; loss: 2.0; acc: 0.25
Batch: 620; loss: 1.82; acc: 0.34
Batch: 640; loss: 1.9; acc: 0.27
Batch: 660; loss: 1.95; acc: 0.36
Batch: 680; loss: 1.86; acc: 0.36
Batch: 700; loss: 1.64; acc: 0.45
Batch: 720; loss: 1.72; acc: 0.48
Batch: 740; loss: 1.89; acc: 0.27
Batch: 760; loss: 1.74; acc: 0.41
Batch: 780; loss: 1.83; acc: 0.38
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.92; acc: 0.3
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.69; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.44
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.39
Batch: 140; loss: 1.74; acc: 0.36
Val Epoch over. val_loss: 1.8076095725320707; val_accuracy: 0.3664410828025478 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.0; acc: 0.3
Batch: 20; loss: 1.73; acc: 0.39
Batch: 40; loss: 1.81; acc: 0.39
Batch: 60; loss: 1.77; acc: 0.41
Batch: 80; loss: 2.0; acc: 0.3
Batch: 100; loss: 1.85; acc: 0.28
Batch: 120; loss: 1.86; acc: 0.23
Batch: 140; loss: 1.8; acc: 0.38
Batch: 160; loss: 1.72; acc: 0.47
Batch: 180; loss: 1.82; acc: 0.38
Batch: 200; loss: 1.93; acc: 0.23
Batch: 220; loss: 1.75; acc: 0.42
Batch: 240; loss: 1.76; acc: 0.41
Batch: 260; loss: 1.77; acc: 0.34
Batch: 280; loss: 1.86; acc: 0.36
Batch: 300; loss: 1.9; acc: 0.25
Batch: 320; loss: 1.78; acc: 0.33
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.99; acc: 0.25
Batch: 380; loss: 1.91; acc: 0.31
Batch: 400; loss: 1.72; acc: 0.42
Batch: 420; loss: 1.85; acc: 0.3
Batch: 440; loss: 2.02; acc: 0.23
Batch: 460; loss: 1.8; acc: 0.33
Batch: 480; loss: 1.84; acc: 0.3
Batch: 500; loss: 1.9; acc: 0.33
Batch: 520; loss: 1.76; acc: 0.34
Batch: 540; loss: 1.69; acc: 0.44
Batch: 560; loss: 1.96; acc: 0.27
Batch: 580; loss: 1.7; acc: 0.38
Batch: 600; loss: 1.78; acc: 0.36
Batch: 620; loss: 2.0; acc: 0.25
Batch: 640; loss: 1.84; acc: 0.41
Batch: 660; loss: 1.73; acc: 0.42
Batch: 680; loss: 1.81; acc: 0.33
Batch: 700; loss: 1.92; acc: 0.28
Batch: 720; loss: 1.9; acc: 0.31
Batch: 740; loss: 1.77; acc: 0.34
Batch: 760; loss: 1.76; acc: 0.39
Batch: 780; loss: 1.7; acc: 0.48
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.38
Val Epoch over. val_loss: 1.8070629174542274; val_accuracy: 0.37121815286624205 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.91; acc: 0.36
Batch: 20; loss: 1.81; acc: 0.38
Batch: 40; loss: 1.69; acc: 0.41
Batch: 60; loss: 1.9; acc: 0.36
Batch: 80; loss: 1.86; acc: 0.31
Batch: 100; loss: 1.94; acc: 0.34
Batch: 120; loss: 1.81; acc: 0.39
Batch: 140; loss: 1.77; acc: 0.44
Batch: 160; loss: 1.82; acc: 0.34
Batch: 180; loss: 1.83; acc: 0.36
Batch: 200; loss: 1.72; acc: 0.45
Batch: 220; loss: 1.87; acc: 0.27
Batch: 240; loss: 1.94; acc: 0.34
Batch: 260; loss: 1.93; acc: 0.28
Batch: 280; loss: 1.84; acc: 0.3
Batch: 300; loss: 1.85; acc: 0.3
Batch: 320; loss: 1.79; acc: 0.38
Batch: 340; loss: 1.8; acc: 0.3
Batch: 360; loss: 1.58; acc: 0.41
Batch: 380; loss: 1.83; acc: 0.41
Batch: 400; loss: 1.75; acc: 0.38
Batch: 420; loss: 1.89; acc: 0.3
Batch: 440; loss: 1.86; acc: 0.38
Batch: 460; loss: 1.8; acc: 0.39
Batch: 480; loss: 1.93; acc: 0.34
Batch: 500; loss: 1.89; acc: 0.41
Batch: 520; loss: 1.76; acc: 0.36
Batch: 540; loss: 1.95; acc: 0.3
Batch: 560; loss: 1.86; acc: 0.3
Batch: 580; loss: 1.88; acc: 0.36
Batch: 600; loss: 1.75; acc: 0.45
Batch: 620; loss: 1.92; acc: 0.27
Batch: 640; loss: 1.83; acc: 0.38
Batch: 660; loss: 1.87; acc: 0.39
Batch: 680; loss: 1.99; acc: 0.27
Batch: 700; loss: 1.78; acc: 0.41
Batch: 720; loss: 1.92; acc: 0.33
Batch: 740; loss: 1.77; acc: 0.42
Batch: 760; loss: 1.82; acc: 0.36
Batch: 780; loss: 1.73; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.69; acc: 0.5
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074979440421814; val_accuracy: 0.37121815286624205 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.85; acc: 0.41
Batch: 20; loss: 1.79; acc: 0.39
Batch: 40; loss: 1.73; acc: 0.3
Batch: 60; loss: 1.78; acc: 0.36
Batch: 80; loss: 1.88; acc: 0.28
Batch: 100; loss: 1.75; acc: 0.36
Batch: 120; loss: 1.87; acc: 0.33
Batch: 140; loss: 1.82; acc: 0.34
Batch: 160; loss: 1.89; acc: 0.3
Batch: 180; loss: 1.82; acc: 0.36
Batch: 200; loss: 1.92; acc: 0.27
Batch: 220; loss: 1.74; acc: 0.33
Batch: 240; loss: 1.85; acc: 0.47
Batch: 260; loss: 2.03; acc: 0.2
Batch: 280; loss: 1.82; acc: 0.34
Batch: 300; loss: 1.88; acc: 0.28
Batch: 320; loss: 1.82; acc: 0.38
Batch: 340; loss: 1.9; acc: 0.31
Batch: 360; loss: 1.82; acc: 0.31
Batch: 380; loss: 1.79; acc: 0.41
Batch: 400; loss: 1.82; acc: 0.36
Batch: 420; loss: 1.62; acc: 0.47
Batch: 440; loss: 1.74; acc: 0.41
Batch: 460; loss: 1.75; acc: 0.44
Batch: 480; loss: 1.88; acc: 0.28
Batch: 500; loss: 1.96; acc: 0.39
Batch: 520; loss: 1.86; acc: 0.38
Batch: 540; loss: 1.73; acc: 0.36
Batch: 560; loss: 1.83; acc: 0.44
Batch: 580; loss: 1.94; acc: 0.27
Batch: 600; loss: 1.86; acc: 0.36
Batch: 620; loss: 1.8; acc: 0.39
Batch: 640; loss: 1.69; acc: 0.45
Batch: 660; loss: 1.73; acc: 0.48
Batch: 680; loss: 1.86; acc: 0.3
Batch: 700; loss: 1.83; acc: 0.36
Batch: 720; loss: 1.92; acc: 0.31
Batch: 740; loss: 1.76; acc: 0.45
Batch: 760; loss: 2.04; acc: 0.28
Batch: 780; loss: 1.91; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8075251457797494; val_accuracy: 0.37091958598726116 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.65; acc: 0.48
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.89; acc: 0.28
Batch: 60; loss: 1.89; acc: 0.31
Batch: 80; loss: 1.82; acc: 0.39
Batch: 100; loss: 1.79; acc: 0.39
Batch: 120; loss: 1.79; acc: 0.31
Batch: 140; loss: 1.89; acc: 0.31
Batch: 160; loss: 1.86; acc: 0.36
Batch: 180; loss: 1.85; acc: 0.36
Batch: 200; loss: 1.62; acc: 0.47
Batch: 220; loss: 1.55; acc: 0.45
Batch: 240; loss: 1.94; acc: 0.3
Batch: 260; loss: 1.79; acc: 0.34
Batch: 280; loss: 1.93; acc: 0.3
Batch: 300; loss: 1.91; acc: 0.31
Batch: 320; loss: 1.88; acc: 0.33
Batch: 340; loss: 1.84; acc: 0.38
Batch: 360; loss: 1.99; acc: 0.31
Batch: 380; loss: 1.83; acc: 0.34
Batch: 400; loss: 1.81; acc: 0.38
Batch: 420; loss: 1.99; acc: 0.31
Batch: 440; loss: 1.71; acc: 0.44
Batch: 460; loss: 1.85; acc: 0.34
Batch: 480; loss: 1.98; acc: 0.31
Batch: 500; loss: 2.07; acc: 0.33
Batch: 520; loss: 1.86; acc: 0.3
Batch: 540; loss: 1.81; acc: 0.28
Batch: 560; loss: 1.96; acc: 0.23
Batch: 580; loss: 1.89; acc: 0.33
Batch: 600; loss: 1.77; acc: 0.33
Batch: 620; loss: 1.86; acc: 0.34
Batch: 640; loss: 1.79; acc: 0.36
Batch: 660; loss: 1.86; acc: 0.38
Batch: 680; loss: 1.71; acc: 0.45
Batch: 700; loss: 1.89; acc: 0.31
Batch: 720; loss: 1.82; acc: 0.3
Batch: 740; loss: 1.76; acc: 0.41
Batch: 760; loss: 1.89; acc: 0.36
Batch: 780; loss: 1.9; acc: 0.27
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.36
Batch: 40; loss: 1.74; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8076442419343692; val_accuracy: 0.3705214968152866 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.72; acc: 0.42
Batch: 20; loss: 1.74; acc: 0.42
Batch: 40; loss: 1.9; acc: 0.27
Batch: 60; loss: 1.85; acc: 0.38
Batch: 80; loss: 2.15; acc: 0.25
Batch: 100; loss: 1.87; acc: 0.36
Batch: 120; loss: 1.68; acc: 0.39
Batch: 140; loss: 1.78; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.36
Batch: 180; loss: 1.87; acc: 0.34
Batch: 200; loss: 1.86; acc: 0.28
Batch: 220; loss: 1.88; acc: 0.33
Batch: 240; loss: 1.68; acc: 0.36
Batch: 260; loss: 1.87; acc: 0.34
Batch: 280; loss: 1.82; acc: 0.33
Batch: 300; loss: 1.79; acc: 0.38
Batch: 320; loss: 1.78; acc: 0.41
Batch: 340; loss: 1.94; acc: 0.27
Batch: 360; loss: 1.67; acc: 0.47
Batch: 380; loss: 1.7; acc: 0.45
Batch: 400; loss: 1.76; acc: 0.42
Batch: 420; loss: 1.85; acc: 0.34
Batch: 440; loss: 1.92; acc: 0.28
Batch: 460; loss: 1.86; acc: 0.36
Batch: 480; loss: 1.72; acc: 0.47
Batch: 500; loss: 1.81; acc: 0.34
Batch: 520; loss: 1.81; acc: 0.36
Batch: 540; loss: 1.8; acc: 0.33
Batch: 560; loss: 1.89; acc: 0.28
Batch: 580; loss: 1.86; acc: 0.33
Batch: 600; loss: 1.72; acc: 0.38
Batch: 620; loss: 1.78; acc: 0.38
Batch: 640; loss: 1.81; acc: 0.34
Batch: 660; loss: 1.78; acc: 0.38
Batch: 680; loss: 1.8; acc: 0.39
Batch: 700; loss: 1.84; acc: 0.34
Batch: 720; loss: 1.98; acc: 0.36
Batch: 740; loss: 1.94; acc: 0.34
Batch: 760; loss: 1.77; acc: 0.39
Batch: 780; loss: 1.81; acc: 0.36
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8071829566530362; val_accuracy: 0.3703224522292994 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.83; acc: 0.44
Batch: 20; loss: 1.75; acc: 0.39
Batch: 40; loss: 1.89; acc: 0.39
Batch: 60; loss: 1.69; acc: 0.36
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.8; acc: 0.34
Batch: 120; loss: 1.91; acc: 0.3
Batch: 140; loss: 1.93; acc: 0.28
Batch: 160; loss: 1.88; acc: 0.34
Batch: 180; loss: 1.92; acc: 0.28
Batch: 200; loss: 1.69; acc: 0.39
Batch: 220; loss: 1.83; acc: 0.36
Batch: 240; loss: 1.93; acc: 0.28
Batch: 260; loss: 1.74; acc: 0.36
Batch: 280; loss: 1.71; acc: 0.42
Batch: 300; loss: 1.92; acc: 0.27
Batch: 320; loss: 1.91; acc: 0.25
Batch: 340; loss: 1.85; acc: 0.28
Batch: 360; loss: 1.9; acc: 0.38
Batch: 380; loss: 1.95; acc: 0.34
Batch: 400; loss: 1.8; acc: 0.34
Batch: 420; loss: 1.67; acc: 0.33
Batch: 440; loss: 1.73; acc: 0.47
Batch: 460; loss: 1.89; acc: 0.36
Batch: 480; loss: 1.97; acc: 0.27
Batch: 500; loss: 1.86; acc: 0.34
Batch: 520; loss: 2.01; acc: 0.31
Batch: 540; loss: 1.89; acc: 0.31
Batch: 560; loss: 1.98; acc: 0.34
Batch: 580; loss: 1.62; acc: 0.45
Batch: 600; loss: 1.84; acc: 0.42
Batch: 620; loss: 1.75; acc: 0.42
Batch: 640; loss: 1.73; acc: 0.47
Batch: 660; loss: 1.8; acc: 0.36
Batch: 680; loss: 1.86; acc: 0.39
Batch: 700; loss: 1.76; acc: 0.33
Batch: 720; loss: 1.72; acc: 0.38
Batch: 740; loss: 1.77; acc: 0.41
Batch: 760; loss: 1.98; acc: 0.31
Batch: 780; loss: 1.89; acc: 0.34
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.39
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807287471309589; val_accuracy: 0.3705214968152866 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 1.8; acc: 0.41
Batch: 40; loss: 1.87; acc: 0.41
Batch: 60; loss: 1.74; acc: 0.45
Batch: 80; loss: 1.91; acc: 0.33
Batch: 100; loss: 1.85; acc: 0.27
Batch: 120; loss: 1.78; acc: 0.39
Batch: 140; loss: 1.92; acc: 0.31
Batch: 160; loss: 1.78; acc: 0.41
Batch: 180; loss: 1.82; acc: 0.39
Batch: 200; loss: 1.95; acc: 0.31
Batch: 220; loss: 1.69; acc: 0.53
Batch: 240; loss: 1.93; acc: 0.41
Batch: 260; loss: 1.71; acc: 0.42
Batch: 280; loss: 1.83; acc: 0.3
Batch: 300; loss: 1.9; acc: 0.28
Batch: 320; loss: 1.93; acc: 0.33
Batch: 340; loss: 1.97; acc: 0.28
Batch: 360; loss: 1.7; acc: 0.41
Batch: 380; loss: 1.86; acc: 0.31
Batch: 400; loss: 1.81; acc: 0.39
Batch: 420; loss: 1.73; acc: 0.42
Batch: 440; loss: 1.79; acc: 0.31
Batch: 460; loss: 1.82; acc: 0.34
Batch: 480; loss: 1.75; acc: 0.41
Batch: 500; loss: 1.51; acc: 0.55
Batch: 520; loss: 1.7; acc: 0.36
Batch: 540; loss: 1.74; acc: 0.44
Batch: 560; loss: 1.75; acc: 0.36
Batch: 580; loss: 1.91; acc: 0.28
Batch: 600; loss: 1.89; acc: 0.31
Batch: 620; loss: 1.85; acc: 0.23
Batch: 640; loss: 1.96; acc: 0.27
Batch: 660; loss: 1.81; acc: 0.39
Batch: 680; loss: 1.75; acc: 0.36
Batch: 700; loss: 1.8; acc: 0.36
Batch: 720; loss: 1.81; acc: 0.38
Batch: 740; loss: 1.84; acc: 0.34
Batch: 760; loss: 1.92; acc: 0.34
Batch: 780; loss: 1.83; acc: 0.38
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074233046003207; val_accuracy: 0.3701234076433121 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.88; acc: 0.31
Batch: 20; loss: 1.79; acc: 0.42
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.84; acc: 0.3
Batch: 80; loss: 1.78; acc: 0.42
Batch: 100; loss: 1.9; acc: 0.22
Batch: 120; loss: 1.8; acc: 0.28
Batch: 140; loss: 1.77; acc: 0.41
Batch: 160; loss: 1.88; acc: 0.34
Batch: 180; loss: 1.83; acc: 0.41
Batch: 200; loss: 1.98; acc: 0.34
Batch: 220; loss: 1.8; acc: 0.41
Batch: 240; loss: 1.96; acc: 0.36
Batch: 260; loss: 1.85; acc: 0.34
Batch: 280; loss: 1.88; acc: 0.39
Batch: 300; loss: 1.75; acc: 0.38
Batch: 320; loss: 1.92; acc: 0.23
Batch: 340; loss: 1.9; acc: 0.31
Batch: 360; loss: 1.91; acc: 0.33
Batch: 380; loss: 1.92; acc: 0.38
Batch: 400; loss: 1.95; acc: 0.33
Batch: 420; loss: 1.84; acc: 0.31
Batch: 440; loss: 2.0; acc: 0.3
Batch: 460; loss: 1.81; acc: 0.34
Batch: 480; loss: 1.82; acc: 0.31
Batch: 500; loss: 1.94; acc: 0.28
Batch: 520; loss: 1.72; acc: 0.5
Batch: 540; loss: 1.89; acc: 0.3
Batch: 560; loss: 1.68; acc: 0.48
Batch: 580; loss: 2.1; acc: 0.23
Batch: 600; loss: 1.71; acc: 0.47
Batch: 620; loss: 1.77; acc: 0.34
Batch: 640; loss: 1.79; acc: 0.44
Batch: 660; loss: 1.88; acc: 0.34
Batch: 680; loss: 1.82; acc: 0.39
Batch: 700; loss: 1.63; acc: 0.47
Batch: 720; loss: 1.76; acc: 0.34
Batch: 740; loss: 1.75; acc: 0.36
Batch: 760; loss: 1.78; acc: 0.33
Batch: 780; loss: 1.69; acc: 0.39
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8075225345648018; val_accuracy: 0.3695262738853503 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.81; acc: 0.39
Batch: 20; loss: 1.77; acc: 0.47
Batch: 40; loss: 1.82; acc: 0.31
Batch: 60; loss: 1.79; acc: 0.39
Batch: 80; loss: 1.74; acc: 0.42
Batch: 100; loss: 1.92; acc: 0.3
Batch: 120; loss: 1.97; acc: 0.25
Batch: 140; loss: 1.8; acc: 0.41
Batch: 160; loss: 1.66; acc: 0.47
Batch: 180; loss: 1.76; acc: 0.34
Batch: 200; loss: 1.98; acc: 0.3
Batch: 220; loss: 1.84; acc: 0.34
Batch: 240; loss: 1.87; acc: 0.36
Batch: 260; loss: 1.85; acc: 0.45
Batch: 280; loss: 1.79; acc: 0.39
Batch: 300; loss: 1.79; acc: 0.39
Batch: 320; loss: 1.9; acc: 0.31
Batch: 340; loss: 1.97; acc: 0.33
Batch: 360; loss: 1.87; acc: 0.33
Batch: 380; loss: 1.9; acc: 0.31
Batch: 400; loss: 1.76; acc: 0.38
Batch: 420; loss: 1.95; acc: 0.34
Batch: 440; loss: 1.81; acc: 0.36
Batch: 460; loss: 1.84; acc: 0.31
Batch: 480; loss: 1.69; acc: 0.39
Batch: 500; loss: 1.7; acc: 0.41
Batch: 520; loss: 1.79; acc: 0.39
Batch: 540; loss: 1.86; acc: 0.39
Batch: 560; loss: 1.93; acc: 0.31
Batch: 580; loss: 1.88; acc: 0.33
Batch: 600; loss: 1.85; acc: 0.3
Batch: 620; loss: 1.67; acc: 0.48
Batch: 640; loss: 1.97; acc: 0.3
Batch: 660; loss: 1.76; acc: 0.42
Batch: 680; loss: 1.62; acc: 0.48
Batch: 700; loss: 1.87; acc: 0.36
Batch: 720; loss: 1.91; acc: 0.33
Batch: 740; loss: 1.92; acc: 0.27
Batch: 760; loss: 1.82; acc: 0.33
Batch: 780; loss: 1.89; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8074715122295792; val_accuracy: 0.37062101910828027 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.86; acc: 0.31
Batch: 20; loss: 1.74; acc: 0.34
Batch: 40; loss: 2.05; acc: 0.28
Batch: 60; loss: 1.84; acc: 0.34
Batch: 80; loss: 1.64; acc: 0.45
Batch: 100; loss: 2.0; acc: 0.33
Batch: 120; loss: 1.89; acc: 0.3
Batch: 140; loss: 2.04; acc: 0.19
Batch: 160; loss: 1.96; acc: 0.3
Batch: 180; loss: 1.81; acc: 0.38
Batch: 200; loss: 1.91; acc: 0.3
Batch: 220; loss: 1.65; acc: 0.42
Batch: 240; loss: 1.83; acc: 0.41
Batch: 260; loss: 1.83; acc: 0.36
Batch: 280; loss: 1.94; acc: 0.33
Batch: 300; loss: 1.63; acc: 0.42
Batch: 320; loss: 1.92; acc: 0.31
Batch: 340; loss: 1.83; acc: 0.41
Batch: 360; loss: 1.83; acc: 0.34
Batch: 380; loss: 1.88; acc: 0.34
Batch: 400; loss: 1.83; acc: 0.36
Batch: 420; loss: 1.82; acc: 0.38
Batch: 440; loss: 1.74; acc: 0.34
Batch: 460; loss: 1.81; acc: 0.3
Batch: 480; loss: 1.84; acc: 0.41
Batch: 500; loss: 1.83; acc: 0.39
Batch: 520; loss: 2.02; acc: 0.34
Batch: 540; loss: 1.8; acc: 0.36
Batch: 560; loss: 1.84; acc: 0.33
Batch: 580; loss: 1.76; acc: 0.39
Batch: 600; loss: 1.85; acc: 0.31
Batch: 620; loss: 1.83; acc: 0.39
Batch: 640; loss: 1.67; acc: 0.52
Batch: 660; loss: 1.8; acc: 0.38
Batch: 680; loss: 1.73; acc: 0.47
Batch: 700; loss: 1.6; acc: 0.55
Batch: 720; loss: 1.89; acc: 0.38
Batch: 740; loss: 1.91; acc: 0.41
Batch: 760; loss: 1.9; acc: 0.31
Batch: 780; loss: 1.67; acc: 0.42
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.5
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8073301231785186; val_accuracy: 0.370421974522293 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.78; acc: 0.39
Batch: 20; loss: 1.89; acc: 0.34
Batch: 40; loss: 2.0; acc: 0.3
Batch: 60; loss: 1.71; acc: 0.38
Batch: 80; loss: 1.91; acc: 0.31
Batch: 100; loss: 1.66; acc: 0.45
Batch: 120; loss: 1.72; acc: 0.48
Batch: 140; loss: 1.81; acc: 0.36
Batch: 160; loss: 1.87; acc: 0.39
Batch: 180; loss: 1.87; acc: 0.3
Batch: 200; loss: 1.95; acc: 0.3
Batch: 220; loss: 1.75; acc: 0.3
Batch: 240; loss: 1.89; acc: 0.31
Batch: 260; loss: 1.89; acc: 0.27
Batch: 280; loss: 1.88; acc: 0.28
Batch: 300; loss: 1.89; acc: 0.33
Batch: 320; loss: 1.84; acc: 0.34
Batch: 340; loss: 1.77; acc: 0.33
Batch: 360; loss: 1.72; acc: 0.44
Batch: 380; loss: 1.99; acc: 0.22
Batch: 400; loss: 1.87; acc: 0.39
Batch: 420; loss: 1.78; acc: 0.39
Batch: 440; loss: 2.04; acc: 0.27
Batch: 460; loss: 1.88; acc: 0.33
Batch: 480; loss: 1.94; acc: 0.3
Batch: 500; loss: 1.85; acc: 0.34
Batch: 520; loss: 1.67; acc: 0.45
Batch: 540; loss: 1.83; acc: 0.39
Batch: 560; loss: 1.88; acc: 0.39
Batch: 580; loss: 1.87; acc: 0.33
Batch: 600; loss: 1.85; acc: 0.36
Batch: 620; loss: 1.82; acc: 0.33
Batch: 640; loss: 1.94; acc: 0.27
Batch: 660; loss: 1.83; acc: 0.3
Batch: 680; loss: 1.82; acc: 0.34
Batch: 700; loss: 1.94; acc: 0.22
Batch: 720; loss: 1.76; acc: 0.39
Batch: 740; loss: 1.87; acc: 0.34
Batch: 760; loss: 1.71; acc: 0.41
Batch: 780; loss: 1.8; acc: 0.48
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807605908175183; val_accuracy: 0.36992436305732485 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.99; acc: 0.31
Batch: 20; loss: 1.96; acc: 0.36
Batch: 40; loss: 1.77; acc: 0.41
Batch: 60; loss: 1.65; acc: 0.44
Batch: 80; loss: 1.86; acc: 0.39
Batch: 100; loss: 1.74; acc: 0.52
Batch: 120; loss: 1.83; acc: 0.41
Batch: 140; loss: 1.85; acc: 0.31
Batch: 160; loss: 1.76; acc: 0.45
Batch: 180; loss: 1.7; acc: 0.44
Batch: 200; loss: 1.66; acc: 0.45
Batch: 220; loss: 1.91; acc: 0.33
Batch: 240; loss: 2.03; acc: 0.3
Batch: 260; loss: 1.69; acc: 0.38
Batch: 280; loss: 2.13; acc: 0.27
Batch: 300; loss: 1.98; acc: 0.36
Batch: 320; loss: 1.77; acc: 0.39
Batch: 340; loss: 1.76; acc: 0.41
Batch: 360; loss: 2.14; acc: 0.23
Batch: 380; loss: 1.91; acc: 0.23
Batch: 400; loss: 1.91; acc: 0.34
Batch: 420; loss: 1.93; acc: 0.33
Batch: 440; loss: 1.79; acc: 0.33
Batch: 460; loss: 1.89; acc: 0.38
Batch: 480; loss: 1.88; acc: 0.33
Batch: 500; loss: 1.87; acc: 0.36
Batch: 520; loss: 1.82; acc: 0.44
Batch: 540; loss: 1.78; acc: 0.39
Batch: 560; loss: 1.87; acc: 0.31
Batch: 580; loss: 1.96; acc: 0.28
Batch: 600; loss: 1.8; acc: 0.39
Batch: 620; loss: 2.03; acc: 0.31
Batch: 640; loss: 1.93; acc: 0.34
Batch: 660; loss: 1.92; acc: 0.34
Batch: 680; loss: 1.79; acc: 0.3
Batch: 700; loss: 1.9; acc: 0.31
Batch: 720; loss: 1.74; acc: 0.42
Batch: 740; loss: 1.93; acc: 0.28
Batch: 760; loss: 1.89; acc: 0.34
Batch: 780; loss: 1.93; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.36
Val Epoch over. val_loss: 1.8073714751346854; val_accuracy: 0.370421974522293 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.77; acc: 0.39
Batch: 20; loss: 1.82; acc: 0.36
Batch: 40; loss: 1.75; acc: 0.44
Batch: 60; loss: 1.68; acc: 0.45
Batch: 80; loss: 1.98; acc: 0.23
Batch: 100; loss: 1.81; acc: 0.38
Batch: 120; loss: 1.84; acc: 0.28
Batch: 140; loss: 1.89; acc: 0.3
Batch: 160; loss: 1.85; acc: 0.34
Batch: 180; loss: 1.88; acc: 0.38
Batch: 200; loss: 1.77; acc: 0.38
Batch: 220; loss: 1.7; acc: 0.42
Batch: 240; loss: 1.84; acc: 0.31
Batch: 260; loss: 1.93; acc: 0.3
Batch: 280; loss: 1.78; acc: 0.36
Batch: 300; loss: 1.83; acc: 0.3
Batch: 320; loss: 1.75; acc: 0.45
Batch: 340; loss: 1.92; acc: 0.34
Batch: 360; loss: 1.86; acc: 0.38
Batch: 380; loss: 1.93; acc: 0.27
Batch: 400; loss: 1.96; acc: 0.3
Batch: 420; loss: 1.87; acc: 0.36
Batch: 440; loss: 1.85; acc: 0.44
Batch: 460; loss: 1.86; acc: 0.38
Batch: 480; loss: 1.73; acc: 0.41
Batch: 500; loss: 1.78; acc: 0.27
Batch: 520; loss: 1.76; acc: 0.34
Batch: 540; loss: 1.74; acc: 0.34
Batch: 560; loss: 1.72; acc: 0.41
Batch: 580; loss: 1.82; acc: 0.34
Batch: 600; loss: 1.65; acc: 0.47
Batch: 620; loss: 1.88; acc: 0.33
Batch: 640; loss: 2.05; acc: 0.28
Batch: 660; loss: 1.87; acc: 0.31
Batch: 680; loss: 1.83; acc: 0.44
Batch: 700; loss: 1.93; acc: 0.39
Batch: 720; loss: 1.87; acc: 0.39
Batch: 740; loss: 1.92; acc: 0.39
Batch: 760; loss: 2.06; acc: 0.28
Batch: 780; loss: 1.88; acc: 0.42
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8076779637367102; val_accuracy: 0.3701234076433121 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.92; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.34
Batch: 40; loss: 1.81; acc: 0.33
Batch: 60; loss: 1.93; acc: 0.34
Batch: 80; loss: 1.77; acc: 0.45
Batch: 100; loss: 1.89; acc: 0.34
Batch: 120; loss: 1.93; acc: 0.28
Batch: 140; loss: 2.01; acc: 0.31
Batch: 160; loss: 1.99; acc: 0.2
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.86; acc: 0.34
Batch: 220; loss: 1.82; acc: 0.41
Batch: 240; loss: 1.93; acc: 0.27
Batch: 260; loss: 1.65; acc: 0.41
Batch: 280; loss: 1.91; acc: 0.31
Batch: 300; loss: 1.75; acc: 0.42
Batch: 320; loss: 1.83; acc: 0.33
Batch: 340; loss: 1.77; acc: 0.48
Batch: 360; loss: 1.94; acc: 0.36
Batch: 380; loss: 1.8; acc: 0.38
Batch: 400; loss: 1.86; acc: 0.33
Batch: 420; loss: 1.8; acc: 0.38
Batch: 440; loss: 1.7; acc: 0.36
Batch: 460; loss: 1.79; acc: 0.41
Batch: 480; loss: 1.77; acc: 0.39
Batch: 500; loss: 1.65; acc: 0.39
Batch: 520; loss: 1.85; acc: 0.39
Batch: 540; loss: 1.9; acc: 0.3
Batch: 560; loss: 1.92; acc: 0.28
Batch: 580; loss: 1.84; acc: 0.39
Batch: 600; loss: 1.84; acc: 0.42
Batch: 620; loss: 1.94; acc: 0.28
Batch: 640; loss: 1.74; acc: 0.39
Batch: 660; loss: 1.8; acc: 0.42
Batch: 680; loss: 1.93; acc: 0.27
Batch: 700; loss: 1.75; acc: 0.42
Batch: 720; loss: 1.8; acc: 0.34
Batch: 740; loss: 1.78; acc: 0.41
Batch: 760; loss: 1.92; acc: 0.3
Batch: 780; loss: 1.73; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807491926630591; val_accuracy: 0.36962579617834396 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.78; acc: 0.45
Batch: 20; loss: 1.77; acc: 0.42
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.78; acc: 0.38
Batch: 80; loss: 1.88; acc: 0.36
Batch: 100; loss: 1.9; acc: 0.34
Batch: 120; loss: 1.64; acc: 0.41
Batch: 140; loss: 2.02; acc: 0.31
Batch: 160; loss: 1.86; acc: 0.36
Batch: 180; loss: 1.69; acc: 0.42
Batch: 200; loss: 1.85; acc: 0.42
Batch: 220; loss: 1.89; acc: 0.41
Batch: 240; loss: 1.81; acc: 0.33
Batch: 260; loss: 1.68; acc: 0.38
Batch: 280; loss: 1.97; acc: 0.33
Batch: 300; loss: 1.95; acc: 0.33
Batch: 320; loss: 1.85; acc: 0.34
Batch: 340; loss: 1.73; acc: 0.45
Batch: 360; loss: 1.91; acc: 0.3
Batch: 380; loss: 1.77; acc: 0.42
Batch: 400; loss: 1.79; acc: 0.36
Batch: 420; loss: 1.97; acc: 0.36
Batch: 440; loss: 1.82; acc: 0.34
Batch: 460; loss: 1.78; acc: 0.36
Batch: 480; loss: 2.01; acc: 0.27
Batch: 500; loss: 1.78; acc: 0.34
Batch: 520; loss: 1.77; acc: 0.41
Batch: 540; loss: 1.8; acc: 0.39
Batch: 560; loss: 1.81; acc: 0.34
Batch: 580; loss: 1.91; acc: 0.31
Batch: 600; loss: 1.73; acc: 0.44
Batch: 620; loss: 1.75; acc: 0.33
Batch: 640; loss: 1.88; acc: 0.39
Batch: 660; loss: 1.89; acc: 0.31
Batch: 680; loss: 2.05; acc: 0.23
Batch: 700; loss: 1.9; acc: 0.33
Batch: 720; loss: 1.84; acc: 0.36
Batch: 740; loss: 1.71; acc: 0.42
Batch: 760; loss: 1.81; acc: 0.41
Batch: 780; loss: 1.99; acc: 0.2
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074500773363054; val_accuracy: 0.3705214968152866 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.86; acc: 0.36
Batch: 20; loss: 1.89; acc: 0.33
Batch: 40; loss: 1.77; acc: 0.38
Batch: 60; loss: 1.66; acc: 0.5
Batch: 80; loss: 1.65; acc: 0.44
Batch: 100; loss: 1.83; acc: 0.34
Batch: 120; loss: 1.62; acc: 0.48
Batch: 140; loss: 1.78; acc: 0.38
Batch: 160; loss: 1.79; acc: 0.41
Batch: 180; loss: 1.64; acc: 0.48
Batch: 200; loss: 1.74; acc: 0.45
Batch: 220; loss: 1.99; acc: 0.31
Batch: 240; loss: 1.73; acc: 0.34
Batch: 260; loss: 1.79; acc: 0.39
Batch: 280; loss: 1.82; acc: 0.36
Batch: 300; loss: 1.73; acc: 0.42
Batch: 320; loss: 1.75; acc: 0.42
Batch: 340; loss: 1.98; acc: 0.28
Batch: 360; loss: 1.7; acc: 0.41
Batch: 380; loss: 1.77; acc: 0.39
Batch: 400; loss: 1.94; acc: 0.36
Batch: 420; loss: 1.89; acc: 0.34
Batch: 440; loss: 1.89; acc: 0.36
Batch: 460; loss: 1.68; acc: 0.44
Batch: 480; loss: 1.96; acc: 0.28
Batch: 500; loss: 2.15; acc: 0.27
Batch: 520; loss: 1.93; acc: 0.34
Batch: 540; loss: 1.86; acc: 0.38
Batch: 560; loss: 1.86; acc: 0.31
Batch: 580; loss: 1.9; acc: 0.36
Batch: 600; loss: 1.89; acc: 0.39
Batch: 620; loss: 1.89; acc: 0.33
Batch: 640; loss: 1.93; acc: 0.28
Batch: 660; loss: 1.82; acc: 0.33
Batch: 680; loss: 1.86; acc: 0.41
Batch: 700; loss: 1.97; acc: 0.27
Batch: 720; loss: 1.82; acc: 0.42
Batch: 740; loss: 1.88; acc: 0.36
Batch: 760; loss: 1.82; acc: 0.41
Batch: 780; loss: 1.69; acc: 0.47
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074335444505047; val_accuracy: 0.37072054140127386 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.72; acc: 0.41
Batch: 20; loss: 1.89; acc: 0.42
Batch: 40; loss: 1.82; acc: 0.36
Batch: 60; loss: 1.73; acc: 0.44
Batch: 80; loss: 1.73; acc: 0.5
Batch: 100; loss: 1.7; acc: 0.47
Batch: 120; loss: 1.75; acc: 0.38
Batch: 140; loss: 1.79; acc: 0.34
Batch: 160; loss: 1.74; acc: 0.42
Batch: 180; loss: 2.09; acc: 0.25
Batch: 200; loss: 1.91; acc: 0.31
Batch: 220; loss: 1.84; acc: 0.33
Batch: 240; loss: 1.76; acc: 0.42
Batch: 260; loss: 1.88; acc: 0.39
Batch: 280; loss: 1.78; acc: 0.39
Batch: 300; loss: 1.77; acc: 0.42
Batch: 320; loss: 1.83; acc: 0.33
Batch: 340; loss: 1.74; acc: 0.38
Batch: 360; loss: 1.95; acc: 0.28
Batch: 380; loss: 1.75; acc: 0.39
Batch: 400; loss: 1.83; acc: 0.42
Batch: 420; loss: 1.93; acc: 0.27
Batch: 440; loss: 1.91; acc: 0.33
Batch: 460; loss: 1.85; acc: 0.33
Batch: 480; loss: 1.84; acc: 0.36
Batch: 500; loss: 1.91; acc: 0.28
Batch: 520; loss: 1.87; acc: 0.38
Batch: 540; loss: 1.84; acc: 0.36
Batch: 560; loss: 1.69; acc: 0.42
Batch: 580; loss: 1.89; acc: 0.34
Batch: 600; loss: 1.82; acc: 0.36
Batch: 620; loss: 1.76; acc: 0.42
Batch: 640; loss: 1.92; acc: 0.38
Batch: 660; loss: 1.99; acc: 0.39
Batch: 680; loss: 1.9; acc: 0.31
Batch: 700; loss: 1.99; acc: 0.28
Batch: 720; loss: 1.86; acc: 0.33
Batch: 740; loss: 1.81; acc: 0.31
Batch: 760; loss: 1.85; acc: 0.34
Batch: 780; loss: 1.74; acc: 0.41
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074641334023445; val_accuracy: 0.3703224522292994 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.88; acc: 0.34
Batch: 20; loss: 1.68; acc: 0.45
Batch: 40; loss: 1.84; acc: 0.34
Batch: 60; loss: 2.08; acc: 0.23
Batch: 80; loss: 1.8; acc: 0.38
Batch: 100; loss: 1.58; acc: 0.5
Batch: 120; loss: 1.85; acc: 0.27
Batch: 140; loss: 1.68; acc: 0.47
Batch: 160; loss: 1.74; acc: 0.36
Batch: 180; loss: 1.94; acc: 0.34
Batch: 200; loss: 1.84; acc: 0.38
Batch: 220; loss: 1.91; acc: 0.3
Batch: 240; loss: 1.96; acc: 0.28
Batch: 260; loss: 1.65; acc: 0.52
Batch: 280; loss: 1.96; acc: 0.31
Batch: 300; loss: 1.8; acc: 0.3
Batch: 320; loss: 1.9; acc: 0.33
Batch: 340; loss: 1.76; acc: 0.38
Batch: 360; loss: 1.75; acc: 0.42
Batch: 380; loss: 1.85; acc: 0.33
Batch: 400; loss: 1.76; acc: 0.45
Batch: 420; loss: 1.84; acc: 0.3
Batch: 440; loss: 1.8; acc: 0.41
Batch: 460; loss: 1.66; acc: 0.45
Batch: 480; loss: 2.03; acc: 0.23
Batch: 500; loss: 1.77; acc: 0.41
Batch: 520; loss: 1.65; acc: 0.44
Batch: 540; loss: 1.94; acc: 0.34
Batch: 560; loss: 1.8; acc: 0.39
Batch: 580; loss: 1.79; acc: 0.33
Batch: 600; loss: 1.73; acc: 0.39
Batch: 620; loss: 1.79; acc: 0.42
Batch: 640; loss: 1.68; acc: 0.44
Batch: 660; loss: 1.65; acc: 0.41
Batch: 680; loss: 1.76; acc: 0.41
Batch: 700; loss: 1.7; acc: 0.39
Batch: 720; loss: 1.83; acc: 0.25
Batch: 740; loss: 1.88; acc: 0.39
Batch: 760; loss: 1.66; acc: 0.47
Batch: 780; loss: 1.92; acc: 0.34
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074073123324448; val_accuracy: 0.37091958598726116 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.75; acc: 0.41
Batch: 20; loss: 1.76; acc: 0.45
Batch: 40; loss: 1.82; acc: 0.38
Batch: 60; loss: 1.81; acc: 0.31
Batch: 80; loss: 1.94; acc: 0.36
Batch: 100; loss: 1.81; acc: 0.39
Batch: 120; loss: 1.8; acc: 0.33
Batch: 140; loss: 1.93; acc: 0.28
Batch: 160; loss: 1.89; acc: 0.31
Batch: 180; loss: 1.73; acc: 0.39
Batch: 200; loss: 1.72; acc: 0.42
Batch: 220; loss: 1.81; acc: 0.31
Batch: 240; loss: 1.9; acc: 0.33
Batch: 260; loss: 1.7; acc: 0.44
Batch: 280; loss: 1.82; acc: 0.28
Batch: 300; loss: 1.77; acc: 0.41
Batch: 320; loss: 1.82; acc: 0.36
Batch: 340; loss: 1.85; acc: 0.36
Batch: 360; loss: 1.95; acc: 0.28
Batch: 380; loss: 1.76; acc: 0.38
Batch: 400; loss: 1.79; acc: 0.38
Batch: 420; loss: 1.98; acc: 0.28
Batch: 440; loss: 1.88; acc: 0.3
Batch: 460; loss: 1.9; acc: 0.36
Batch: 480; loss: 1.74; acc: 0.39
Batch: 500; loss: 2.0; acc: 0.3
Batch: 520; loss: 1.82; acc: 0.39
Batch: 540; loss: 1.77; acc: 0.28
Batch: 560; loss: 1.86; acc: 0.38
Batch: 580; loss: 1.96; acc: 0.33
Batch: 600; loss: 2.01; acc: 0.25
Batch: 620; loss: 1.95; acc: 0.23
Batch: 640; loss: 1.86; acc: 0.31
Batch: 660; loss: 1.92; acc: 0.25
Batch: 680; loss: 1.86; acc: 0.33
Batch: 700; loss: 1.76; acc: 0.38
Batch: 720; loss: 1.7; acc: 0.52
Batch: 740; loss: 1.94; acc: 0.3
Batch: 760; loss: 1.69; acc: 0.44
Batch: 780; loss: 1.9; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807407939509981; val_accuracy: 0.37091958598726116 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.04; acc: 0.25
Batch: 20; loss: 1.79; acc: 0.39
Batch: 40; loss: 1.82; acc: 0.41
Batch: 60; loss: 1.79; acc: 0.34
Batch: 80; loss: 1.89; acc: 0.33
Batch: 100; loss: 1.92; acc: 0.38
Batch: 120; loss: 1.71; acc: 0.39
Batch: 140; loss: 1.78; acc: 0.39
Batch: 160; loss: 1.9; acc: 0.34
Batch: 180; loss: 1.74; acc: 0.36
Batch: 200; loss: 1.89; acc: 0.28
Batch: 220; loss: 1.9; acc: 0.28
Batch: 240; loss: 1.81; acc: 0.36
Batch: 260; loss: 1.73; acc: 0.38
Batch: 280; loss: 1.8; acc: 0.38
Batch: 300; loss: 1.88; acc: 0.31
Batch: 320; loss: 1.79; acc: 0.41
Batch: 340; loss: 1.61; acc: 0.52
Batch: 360; loss: 1.74; acc: 0.42
Batch: 380; loss: 1.87; acc: 0.33
Batch: 400; loss: 1.9; acc: 0.36
Batch: 420; loss: 1.99; acc: 0.27
Batch: 440; loss: 1.96; acc: 0.31
Batch: 460; loss: 2.01; acc: 0.28
Batch: 480; loss: 1.79; acc: 0.42
Batch: 500; loss: 1.81; acc: 0.34
Batch: 520; loss: 1.99; acc: 0.25
Batch: 540; loss: 1.81; acc: 0.36
Batch: 560; loss: 1.95; acc: 0.34
Batch: 580; loss: 1.92; acc: 0.39
Batch: 600; loss: 1.77; acc: 0.39
Batch: 620; loss: 1.65; acc: 0.53
Batch: 640; loss: 1.95; acc: 0.34
Batch: 660; loss: 1.94; acc: 0.34
Batch: 680; loss: 1.86; acc: 0.38
Batch: 700; loss: 1.82; acc: 0.39
Batch: 720; loss: 1.79; acc: 0.42
Batch: 740; loss: 1.83; acc: 0.41
Batch: 760; loss: 1.83; acc: 0.33
Batch: 780; loss: 1.82; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807383572979338; val_accuracy: 0.3708200636942675 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.9; acc: 0.34
Batch: 20; loss: 1.82; acc: 0.31
Batch: 40; loss: 1.75; acc: 0.38
Batch: 60; loss: 1.89; acc: 0.27
Batch: 80; loss: 1.63; acc: 0.47
Batch: 100; loss: 1.93; acc: 0.38
Batch: 120; loss: 1.87; acc: 0.31
Batch: 140; loss: 2.03; acc: 0.28
Batch: 160; loss: 1.77; acc: 0.39
Batch: 180; loss: 1.85; acc: 0.33
Batch: 200; loss: 1.89; acc: 0.33
Batch: 220; loss: 1.82; acc: 0.34
Batch: 240; loss: 1.76; acc: 0.36
Batch: 260; loss: 1.91; acc: 0.42
Batch: 280; loss: 1.78; acc: 0.34
Batch: 300; loss: 1.92; acc: 0.33
Batch: 320; loss: 1.81; acc: 0.41
Batch: 340; loss: 1.82; acc: 0.34
Batch: 360; loss: 1.9; acc: 0.31
Batch: 380; loss: 1.79; acc: 0.38
Batch: 400; loss: 1.83; acc: 0.36
Batch: 420; loss: 1.77; acc: 0.41
Batch: 440; loss: 1.78; acc: 0.44
Batch: 460; loss: 1.8; acc: 0.36
Batch: 480; loss: 1.78; acc: 0.36
Batch: 500; loss: 1.97; acc: 0.27
Batch: 520; loss: 1.77; acc: 0.42
Batch: 540; loss: 1.81; acc: 0.33
Batch: 560; loss: 1.85; acc: 0.31
Batch: 580; loss: 1.88; acc: 0.33
Batch: 600; loss: 1.73; acc: 0.42
Batch: 620; loss: 1.93; acc: 0.31
Batch: 640; loss: 1.8; acc: 0.42
Batch: 660; loss: 1.83; acc: 0.36
Batch: 680; loss: 1.59; acc: 0.5
Batch: 700; loss: 1.9; acc: 0.38
Batch: 720; loss: 1.96; acc: 0.3
Batch: 740; loss: 1.92; acc: 0.31
Batch: 760; loss: 1.77; acc: 0.36
Batch: 780; loss: 1.84; acc: 0.33
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807447109252784; val_accuracy: 0.3705214968152866 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.74; acc: 0.42
Batch: 20; loss: 1.76; acc: 0.47
Batch: 40; loss: 1.79; acc: 0.41
Batch: 60; loss: 1.92; acc: 0.31
Batch: 80; loss: 1.77; acc: 0.41
Batch: 100; loss: 1.85; acc: 0.27
Batch: 120; loss: 1.76; acc: 0.36
Batch: 140; loss: 1.92; acc: 0.33
Batch: 160; loss: 1.78; acc: 0.41
Batch: 180; loss: 1.89; acc: 0.31
Batch: 200; loss: 1.97; acc: 0.22
Batch: 220; loss: 1.86; acc: 0.34
Batch: 240; loss: 1.8; acc: 0.33
Batch: 260; loss: 1.61; acc: 0.42
Batch: 280; loss: 1.77; acc: 0.42
Batch: 300; loss: 1.83; acc: 0.36
Batch: 320; loss: 1.67; acc: 0.39
Batch: 340; loss: 2.04; acc: 0.28
Batch: 360; loss: 1.9; acc: 0.3
Batch: 380; loss: 1.83; acc: 0.36
Batch: 400; loss: 1.88; acc: 0.39
Batch: 420; loss: 1.69; acc: 0.42
Batch: 440; loss: 1.84; acc: 0.38
Batch: 460; loss: 1.78; acc: 0.33
Batch: 480; loss: 1.82; acc: 0.34
Batch: 500; loss: 2.0; acc: 0.25
Batch: 520; loss: 1.72; acc: 0.3
Batch: 540; loss: 1.7; acc: 0.39
Batch: 560; loss: 1.96; acc: 0.33
Batch: 580; loss: 1.69; acc: 0.47
Batch: 600; loss: 1.92; acc: 0.28
Batch: 620; loss: 1.92; acc: 0.38
Batch: 640; loss: 1.92; acc: 0.28
Batch: 660; loss: 1.66; acc: 0.47
Batch: 680; loss: 1.71; acc: 0.44
Batch: 700; loss: 1.95; acc: 0.28
Batch: 720; loss: 1.86; acc: 0.34
Batch: 740; loss: 1.94; acc: 0.33
Batch: 760; loss: 1.87; acc: 0.34
Batch: 780; loss: 1.86; acc: 0.36
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8074556596719535; val_accuracy: 0.370421974522293 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.77; acc: 0.39
Batch: 20; loss: 1.74; acc: 0.39
Batch: 40; loss: 1.77; acc: 0.39
Batch: 60; loss: 1.76; acc: 0.39
Batch: 80; loss: 1.8; acc: 0.38
Batch: 100; loss: 1.6; acc: 0.48
Batch: 120; loss: 1.85; acc: 0.33
Batch: 140; loss: 1.91; acc: 0.34
Batch: 160; loss: 1.82; acc: 0.39
Batch: 180; loss: 1.74; acc: 0.39
Batch: 200; loss: 1.77; acc: 0.39
Batch: 220; loss: 1.75; acc: 0.39
Batch: 240; loss: 1.7; acc: 0.39
Batch: 260; loss: 1.97; acc: 0.31
Batch: 280; loss: 1.64; acc: 0.39
Batch: 300; loss: 1.85; acc: 0.41
Batch: 320; loss: 1.83; acc: 0.34
Batch: 340; loss: 1.79; acc: 0.39
Batch: 360; loss: 1.77; acc: 0.42
Batch: 380; loss: 1.67; acc: 0.42
Batch: 400; loss: 1.77; acc: 0.42
Batch: 420; loss: 1.94; acc: 0.33
Batch: 440; loss: 1.85; acc: 0.39
Batch: 460; loss: 1.94; acc: 0.31
Batch: 480; loss: 1.93; acc: 0.3
Batch: 500; loss: 1.94; acc: 0.3
Batch: 520; loss: 2.04; acc: 0.31
Batch: 540; loss: 1.97; acc: 0.27
Batch: 560; loss: 1.76; acc: 0.38
Batch: 580; loss: 1.96; acc: 0.36
Batch: 600; loss: 1.83; acc: 0.36
Batch: 620; loss: 1.71; acc: 0.44
Batch: 640; loss: 1.71; acc: 0.47
Batch: 660; loss: 1.94; acc: 0.33
Batch: 680; loss: 1.95; acc: 0.33
Batch: 700; loss: 1.64; acc: 0.52
Batch: 720; loss: 2.09; acc: 0.25
Batch: 740; loss: 1.75; acc: 0.47
Batch: 760; loss: 1.75; acc: 0.45
Batch: 780; loss: 1.95; acc: 0.31
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.85; acc: 0.38
Batch: 40; loss: 1.74; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.807481519735543; val_accuracy: 0.3708200636942675 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.83; acc: 0.36
Batch: 20; loss: 1.81; acc: 0.33
Batch: 40; loss: 1.77; acc: 0.34
Batch: 60; loss: 1.94; acc: 0.34
Batch: 80; loss: 1.93; acc: 0.3
Batch: 100; loss: 1.85; acc: 0.38
Batch: 120; loss: 1.69; acc: 0.44
Batch: 140; loss: 1.82; acc: 0.36
Batch: 160; loss: 1.78; acc: 0.34
Batch: 180; loss: 1.65; acc: 0.47
Batch: 200; loss: 1.68; acc: 0.41
Batch: 220; loss: 1.85; acc: 0.39
Batch: 240; loss: 1.81; acc: 0.34
Batch: 260; loss: 1.89; acc: 0.28
Batch: 280; loss: 1.84; acc: 0.33
Batch: 300; loss: 1.86; acc: 0.38
Batch: 320; loss: 1.86; acc: 0.36
Batch: 340; loss: 1.88; acc: 0.33
Batch: 360; loss: 1.79; acc: 0.39
Batch: 380; loss: 1.72; acc: 0.45
Batch: 400; loss: 1.79; acc: 0.42
Batch: 420; loss: 1.81; acc: 0.34
Batch: 440; loss: 1.82; acc: 0.41
Batch: 460; loss: 1.6; acc: 0.5
Batch: 480; loss: 1.95; acc: 0.28
Batch: 500; loss: 2.04; acc: 0.3
Batch: 520; loss: 1.77; acc: 0.41
Batch: 540; loss: 1.86; acc: 0.33
Batch: 560; loss: 1.72; acc: 0.48
Batch: 580; loss: 1.74; acc: 0.39
Batch: 600; loss: 1.83; acc: 0.34
Batch: 620; loss: 1.99; acc: 0.22
Batch: 640; loss: 1.77; acc: 0.39
Batch: 660; loss: 1.8; acc: 0.41
Batch: 680; loss: 1.85; acc: 0.39
Batch: 700; loss: 1.96; acc: 0.33
Batch: 720; loss: 1.85; acc: 0.34
Batch: 740; loss: 1.86; acc: 0.36
Batch: 760; loss: 1.66; acc: 0.38
Batch: 780; loss: 1.86; acc: 0.42
Train Epoch over. train_loss: 1.83; train_accuracy: 0.36 

Batch: 0; loss: 1.93; acc: 0.31
Batch: 20; loss: 1.86; acc: 0.38
Batch: 40; loss: 1.73; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.52
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.75; acc: 0.41
Batch: 140; loss: 1.73; acc: 0.39
Val Epoch over. val_loss: 1.8075340682533896; val_accuracy: 0.370421974522293 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_25_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 2221300
elements in E: 2221300
fraction nonzero: 1.0
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.05
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.31; acc: 0.06
Batch: 60; loss: 2.31; acc: 0.09
Batch: 80; loss: 2.29; acc: 0.17
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.29; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.08
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.3; acc: 0.12
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.31; acc: 0.09
Batch: 260; loss: 2.29; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.31; acc: 0.09
Batch: 320; loss: 2.29; acc: 0.08
Batch: 340; loss: 2.31; acc: 0.08
Batch: 360; loss: 2.3; acc: 0.12
Batch: 380; loss: 2.3; acc: 0.12
Batch: 400; loss: 2.31; acc: 0.05
Batch: 420; loss: 2.3; acc: 0.05
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.29; acc: 0.11
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.29; acc: 0.09
Batch: 520; loss: 2.27; acc: 0.14
Batch: 540; loss: 2.3; acc: 0.05
Batch: 560; loss: 2.28; acc: 0.12
Batch: 580; loss: 2.28; acc: 0.14
Batch: 600; loss: 2.3; acc: 0.08
Batch: 620; loss: 2.28; acc: 0.16
Batch: 640; loss: 2.29; acc: 0.08
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.29; acc: 0.05
Batch: 700; loss: 2.27; acc: 0.2
Batch: 720; loss: 2.29; acc: 0.08
Batch: 740; loss: 2.28; acc: 0.2
Batch: 760; loss: 2.27; acc: 0.19
Batch: 780; loss: 2.27; acc: 0.22
Train Epoch over. train_loss: 2.29; train_accuracy: 0.11 

Batch: 0; loss: 2.28; acc: 0.22
Batch: 20; loss: 2.26; acc: 0.31
Batch: 40; loss: 2.27; acc: 0.3
Batch: 60; loss: 2.27; acc: 0.2
Batch: 80; loss: 2.27; acc: 0.2
Batch: 100; loss: 2.29; acc: 0.17
Batch: 120; loss: 2.28; acc: 0.22
Batch: 140; loss: 2.27; acc: 0.2
Val Epoch over. val_loss: 2.275382417022802; val_accuracy: 0.19864649681528662 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.28; acc: 0.22
Batch: 40; loss: 2.28; acc: 0.22
Batch: 60; loss: 2.26; acc: 0.27
Batch: 80; loss: 2.27; acc: 0.14
Batch: 100; loss: 2.27; acc: 0.27
Batch: 120; loss: 2.26; acc: 0.33
Batch: 140; loss: 2.25; acc: 0.31
Batch: 160; loss: 2.26; acc: 0.23
Batch: 180; loss: 2.25; acc: 0.3
Batch: 200; loss: 2.27; acc: 0.27
Batch: 220; loss: 2.26; acc: 0.28
Batch: 240; loss: 2.25; acc: 0.22
Batch: 260; loss: 2.22; acc: 0.38
Batch: 280; loss: 2.23; acc: 0.38
Batch: 300; loss: 2.24; acc: 0.25
Batch: 320; loss: 2.22; acc: 0.33
Batch: 340; loss: 2.23; acc: 0.3
Batch: 360; loss: 2.22; acc: 0.27
Batch: 380; loss: 2.2; acc: 0.34
Batch: 400; loss: 2.2; acc: 0.3
Batch: 420; loss: 2.27; acc: 0.11
Batch: 440; loss: 2.17; acc: 0.38
Batch: 460; loss: 2.25; acc: 0.19
Batch: 480; loss: 2.17; acc: 0.31
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.1; acc: 0.34
Batch: 540; loss: 2.12; acc: 0.34
Batch: 560; loss: 2.13; acc: 0.28
Batch: 580; loss: 2.15; acc: 0.33
Batch: 600; loss: 2.11; acc: 0.28
Batch: 620; loss: 2.13; acc: 0.33
Batch: 640; loss: 2.09; acc: 0.3
Batch: 660; loss: 1.99; acc: 0.33
Batch: 680; loss: 2.07; acc: 0.27
Batch: 700; loss: 1.95; acc: 0.34
Batch: 720; loss: 1.94; acc: 0.39
Batch: 740; loss: 1.87; acc: 0.38
Batch: 760; loss: 1.86; acc: 0.41
Batch: 780; loss: 1.85; acc: 0.39
Train Epoch over. train_loss: 2.17; train_accuracy: 0.28 

Batch: 0; loss: 1.83; acc: 0.38
Batch: 20; loss: 1.77; acc: 0.47
Batch: 40; loss: 1.61; acc: 0.52
Batch: 60; loss: 1.76; acc: 0.52
Batch: 80; loss: 1.82; acc: 0.42
Batch: 100; loss: 1.89; acc: 0.38
Batch: 120; loss: 1.87; acc: 0.42
Batch: 140; loss: 1.81; acc: 0.36
Val Epoch over. val_loss: 1.8392461492757128; val_accuracy: 0.3890326433121019 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.78; acc: 0.41
Batch: 20; loss: 1.65; acc: 0.48
Batch: 40; loss: 1.66; acc: 0.41
Batch: 60; loss: 1.73; acc: 0.36
Batch: 80; loss: 1.85; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.33
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.28; acc: 0.61
Batch: 160; loss: 1.56; acc: 0.52
Batch: 180; loss: 1.59; acc: 0.39
Batch: 200; loss: 1.47; acc: 0.53
Batch: 220; loss: 1.47; acc: 0.52
Batch: 240; loss: 1.48; acc: 0.44
Batch: 260; loss: 1.55; acc: 0.52
Batch: 280; loss: 1.52; acc: 0.41
Batch: 300; loss: 1.48; acc: 0.44
Batch: 320; loss: 1.31; acc: 0.56
Batch: 340; loss: 1.25; acc: 0.59
Batch: 360; loss: 1.36; acc: 0.5
Batch: 380; loss: 1.33; acc: 0.56
Batch: 400; loss: 1.22; acc: 0.64
Batch: 420; loss: 1.67; acc: 0.47
Batch: 440; loss: 1.31; acc: 0.53
Batch: 460; loss: 1.61; acc: 0.45
Batch: 480; loss: 1.23; acc: 0.56
Batch: 500; loss: 1.28; acc: 0.48
Batch: 520; loss: 1.53; acc: 0.47
Batch: 540; loss: 1.44; acc: 0.53
Batch: 560; loss: 1.54; acc: 0.45
Batch: 580; loss: 1.47; acc: 0.5
Batch: 600; loss: 1.27; acc: 0.58
Batch: 620; loss: 1.4; acc: 0.5
Batch: 640; loss: 1.29; acc: 0.64
Batch: 660; loss: 1.6; acc: 0.42
Batch: 680; loss: 1.51; acc: 0.5
Batch: 700; loss: 1.2; acc: 0.53
Batch: 720; loss: 1.25; acc: 0.59
Batch: 740; loss: 1.44; acc: 0.56
Batch: 760; loss: 1.65; acc: 0.42
Batch: 780; loss: 1.12; acc: 0.58
Train Epoch over. train_loss: 1.5; train_accuracy: 0.49 

Batch: 0; loss: 1.56; acc: 0.48
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 1.12; acc: 0.56
Batch: 60; loss: 1.34; acc: 0.52
Batch: 80; loss: 1.19; acc: 0.62
Batch: 100; loss: 1.49; acc: 0.45
Batch: 120; loss: 1.54; acc: 0.45
Batch: 140; loss: 1.34; acc: 0.5
Val Epoch over. val_loss: 1.4065129327925907; val_accuracy: 0.5166202229299363 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.42; acc: 0.53
Batch: 20; loss: 1.58; acc: 0.45
Batch: 40; loss: 1.48; acc: 0.45
Batch: 60; loss: 1.4; acc: 0.45
Batch: 80; loss: 1.66; acc: 0.41
Batch: 100; loss: 1.29; acc: 0.53
Batch: 120; loss: 1.43; acc: 0.52
Batch: 140; loss: 1.59; acc: 0.44
Batch: 160; loss: 1.53; acc: 0.42
Batch: 180; loss: 1.53; acc: 0.5
Batch: 200; loss: 1.49; acc: 0.41
Batch: 220; loss: 1.34; acc: 0.58
Batch: 240; loss: 1.32; acc: 0.53
Batch: 260; loss: 1.58; acc: 0.36
Batch: 280; loss: 1.62; acc: 0.38
Batch: 300; loss: 1.23; acc: 0.56
Batch: 320; loss: 1.28; acc: 0.59
Batch: 340; loss: 1.38; acc: 0.58
Batch: 360; loss: 1.45; acc: 0.5
Batch: 380; loss: 1.63; acc: 0.45
Batch: 400; loss: 1.23; acc: 0.56
Batch: 420; loss: 1.3; acc: 0.56
Batch: 440; loss: 1.53; acc: 0.44
Batch: 460; loss: 1.4; acc: 0.53
Batch: 480; loss: 1.68; acc: 0.41
Batch: 500; loss: 1.46; acc: 0.47
Batch: 520; loss: 1.27; acc: 0.56
Batch: 540; loss: 1.43; acc: 0.55
Batch: 560; loss: 1.34; acc: 0.59
Batch: 580; loss: 1.79; acc: 0.41
Batch: 600; loss: 1.6; acc: 0.5
Batch: 620; loss: 1.57; acc: 0.44
Batch: 640; loss: 1.25; acc: 0.55
Batch: 660; loss: 1.43; acc: 0.44
Batch: 680; loss: 1.34; acc: 0.61
Batch: 700; loss: 1.32; acc: 0.56
Batch: 720; loss: 1.23; acc: 0.53
Batch: 740; loss: 1.58; acc: 0.44
Batch: 760; loss: 1.46; acc: 0.53
Batch: 780; loss: 1.37; acc: 0.61
Train Epoch over. train_loss: 1.42; train_accuracy: 0.52 

Batch: 0; loss: 1.65; acc: 0.42
Batch: 20; loss: 1.28; acc: 0.58
Batch: 40; loss: 1.05; acc: 0.62
Batch: 60; loss: 1.35; acc: 0.5
Batch: 80; loss: 1.16; acc: 0.53
Batch: 100; loss: 1.43; acc: 0.52
Batch: 120; loss: 1.5; acc: 0.48
Batch: 140; loss: 1.27; acc: 0.52
Val Epoch over. val_loss: 1.3689274150095168; val_accuracy: 0.5237858280254777 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.45; acc: 0.48
Batch: 20; loss: 1.54; acc: 0.47
Batch: 40; loss: 1.38; acc: 0.59
Batch: 60; loss: 1.62; acc: 0.45
Batch: 80; loss: 1.34; acc: 0.5
Batch: 100; loss: 1.3; acc: 0.64
Batch: 120; loss: 1.32; acc: 0.56
Batch: 140; loss: 1.15; acc: 0.61
Batch: 160; loss: 1.24; acc: 0.53
Batch: 180; loss: 1.24; acc: 0.59
Batch: 200; loss: 1.5; acc: 0.42
Batch: 220; loss: 1.38; acc: 0.53
Batch: 240; loss: 1.42; acc: 0.5
Batch: 260; loss: 1.29; acc: 0.56
Batch: 280; loss: 1.34; acc: 0.5
Batch: 300; loss: 1.43; acc: 0.5
Batch: 320; loss: 1.31; acc: 0.58
Batch: 340; loss: 1.33; acc: 0.52
Batch: 360; loss: 1.5; acc: 0.5
Batch: 380; loss: 1.38; acc: 0.55
Batch: 400; loss: 1.3; acc: 0.53
Batch: 420; loss: 1.29; acc: 0.64
Batch: 440; loss: 1.25; acc: 0.56
Batch: 460; loss: 1.24; acc: 0.52
Batch: 480; loss: 1.54; acc: 0.5
Batch: 500; loss: 1.45; acc: 0.48
Batch: 520; loss: 1.16; acc: 0.61
Batch: 540; loss: 1.25; acc: 0.58
Batch: 560; loss: 1.44; acc: 0.56
Batch: 580; loss: 1.24; acc: 0.62
Batch: 600; loss: 1.51; acc: 0.47
Batch: 620; loss: 1.05; acc: 0.67
Batch: 640; loss: 1.08; acc: 0.64
Batch: 660; loss: 1.36; acc: 0.53
Batch: 680; loss: 1.47; acc: 0.47
Batch: 700; loss: 1.28; acc: 0.62
Batch: 720; loss: 1.28; acc: 0.56
Batch: 740; loss: 1.01; acc: 0.64
Batch: 760; loss: 1.14; acc: 0.61
Batch: 780; loss: 1.19; acc: 0.56
Train Epoch over. train_loss: 1.34; train_accuracy: 0.55 

Batch: 0; loss: 1.4; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.59
Batch: 40; loss: 0.9; acc: 0.7
Batch: 60; loss: 1.29; acc: 0.56
Batch: 80; loss: 0.96; acc: 0.61
Batch: 100; loss: 1.32; acc: 0.5
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 1.12; acc: 0.62
Val Epoch over. val_loss: 1.2421949839895698; val_accuracy: 0.5870820063694268 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.59
Batch: 20; loss: 1.33; acc: 0.55
Batch: 40; loss: 1.08; acc: 0.66
Batch: 60; loss: 1.58; acc: 0.47
Batch: 80; loss: 1.07; acc: 0.7
Batch: 100; loss: 1.28; acc: 0.55
Batch: 120; loss: 1.58; acc: 0.52
Batch: 140; loss: 1.5; acc: 0.55
Batch: 160; loss: 1.3; acc: 0.52
Batch: 180; loss: 1.02; acc: 0.61
Batch: 200; loss: 1.47; acc: 0.56
Batch: 220; loss: 1.5; acc: 0.47
Batch: 240; loss: 1.38; acc: 0.52
Batch: 260; loss: 1.44; acc: 0.55
Batch: 280; loss: 1.71; acc: 0.42
Batch: 300; loss: 1.27; acc: 0.61
Batch: 320; loss: 1.28; acc: 0.56
Batch: 340; loss: 1.12; acc: 0.66
Batch: 360; loss: 1.19; acc: 0.58
Batch: 380; loss: 1.39; acc: 0.59
Batch: 400; loss: 1.2; acc: 0.58
Batch: 420; loss: 1.5; acc: 0.47
Batch: 440; loss: 1.42; acc: 0.52
Batch: 460; loss: 1.47; acc: 0.5
Batch: 480; loss: 1.46; acc: 0.56
Batch: 500; loss: 1.31; acc: 0.62
Batch: 520; loss: 1.39; acc: 0.58
Batch: 540; loss: 0.97; acc: 0.66
Batch: 560; loss: 1.38; acc: 0.56
Batch: 580; loss: 1.07; acc: 0.69
Batch: 600; loss: 1.23; acc: 0.56
Batch: 620; loss: 1.23; acc: 0.64
Batch: 640; loss: 1.37; acc: 0.47
Batch: 660; loss: 1.17; acc: 0.64
Batch: 680; loss: 1.38; acc: 0.52
Batch: 700; loss: 1.31; acc: 0.62
Batch: 720; loss: 1.2; acc: 0.56
Batch: 740; loss: 1.04; acc: 0.69
Batch: 760; loss: 1.18; acc: 0.61
Batch: 780; loss: 1.19; acc: 0.61
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.37; acc: 0.56
Batch: 20; loss: 1.28; acc: 0.61
Batch: 40; loss: 0.92; acc: 0.72
Batch: 60; loss: 1.31; acc: 0.55
Batch: 80; loss: 0.93; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.24; acc: 0.62
Val Epoch over. val_loss: 1.2202874186691965; val_accuracy: 0.6000199044585988 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.58
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 1.31; acc: 0.55
Batch: 60; loss: 1.19; acc: 0.59
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.22; acc: 0.53
Batch: 120; loss: 1.4; acc: 0.55
Batch: 140; loss: 1.04; acc: 0.64
Batch: 160; loss: 1.25; acc: 0.64
Batch: 180; loss: 1.25; acc: 0.55
Batch: 200; loss: 1.56; acc: 0.44
Batch: 220; loss: 1.19; acc: 0.64
Batch: 240; loss: 1.22; acc: 0.61
Batch: 260; loss: 1.51; acc: 0.5
Batch: 280; loss: 1.13; acc: 0.66
Batch: 300; loss: 1.39; acc: 0.59
Batch: 320; loss: 1.39; acc: 0.45
Batch: 340; loss: 1.26; acc: 0.55
Batch: 360; loss: 1.33; acc: 0.58
Batch: 380; loss: 1.61; acc: 0.48
Batch: 400; loss: 1.04; acc: 0.61
Batch: 420; loss: 1.19; acc: 0.61
Batch: 440; loss: 1.26; acc: 0.61
Batch: 460; loss: 1.21; acc: 0.67
Batch: 480; loss: 1.27; acc: 0.59
Batch: 500; loss: 1.19; acc: 0.66
Batch: 520; loss: 1.16; acc: 0.62
Batch: 540; loss: 1.34; acc: 0.58
Batch: 560; loss: 1.38; acc: 0.48
Batch: 580; loss: 1.41; acc: 0.53
Batch: 600; loss: 1.48; acc: 0.5
Batch: 620; loss: 1.18; acc: 0.61
Batch: 640; loss: 1.17; acc: 0.67
Batch: 660; loss: 1.42; acc: 0.52
Batch: 680; loss: 1.37; acc: 0.58
Batch: 700; loss: 1.21; acc: 0.53
Batch: 720; loss: 1.23; acc: 0.55
Batch: 740; loss: 1.21; acc: 0.55
Batch: 760; loss: 1.27; acc: 0.59
Batch: 780; loss: 1.14; acc: 0.64
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.33; acc: 0.53
Batch: 20; loss: 1.22; acc: 0.61
Batch: 40; loss: 0.94; acc: 0.66
Batch: 60; loss: 1.27; acc: 0.58
Batch: 80; loss: 0.97; acc: 0.66
Batch: 100; loss: 1.3; acc: 0.53
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.24; acc: 0.56
Val Epoch over. val_loss: 1.2045406581489904; val_accuracy: 0.6004179936305732 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.48
Batch: 20; loss: 1.41; acc: 0.53
Batch: 40; loss: 1.25; acc: 0.61
Batch: 60; loss: 1.25; acc: 0.53
Batch: 80; loss: 1.26; acc: 0.66
Batch: 100; loss: 1.25; acc: 0.61
Batch: 120; loss: 1.21; acc: 0.58
Batch: 140; loss: 1.14; acc: 0.64
Batch: 160; loss: 1.31; acc: 0.58
Batch: 180; loss: 1.44; acc: 0.52
Batch: 200; loss: 1.34; acc: 0.62
Batch: 220; loss: 1.24; acc: 0.62
Batch: 240; loss: 1.26; acc: 0.58
Batch: 260; loss: 1.42; acc: 0.52
Batch: 280; loss: 1.38; acc: 0.52
Batch: 300; loss: 1.22; acc: 0.67
Batch: 320; loss: 1.31; acc: 0.59
Batch: 340; loss: 1.18; acc: 0.62
Batch: 360; loss: 1.49; acc: 0.47
Batch: 380; loss: 1.13; acc: 0.64
Batch: 400; loss: 0.92; acc: 0.7
Batch: 420; loss: 1.32; acc: 0.53
Batch: 440; loss: 1.22; acc: 0.52
Batch: 460; loss: 1.11; acc: 0.64
Batch: 480; loss: 1.49; acc: 0.48
Batch: 500; loss: 1.47; acc: 0.53
Batch: 520; loss: 1.07; acc: 0.53
Batch: 540; loss: 1.07; acc: 0.67
Batch: 560; loss: 1.43; acc: 0.52
Batch: 580; loss: 1.25; acc: 0.72
Batch: 600; loss: 1.38; acc: 0.52
Batch: 620; loss: 1.42; acc: 0.64
Batch: 640; loss: 1.08; acc: 0.67
Batch: 660; loss: 1.36; acc: 0.56
Batch: 680; loss: 1.08; acc: 0.66
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 1.18; acc: 0.61
Batch: 740; loss: 1.24; acc: 0.59
Batch: 760; loss: 1.15; acc: 0.58
Batch: 780; loss: 1.28; acc: 0.52
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.33; acc: 0.55
Batch: 20; loss: 1.24; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.64
Batch: 60; loss: 1.31; acc: 0.58
Batch: 80; loss: 1.01; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.18; acc: 0.69
Batch: 140; loss: 1.23; acc: 0.56
Val Epoch over. val_loss: 1.2112988461354735; val_accuracy: 0.5977308917197452 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.44
Batch: 20; loss: 1.47; acc: 0.48
Batch: 40; loss: 1.29; acc: 0.58
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.07; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.61
Batch: 140; loss: 1.18; acc: 0.55
Batch: 160; loss: 1.18; acc: 0.61
Batch: 180; loss: 1.22; acc: 0.61
Batch: 200; loss: 1.35; acc: 0.58
Batch: 220; loss: 0.87; acc: 0.75
Batch: 240; loss: 1.2; acc: 0.7
Batch: 260; loss: 1.08; acc: 0.66
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 1.35; acc: 0.53
Batch: 320; loss: 1.12; acc: 0.61
Batch: 340; loss: 1.47; acc: 0.59
Batch: 360; loss: 1.25; acc: 0.61
Batch: 380; loss: 1.32; acc: 0.48
Batch: 400; loss: 1.09; acc: 0.62
Batch: 420; loss: 1.19; acc: 0.61
Batch: 440; loss: 1.23; acc: 0.58
Batch: 460; loss: 1.22; acc: 0.55
Batch: 480; loss: 1.42; acc: 0.53
Batch: 500; loss: 1.1; acc: 0.67
Batch: 520; loss: 1.31; acc: 0.55
Batch: 540; loss: 1.18; acc: 0.59
Batch: 560; loss: 1.08; acc: 0.67
Batch: 580; loss: 1.36; acc: 0.56
Batch: 600; loss: 1.33; acc: 0.56
Batch: 620; loss: 1.49; acc: 0.55
Batch: 640; loss: 1.22; acc: 0.55
Batch: 660; loss: 1.22; acc: 0.59
Batch: 680; loss: 1.33; acc: 0.5
Batch: 700; loss: 1.17; acc: 0.64
Batch: 720; loss: 1.11; acc: 0.67
Batch: 740; loss: 1.19; acc: 0.58
Batch: 760; loss: 1.25; acc: 0.56
Batch: 780; loss: 1.21; acc: 0.56
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.55
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 0.91; acc: 0.69
Batch: 60; loss: 1.31; acc: 0.58
Batch: 80; loss: 1.0; acc: 0.69
Batch: 100; loss: 1.26; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.59
Val Epoch over. val_loss: 1.2022382263924665; val_accuracy: 0.6057921974522293 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.66
Batch: 20; loss: 1.04; acc: 0.58
Batch: 40; loss: 1.49; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.16; acc: 0.66
Batch: 100; loss: 1.41; acc: 0.53
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 1.44; acc: 0.55
Batch: 160; loss: 1.17; acc: 0.58
Batch: 180; loss: 1.15; acc: 0.59
Batch: 200; loss: 1.22; acc: 0.64
Batch: 220; loss: 1.28; acc: 0.52
Batch: 240; loss: 1.22; acc: 0.55
Batch: 260; loss: 1.19; acc: 0.59
Batch: 280; loss: 1.16; acc: 0.59
Batch: 300; loss: 1.33; acc: 0.58
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.19; acc: 0.58
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.16; acc: 0.59
Batch: 400; loss: 1.38; acc: 0.53
Batch: 420; loss: 1.28; acc: 0.58
Batch: 440; loss: 1.34; acc: 0.53
Batch: 460; loss: 1.59; acc: 0.48
Batch: 480; loss: 1.4; acc: 0.56
Batch: 500; loss: 1.36; acc: 0.58
Batch: 520; loss: 0.96; acc: 0.7
Batch: 540; loss: 1.37; acc: 0.56
Batch: 560; loss: 1.28; acc: 0.53
Batch: 580; loss: 1.24; acc: 0.53
Batch: 600; loss: 1.35; acc: 0.61
Batch: 620; loss: 1.08; acc: 0.58
Batch: 640; loss: 0.94; acc: 0.69
Batch: 660; loss: 1.37; acc: 0.58
Batch: 680; loss: 1.36; acc: 0.47
Batch: 700; loss: 1.13; acc: 0.64
Batch: 720; loss: 1.22; acc: 0.61
Batch: 740; loss: 1.28; acc: 0.58
Batch: 760; loss: 1.41; acc: 0.5
Batch: 780; loss: 1.34; acc: 0.47
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.27; acc: 0.59
Batch: 20; loss: 1.25; acc: 0.58
Batch: 40; loss: 0.91; acc: 0.67
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.01; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.24; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1994379174177814; val_accuracy: 0.6065883757961783 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.36; acc: 0.58
Batch: 20; loss: 1.29; acc: 0.61
Batch: 40; loss: 1.33; acc: 0.61
Batch: 60; loss: 1.08; acc: 0.67
Batch: 80; loss: 1.33; acc: 0.52
Batch: 100; loss: 1.62; acc: 0.52
Batch: 120; loss: 1.47; acc: 0.52
Batch: 140; loss: 1.52; acc: 0.61
Batch: 160; loss: 1.14; acc: 0.55
Batch: 180; loss: 1.44; acc: 0.42
Batch: 200; loss: 1.22; acc: 0.56
Batch: 220; loss: 1.38; acc: 0.55
Batch: 240; loss: 1.32; acc: 0.58
Batch: 260; loss: 1.15; acc: 0.59
Batch: 280; loss: 1.03; acc: 0.64
Batch: 300; loss: 1.34; acc: 0.66
Batch: 320; loss: 0.98; acc: 0.67
Batch: 340; loss: 1.21; acc: 0.62
Batch: 360; loss: 1.15; acc: 0.64
Batch: 380; loss: 1.28; acc: 0.62
Batch: 400; loss: 1.22; acc: 0.58
Batch: 420; loss: 1.25; acc: 0.62
Batch: 440; loss: 1.45; acc: 0.52
Batch: 460; loss: 1.39; acc: 0.55
Batch: 480; loss: 1.0; acc: 0.7
Batch: 500; loss: 1.26; acc: 0.55
Batch: 520; loss: 1.3; acc: 0.61
Batch: 540; loss: 1.2; acc: 0.55
Batch: 560; loss: 1.08; acc: 0.59
Batch: 580; loss: 1.35; acc: 0.48
Batch: 600; loss: 1.07; acc: 0.61
Batch: 620; loss: 1.15; acc: 0.58
Batch: 640; loss: 1.33; acc: 0.58
Batch: 660; loss: 1.33; acc: 0.58
Batch: 680; loss: 1.14; acc: 0.55
Batch: 700; loss: 1.05; acc: 0.67
Batch: 720; loss: 1.21; acc: 0.56
Batch: 740; loss: 1.42; acc: 0.55
Batch: 760; loss: 1.35; acc: 0.58
Batch: 780; loss: 1.22; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.26; acc: 0.59
Batch: 20; loss: 1.23; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.67
Batch: 60; loss: 1.3; acc: 0.59
Batch: 80; loss: 1.01; acc: 0.69
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.62
Val Epoch over. val_loss: 1.1883382683346986; val_accuracy: 0.6097730891719745 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.29; acc: 0.53
Batch: 20; loss: 1.34; acc: 0.59
Batch: 40; loss: 1.49; acc: 0.52
Batch: 60; loss: 1.3; acc: 0.59
Batch: 80; loss: 1.25; acc: 0.58
Batch: 100; loss: 1.13; acc: 0.62
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 1.26; acc: 0.61
Batch: 160; loss: 1.27; acc: 0.53
Batch: 180; loss: 1.52; acc: 0.47
Batch: 200; loss: 1.35; acc: 0.58
Batch: 220; loss: 1.07; acc: 0.67
Batch: 240; loss: 1.34; acc: 0.62
Batch: 260; loss: 1.18; acc: 0.64
Batch: 280; loss: 1.29; acc: 0.61
Batch: 300; loss: 1.44; acc: 0.52
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.32; acc: 0.61
Batch: 360; loss: 1.27; acc: 0.53
Batch: 380; loss: 1.41; acc: 0.52
Batch: 400; loss: 1.34; acc: 0.52
Batch: 420; loss: 1.32; acc: 0.55
Batch: 440; loss: 1.21; acc: 0.59
Batch: 460; loss: 1.13; acc: 0.62
Batch: 480; loss: 1.29; acc: 0.55
Batch: 500; loss: 1.0; acc: 0.69
Batch: 520; loss: 1.13; acc: 0.64
Batch: 540; loss: 1.27; acc: 0.59
Batch: 560; loss: 1.46; acc: 0.56
Batch: 580; loss: 1.34; acc: 0.52
Batch: 600; loss: 1.23; acc: 0.59
Batch: 620; loss: 1.27; acc: 0.61
Batch: 640; loss: 1.38; acc: 0.56
Batch: 660; loss: 1.34; acc: 0.59
Batch: 680; loss: 1.32; acc: 0.61
Batch: 700; loss: 1.32; acc: 0.55
Batch: 720; loss: 1.29; acc: 0.61
Batch: 740; loss: 1.35; acc: 0.5
Batch: 760; loss: 1.37; acc: 0.58
Batch: 780; loss: 1.43; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.24; acc: 0.62
Batch: 20; loss: 1.22; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.67
Batch: 60; loss: 1.27; acc: 0.59
Batch: 80; loss: 1.01; acc: 0.67
Batch: 100; loss: 1.25; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 1.28; acc: 0.56
Val Epoch over. val_loss: 1.1901118835066533; val_accuracy: 0.6096735668789809 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.34; acc: 0.52
Batch: 20; loss: 1.24; acc: 0.66
Batch: 40; loss: 1.17; acc: 0.59
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.4; acc: 0.56
Batch: 100; loss: 1.05; acc: 0.55
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 1.19; acc: 0.64
Batch: 160; loss: 1.19; acc: 0.56
Batch: 180; loss: 1.16; acc: 0.62
Batch: 200; loss: 1.2; acc: 0.58
Batch: 220; loss: 1.19; acc: 0.67
Batch: 240; loss: 1.2; acc: 0.55
Batch: 260; loss: 1.14; acc: 0.66
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 1.09; acc: 0.67
Batch: 320; loss: 1.18; acc: 0.64
Batch: 340; loss: 1.08; acc: 0.66
Batch: 360; loss: 1.02; acc: 0.66
Batch: 380; loss: 0.87; acc: 0.72
Batch: 400; loss: 1.35; acc: 0.61
Batch: 420; loss: 1.2; acc: 0.62
Batch: 440; loss: 1.27; acc: 0.62
Batch: 460; loss: 1.35; acc: 0.56
Batch: 480; loss: 1.03; acc: 0.7
Batch: 500; loss: 1.11; acc: 0.62
Batch: 520; loss: 1.05; acc: 0.72
Batch: 540; loss: 1.2; acc: 0.58
Batch: 560; loss: 1.22; acc: 0.52
Batch: 580; loss: 1.07; acc: 0.69
Batch: 600; loss: 1.4; acc: 0.59
Batch: 620; loss: 1.21; acc: 0.56
Batch: 640; loss: 1.24; acc: 0.61
Batch: 660; loss: 1.35; acc: 0.52
Batch: 680; loss: 0.94; acc: 0.7
Batch: 700; loss: 1.02; acc: 0.7
Batch: 720; loss: 1.43; acc: 0.52
Batch: 740; loss: 1.59; acc: 0.47
Batch: 760; loss: 1.25; acc: 0.55
Batch: 780; loss: 1.29; acc: 0.5
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.64
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.3; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.188326119617292; val_accuracy: 0.6106687898089171 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.55; acc: 0.58
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 1.37; acc: 0.55
Batch: 60; loss: 1.15; acc: 0.62
Batch: 80; loss: 1.27; acc: 0.64
Batch: 100; loss: 1.31; acc: 0.53
Batch: 120; loss: 1.37; acc: 0.55
Batch: 140; loss: 1.74; acc: 0.45
Batch: 160; loss: 1.18; acc: 0.64
Batch: 180; loss: 1.15; acc: 0.64
Batch: 200; loss: 1.49; acc: 0.52
Batch: 220; loss: 1.2; acc: 0.72
Batch: 240; loss: 1.19; acc: 0.66
Batch: 260; loss: 1.21; acc: 0.55
Batch: 280; loss: 1.27; acc: 0.55
Batch: 300; loss: 1.31; acc: 0.64
Batch: 320; loss: 0.98; acc: 0.62
Batch: 340; loss: 1.38; acc: 0.52
Batch: 360; loss: 1.39; acc: 0.53
Batch: 380; loss: 1.19; acc: 0.61
Batch: 400; loss: 1.28; acc: 0.61
Batch: 420; loss: 1.14; acc: 0.62
Batch: 440; loss: 1.14; acc: 0.67
Batch: 460; loss: 1.29; acc: 0.56
Batch: 480; loss: 1.26; acc: 0.62
Batch: 500; loss: 1.36; acc: 0.48
Batch: 520; loss: 1.3; acc: 0.55
Batch: 540; loss: 1.02; acc: 0.66
Batch: 560; loss: 1.4; acc: 0.53
Batch: 580; loss: 1.38; acc: 0.61
Batch: 600; loss: 1.06; acc: 0.69
Batch: 620; loss: 1.13; acc: 0.64
Batch: 640; loss: 1.25; acc: 0.58
Batch: 660; loss: 0.99; acc: 0.69
Batch: 680; loss: 1.32; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.55
Batch: 720; loss: 1.49; acc: 0.48
Batch: 740; loss: 1.51; acc: 0.5
Batch: 760; loss: 1.0; acc: 0.61
Batch: 780; loss: 1.23; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.17; acc: 0.62
Batch: 40; loss: 0.9; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.62
Val Epoch over. val_loss: 1.1871674550566704; val_accuracy: 0.6129578025477707 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.17; acc: 0.59
Batch: 40; loss: 1.14; acc: 0.59
Batch: 60; loss: 1.29; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.47
Batch: 100; loss: 1.24; acc: 0.62
Batch: 120; loss: 1.25; acc: 0.62
Batch: 140; loss: 1.17; acc: 0.56
Batch: 160; loss: 1.17; acc: 0.58
Batch: 180; loss: 0.91; acc: 0.7
Batch: 200; loss: 1.46; acc: 0.48
Batch: 220; loss: 1.35; acc: 0.59
Batch: 240; loss: 1.28; acc: 0.56
Batch: 260; loss: 1.06; acc: 0.59
Batch: 280; loss: 1.14; acc: 0.67
Batch: 300; loss: 1.4; acc: 0.53
Batch: 320; loss: 1.2; acc: 0.61
Batch: 340; loss: 1.29; acc: 0.47
Batch: 360; loss: 1.21; acc: 0.53
Batch: 380; loss: 1.13; acc: 0.66
Batch: 400; loss: 1.23; acc: 0.59
Batch: 420; loss: 1.27; acc: 0.58
Batch: 440; loss: 1.1; acc: 0.56
Batch: 460; loss: 1.11; acc: 0.64
Batch: 480; loss: 1.34; acc: 0.55
Batch: 500; loss: 1.21; acc: 0.55
Batch: 520; loss: 1.1; acc: 0.64
Batch: 540; loss: 1.14; acc: 0.62
Batch: 560; loss: 1.41; acc: 0.61
Batch: 580; loss: 1.31; acc: 0.59
Batch: 600; loss: 1.24; acc: 0.58
Batch: 620; loss: 1.37; acc: 0.55
Batch: 640; loss: 0.96; acc: 0.62
Batch: 660; loss: 1.2; acc: 0.61
Batch: 680; loss: 1.08; acc: 0.62
Batch: 700; loss: 1.1; acc: 0.61
Batch: 720; loss: 1.39; acc: 0.52
Batch: 740; loss: 1.17; acc: 0.66
Batch: 760; loss: 1.08; acc: 0.62
Batch: 780; loss: 1.33; acc: 0.55
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.66
Batch: 60; loss: 1.27; acc: 0.62
Batch: 80; loss: 1.02; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.58
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 1.27; acc: 0.59
Val Epoch over. val_loss: 1.185463887491044; val_accuracy: 0.6141520700636943 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 1.13; acc: 0.58
Batch: 40; loss: 1.29; acc: 0.53
Batch: 60; loss: 1.44; acc: 0.53
Batch: 80; loss: 1.18; acc: 0.55
Batch: 100; loss: 1.24; acc: 0.56
Batch: 120; loss: 1.14; acc: 0.62
Batch: 140; loss: 1.03; acc: 0.66
Batch: 160; loss: 0.95; acc: 0.67
Batch: 180; loss: 1.27; acc: 0.66
Batch: 200; loss: 1.28; acc: 0.66
Batch: 220; loss: 1.16; acc: 0.62
Batch: 240; loss: 1.04; acc: 0.66
Batch: 260; loss: 1.24; acc: 0.62
Batch: 280; loss: 1.15; acc: 0.59
Batch: 300; loss: 1.48; acc: 0.5
Batch: 320; loss: 0.96; acc: 0.75
Batch: 340; loss: 1.34; acc: 0.56
Batch: 360; loss: 1.35; acc: 0.53
Batch: 380; loss: 1.32; acc: 0.58
Batch: 400; loss: 1.02; acc: 0.64
Batch: 420; loss: 1.27; acc: 0.53
Batch: 440; loss: 1.28; acc: 0.64
Batch: 460; loss: 1.42; acc: 0.48
Batch: 480; loss: 1.33; acc: 0.55
Batch: 500; loss: 1.31; acc: 0.58
Batch: 520; loss: 1.2; acc: 0.56
Batch: 540; loss: 1.19; acc: 0.61
Batch: 560; loss: 1.39; acc: 0.5
Batch: 580; loss: 1.26; acc: 0.64
Batch: 600; loss: 1.22; acc: 0.61
Batch: 620; loss: 1.33; acc: 0.56
Batch: 640; loss: 1.58; acc: 0.45
Batch: 660; loss: 1.76; acc: 0.39
Batch: 680; loss: 1.36; acc: 0.55
Batch: 700; loss: 1.39; acc: 0.61
Batch: 720; loss: 1.04; acc: 0.67
Batch: 740; loss: 1.18; acc: 0.56
Batch: 760; loss: 1.12; acc: 0.61
Batch: 780; loss: 1.25; acc: 0.5
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.66
Batch: 60; loss: 1.29; acc: 0.59
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.24; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1875067417788658; val_accuracy: 0.6125597133757962 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.13; acc: 0.61
Batch: 20; loss: 1.14; acc: 0.55
Batch: 40; loss: 1.54; acc: 0.47
Batch: 60; loss: 1.39; acc: 0.53
Batch: 80; loss: 1.21; acc: 0.62
Batch: 100; loss: 1.34; acc: 0.52
Batch: 120; loss: 1.09; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.59
Batch: 160; loss: 1.06; acc: 0.61
Batch: 180; loss: 1.25; acc: 0.64
Batch: 200; loss: 1.58; acc: 0.5
Batch: 220; loss: 1.39; acc: 0.53
Batch: 240; loss: 1.07; acc: 0.62
Batch: 260; loss: 1.28; acc: 0.59
Batch: 280; loss: 1.29; acc: 0.55
Batch: 300; loss: 1.32; acc: 0.66
Batch: 320; loss: 0.96; acc: 0.61
Batch: 340; loss: 1.5; acc: 0.5
Batch: 360; loss: 1.23; acc: 0.58
Batch: 380; loss: 1.28; acc: 0.56
Batch: 400; loss: 1.13; acc: 0.72
Batch: 420; loss: 1.35; acc: 0.53
Batch: 440; loss: 1.36; acc: 0.58
Batch: 460; loss: 1.3; acc: 0.58
Batch: 480; loss: 1.3; acc: 0.52
Batch: 500; loss: 1.01; acc: 0.7
Batch: 520; loss: 1.12; acc: 0.59
Batch: 540; loss: 1.11; acc: 0.66
Batch: 560; loss: 1.28; acc: 0.56
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.42; acc: 0.53
Batch: 620; loss: 1.44; acc: 0.53
Batch: 640; loss: 1.52; acc: 0.55
Batch: 660; loss: 1.33; acc: 0.59
Batch: 680; loss: 1.29; acc: 0.59
Batch: 700; loss: 1.49; acc: 0.5
Batch: 720; loss: 1.28; acc: 0.55
Batch: 740; loss: 1.15; acc: 0.64
Batch: 760; loss: 1.21; acc: 0.67
Batch: 780; loss: 1.51; acc: 0.5
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.19; acc: 0.61
Batch: 40; loss: 0.9; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 1.31; acc: 0.59
Val Epoch over. val_loss: 1.185999929145643; val_accuracy: 0.6153463375796179 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.18; acc: 0.61
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.99; acc: 0.72
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 1.21; acc: 0.67
Batch: 100; loss: 1.32; acc: 0.53
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 1.19; acc: 0.64
Batch: 160; loss: 1.3; acc: 0.58
Batch: 180; loss: 1.17; acc: 0.58
Batch: 200; loss: 1.35; acc: 0.67
Batch: 220; loss: 1.07; acc: 0.62
Batch: 240; loss: 1.07; acc: 0.7
Batch: 260; loss: 1.31; acc: 0.52
Batch: 280; loss: 1.3; acc: 0.59
Batch: 300; loss: 1.22; acc: 0.61
Batch: 320; loss: 1.36; acc: 0.59
Batch: 340; loss: 0.92; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.66
Batch: 380; loss: 1.47; acc: 0.56
Batch: 400; loss: 1.29; acc: 0.62
Batch: 420; loss: 1.25; acc: 0.53
Batch: 440; loss: 1.04; acc: 0.62
Batch: 460; loss: 1.47; acc: 0.47
Batch: 480; loss: 1.47; acc: 0.53
Batch: 500; loss: 1.1; acc: 0.64
Batch: 520; loss: 1.48; acc: 0.58
Batch: 540; loss: 1.01; acc: 0.69
Batch: 560; loss: 1.31; acc: 0.62
Batch: 580; loss: 1.08; acc: 0.67
Batch: 600; loss: 1.32; acc: 0.55
Batch: 620; loss: 1.23; acc: 0.53
Batch: 640; loss: 1.28; acc: 0.62
Batch: 660; loss: 1.37; acc: 0.58
Batch: 680; loss: 1.12; acc: 0.66
Batch: 700; loss: 1.28; acc: 0.58
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.29; acc: 0.58
Batch: 760; loss: 1.11; acc: 0.64
Batch: 780; loss: 1.11; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.26; acc: 0.56
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.64
Batch: 60; loss: 1.31; acc: 0.64
Batch: 80; loss: 1.06; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 1.32; acc: 0.56
Val Epoch over. val_loss: 1.1902980257751077; val_accuracy: 0.6090764331210191 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.49; acc: 0.5
Batch: 20; loss: 1.18; acc: 0.61
Batch: 40; loss: 1.17; acc: 0.55
Batch: 60; loss: 1.3; acc: 0.56
Batch: 80; loss: 1.4; acc: 0.52
Batch: 100; loss: 1.21; acc: 0.56
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 1.22; acc: 0.59
Batch: 160; loss: 1.24; acc: 0.62
Batch: 180; loss: 1.22; acc: 0.61
Batch: 200; loss: 1.39; acc: 0.56
Batch: 220; loss: 1.57; acc: 0.47
Batch: 240; loss: 1.49; acc: 0.55
Batch: 260; loss: 0.96; acc: 0.69
Batch: 280; loss: 1.39; acc: 0.58
Batch: 300; loss: 1.21; acc: 0.56
Batch: 320; loss: 1.38; acc: 0.56
Batch: 340; loss: 1.14; acc: 0.67
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 1.22; acc: 0.59
Batch: 400; loss: 1.28; acc: 0.58
Batch: 420; loss: 1.4; acc: 0.59
Batch: 440; loss: 1.17; acc: 0.62
Batch: 460; loss: 1.21; acc: 0.7
Batch: 480; loss: 1.0; acc: 0.59
Batch: 500; loss: 1.21; acc: 0.59
Batch: 520; loss: 1.22; acc: 0.59
Batch: 540; loss: 1.6; acc: 0.55
Batch: 560; loss: 1.35; acc: 0.59
Batch: 580; loss: 1.34; acc: 0.55
Batch: 600; loss: 1.16; acc: 0.59
Batch: 620; loss: 1.18; acc: 0.59
Batch: 640; loss: 1.4; acc: 0.56
Batch: 660; loss: 1.23; acc: 0.58
Batch: 680; loss: 1.1; acc: 0.64
Batch: 700; loss: 1.16; acc: 0.67
Batch: 720; loss: 1.45; acc: 0.48
Batch: 740; loss: 1.51; acc: 0.61
Batch: 760; loss: 1.28; acc: 0.56
Batch: 780; loss: 1.32; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.31; acc: 0.61
Val Epoch over. val_loss: 1.1847406747234854; val_accuracy: 0.6118630573248408 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.72
Batch: 20; loss: 1.53; acc: 0.45
Batch: 40; loss: 1.16; acc: 0.61
Batch: 60; loss: 1.02; acc: 0.62
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.2; acc: 0.56
Batch: 120; loss: 1.27; acc: 0.56
Batch: 140; loss: 0.87; acc: 0.72
Batch: 160; loss: 1.4; acc: 0.59
Batch: 180; loss: 1.19; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.53
Batch: 220; loss: 1.18; acc: 0.61
Batch: 240; loss: 1.24; acc: 0.53
Batch: 260; loss: 1.07; acc: 0.66
Batch: 280; loss: 1.36; acc: 0.52
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.27; acc: 0.58
Batch: 340; loss: 1.25; acc: 0.64
Batch: 360; loss: 1.3; acc: 0.55
Batch: 380; loss: 1.25; acc: 0.55
Batch: 400; loss: 1.14; acc: 0.66
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 1.09; acc: 0.7
Batch: 460; loss: 1.29; acc: 0.64
Batch: 480; loss: 1.44; acc: 0.55
Batch: 500; loss: 1.41; acc: 0.48
Batch: 520; loss: 1.15; acc: 0.61
Batch: 540; loss: 1.08; acc: 0.62
Batch: 560; loss: 1.26; acc: 0.55
Batch: 580; loss: 1.21; acc: 0.58
Batch: 600; loss: 1.14; acc: 0.64
Batch: 620; loss: 1.03; acc: 0.72
Batch: 640; loss: 1.36; acc: 0.55
Batch: 660; loss: 1.14; acc: 0.62
Batch: 680; loss: 1.24; acc: 0.61
Batch: 700; loss: 1.09; acc: 0.62
Batch: 720; loss: 1.13; acc: 0.55
Batch: 740; loss: 1.25; acc: 0.62
Batch: 760; loss: 1.12; acc: 0.64
Batch: 780; loss: 1.19; acc: 0.67
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.24; acc: 0.59
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.87; acc: 0.64
Batch: 60; loss: 1.3; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.59
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.31; acc: 0.58
Val Epoch over. val_loss: 1.186293593637503; val_accuracy: 0.6122611464968153 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.37; acc: 0.59
Batch: 20; loss: 1.43; acc: 0.47
Batch: 40; loss: 1.12; acc: 0.66
Batch: 60; loss: 1.17; acc: 0.59
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.4; acc: 0.58
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.69; acc: 0.42
Batch: 160; loss: 1.31; acc: 0.61
Batch: 180; loss: 1.2; acc: 0.62
Batch: 200; loss: 1.14; acc: 0.59
Batch: 220; loss: 1.75; acc: 0.5
Batch: 240; loss: 1.19; acc: 0.62
Batch: 260; loss: 1.24; acc: 0.56
Batch: 280; loss: 1.11; acc: 0.62
Batch: 300; loss: 1.15; acc: 0.61
Batch: 320; loss: 1.34; acc: 0.55
Batch: 340; loss: 1.3; acc: 0.58
Batch: 360; loss: 1.31; acc: 0.53
Batch: 380; loss: 1.14; acc: 0.62
Batch: 400; loss: 1.34; acc: 0.56
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 1.19; acc: 0.62
Batch: 460; loss: 1.33; acc: 0.56
Batch: 480; loss: 1.44; acc: 0.52
Batch: 500; loss: 1.36; acc: 0.58
Batch: 520; loss: 1.3; acc: 0.56
Batch: 540; loss: 1.1; acc: 0.61
Batch: 560; loss: 1.41; acc: 0.55
Batch: 580; loss: 1.07; acc: 0.61
Batch: 600; loss: 1.3; acc: 0.56
Batch: 620; loss: 1.25; acc: 0.52
Batch: 640; loss: 1.54; acc: 0.48
Batch: 660; loss: 1.47; acc: 0.55
Batch: 680; loss: 1.43; acc: 0.48
Batch: 700; loss: 1.12; acc: 0.62
Batch: 720; loss: 1.51; acc: 0.53
Batch: 740; loss: 1.4; acc: 0.55
Batch: 760; loss: 1.13; acc: 0.62
Batch: 780; loss: 1.3; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.61
Val Epoch over. val_loss: 1.1846467450166205; val_accuracy: 0.6136544585987261 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.22; acc: 0.58
Batch: 20; loss: 1.4; acc: 0.58
Batch: 40; loss: 1.06; acc: 0.66
Batch: 60; loss: 1.44; acc: 0.58
Batch: 80; loss: 1.12; acc: 0.64
Batch: 100; loss: 1.26; acc: 0.58
Batch: 120; loss: 1.31; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.62
Batch: 160; loss: 1.48; acc: 0.53
Batch: 180; loss: 1.29; acc: 0.56
Batch: 200; loss: 1.17; acc: 0.59
Batch: 220; loss: 1.35; acc: 0.52
Batch: 240; loss: 0.94; acc: 0.75
Batch: 260; loss: 1.29; acc: 0.53
Batch: 280; loss: 1.39; acc: 0.58
Batch: 300; loss: 1.3; acc: 0.58
Batch: 320; loss: 1.23; acc: 0.55
Batch: 340; loss: 1.36; acc: 0.52
Batch: 360; loss: 1.28; acc: 0.48
Batch: 380; loss: 1.13; acc: 0.62
Batch: 400; loss: 0.9; acc: 0.69
Batch: 420; loss: 1.18; acc: 0.62
Batch: 440; loss: 1.12; acc: 0.66
Batch: 460; loss: 1.25; acc: 0.5
Batch: 480; loss: 1.63; acc: 0.5
Batch: 500; loss: 1.54; acc: 0.59
Batch: 520; loss: 1.21; acc: 0.67
Batch: 540; loss: 1.28; acc: 0.59
Batch: 560; loss: 1.2; acc: 0.56
Batch: 580; loss: 1.17; acc: 0.56
Batch: 600; loss: 1.15; acc: 0.62
Batch: 620; loss: 1.18; acc: 0.53
Batch: 640; loss: 1.34; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.61
Batch: 680; loss: 1.12; acc: 0.61
Batch: 700; loss: 1.35; acc: 0.61
Batch: 720; loss: 1.06; acc: 0.69
Batch: 740; loss: 1.4; acc: 0.48
Batch: 760; loss: 1.43; acc: 0.55
Batch: 780; loss: 1.19; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1822293153993644; val_accuracy: 0.6155453821656051 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.17; acc: 0.66
Batch: 20; loss: 1.5; acc: 0.52
Batch: 40; loss: 1.35; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.62
Batch: 80; loss: 1.2; acc: 0.59
Batch: 100; loss: 1.13; acc: 0.64
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.94; acc: 0.67
Batch: 160; loss: 1.0; acc: 0.66
Batch: 180; loss: 1.56; acc: 0.45
Batch: 200; loss: 1.11; acc: 0.67
Batch: 220; loss: 1.3; acc: 0.55
Batch: 240; loss: 1.06; acc: 0.62
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 1.45; acc: 0.53
Batch: 300; loss: 1.24; acc: 0.55
Batch: 320; loss: 1.1; acc: 0.64
Batch: 340; loss: 1.25; acc: 0.67
Batch: 360; loss: 1.11; acc: 0.64
Batch: 380; loss: 1.26; acc: 0.67
Batch: 400; loss: 1.42; acc: 0.55
Batch: 420; loss: 1.42; acc: 0.48
Batch: 440; loss: 1.18; acc: 0.62
Batch: 460; loss: 1.14; acc: 0.67
Batch: 480; loss: 1.14; acc: 0.66
Batch: 500; loss: 1.11; acc: 0.64
Batch: 520; loss: 1.07; acc: 0.66
Batch: 540; loss: 1.4; acc: 0.55
Batch: 560; loss: 1.06; acc: 0.59
Batch: 580; loss: 1.3; acc: 0.61
Batch: 600; loss: 1.23; acc: 0.59
Batch: 620; loss: 1.42; acc: 0.55
Batch: 640; loss: 1.32; acc: 0.64
Batch: 660; loss: 1.36; acc: 0.61
Batch: 680; loss: 1.26; acc: 0.59
Batch: 700; loss: 0.93; acc: 0.69
Batch: 720; loss: 1.29; acc: 0.58
Batch: 740; loss: 1.06; acc: 0.61
Batch: 760; loss: 1.28; acc: 0.59
Batch: 780; loss: 1.14; acc: 0.67
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.29; acc: 0.62
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.3; acc: 0.58
Val Epoch over. val_loss: 1.1829856634140015; val_accuracy: 0.6144506369426752 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.12; acc: 0.61
Batch: 20; loss: 1.22; acc: 0.61
Batch: 40; loss: 1.18; acc: 0.58
Batch: 60; loss: 1.04; acc: 0.67
Batch: 80; loss: 1.58; acc: 0.52
Batch: 100; loss: 1.0; acc: 0.64
Batch: 120; loss: 1.48; acc: 0.48
Batch: 140; loss: 1.16; acc: 0.59
Batch: 160; loss: 1.42; acc: 0.56
Batch: 180; loss: 1.32; acc: 0.5
Batch: 200; loss: 1.07; acc: 0.7
Batch: 220; loss: 1.56; acc: 0.52
Batch: 240; loss: 1.28; acc: 0.53
Batch: 260; loss: 1.13; acc: 0.62
Batch: 280; loss: 1.18; acc: 0.67
Batch: 300; loss: 1.23; acc: 0.56
Batch: 320; loss: 0.9; acc: 0.7
Batch: 340; loss: 1.36; acc: 0.5
Batch: 360; loss: 1.51; acc: 0.48
Batch: 380; loss: 1.42; acc: 0.56
Batch: 400; loss: 1.44; acc: 0.52
Batch: 420; loss: 1.28; acc: 0.55
Batch: 440; loss: 1.41; acc: 0.56
Batch: 460; loss: 1.41; acc: 0.5
Batch: 480; loss: 1.18; acc: 0.69
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.01; acc: 0.59
Batch: 540; loss: 1.15; acc: 0.58
Batch: 560; loss: 1.18; acc: 0.62
Batch: 580; loss: 1.36; acc: 0.5
Batch: 600; loss: 1.3; acc: 0.59
Batch: 620; loss: 1.22; acc: 0.58
Batch: 640; loss: 1.01; acc: 0.7
Batch: 660; loss: 1.36; acc: 0.56
Batch: 680; loss: 1.55; acc: 0.34
Batch: 700; loss: 1.4; acc: 0.53
Batch: 720; loss: 1.13; acc: 0.61
Batch: 740; loss: 1.36; acc: 0.59
Batch: 760; loss: 1.25; acc: 0.59
Batch: 780; loss: 1.21; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.26; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1847595750905906; val_accuracy: 0.6137539808917197 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.18; acc: 0.62
Batch: 20; loss: 1.1; acc: 0.66
Batch: 40; loss: 1.39; acc: 0.55
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 1.05; acc: 0.62
Batch: 120; loss: 1.31; acc: 0.53
Batch: 140; loss: 1.48; acc: 0.53
Batch: 160; loss: 1.2; acc: 0.59
Batch: 180; loss: 1.14; acc: 0.58
Batch: 200; loss: 1.41; acc: 0.56
Batch: 220; loss: 1.17; acc: 0.59
Batch: 240; loss: 1.19; acc: 0.62
Batch: 260; loss: 1.33; acc: 0.56
Batch: 280; loss: 1.39; acc: 0.53
Batch: 300; loss: 1.08; acc: 0.64
Batch: 320; loss: 1.33; acc: 0.59
Batch: 340; loss: 1.09; acc: 0.66
Batch: 360; loss: 1.26; acc: 0.69
Batch: 380; loss: 1.01; acc: 0.61
Batch: 400; loss: 1.31; acc: 0.53
Batch: 420; loss: 1.29; acc: 0.55
Batch: 440; loss: 1.2; acc: 0.59
Batch: 460; loss: 1.24; acc: 0.55
Batch: 480; loss: 1.26; acc: 0.59
Batch: 500; loss: 1.24; acc: 0.58
Batch: 520; loss: 1.26; acc: 0.55
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.08; acc: 0.64
Batch: 580; loss: 1.24; acc: 0.52
Batch: 600; loss: 1.05; acc: 0.69
Batch: 620; loss: 1.13; acc: 0.61
Batch: 640; loss: 1.46; acc: 0.53
Batch: 660; loss: 1.09; acc: 0.61
Batch: 680; loss: 1.55; acc: 0.42
Batch: 700; loss: 1.34; acc: 0.64
Batch: 720; loss: 1.2; acc: 0.58
Batch: 740; loss: 1.3; acc: 0.53
Batch: 760; loss: 1.35; acc: 0.56
Batch: 780; loss: 1.16; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.66
Batch: 60; loss: 1.29; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.64
Batch: 100; loss: 1.29; acc: 0.55
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.64
Val Epoch over. val_loss: 1.1828102454258378; val_accuracy: 0.6153463375796179 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.58; acc: 0.48
Batch: 20; loss: 1.14; acc: 0.64
Batch: 40; loss: 1.45; acc: 0.53
Batch: 60; loss: 1.31; acc: 0.58
Batch: 80; loss: 1.12; acc: 0.62
Batch: 100; loss: 1.02; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.58
Batch: 140; loss: 1.3; acc: 0.59
Batch: 160; loss: 1.23; acc: 0.61
Batch: 180; loss: 1.35; acc: 0.53
Batch: 200; loss: 1.39; acc: 0.61
Batch: 220; loss: 1.21; acc: 0.64
Batch: 240; loss: 0.98; acc: 0.7
Batch: 260; loss: 1.34; acc: 0.52
Batch: 280; loss: 1.34; acc: 0.53
Batch: 300; loss: 1.25; acc: 0.62
Batch: 320; loss: 1.32; acc: 0.56
Batch: 340; loss: 1.43; acc: 0.5
Batch: 360; loss: 1.06; acc: 0.62
Batch: 380; loss: 1.18; acc: 0.61
Batch: 400; loss: 1.3; acc: 0.58
Batch: 420; loss: 1.1; acc: 0.61
Batch: 440; loss: 1.47; acc: 0.53
Batch: 460; loss: 1.01; acc: 0.69
Batch: 480; loss: 1.22; acc: 0.62
Batch: 500; loss: 1.1; acc: 0.62
Batch: 520; loss: 1.3; acc: 0.53
Batch: 540; loss: 1.07; acc: 0.62
Batch: 560; loss: 1.25; acc: 0.59
Batch: 580; loss: 1.11; acc: 0.58
Batch: 600; loss: 1.44; acc: 0.48
Batch: 620; loss: 1.15; acc: 0.53
Batch: 640; loss: 1.17; acc: 0.61
Batch: 660; loss: 1.32; acc: 0.55
Batch: 680; loss: 1.28; acc: 0.64
Batch: 700; loss: 1.21; acc: 0.59
Batch: 720; loss: 1.32; acc: 0.59
Batch: 740; loss: 1.25; acc: 0.53
Batch: 760; loss: 1.31; acc: 0.61
Batch: 780; loss: 1.29; acc: 0.58
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.66
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.26; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1823065425180326; val_accuracy: 0.6149482484076433 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.47; acc: 0.55
Batch: 20; loss: 1.23; acc: 0.62
Batch: 40; loss: 1.47; acc: 0.45
Batch: 60; loss: 1.59; acc: 0.42
Batch: 80; loss: 1.1; acc: 0.69
Batch: 100; loss: 1.19; acc: 0.58
Batch: 120; loss: 1.24; acc: 0.59
Batch: 140; loss: 1.36; acc: 0.58
Batch: 160; loss: 1.22; acc: 0.58
Batch: 180; loss: 1.53; acc: 0.52
Batch: 200; loss: 1.34; acc: 0.59
Batch: 220; loss: 1.56; acc: 0.52
Batch: 240; loss: 1.13; acc: 0.61
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 1.47; acc: 0.53
Batch: 300; loss: 1.12; acc: 0.58
Batch: 320; loss: 1.36; acc: 0.53
Batch: 340; loss: 1.22; acc: 0.61
Batch: 360; loss: 1.3; acc: 0.56
Batch: 380; loss: 1.2; acc: 0.66
Batch: 400; loss: 1.17; acc: 0.61
Batch: 420; loss: 1.23; acc: 0.66
Batch: 440; loss: 1.04; acc: 0.64
Batch: 460; loss: 1.27; acc: 0.67
Batch: 480; loss: 1.21; acc: 0.61
Batch: 500; loss: 1.36; acc: 0.61
Batch: 520; loss: 1.27; acc: 0.58
Batch: 540; loss: 1.31; acc: 0.59
Batch: 560; loss: 1.21; acc: 0.62
Batch: 580; loss: 1.22; acc: 0.61
Batch: 600; loss: 1.16; acc: 0.66
Batch: 620; loss: 1.46; acc: 0.52
Batch: 640; loss: 1.18; acc: 0.53
Batch: 660; loss: 1.4; acc: 0.47
Batch: 680; loss: 1.5; acc: 0.52
Batch: 700; loss: 1.26; acc: 0.58
Batch: 720; loss: 1.07; acc: 0.64
Batch: 740; loss: 1.15; acc: 0.64
Batch: 760; loss: 1.24; acc: 0.56
Batch: 780; loss: 1.26; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.9; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1858805949520912; val_accuracy: 0.6132563694267515 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.03; acc: 0.66
Batch: 20; loss: 1.27; acc: 0.56
Batch: 40; loss: 1.19; acc: 0.67
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 1.22; acc: 0.56
Batch: 100; loss: 1.35; acc: 0.58
Batch: 120; loss: 1.29; acc: 0.58
Batch: 140; loss: 1.25; acc: 0.56
Batch: 160; loss: 1.14; acc: 0.61
Batch: 180; loss: 1.24; acc: 0.61
Batch: 200; loss: 1.27; acc: 0.66
Batch: 220; loss: 1.36; acc: 0.62
Batch: 240; loss: 1.17; acc: 0.62
Batch: 260; loss: 1.21; acc: 0.66
Batch: 280; loss: 1.71; acc: 0.45
Batch: 300; loss: 1.3; acc: 0.55
Batch: 320; loss: 1.43; acc: 0.5
Batch: 340; loss: 1.11; acc: 0.66
Batch: 360; loss: 1.18; acc: 0.59
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.3; acc: 0.55
Batch: 420; loss: 1.17; acc: 0.61
Batch: 440; loss: 1.46; acc: 0.5
Batch: 460; loss: 1.15; acc: 0.59
Batch: 480; loss: 1.44; acc: 0.48
Batch: 500; loss: 1.12; acc: 0.59
Batch: 520; loss: 1.15; acc: 0.62
Batch: 540; loss: 1.25; acc: 0.61
Batch: 560; loss: 1.38; acc: 0.52
Batch: 580; loss: 1.61; acc: 0.44
Batch: 600; loss: 1.24; acc: 0.59
Batch: 620; loss: 1.09; acc: 0.69
Batch: 640; loss: 1.12; acc: 0.61
Batch: 660; loss: 1.42; acc: 0.58
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.24; acc: 0.62
Batch: 720; loss: 1.27; acc: 0.56
Batch: 740; loss: 1.26; acc: 0.66
Batch: 760; loss: 1.29; acc: 0.64
Batch: 780; loss: 1.27; acc: 0.55
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.62
Val Epoch over. val_loss: 1.1813674398288605; val_accuracy: 0.6155453821656051 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.35; acc: 0.56
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 1.05; acc: 0.61
Batch: 60; loss: 1.47; acc: 0.5
Batch: 80; loss: 1.13; acc: 0.61
Batch: 100; loss: 1.22; acc: 0.64
Batch: 120; loss: 1.02; acc: 0.64
Batch: 140; loss: 1.18; acc: 0.62
Batch: 160; loss: 1.16; acc: 0.66
Batch: 180; loss: 1.11; acc: 0.59
Batch: 200; loss: 1.44; acc: 0.58
Batch: 220; loss: 1.2; acc: 0.62
Batch: 240; loss: 1.1; acc: 0.64
Batch: 260; loss: 1.04; acc: 0.66
Batch: 280; loss: 1.4; acc: 0.48
Batch: 300; loss: 1.27; acc: 0.56
Batch: 320; loss: 1.25; acc: 0.59
Batch: 340; loss: 1.21; acc: 0.62
Batch: 360; loss: 1.44; acc: 0.5
Batch: 380; loss: 1.3; acc: 0.61
Batch: 400; loss: 1.45; acc: 0.55
Batch: 420; loss: 1.36; acc: 0.59
Batch: 440; loss: 1.13; acc: 0.62
Batch: 460; loss: 1.2; acc: 0.58
Batch: 480; loss: 1.14; acc: 0.56
Batch: 500; loss: 1.52; acc: 0.48
Batch: 520; loss: 1.21; acc: 0.53
Batch: 540; loss: 1.12; acc: 0.67
Batch: 560; loss: 1.2; acc: 0.5
Batch: 580; loss: 1.25; acc: 0.59
Batch: 600; loss: 1.2; acc: 0.62
Batch: 620; loss: 1.17; acc: 0.61
Batch: 640; loss: 1.2; acc: 0.58
Batch: 660; loss: 1.36; acc: 0.59
Batch: 680; loss: 1.25; acc: 0.58
Batch: 700; loss: 1.56; acc: 0.53
Batch: 720; loss: 1.18; acc: 0.64
Batch: 740; loss: 1.32; acc: 0.59
Batch: 760; loss: 1.21; acc: 0.64
Batch: 780; loss: 1.47; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.3; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.62
Val Epoch over. val_loss: 1.1823142303782663; val_accuracy: 0.6142515923566879 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.06; acc: 0.69
Batch: 20; loss: 1.22; acc: 0.62
Batch: 40; loss: 1.19; acc: 0.67
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.19; acc: 0.62
Batch: 100; loss: 1.44; acc: 0.47
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 1.1; acc: 0.66
Batch: 160; loss: 1.43; acc: 0.55
Batch: 180; loss: 1.09; acc: 0.64
Batch: 200; loss: 1.43; acc: 0.56
Batch: 220; loss: 1.26; acc: 0.59
Batch: 240; loss: 1.57; acc: 0.47
Batch: 260; loss: 1.32; acc: 0.55
Batch: 280; loss: 1.28; acc: 0.58
Batch: 300; loss: 1.06; acc: 0.67
Batch: 320; loss: 1.11; acc: 0.66
Batch: 340; loss: 1.06; acc: 0.69
Batch: 360; loss: 1.53; acc: 0.5
Batch: 380; loss: 1.14; acc: 0.55
Batch: 400; loss: 1.18; acc: 0.62
Batch: 420; loss: 1.27; acc: 0.61
Batch: 440; loss: 1.07; acc: 0.61
Batch: 460; loss: 1.12; acc: 0.64
Batch: 480; loss: 0.96; acc: 0.72
Batch: 500; loss: 0.92; acc: 0.67
Batch: 520; loss: 1.32; acc: 0.55
Batch: 540; loss: 0.94; acc: 0.67
Batch: 560; loss: 1.25; acc: 0.58
Batch: 580; loss: 1.23; acc: 0.5
Batch: 600; loss: 1.04; acc: 0.66
Batch: 620; loss: 1.2; acc: 0.59
Batch: 640; loss: 1.27; acc: 0.56
Batch: 660; loss: 1.26; acc: 0.64
Batch: 680; loss: 1.55; acc: 0.52
Batch: 700; loss: 1.36; acc: 0.56
Batch: 720; loss: 1.27; acc: 0.55
Batch: 740; loss: 1.02; acc: 0.64
Batch: 760; loss: 1.41; acc: 0.58
Batch: 780; loss: 1.24; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.59
Batch: 20; loss: 1.17; acc: 0.64
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.02; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1830308490498052; val_accuracy: 0.6123606687898089 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.95; acc: 0.67
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 1.27; acc: 0.55
Batch: 60; loss: 1.06; acc: 0.67
Batch: 80; loss: 1.19; acc: 0.62
Batch: 100; loss: 1.73; acc: 0.45
Batch: 120; loss: 1.15; acc: 0.72
Batch: 140; loss: 1.39; acc: 0.53
Batch: 160; loss: 1.32; acc: 0.55
Batch: 180; loss: 1.13; acc: 0.62
Batch: 200; loss: 1.19; acc: 0.62
Batch: 220; loss: 1.24; acc: 0.59
Batch: 240; loss: 1.21; acc: 0.64
Batch: 260; loss: 1.58; acc: 0.45
Batch: 280; loss: 0.91; acc: 0.69
Batch: 300; loss: 1.2; acc: 0.61
Batch: 320; loss: 1.06; acc: 0.62
Batch: 340; loss: 1.12; acc: 0.56
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 1.25; acc: 0.52
Batch: 400; loss: 1.35; acc: 0.55
Batch: 420; loss: 1.27; acc: 0.48
Batch: 440; loss: 1.34; acc: 0.56
Batch: 460; loss: 1.1; acc: 0.62
Batch: 480; loss: 1.33; acc: 0.62
Batch: 500; loss: 1.48; acc: 0.53
Batch: 520; loss: 1.25; acc: 0.61
Batch: 540; loss: 1.34; acc: 0.56
Batch: 560; loss: 1.36; acc: 0.48
Batch: 580; loss: 1.21; acc: 0.59
Batch: 600; loss: 1.24; acc: 0.61
Batch: 620; loss: 1.11; acc: 0.61
Batch: 640; loss: 1.15; acc: 0.58
Batch: 660; loss: 1.27; acc: 0.59
Batch: 680; loss: 1.35; acc: 0.56
Batch: 700; loss: 1.0; acc: 0.64
Batch: 720; loss: 1.34; acc: 0.52
Batch: 740; loss: 1.29; acc: 0.59
Batch: 760; loss: 1.18; acc: 0.59
Batch: 780; loss: 1.13; acc: 0.7
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.28; acc: 0.59
Val Epoch over. val_loss: 1.1817025401789671; val_accuracy: 0.6144506369426752 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.58; acc: 0.56
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 1.07; acc: 0.61
Batch: 60; loss: 1.2; acc: 0.61
Batch: 80; loss: 1.17; acc: 0.61
Batch: 100; loss: 1.11; acc: 0.62
Batch: 120; loss: 1.17; acc: 0.61
Batch: 140; loss: 1.3; acc: 0.58
Batch: 160; loss: 1.19; acc: 0.58
Batch: 180; loss: 1.34; acc: 0.64
Batch: 200; loss: 1.3; acc: 0.61
Batch: 220; loss: 1.53; acc: 0.48
Batch: 240; loss: 1.2; acc: 0.61
Batch: 260; loss: 1.26; acc: 0.59
Batch: 280; loss: 0.96; acc: 0.69
Batch: 300; loss: 1.39; acc: 0.58
Batch: 320; loss: 1.21; acc: 0.56
Batch: 340; loss: 1.34; acc: 0.59
Batch: 360; loss: 1.28; acc: 0.59
Batch: 380; loss: 1.27; acc: 0.5
Batch: 400; loss: 1.74; acc: 0.42
Batch: 420; loss: 1.38; acc: 0.5
Batch: 440; loss: 1.16; acc: 0.62
Batch: 460; loss: 1.21; acc: 0.59
Batch: 480; loss: 1.17; acc: 0.59
Batch: 500; loss: 1.46; acc: 0.5
Batch: 520; loss: 1.01; acc: 0.67
Batch: 540; loss: 1.56; acc: 0.53
Batch: 560; loss: 1.27; acc: 0.58
Batch: 580; loss: 1.22; acc: 0.59
Batch: 600; loss: 0.94; acc: 0.72
Batch: 620; loss: 1.34; acc: 0.58
Batch: 640; loss: 1.05; acc: 0.7
Batch: 660; loss: 1.23; acc: 0.61
Batch: 680; loss: 1.12; acc: 0.62
Batch: 700; loss: 1.24; acc: 0.58
Batch: 720; loss: 0.88; acc: 0.78
Batch: 740; loss: 1.15; acc: 0.64
Batch: 760; loss: 1.12; acc: 0.64
Batch: 780; loss: 1.45; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.28; acc: 0.58
Val Epoch over. val_loss: 1.1818390834103725; val_accuracy: 0.6138535031847133 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.13; acc: 0.58
Batch: 20; loss: 1.27; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.67
Batch: 60; loss: 1.28; acc: 0.53
Batch: 80; loss: 1.19; acc: 0.61
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 1.41; acc: 0.58
Batch: 140; loss: 1.14; acc: 0.61
Batch: 160; loss: 1.17; acc: 0.64
Batch: 180; loss: 1.16; acc: 0.61
Batch: 200; loss: 1.18; acc: 0.66
Batch: 220; loss: 1.28; acc: 0.53
Batch: 240; loss: 1.35; acc: 0.64
Batch: 260; loss: 1.28; acc: 0.64
Batch: 280; loss: 1.31; acc: 0.56
Batch: 300; loss: 1.23; acc: 0.64
Batch: 320; loss: 1.32; acc: 0.55
Batch: 340; loss: 1.05; acc: 0.7
Batch: 360; loss: 1.25; acc: 0.61
Batch: 380; loss: 1.31; acc: 0.58
Batch: 400; loss: 1.31; acc: 0.52
Batch: 420; loss: 0.97; acc: 0.7
Batch: 440; loss: 1.36; acc: 0.48
Batch: 460; loss: 1.18; acc: 0.64
Batch: 480; loss: 1.07; acc: 0.7
Batch: 500; loss: 1.38; acc: 0.56
Batch: 520; loss: 1.29; acc: 0.62
Batch: 540; loss: 1.05; acc: 0.67
Batch: 560; loss: 1.39; acc: 0.56
Batch: 580; loss: 1.26; acc: 0.59
Batch: 600; loss: 1.4; acc: 0.44
Batch: 620; loss: 1.3; acc: 0.66
Batch: 640; loss: 1.3; acc: 0.62
Batch: 660; loss: 1.2; acc: 0.62
Batch: 680; loss: 1.16; acc: 0.64
Batch: 700; loss: 1.55; acc: 0.52
Batch: 720; loss: 1.39; acc: 0.53
Batch: 740; loss: 1.42; acc: 0.5
Batch: 760; loss: 1.49; acc: 0.52
Batch: 780; loss: 1.45; acc: 0.5
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.182546469815977; val_accuracy: 0.6134554140127388 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.31; acc: 0.58
Batch: 20; loss: 1.23; acc: 0.53
Batch: 40; loss: 1.31; acc: 0.61
Batch: 60; loss: 1.04; acc: 0.69
Batch: 80; loss: 1.27; acc: 0.62
Batch: 100; loss: 1.48; acc: 0.45
Batch: 120; loss: 1.18; acc: 0.58
Batch: 140; loss: 1.18; acc: 0.67
Batch: 160; loss: 1.28; acc: 0.59
Batch: 180; loss: 1.21; acc: 0.61
Batch: 200; loss: 1.3; acc: 0.52
Batch: 220; loss: 1.28; acc: 0.56
Batch: 240; loss: 1.42; acc: 0.55
Batch: 260; loss: 1.05; acc: 0.59
Batch: 280; loss: 1.53; acc: 0.5
Batch: 300; loss: 1.16; acc: 0.64
Batch: 320; loss: 1.28; acc: 0.58
Batch: 340; loss: 1.34; acc: 0.55
Batch: 360; loss: 0.9; acc: 0.7
Batch: 380; loss: 1.13; acc: 0.64
Batch: 400; loss: 1.28; acc: 0.58
Batch: 420; loss: 1.22; acc: 0.59
Batch: 440; loss: 1.5; acc: 0.41
Batch: 460; loss: 1.14; acc: 0.62
Batch: 480; loss: 1.25; acc: 0.58
Batch: 500; loss: 1.12; acc: 0.61
Batch: 520; loss: 1.31; acc: 0.53
Batch: 540; loss: 1.26; acc: 0.64
Batch: 560; loss: 1.43; acc: 0.48
Batch: 580; loss: 1.1; acc: 0.66
Batch: 600; loss: 1.12; acc: 0.64
Batch: 620; loss: 1.26; acc: 0.52
Batch: 640; loss: 1.22; acc: 0.55
Batch: 660; loss: 1.25; acc: 0.59
Batch: 680; loss: 1.22; acc: 0.61
Batch: 700; loss: 1.25; acc: 0.61
Batch: 720; loss: 1.37; acc: 0.58
Batch: 740; loss: 1.12; acc: 0.66
Batch: 760; loss: 1.08; acc: 0.66
Batch: 780; loss: 0.94; acc: 0.75
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 0.88; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.58
Val Epoch over. val_loss: 1.18173856074643; val_accuracy: 0.6140525477707006 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.06; acc: 0.7
Batch: 20; loss: 1.37; acc: 0.55
Batch: 40; loss: 1.25; acc: 0.56
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.33; acc: 0.61
Batch: 100; loss: 1.3; acc: 0.52
Batch: 120; loss: 1.33; acc: 0.58
Batch: 140; loss: 1.44; acc: 0.58
Batch: 160; loss: 1.12; acc: 0.64
Batch: 180; loss: 1.24; acc: 0.61
Batch: 200; loss: 1.22; acc: 0.56
Batch: 220; loss: 1.44; acc: 0.59
Batch: 240; loss: 1.28; acc: 0.58
Batch: 260; loss: 1.38; acc: 0.53
Batch: 280; loss: 1.32; acc: 0.56
Batch: 300; loss: 1.42; acc: 0.5
Batch: 320; loss: 1.23; acc: 0.59
Batch: 340; loss: 1.2; acc: 0.62
Batch: 360; loss: 1.29; acc: 0.61
Batch: 380; loss: 1.35; acc: 0.62
Batch: 400; loss: 1.19; acc: 0.58
Batch: 420; loss: 1.21; acc: 0.55
Batch: 440; loss: 1.1; acc: 0.64
Batch: 460; loss: 1.48; acc: 0.55
Batch: 480; loss: 1.02; acc: 0.67
Batch: 500; loss: 1.25; acc: 0.59
Batch: 520; loss: 1.2; acc: 0.61
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.49; acc: 0.52
Batch: 580; loss: 1.16; acc: 0.59
Batch: 600; loss: 1.05; acc: 0.66
Batch: 620; loss: 1.23; acc: 0.5
Batch: 640; loss: 1.28; acc: 0.58
Batch: 660; loss: 1.39; acc: 0.56
Batch: 680; loss: 1.43; acc: 0.56
Batch: 700; loss: 1.36; acc: 0.59
Batch: 720; loss: 1.28; acc: 0.59
Batch: 740; loss: 1.13; acc: 0.58
Batch: 760; loss: 1.15; acc: 0.61
Batch: 780; loss: 1.24; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.64
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.62
Val Epoch over. val_loss: 1.1810825706287553; val_accuracy: 0.6157444267515924 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.13; acc: 0.58
Batch: 20; loss: 1.33; acc: 0.61
Batch: 40; loss: 1.09; acc: 0.61
Batch: 60; loss: 1.14; acc: 0.55
Batch: 80; loss: 1.21; acc: 0.62
Batch: 100; loss: 1.21; acc: 0.56
Batch: 120; loss: 1.34; acc: 0.53
Batch: 140; loss: 1.12; acc: 0.64
Batch: 160; loss: 1.26; acc: 0.59
Batch: 180; loss: 1.16; acc: 0.67
Batch: 200; loss: 0.93; acc: 0.75
Batch: 220; loss: 1.12; acc: 0.62
Batch: 240; loss: 1.11; acc: 0.67
Batch: 260; loss: 1.56; acc: 0.55
Batch: 280; loss: 1.31; acc: 0.58
Batch: 300; loss: 1.13; acc: 0.58
Batch: 320; loss: 1.25; acc: 0.61
Batch: 340; loss: 1.43; acc: 0.47
Batch: 360; loss: 1.13; acc: 0.7
Batch: 380; loss: 1.29; acc: 0.58
Batch: 400; loss: 1.19; acc: 0.59
Batch: 420; loss: 1.37; acc: 0.52
Batch: 440; loss: 1.13; acc: 0.62
Batch: 460; loss: 1.21; acc: 0.58
Batch: 480; loss: 1.26; acc: 0.61
Batch: 500; loss: 1.1; acc: 0.64
Batch: 520; loss: 1.71; acc: 0.47
Batch: 540; loss: 1.15; acc: 0.66
Batch: 560; loss: 1.22; acc: 0.61
Batch: 580; loss: 1.34; acc: 0.61
Batch: 600; loss: 1.02; acc: 0.69
Batch: 620; loss: 1.32; acc: 0.62
Batch: 640; loss: 1.03; acc: 0.62
Batch: 660; loss: 1.55; acc: 0.53
Batch: 680; loss: 1.2; acc: 0.61
Batch: 700; loss: 1.36; acc: 0.53
Batch: 720; loss: 1.19; acc: 0.64
Batch: 740; loss: 1.59; acc: 0.5
Batch: 760; loss: 1.39; acc: 0.53
Batch: 780; loss: 0.96; acc: 0.7
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 1.28; acc: 0.56
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1817467710015122; val_accuracy: 0.6157444267515924 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.28; acc: 0.52
Batch: 20; loss: 1.21; acc: 0.55
Batch: 40; loss: 1.27; acc: 0.59
Batch: 60; loss: 1.23; acc: 0.58
Batch: 80; loss: 1.46; acc: 0.64
Batch: 100; loss: 1.47; acc: 0.56
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 1.32; acc: 0.56
Batch: 160; loss: 1.07; acc: 0.72
Batch: 180; loss: 1.46; acc: 0.53
Batch: 200; loss: 1.22; acc: 0.64
Batch: 220; loss: 1.37; acc: 0.55
Batch: 240; loss: 1.3; acc: 0.56
Batch: 260; loss: 1.26; acc: 0.58
Batch: 280; loss: 1.1; acc: 0.61
Batch: 300; loss: 1.22; acc: 0.59
Batch: 320; loss: 1.18; acc: 0.62
Batch: 340; loss: 1.44; acc: 0.61
Batch: 360; loss: 1.36; acc: 0.62
Batch: 380; loss: 1.39; acc: 0.5
Batch: 400; loss: 1.32; acc: 0.59
Batch: 420; loss: 1.37; acc: 0.45
Batch: 440; loss: 1.08; acc: 0.59
Batch: 460; loss: 1.04; acc: 0.66
Batch: 480; loss: 1.2; acc: 0.66
Batch: 500; loss: 1.3; acc: 0.52
Batch: 520; loss: 1.51; acc: 0.52
Batch: 540; loss: 1.27; acc: 0.62
Batch: 560; loss: 1.48; acc: 0.53
Batch: 580; loss: 1.31; acc: 0.52
Batch: 600; loss: 1.21; acc: 0.58
Batch: 620; loss: 1.3; acc: 0.58
Batch: 640; loss: 1.25; acc: 0.53
Batch: 660; loss: 1.05; acc: 0.64
Batch: 680; loss: 1.39; acc: 0.56
Batch: 700; loss: 1.24; acc: 0.56
Batch: 720; loss: 1.35; acc: 0.5
Batch: 740; loss: 1.27; acc: 0.52
Batch: 760; loss: 1.3; acc: 0.59
Batch: 780; loss: 1.09; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.28; acc: 0.61
Val Epoch over. val_loss: 1.1813265666080888; val_accuracy: 0.6143511146496815 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 1.36; acc: 0.47
Batch: 80; loss: 1.25; acc: 0.59
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 1.34; acc: 0.58
Batch: 140; loss: 1.19; acc: 0.58
Batch: 160; loss: 1.24; acc: 0.58
Batch: 180; loss: 1.43; acc: 0.62
Batch: 200; loss: 1.21; acc: 0.59
Batch: 220; loss: 1.07; acc: 0.62
Batch: 240; loss: 1.47; acc: 0.45
Batch: 260; loss: 1.5; acc: 0.5
Batch: 280; loss: 1.57; acc: 0.5
Batch: 300; loss: 1.48; acc: 0.48
Batch: 320; loss: 1.25; acc: 0.59
Batch: 340; loss: 1.25; acc: 0.48
Batch: 360; loss: 1.29; acc: 0.61
Batch: 380; loss: 1.34; acc: 0.67
Batch: 400; loss: 1.16; acc: 0.59
Batch: 420; loss: 1.52; acc: 0.53
Batch: 440; loss: 1.31; acc: 0.69
Batch: 460; loss: 1.08; acc: 0.67
Batch: 480; loss: 1.18; acc: 0.62
Batch: 500; loss: 1.58; acc: 0.45
Batch: 520; loss: 1.05; acc: 0.64
Batch: 540; loss: 1.16; acc: 0.72
Batch: 560; loss: 1.13; acc: 0.64
Batch: 580; loss: 1.3; acc: 0.58
Batch: 600; loss: 1.3; acc: 0.59
Batch: 620; loss: 0.93; acc: 0.67
Batch: 640; loss: 1.14; acc: 0.62
Batch: 660; loss: 1.28; acc: 0.53
Batch: 680; loss: 1.07; acc: 0.69
Batch: 700; loss: 1.38; acc: 0.52
Batch: 720; loss: 1.19; acc: 0.59
Batch: 740; loss: 1.59; acc: 0.45
Batch: 760; loss: 1.12; acc: 0.67
Batch: 780; loss: 1.16; acc: 0.67
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.64
Batch: 40; loss: 0.88; acc: 0.66
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.19; acc: 0.66
Batch: 140; loss: 1.29; acc: 0.56
Val Epoch over. val_loss: 1.1828284240831994; val_accuracy: 0.6132563694267515 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.1; acc: 0.61
Batch: 40; loss: 1.28; acc: 0.58
Batch: 60; loss: 1.47; acc: 0.52
Batch: 80; loss: 1.0; acc: 0.64
Batch: 100; loss: 1.51; acc: 0.47
Batch: 120; loss: 1.23; acc: 0.55
Batch: 140; loss: 1.37; acc: 0.59
Batch: 160; loss: 1.2; acc: 0.59
Batch: 180; loss: 1.22; acc: 0.56
Batch: 200; loss: 1.47; acc: 0.59
Batch: 220; loss: 1.37; acc: 0.62
Batch: 240; loss: 1.48; acc: 0.5
Batch: 260; loss: 1.14; acc: 0.64
Batch: 280; loss: 1.24; acc: 0.61
Batch: 300; loss: 1.48; acc: 0.48
Batch: 320; loss: 1.28; acc: 0.64
Batch: 340; loss: 1.0; acc: 0.62
Batch: 360; loss: 1.31; acc: 0.59
Batch: 380; loss: 1.36; acc: 0.55
Batch: 400; loss: 1.51; acc: 0.52
Batch: 420; loss: 1.51; acc: 0.56
Batch: 440; loss: 1.48; acc: 0.44
Batch: 460; loss: 1.21; acc: 0.52
Batch: 480; loss: 1.22; acc: 0.61
Batch: 500; loss: 1.1; acc: 0.58
Batch: 520; loss: 1.35; acc: 0.55
Batch: 540; loss: 1.29; acc: 0.58
Batch: 560; loss: 1.31; acc: 0.53
Batch: 580; loss: 1.09; acc: 0.67
Batch: 600; loss: 1.17; acc: 0.64
Batch: 620; loss: 1.11; acc: 0.66
Batch: 640; loss: 1.38; acc: 0.56
Batch: 660; loss: 1.58; acc: 0.5
Batch: 680; loss: 1.35; acc: 0.5
Batch: 700; loss: 1.05; acc: 0.7
Batch: 720; loss: 1.23; acc: 0.61
Batch: 740; loss: 1.57; acc: 0.53
Batch: 760; loss: 1.36; acc: 0.58
Batch: 780; loss: 1.23; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.29; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.58
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1810772775844405; val_accuracy: 0.6156449044585988 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.98; acc: 0.69
Batch: 20; loss: 1.5; acc: 0.5
Batch: 40; loss: 1.26; acc: 0.64
Batch: 60; loss: 1.34; acc: 0.58
Batch: 80; loss: 1.61; acc: 0.48
Batch: 100; loss: 1.18; acc: 0.56
Batch: 120; loss: 1.16; acc: 0.53
Batch: 140; loss: 1.29; acc: 0.59
Batch: 160; loss: 1.19; acc: 0.58
Batch: 180; loss: 1.26; acc: 0.61
Batch: 200; loss: 1.17; acc: 0.66
Batch: 220; loss: 1.34; acc: 0.55
Batch: 240; loss: 1.16; acc: 0.62
Batch: 260; loss: 1.06; acc: 0.66
Batch: 280; loss: 1.31; acc: 0.56
Batch: 300; loss: 1.16; acc: 0.66
Batch: 320; loss: 1.18; acc: 0.53
Batch: 340; loss: 1.35; acc: 0.59
Batch: 360; loss: 1.41; acc: 0.56
Batch: 380; loss: 1.21; acc: 0.61
Batch: 400; loss: 1.08; acc: 0.64
Batch: 420; loss: 1.21; acc: 0.61
Batch: 440; loss: 1.26; acc: 0.58
Batch: 460; loss: 1.27; acc: 0.58
Batch: 480; loss: 1.21; acc: 0.58
Batch: 500; loss: 1.11; acc: 0.61
Batch: 520; loss: 1.54; acc: 0.48
Batch: 540; loss: 1.36; acc: 0.56
Batch: 560; loss: 1.25; acc: 0.56
Batch: 580; loss: 0.97; acc: 0.67
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 1.34; acc: 0.52
Batch: 640; loss: 1.3; acc: 0.55
Batch: 660; loss: 1.38; acc: 0.52
Batch: 680; loss: 1.16; acc: 0.64
Batch: 700; loss: 1.1; acc: 0.62
Batch: 720; loss: 1.15; acc: 0.64
Batch: 740; loss: 1.18; acc: 0.61
Batch: 760; loss: 1.15; acc: 0.61
Batch: 780; loss: 1.05; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1817359521890143; val_accuracy: 0.6130573248407644 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.19; acc: 0.58
Batch: 20; loss: 1.08; acc: 0.67
Batch: 40; loss: 1.31; acc: 0.56
Batch: 60; loss: 1.33; acc: 0.5
Batch: 80; loss: 1.17; acc: 0.62
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.55
Batch: 140; loss: 1.13; acc: 0.59
Batch: 160; loss: 1.39; acc: 0.56
Batch: 180; loss: 1.33; acc: 0.66
Batch: 200; loss: 1.42; acc: 0.5
Batch: 220; loss: 1.1; acc: 0.61
Batch: 240; loss: 1.17; acc: 0.56
Batch: 260; loss: 0.93; acc: 0.72
Batch: 280; loss: 1.11; acc: 0.62
Batch: 300; loss: 1.04; acc: 0.66
Batch: 320; loss: 1.22; acc: 0.58
Batch: 340; loss: 0.99; acc: 0.72
Batch: 360; loss: 1.26; acc: 0.64
Batch: 380; loss: 0.93; acc: 0.69
Batch: 400; loss: 1.26; acc: 0.56
Batch: 420; loss: 1.47; acc: 0.48
Batch: 440; loss: 0.9; acc: 0.66
Batch: 460; loss: 1.35; acc: 0.62
Batch: 480; loss: 1.19; acc: 0.61
Batch: 500; loss: 1.28; acc: 0.64
Batch: 520; loss: 1.14; acc: 0.59
Batch: 540; loss: 1.41; acc: 0.55
Batch: 560; loss: 1.03; acc: 0.69
Batch: 580; loss: 1.29; acc: 0.56
Batch: 600; loss: 1.29; acc: 0.58
Batch: 620; loss: 1.14; acc: 0.66
Batch: 640; loss: 1.45; acc: 0.55
Batch: 660; loss: 1.44; acc: 0.45
Batch: 680; loss: 0.97; acc: 0.69
Batch: 700; loss: 1.17; acc: 0.59
Batch: 720; loss: 1.31; acc: 0.64
Batch: 740; loss: 1.3; acc: 0.58
Batch: 760; loss: 1.01; acc: 0.69
Batch: 780; loss: 0.99; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.61
Val Epoch over. val_loss: 1.1816990907025184; val_accuracy: 0.6132563694267515 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.39; acc: 0.58
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 0.95; acc: 0.7
Batch: 60; loss: 1.28; acc: 0.53
Batch: 80; loss: 1.02; acc: 0.62
Batch: 100; loss: 1.18; acc: 0.59
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 1.37; acc: 0.52
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.52; acc: 0.48
Batch: 220; loss: 1.08; acc: 0.64
Batch: 240; loss: 1.46; acc: 0.58
Batch: 260; loss: 1.04; acc: 0.66
Batch: 280; loss: 1.53; acc: 0.56
Batch: 300; loss: 1.04; acc: 0.66
Batch: 320; loss: 1.05; acc: 0.66
Batch: 340; loss: 1.21; acc: 0.62
Batch: 360; loss: 0.98; acc: 0.72
Batch: 380; loss: 1.14; acc: 0.62
Batch: 400; loss: 1.16; acc: 0.62
Batch: 420; loss: 1.52; acc: 0.47
Batch: 440; loss: 1.03; acc: 0.67
Batch: 460; loss: 1.2; acc: 0.55
Batch: 480; loss: 1.23; acc: 0.69
Batch: 500; loss: 1.24; acc: 0.58
Batch: 520; loss: 1.29; acc: 0.61
Batch: 540; loss: 1.16; acc: 0.58
Batch: 560; loss: 1.12; acc: 0.69
Batch: 580; loss: 1.58; acc: 0.55
Batch: 600; loss: 1.2; acc: 0.62
Batch: 620; loss: 1.25; acc: 0.58
Batch: 640; loss: 1.27; acc: 0.66
Batch: 660; loss: 1.43; acc: 0.56
Batch: 680; loss: 1.24; acc: 0.62
Batch: 700; loss: 1.2; acc: 0.55
Batch: 720; loss: 1.44; acc: 0.56
Batch: 740; loss: 1.12; acc: 0.62
Batch: 760; loss: 1.29; acc: 0.59
Batch: 780; loss: 1.31; acc: 0.59
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.28; acc: 0.61
Val Epoch over. val_loss: 1.1815595307927222; val_accuracy: 0.6138535031847133 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.23; acc: 0.61
Batch: 20; loss: 1.32; acc: 0.53
Batch: 40; loss: 1.45; acc: 0.56
Batch: 60; loss: 1.25; acc: 0.61
Batch: 80; loss: 1.38; acc: 0.55
Batch: 100; loss: 1.09; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.35; acc: 0.55
Batch: 160; loss: 1.15; acc: 0.62
Batch: 180; loss: 1.29; acc: 0.53
Batch: 200; loss: 1.22; acc: 0.59
Batch: 220; loss: 1.32; acc: 0.55
Batch: 240; loss: 1.2; acc: 0.62
Batch: 260; loss: 1.48; acc: 0.5
Batch: 280; loss: 1.22; acc: 0.58
Batch: 300; loss: 1.19; acc: 0.59
Batch: 320; loss: 1.21; acc: 0.59
Batch: 340; loss: 1.36; acc: 0.58
Batch: 360; loss: 1.36; acc: 0.56
Batch: 380; loss: 1.13; acc: 0.66
Batch: 400; loss: 1.12; acc: 0.64
Batch: 420; loss: 1.28; acc: 0.59
Batch: 440; loss: 1.43; acc: 0.59
Batch: 460; loss: 1.33; acc: 0.56
Batch: 480; loss: 1.36; acc: 0.5
Batch: 500; loss: 1.3; acc: 0.64
Batch: 520; loss: 1.53; acc: 0.47
Batch: 540; loss: 1.27; acc: 0.59
Batch: 560; loss: 1.0; acc: 0.69
Batch: 580; loss: 1.16; acc: 0.66
Batch: 600; loss: 1.29; acc: 0.55
Batch: 620; loss: 1.36; acc: 0.53
Batch: 640; loss: 1.32; acc: 0.56
Batch: 660; loss: 1.24; acc: 0.61
Batch: 680; loss: 1.4; acc: 0.48
Batch: 700; loss: 1.18; acc: 0.53
Batch: 720; loss: 1.33; acc: 0.61
Batch: 740; loss: 1.3; acc: 0.61
Batch: 760; loss: 1.19; acc: 0.61
Batch: 780; loss: 1.05; acc: 0.69
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1812093007336757; val_accuracy: 0.6143511146496815 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.23; acc: 0.56
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 1.48; acc: 0.53
Batch: 60; loss: 1.34; acc: 0.62
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.22; acc: 0.58
Batch: 120; loss: 1.27; acc: 0.55
Batch: 140; loss: 1.1; acc: 0.61
Batch: 160; loss: 1.55; acc: 0.52
Batch: 180; loss: 1.34; acc: 0.62
Batch: 200; loss: 1.29; acc: 0.56
Batch: 220; loss: 1.02; acc: 0.69
Batch: 240; loss: 1.13; acc: 0.66
Batch: 260; loss: 1.61; acc: 0.45
Batch: 280; loss: 1.18; acc: 0.58
Batch: 300; loss: 1.36; acc: 0.59
Batch: 320; loss: 1.27; acc: 0.67
Batch: 340; loss: 1.04; acc: 0.64
Batch: 360; loss: 0.93; acc: 0.72
Batch: 380; loss: 1.58; acc: 0.52
Batch: 400; loss: 1.37; acc: 0.62
Batch: 420; loss: 1.19; acc: 0.64
Batch: 440; loss: 1.02; acc: 0.67
Batch: 460; loss: 1.02; acc: 0.67
Batch: 480; loss: 1.28; acc: 0.56
Batch: 500; loss: 1.33; acc: 0.58
Batch: 520; loss: 1.3; acc: 0.52
Batch: 540; loss: 1.23; acc: 0.62
Batch: 560; loss: 1.14; acc: 0.73
Batch: 580; loss: 1.28; acc: 0.56
Batch: 600; loss: 1.01; acc: 0.69
Batch: 620; loss: 1.22; acc: 0.61
Batch: 640; loss: 0.97; acc: 0.69
Batch: 660; loss: 1.4; acc: 0.55
Batch: 680; loss: 1.27; acc: 0.59
Batch: 700; loss: 0.95; acc: 0.7
Batch: 720; loss: 1.28; acc: 0.58
Batch: 740; loss: 1.16; acc: 0.64
Batch: 760; loss: 1.42; acc: 0.45
Batch: 780; loss: 1.15; acc: 0.62
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.181428868679484; val_accuracy: 0.6145501592356688 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.22; acc: 0.55
Batch: 20; loss: 1.48; acc: 0.5
Batch: 40; loss: 1.35; acc: 0.64
Batch: 60; loss: 1.54; acc: 0.48
Batch: 80; loss: 0.96; acc: 0.7
Batch: 100; loss: 1.26; acc: 0.62
Batch: 120; loss: 1.22; acc: 0.62
Batch: 140; loss: 1.17; acc: 0.62
Batch: 160; loss: 1.21; acc: 0.58
Batch: 180; loss: 1.21; acc: 0.67
Batch: 200; loss: 1.55; acc: 0.5
Batch: 220; loss: 1.26; acc: 0.62
Batch: 240; loss: 1.27; acc: 0.61
Batch: 260; loss: 1.17; acc: 0.59
Batch: 280; loss: 1.51; acc: 0.53
Batch: 300; loss: 1.56; acc: 0.53
Batch: 320; loss: 1.31; acc: 0.5
Batch: 340; loss: 1.28; acc: 0.55
Batch: 360; loss: 1.0; acc: 0.7
Batch: 380; loss: 1.19; acc: 0.59
Batch: 400; loss: 1.41; acc: 0.58
Batch: 420; loss: 1.27; acc: 0.58
Batch: 440; loss: 1.21; acc: 0.55
Batch: 460; loss: 1.3; acc: 0.55
Batch: 480; loss: 1.38; acc: 0.62
Batch: 500; loss: 1.26; acc: 0.67
Batch: 520; loss: 1.25; acc: 0.59
Batch: 540; loss: 1.16; acc: 0.62
Batch: 560; loss: 1.17; acc: 0.7
Batch: 580; loss: 1.38; acc: 0.53
Batch: 600; loss: 1.28; acc: 0.55
Batch: 620; loss: 1.1; acc: 0.59
Batch: 640; loss: 1.51; acc: 0.53
Batch: 660; loss: 1.42; acc: 0.58
Batch: 680; loss: 1.18; acc: 0.62
Batch: 700; loss: 1.3; acc: 0.53
Batch: 720; loss: 1.01; acc: 0.7
Batch: 740; loss: 1.3; acc: 0.64
Batch: 760; loss: 1.37; acc: 0.48
Batch: 780; loss: 1.34; acc: 0.52
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1812720401271892; val_accuracy: 0.6143511146496815 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 1.37; acc: 0.55
Batch: 40; loss: 1.17; acc: 0.59
Batch: 60; loss: 1.4; acc: 0.47
Batch: 80; loss: 1.19; acc: 0.67
Batch: 100; loss: 0.88; acc: 0.66
Batch: 120; loss: 1.58; acc: 0.55
Batch: 140; loss: 1.54; acc: 0.5
Batch: 160; loss: 1.57; acc: 0.47
Batch: 180; loss: 1.5; acc: 0.52
Batch: 200; loss: 1.17; acc: 0.59
Batch: 220; loss: 1.15; acc: 0.62
Batch: 240; loss: 1.31; acc: 0.53
Batch: 260; loss: 1.24; acc: 0.58
Batch: 280; loss: 1.5; acc: 0.48
Batch: 300; loss: 1.29; acc: 0.59
Batch: 320; loss: 1.16; acc: 0.69
Batch: 340; loss: 1.29; acc: 0.53
Batch: 360; loss: 1.2; acc: 0.64
Batch: 380; loss: 1.43; acc: 0.55
Batch: 400; loss: 1.28; acc: 0.62
Batch: 420; loss: 1.47; acc: 0.5
Batch: 440; loss: 1.31; acc: 0.56
Batch: 460; loss: 1.32; acc: 0.64
Batch: 480; loss: 1.22; acc: 0.61
Batch: 500; loss: 1.39; acc: 0.5
Batch: 520; loss: 1.19; acc: 0.61
Batch: 540; loss: 1.22; acc: 0.61
Batch: 560; loss: 1.21; acc: 0.56
Batch: 580; loss: 1.16; acc: 0.67
Batch: 600; loss: 1.16; acc: 0.58
Batch: 620; loss: 1.18; acc: 0.59
Batch: 640; loss: 1.19; acc: 0.61
Batch: 660; loss: 1.52; acc: 0.5
Batch: 680; loss: 1.29; acc: 0.58
Batch: 700; loss: 1.04; acc: 0.66
Batch: 720; loss: 1.38; acc: 0.53
Batch: 740; loss: 1.63; acc: 0.52
Batch: 760; loss: 1.47; acc: 0.52
Batch: 780; loss: 1.33; acc: 0.58
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1812869815310096; val_accuracy: 0.6147492038216561 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.26; acc: 0.67
Batch: 20; loss: 1.21; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.61
Batch: 60; loss: 1.49; acc: 0.52
Batch: 80; loss: 1.68; acc: 0.56
Batch: 100; loss: 1.08; acc: 0.59
Batch: 120; loss: 1.43; acc: 0.41
Batch: 140; loss: 1.27; acc: 0.52
Batch: 160; loss: 1.14; acc: 0.64
Batch: 180; loss: 1.26; acc: 0.53
Batch: 200; loss: 1.5; acc: 0.52
Batch: 220; loss: 0.9; acc: 0.72
Batch: 240; loss: 1.07; acc: 0.66
Batch: 260; loss: 1.17; acc: 0.66
Batch: 280; loss: 1.25; acc: 0.56
Batch: 300; loss: 0.88; acc: 0.72
Batch: 320; loss: 0.96; acc: 0.67
Batch: 340; loss: 1.15; acc: 0.62
Batch: 360; loss: 1.2; acc: 0.58
Batch: 380; loss: 1.49; acc: 0.53
Batch: 400; loss: 1.3; acc: 0.55
Batch: 420; loss: 1.15; acc: 0.59
Batch: 440; loss: 1.22; acc: 0.62
Batch: 460; loss: 1.31; acc: 0.59
Batch: 480; loss: 1.12; acc: 0.66
Batch: 500; loss: 1.31; acc: 0.61
Batch: 520; loss: 1.48; acc: 0.53
Batch: 540; loss: 1.18; acc: 0.66
Batch: 560; loss: 1.07; acc: 0.64
Batch: 580; loss: 1.25; acc: 0.56
Batch: 600; loss: 1.47; acc: 0.53
Batch: 620; loss: 1.4; acc: 0.58
Batch: 640; loss: 1.22; acc: 0.55
Batch: 660; loss: 1.16; acc: 0.61
Batch: 680; loss: 1.54; acc: 0.5
Batch: 700; loss: 1.27; acc: 0.61
Batch: 720; loss: 1.48; acc: 0.59
Batch: 740; loss: 1.49; acc: 0.52
Batch: 760; loss: 1.27; acc: 0.56
Batch: 780; loss: 1.31; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.61
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.28; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1815158937387407; val_accuracy: 0.6144506369426752 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.2; acc: 0.61
Batch: 20; loss: 1.35; acc: 0.58
Batch: 40; loss: 1.38; acc: 0.59
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.06; acc: 0.61
Batch: 100; loss: 1.34; acc: 0.52
Batch: 120; loss: 1.25; acc: 0.5
Batch: 140; loss: 1.44; acc: 0.55
Batch: 160; loss: 1.14; acc: 0.64
Batch: 180; loss: 1.3; acc: 0.52
Batch: 200; loss: 1.16; acc: 0.56
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.22; acc: 0.64
Batch: 260; loss: 1.23; acc: 0.56
Batch: 280; loss: 1.05; acc: 0.64
Batch: 300; loss: 1.14; acc: 0.59
Batch: 320; loss: 1.1; acc: 0.64
Batch: 340; loss: 1.32; acc: 0.56
Batch: 360; loss: 1.33; acc: 0.62
Batch: 380; loss: 1.51; acc: 0.5
Batch: 400; loss: 0.9; acc: 0.64
Batch: 420; loss: 1.37; acc: 0.59
Batch: 440; loss: 1.3; acc: 0.62
Batch: 460; loss: 1.14; acc: 0.62
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 1.44; acc: 0.56
Batch: 520; loss: 1.55; acc: 0.52
Batch: 540; loss: 1.28; acc: 0.53
Batch: 560; loss: 1.32; acc: 0.56
Batch: 580; loss: 1.24; acc: 0.58
Batch: 600; loss: 1.05; acc: 0.59
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 1.42; acc: 0.53
Batch: 660; loss: 0.87; acc: 0.78
Batch: 680; loss: 1.33; acc: 0.61
Batch: 700; loss: 0.97; acc: 0.66
Batch: 720; loss: 1.47; acc: 0.52
Batch: 740; loss: 1.03; acc: 0.7
Batch: 760; loss: 1.45; acc: 0.48
Batch: 780; loss: 1.34; acc: 0.56
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.28; acc: 0.55
Batch: 120; loss: 1.21; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.59
Val Epoch over. val_loss: 1.1815009690394067; val_accuracy: 0.6141520700636943 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.38; acc: 0.61
Batch: 20; loss: 1.36; acc: 0.55
Batch: 40; loss: 1.54; acc: 0.42
Batch: 60; loss: 1.37; acc: 0.56
Batch: 80; loss: 1.3; acc: 0.56
Batch: 100; loss: 1.37; acc: 0.5
Batch: 120; loss: 1.47; acc: 0.44
Batch: 140; loss: 1.36; acc: 0.53
Batch: 160; loss: 1.39; acc: 0.58
Batch: 180; loss: 1.24; acc: 0.53
Batch: 200; loss: 1.18; acc: 0.61
Batch: 220; loss: 1.19; acc: 0.64
Batch: 240; loss: 1.21; acc: 0.61
Batch: 260; loss: 1.24; acc: 0.56
Batch: 280; loss: 1.25; acc: 0.62
Batch: 300; loss: 1.08; acc: 0.62
Batch: 320; loss: 1.24; acc: 0.66
Batch: 340; loss: 1.27; acc: 0.55
Batch: 360; loss: 1.31; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.38; acc: 0.64
Batch: 420; loss: 1.12; acc: 0.62
Batch: 440; loss: 1.16; acc: 0.62
Batch: 460; loss: 1.24; acc: 0.59
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 1.2; acc: 0.56
Batch: 520; loss: 1.05; acc: 0.73
Batch: 540; loss: 1.35; acc: 0.58
Batch: 560; loss: 1.25; acc: 0.58
Batch: 580; loss: 1.57; acc: 0.52
Batch: 600; loss: 1.3; acc: 0.64
Batch: 620; loss: 1.19; acc: 0.62
Batch: 640; loss: 1.32; acc: 0.56
Batch: 660; loss: 1.25; acc: 0.59
Batch: 680; loss: 1.06; acc: 0.66
Batch: 700; loss: 1.41; acc: 0.58
Batch: 720; loss: 1.42; acc: 0.5
Batch: 740; loss: 1.31; acc: 0.64
Batch: 760; loss: 1.28; acc: 0.66
Batch: 780; loss: 1.28; acc: 0.5
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.89; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.64
Batch: 80; loss: 1.03; acc: 0.66
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 1.29; acc: 0.61
Val Epoch over. val_loss: 1.1812721574382417; val_accuracy: 0.6138535031847133 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.26; acc: 0.64
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 1.18; acc: 0.66
Batch: 60; loss: 1.34; acc: 0.59
Batch: 80; loss: 1.29; acc: 0.55
Batch: 100; loss: 1.39; acc: 0.59
Batch: 120; loss: 1.32; acc: 0.55
Batch: 140; loss: 1.25; acc: 0.62
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 1.22; acc: 0.59
Batch: 200; loss: 1.09; acc: 0.61
Batch: 220; loss: 1.17; acc: 0.64
Batch: 240; loss: 1.13; acc: 0.61
Batch: 260; loss: 1.25; acc: 0.62
Batch: 280; loss: 1.18; acc: 0.56
Batch: 300; loss: 1.31; acc: 0.52
Batch: 320; loss: 1.29; acc: 0.47
Batch: 340; loss: 1.49; acc: 0.55
Batch: 360; loss: 1.12; acc: 0.62
Batch: 380; loss: 0.9; acc: 0.69
Batch: 400; loss: 1.17; acc: 0.66
Batch: 420; loss: 1.28; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.52
Batch: 460; loss: 1.19; acc: 0.61
Batch: 480; loss: 1.38; acc: 0.59
Batch: 500; loss: 1.45; acc: 0.55
Batch: 520; loss: 1.13; acc: 0.67
Batch: 540; loss: 1.4; acc: 0.62
Batch: 560; loss: 1.26; acc: 0.55
Batch: 580; loss: 1.28; acc: 0.53
Batch: 600; loss: 1.28; acc: 0.62
Batch: 620; loss: 1.44; acc: 0.53
Batch: 640; loss: 0.8; acc: 0.77
Batch: 660; loss: 1.12; acc: 0.62
Batch: 680; loss: 1.15; acc: 0.66
Batch: 700; loss: 1.37; acc: 0.55
Batch: 720; loss: 1.47; acc: 0.53
Batch: 740; loss: 1.37; acc: 0.53
Batch: 760; loss: 1.15; acc: 0.72
Batch: 780; loss: 1.5; acc: 0.47
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.89; acc: 0.64
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 1.28; acc: 0.61
Val Epoch over. val_loss: 1.1810215130733077; val_accuracy: 0.6149482484076433 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 3331948
elements in E: 3331950
fraction nonzero: 0.9999993997508966
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.32; acc: 0.05
Batch: 40; loss: 2.32; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.17
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.14
Batch: 140; loss: 2.31; acc: 0.11
Batch: 160; loss: 2.31; acc: 0.08
Batch: 180; loss: 2.29; acc: 0.09
Batch: 200; loss: 2.31; acc: 0.03
Batch: 220; loss: 2.29; acc: 0.11
Batch: 240; loss: 2.28; acc: 0.16
Batch: 260; loss: 2.29; acc: 0.11
Batch: 280; loss: 2.29; acc: 0.12
Batch: 300; loss: 2.29; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.11
Batch: 340; loss: 2.28; acc: 0.12
Batch: 360; loss: 2.28; acc: 0.16
Batch: 380; loss: 2.27; acc: 0.12
Batch: 400; loss: 2.28; acc: 0.08
Batch: 420; loss: 2.28; acc: 0.08
Batch: 440; loss: 2.27; acc: 0.14
Batch: 460; loss: 2.28; acc: 0.06
Batch: 480; loss: 2.28; acc: 0.06
Batch: 500; loss: 2.28; acc: 0.05
Batch: 520; loss: 2.25; acc: 0.19
Batch: 540; loss: 2.27; acc: 0.06
Batch: 560; loss: 2.27; acc: 0.12
Batch: 580; loss: 2.26; acc: 0.14
Batch: 600; loss: 2.27; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.16
Batch: 640; loss: 2.27; acc: 0.12
Batch: 660; loss: 2.24; acc: 0.2
Batch: 680; loss: 2.25; acc: 0.14
Batch: 700; loss: 2.24; acc: 0.14
Batch: 720; loss: 2.23; acc: 0.2
Batch: 740; loss: 2.24; acc: 0.14
Batch: 760; loss: 2.23; acc: 0.25
Batch: 780; loss: 2.21; acc: 0.34
Train Epoch over. train_loss: 2.28; train_accuracy: 0.13 

Batch: 0; loss: 2.21; acc: 0.34
Batch: 20; loss: 2.2; acc: 0.33
Batch: 40; loss: 2.18; acc: 0.45
Batch: 60; loss: 2.2; acc: 0.42
Batch: 80; loss: 2.21; acc: 0.39
Batch: 100; loss: 2.21; acc: 0.33
Batch: 120; loss: 2.21; acc: 0.36
Batch: 140; loss: 2.19; acc: 0.41
Val Epoch over. val_loss: 2.2144572917063523; val_accuracy: 0.3171775477707006 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.21; acc: 0.44
Batch: 20; loss: 2.21; acc: 0.3
Batch: 40; loss: 2.2; acc: 0.25
Batch: 60; loss: 2.18; acc: 0.42
Batch: 80; loss: 2.18; acc: 0.34
Batch: 100; loss: 2.14; acc: 0.48
Batch: 120; loss: 2.16; acc: 0.33
Batch: 140; loss: 2.11; acc: 0.33
Batch: 160; loss: 2.11; acc: 0.39
Batch: 180; loss: 2.1; acc: 0.31
Batch: 200; loss: 1.95; acc: 0.45
Batch: 220; loss: 1.96; acc: 0.42
Batch: 240; loss: 1.81; acc: 0.47
Batch: 260; loss: 1.68; acc: 0.58
Batch: 280; loss: 1.59; acc: 0.53
Batch: 300; loss: 1.47; acc: 0.55
Batch: 320; loss: 1.31; acc: 0.56
Batch: 340; loss: 1.47; acc: 0.52
Batch: 360; loss: 1.46; acc: 0.61
Batch: 380; loss: 1.41; acc: 0.59
Batch: 400; loss: 1.26; acc: 0.56
Batch: 420; loss: 1.27; acc: 0.58
Batch: 440; loss: 1.53; acc: 0.42
Batch: 460; loss: 1.07; acc: 0.61
Batch: 480; loss: 1.18; acc: 0.56
Batch: 500; loss: 1.18; acc: 0.64
Batch: 520; loss: 1.05; acc: 0.7
Batch: 540; loss: 0.98; acc: 0.61
Batch: 560; loss: 0.99; acc: 0.62
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.08; acc: 0.7
Batch: 620; loss: 1.19; acc: 0.53
Batch: 640; loss: 1.28; acc: 0.62
Batch: 660; loss: 1.02; acc: 0.7
Batch: 680; loss: 0.97; acc: 0.61
Batch: 700; loss: 1.33; acc: 0.59
Batch: 720; loss: 1.22; acc: 0.61
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.22; acc: 0.61
Batch: 780; loss: 0.91; acc: 0.69
Train Epoch over. train_loss: 1.54; train_accuracy: 0.52 

Batch: 0; loss: 1.29; acc: 0.59
Batch: 20; loss: 1.54; acc: 0.58
Batch: 40; loss: 0.79; acc: 0.72
Batch: 60; loss: 1.03; acc: 0.62
Batch: 80; loss: 0.97; acc: 0.69
Batch: 100; loss: 1.0; acc: 0.64
Batch: 120; loss: 1.43; acc: 0.52
Batch: 140; loss: 0.92; acc: 0.64
Val Epoch over. val_loss: 1.2505439831193086; val_accuracy: 0.5958399681528662 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.55
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 1.27; acc: 0.64
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 0.95; acc: 0.75
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 0.92; acc: 0.69
Batch: 160; loss: 1.06; acc: 0.62
Batch: 180; loss: 1.19; acc: 0.61
Batch: 200; loss: 1.22; acc: 0.56
Batch: 220; loss: 1.4; acc: 0.53
Batch: 240; loss: 0.93; acc: 0.67
Batch: 260; loss: 0.98; acc: 0.61
Batch: 280; loss: 0.96; acc: 0.67
Batch: 300; loss: 0.75; acc: 0.75
Batch: 320; loss: 1.02; acc: 0.55
Batch: 340; loss: 1.17; acc: 0.66
Batch: 360; loss: 1.1; acc: 0.66
Batch: 380; loss: 1.04; acc: 0.66
Batch: 400; loss: 0.93; acc: 0.7
Batch: 420; loss: 1.31; acc: 0.56
Batch: 440; loss: 0.77; acc: 0.69
Batch: 460; loss: 0.91; acc: 0.7
Batch: 480; loss: 1.0; acc: 0.66
Batch: 500; loss: 1.25; acc: 0.66
Batch: 520; loss: 1.34; acc: 0.59
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.71; acc: 0.75
Batch: 580; loss: 1.1; acc: 0.66
Batch: 600; loss: 0.91; acc: 0.66
Batch: 620; loss: 0.84; acc: 0.7
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 1.15; acc: 0.61
Batch: 680; loss: 1.23; acc: 0.64
Batch: 700; loss: 1.3; acc: 0.52
Batch: 720; loss: 0.94; acc: 0.73
Batch: 740; loss: 0.93; acc: 0.7
Batch: 760; loss: 0.94; acc: 0.7
Batch: 780; loss: 0.85; acc: 0.72
Train Epoch over. train_loss: 1.04; train_accuracy: 0.66 

Batch: 0; loss: 1.07; acc: 0.64
Batch: 20; loss: 1.31; acc: 0.47
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 0.8; acc: 0.73
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 0.62; acc: 0.77
Val Epoch over. val_loss: 0.9695958212302749; val_accuracy: 0.6754578025477707 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.96; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 1.29; acc: 0.66
Batch: 60; loss: 0.95; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.59
Batch: 100; loss: 0.92; acc: 0.72
Batch: 120; loss: 1.29; acc: 0.61
Batch: 140; loss: 1.23; acc: 0.62
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 1.09; acc: 0.66
Batch: 200; loss: 1.05; acc: 0.73
Batch: 220; loss: 1.08; acc: 0.64
Batch: 240; loss: 0.87; acc: 0.72
Batch: 260; loss: 1.05; acc: 0.67
Batch: 280; loss: 1.03; acc: 0.59
Batch: 300; loss: 1.15; acc: 0.61
Batch: 320; loss: 0.96; acc: 0.67
Batch: 340; loss: 0.97; acc: 0.66
Batch: 360; loss: 0.93; acc: 0.7
Batch: 380; loss: 1.12; acc: 0.56
Batch: 400; loss: 0.9; acc: 0.69
Batch: 420; loss: 1.38; acc: 0.56
Batch: 440; loss: 0.9; acc: 0.67
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.84; acc: 0.73
Batch: 500; loss: 1.0; acc: 0.66
Batch: 520; loss: 1.16; acc: 0.59
Batch: 540; loss: 1.06; acc: 0.67
Batch: 560; loss: 0.8; acc: 0.77
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.94; acc: 0.69
Batch: 620; loss: 0.95; acc: 0.75
Batch: 640; loss: 1.25; acc: 0.61
Batch: 660; loss: 0.72; acc: 0.84
Batch: 680; loss: 1.06; acc: 0.66
Batch: 700; loss: 1.3; acc: 0.62
Batch: 720; loss: 0.85; acc: 0.7
Batch: 740; loss: 0.94; acc: 0.62
Batch: 760; loss: 1.04; acc: 0.62
Batch: 780; loss: 0.83; acc: 0.75
Train Epoch over. train_loss: 1.01; train_accuracy: 0.67 

Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 0.68; acc: 0.69
Batch: 60; loss: 1.04; acc: 0.66
Batch: 80; loss: 0.8; acc: 0.72
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 1.06; acc: 0.66
Batch: 140; loss: 0.79; acc: 0.75
Val Epoch over. val_loss: 0.9990612511422224; val_accuracy: 0.6686902866242038 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 1.1; acc: 0.58
Batch: 60; loss: 0.94; acc: 0.72
Batch: 80; loss: 0.99; acc: 0.66
Batch: 100; loss: 1.01; acc: 0.66
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 1.01; acc: 0.64
Batch: 160; loss: 1.08; acc: 0.61
Batch: 180; loss: 1.03; acc: 0.64
Batch: 200; loss: 1.08; acc: 0.69
Batch: 220; loss: 1.24; acc: 0.53
Batch: 240; loss: 0.97; acc: 0.66
Batch: 260; loss: 0.93; acc: 0.75
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 1.04; acc: 0.7
Batch: 320; loss: 0.84; acc: 0.72
Batch: 340; loss: 0.93; acc: 0.66
Batch: 360; loss: 1.17; acc: 0.66
Batch: 380; loss: 0.91; acc: 0.78
Batch: 400; loss: 1.01; acc: 0.73
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 1.02; acc: 0.66
Batch: 460; loss: 1.29; acc: 0.58
Batch: 480; loss: 0.85; acc: 0.73
Batch: 500; loss: 0.83; acc: 0.72
Batch: 520; loss: 0.86; acc: 0.75
Batch: 540; loss: 0.92; acc: 0.67
Batch: 560; loss: 0.85; acc: 0.7
Batch: 580; loss: 0.83; acc: 0.67
Batch: 600; loss: 1.25; acc: 0.59
Batch: 620; loss: 0.99; acc: 0.62
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 1.48; acc: 0.58
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 0.98; acc: 0.67
Batch: 720; loss: 0.82; acc: 0.72
Batch: 740; loss: 1.21; acc: 0.62
Batch: 760; loss: 0.78; acc: 0.72
Batch: 780; loss: 0.96; acc: 0.72
Train Epoch over. train_loss: 1.0; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.59
Batch: 20; loss: 1.52; acc: 0.58
Batch: 40; loss: 0.89; acc: 0.67
Batch: 60; loss: 1.08; acc: 0.67
Batch: 80; loss: 0.94; acc: 0.62
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 0.86; acc: 0.7
Val Epoch over. val_loss: 1.071017050439385; val_accuracy: 0.6403264331210191 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.64
Batch: 20; loss: 1.12; acc: 0.67
Batch: 40; loss: 1.03; acc: 0.64
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 1.13; acc: 0.62
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 0.96; acc: 0.64
Batch: 140; loss: 1.01; acc: 0.62
Batch: 160; loss: 0.93; acc: 0.75
Batch: 180; loss: 1.1; acc: 0.66
Batch: 200; loss: 1.19; acc: 0.59
Batch: 220; loss: 0.91; acc: 0.66
Batch: 240; loss: 1.0; acc: 0.72
Batch: 260; loss: 0.99; acc: 0.69
Batch: 280; loss: 0.89; acc: 0.66
Batch: 300; loss: 0.94; acc: 0.69
Batch: 320; loss: 0.97; acc: 0.7
Batch: 340; loss: 0.73; acc: 0.78
Batch: 360; loss: 0.89; acc: 0.69
Batch: 380; loss: 0.93; acc: 0.72
Batch: 400; loss: 1.14; acc: 0.61
Batch: 420; loss: 0.74; acc: 0.75
Batch: 440; loss: 0.94; acc: 0.62
Batch: 460; loss: 0.89; acc: 0.69
Batch: 480; loss: 1.1; acc: 0.67
Batch: 500; loss: 0.91; acc: 0.67
Batch: 520; loss: 1.12; acc: 0.62
Batch: 540; loss: 0.65; acc: 0.69
Batch: 560; loss: 0.94; acc: 0.64
Batch: 580; loss: 0.84; acc: 0.64
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 0.88; acc: 0.75
Batch: 640; loss: 1.05; acc: 0.64
Batch: 660; loss: 0.73; acc: 0.73
Batch: 680; loss: 0.93; acc: 0.72
Batch: 700; loss: 0.81; acc: 0.72
Batch: 720; loss: 1.36; acc: 0.59
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 1.58; acc: 0.45
Batch: 780; loss: 1.25; acc: 0.56
Train Epoch over. train_loss: 0.98; train_accuracy: 0.67 

Batch: 0; loss: 1.13; acc: 0.62
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.69
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 0.79; acc: 0.72
Val Epoch over. val_loss: 1.0117034710896242; val_accuracy: 0.6595342356687898 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.86; acc: 0.72
Batch: 20; loss: 0.89; acc: 0.64
Batch: 40; loss: 0.99; acc: 0.64
Batch: 60; loss: 0.84; acc: 0.66
Batch: 80; loss: 1.02; acc: 0.66
Batch: 100; loss: 0.86; acc: 0.73
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 0.97; acc: 0.67
Batch: 160; loss: 0.84; acc: 0.7
Batch: 180; loss: 0.96; acc: 0.66
Batch: 200; loss: 0.97; acc: 0.64
Batch: 220; loss: 0.79; acc: 0.75
Batch: 240; loss: 1.16; acc: 0.61
Batch: 260; loss: 1.11; acc: 0.59
Batch: 280; loss: 0.7; acc: 0.77
Batch: 300; loss: 1.23; acc: 0.62
Batch: 320; loss: 1.11; acc: 0.62
Batch: 340; loss: 0.86; acc: 0.64
Batch: 360; loss: 0.97; acc: 0.73
Batch: 380; loss: 1.07; acc: 0.7
Batch: 400; loss: 0.99; acc: 0.72
Batch: 420; loss: 0.75; acc: 0.72
Batch: 440; loss: 0.88; acc: 0.72
Batch: 460; loss: 1.32; acc: 0.56
Batch: 480; loss: 0.76; acc: 0.72
Batch: 500; loss: 0.88; acc: 0.75
Batch: 520; loss: 0.82; acc: 0.72
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 1.16; acc: 0.64
Batch: 580; loss: 0.75; acc: 0.78
Batch: 600; loss: 1.0; acc: 0.66
Batch: 620; loss: 0.81; acc: 0.73
Batch: 640; loss: 1.01; acc: 0.72
Batch: 660; loss: 0.97; acc: 0.67
Batch: 680; loss: 0.96; acc: 0.69
Batch: 700; loss: 0.74; acc: 0.75
Batch: 720; loss: 0.93; acc: 0.67
Batch: 740; loss: 1.07; acc: 0.67
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.78; acc: 0.73
Train Epoch over. train_loss: 0.96; train_accuracy: 0.68 

Batch: 0; loss: 0.97; acc: 0.66
Batch: 20; loss: 1.14; acc: 0.58
Batch: 40; loss: 0.76; acc: 0.72
Batch: 60; loss: 0.7; acc: 0.72
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.86; acc: 0.73
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 0.66; acc: 0.78
Val Epoch over. val_loss: 0.9166855893696949; val_accuracy: 0.6976512738853503 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 0.8; acc: 0.72
Batch: 40; loss: 0.75; acc: 0.78
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 0.93; acc: 0.73
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 1.22; acc: 0.59
Batch: 140; loss: 0.95; acc: 0.69
Batch: 160; loss: 0.85; acc: 0.59
Batch: 180; loss: 1.11; acc: 0.56
Batch: 200; loss: 1.0; acc: 0.66
Batch: 220; loss: 0.83; acc: 0.73
Batch: 240; loss: 1.21; acc: 0.59
Batch: 260; loss: 1.12; acc: 0.64
Batch: 280; loss: 0.98; acc: 0.67
Batch: 300; loss: 0.95; acc: 0.75
Batch: 320; loss: 0.75; acc: 0.77
Batch: 340; loss: 1.03; acc: 0.72
Batch: 360; loss: 0.79; acc: 0.7
Batch: 380; loss: 0.78; acc: 0.7
Batch: 400; loss: 0.86; acc: 0.67
Batch: 420; loss: 1.16; acc: 0.66
Batch: 440; loss: 0.86; acc: 0.73
Batch: 460; loss: 0.92; acc: 0.77
Batch: 480; loss: 0.89; acc: 0.75
Batch: 500; loss: 0.98; acc: 0.69
Batch: 520; loss: 0.98; acc: 0.62
Batch: 540; loss: 0.85; acc: 0.73
Batch: 560; loss: 1.09; acc: 0.67
Batch: 580; loss: 1.02; acc: 0.61
Batch: 600; loss: 1.06; acc: 0.59
Batch: 620; loss: 1.12; acc: 0.58
Batch: 640; loss: 1.09; acc: 0.66
Batch: 660; loss: 0.94; acc: 0.64
Batch: 680; loss: 0.72; acc: 0.73
Batch: 700; loss: 0.83; acc: 0.7
Batch: 720; loss: 0.81; acc: 0.8
Batch: 740; loss: 0.76; acc: 0.8
Batch: 760; loss: 0.95; acc: 0.61
Batch: 780; loss: 1.07; acc: 0.67
Train Epoch over. train_loss: 0.96; train_accuracy: 0.68 

Batch: 0; loss: 0.91; acc: 0.69
Batch: 20; loss: 1.37; acc: 0.58
Batch: 40; loss: 0.84; acc: 0.69
Batch: 60; loss: 0.92; acc: 0.62
Batch: 80; loss: 0.77; acc: 0.72
Batch: 100; loss: 0.83; acc: 0.73
Batch: 120; loss: 1.34; acc: 0.56
Batch: 140; loss: 0.78; acc: 0.69
Val Epoch over. val_loss: 1.0299335289153324; val_accuracy: 0.661922770700637 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.73
Batch: 40; loss: 0.84; acc: 0.62
Batch: 60; loss: 1.14; acc: 0.59
Batch: 80; loss: 0.76; acc: 0.72
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 1.15; acc: 0.62
Batch: 160; loss: 0.99; acc: 0.67
Batch: 180; loss: 0.9; acc: 0.69
Batch: 200; loss: 0.94; acc: 0.7
Batch: 220; loss: 0.66; acc: 0.75
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.88; acc: 0.66
Batch: 300; loss: 1.01; acc: 0.66
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 1.28; acc: 0.56
Batch: 360; loss: 1.04; acc: 0.62
Batch: 380; loss: 0.81; acc: 0.73
Batch: 400; loss: 1.08; acc: 0.58
Batch: 420; loss: 1.06; acc: 0.69
Batch: 440; loss: 0.99; acc: 0.67
Batch: 460; loss: 1.0; acc: 0.7
Batch: 480; loss: 0.79; acc: 0.75
Batch: 500; loss: 0.79; acc: 0.67
Batch: 520; loss: 0.69; acc: 0.77
Batch: 540; loss: 0.85; acc: 0.73
Batch: 560; loss: 0.89; acc: 0.69
Batch: 580; loss: 0.83; acc: 0.73
Batch: 600; loss: 0.91; acc: 0.72
Batch: 620; loss: 0.84; acc: 0.73
Batch: 640; loss: 0.89; acc: 0.72
Batch: 660; loss: 0.88; acc: 0.75
Batch: 680; loss: 0.75; acc: 0.7
Batch: 700; loss: 0.78; acc: 0.69
Batch: 720; loss: 0.81; acc: 0.73
Batch: 740; loss: 0.75; acc: 0.7
Batch: 760; loss: 0.88; acc: 0.69
Batch: 780; loss: 0.99; acc: 0.64
Train Epoch over. train_loss: 0.93; train_accuracy: 0.69 

Batch: 0; loss: 0.95; acc: 0.59
Batch: 20; loss: 1.4; acc: 0.58
Batch: 40; loss: 0.88; acc: 0.67
Batch: 60; loss: 0.98; acc: 0.7
Batch: 80; loss: 0.96; acc: 0.66
Batch: 100; loss: 0.81; acc: 0.69
Batch: 120; loss: 1.06; acc: 0.64
Batch: 140; loss: 0.74; acc: 0.69
Val Epoch over. val_loss: 1.0296705065259508; val_accuracy: 0.6576433121019108 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 0.96; acc: 0.73
Batch: 40; loss: 1.0; acc: 0.66
Batch: 60; loss: 0.96; acc: 0.61
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 1.03; acc: 0.64
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.95; acc: 0.64
Batch: 180; loss: 0.72; acc: 0.77
Batch: 200; loss: 1.09; acc: 0.67
Batch: 220; loss: 0.9; acc: 0.64
Batch: 240; loss: 0.95; acc: 0.77
Batch: 260; loss: 0.92; acc: 0.77
Batch: 280; loss: 0.88; acc: 0.73
Batch: 300; loss: 0.8; acc: 0.7
Batch: 320; loss: 0.83; acc: 0.7
Batch: 340; loss: 0.72; acc: 0.77
Batch: 360; loss: 0.83; acc: 0.7
Batch: 380; loss: 0.76; acc: 0.8
Batch: 400; loss: 0.87; acc: 0.67
Batch: 420; loss: 0.58; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.73
Batch: 460; loss: 0.77; acc: 0.75
Batch: 480; loss: 0.88; acc: 0.67
Batch: 500; loss: 0.95; acc: 0.7
Batch: 520; loss: 0.94; acc: 0.69
Batch: 540; loss: 1.02; acc: 0.69
Batch: 560; loss: 0.86; acc: 0.78
Batch: 580; loss: 0.78; acc: 0.73
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 1.01; acc: 0.67
Batch: 640; loss: 0.86; acc: 0.69
Batch: 660; loss: 0.87; acc: 0.61
Batch: 680; loss: 1.21; acc: 0.61
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.83; acc: 0.7
Batch: 740; loss: 0.93; acc: 0.67
Batch: 760; loss: 0.84; acc: 0.73
Batch: 780; loss: 0.8; acc: 0.75
Train Epoch over. train_loss: 0.91; train_accuracy: 0.7 

Batch: 0; loss: 0.94; acc: 0.59
Batch: 20; loss: 1.25; acc: 0.53
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 0.81; acc: 0.78
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 1.07; acc: 0.69
Batch: 140; loss: 0.6; acc: 0.78
Val Epoch over. val_loss: 0.9383076127547367; val_accuracy: 0.6898885350318471 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.11; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 1.03; acc: 0.58
Batch: 80; loss: 0.99; acc: 0.67
Batch: 100; loss: 0.93; acc: 0.72
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 0.96; acc: 0.72
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.8; acc: 0.7
Batch: 200; loss: 0.74; acc: 0.72
Batch: 220; loss: 0.95; acc: 0.7
Batch: 240; loss: 0.98; acc: 0.7
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 0.67; acc: 0.75
Batch: 300; loss: 0.95; acc: 0.66
Batch: 320; loss: 0.87; acc: 0.66
Batch: 340; loss: 1.29; acc: 0.58
Batch: 360; loss: 1.03; acc: 0.67
Batch: 380; loss: 0.71; acc: 0.73
Batch: 400; loss: 0.67; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.67
Batch: 440; loss: 0.83; acc: 0.69
Batch: 460; loss: 0.82; acc: 0.77
Batch: 480; loss: 0.97; acc: 0.7
Batch: 500; loss: 0.94; acc: 0.72
Batch: 520; loss: 0.66; acc: 0.73
Batch: 540; loss: 0.75; acc: 0.75
Batch: 560; loss: 0.72; acc: 0.77
Batch: 580; loss: 0.95; acc: 0.67
Batch: 600; loss: 0.97; acc: 0.73
Batch: 620; loss: 0.82; acc: 0.78
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 1.09; acc: 0.59
Batch: 680; loss: 0.83; acc: 0.75
Batch: 700; loss: 0.77; acc: 0.78
Batch: 720; loss: 0.77; acc: 0.75
Batch: 740; loss: 0.72; acc: 0.83
Batch: 760; loss: 0.96; acc: 0.64
Batch: 780; loss: 0.78; acc: 0.73
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.64
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.71; acc: 0.75
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 0.63; acc: 0.81
Val Epoch over. val_loss: 0.8147681233989206; val_accuracy: 0.7364649681528662 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.89; acc: 0.67
Batch: 40; loss: 0.97; acc: 0.69
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.83; acc: 0.77
Batch: 100; loss: 1.2; acc: 0.66
Batch: 120; loss: 1.0; acc: 0.7
Batch: 140; loss: 0.87; acc: 0.64
Batch: 160; loss: 0.8; acc: 0.8
Batch: 180; loss: 0.99; acc: 0.69
Batch: 200; loss: 0.7; acc: 0.72
Batch: 220; loss: 0.87; acc: 0.62
Batch: 240; loss: 1.12; acc: 0.66
Batch: 260; loss: 0.81; acc: 0.7
Batch: 280; loss: 0.77; acc: 0.78
Batch: 300; loss: 0.96; acc: 0.66
Batch: 320; loss: 0.91; acc: 0.67
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.77
Batch: 400; loss: 0.93; acc: 0.72
Batch: 420; loss: 0.83; acc: 0.75
Batch: 440; loss: 0.92; acc: 0.75
Batch: 460; loss: 0.84; acc: 0.69
Batch: 480; loss: 0.78; acc: 0.78
Batch: 500; loss: 0.83; acc: 0.69
Batch: 520; loss: 0.96; acc: 0.64
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.82; acc: 0.75
Batch: 580; loss: 1.01; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.73
Batch: 620; loss: 0.76; acc: 0.73
Batch: 640; loss: 1.04; acc: 0.7
Batch: 660; loss: 0.73; acc: 0.78
Batch: 680; loss: 0.57; acc: 0.75
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 0.93; acc: 0.72
Batch: 780; loss: 0.92; acc: 0.73
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.89; acc: 0.69
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 1.0; acc: 0.72
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.8357000723006619; val_accuracy: 0.7278065286624203 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.06; acc: 0.66
Batch: 20; loss: 0.95; acc: 0.7
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 0.66; acc: 0.7
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.81; acc: 0.73
Batch: 160; loss: 0.82; acc: 0.69
Batch: 180; loss: 0.87; acc: 0.7
Batch: 200; loss: 0.83; acc: 0.72
Batch: 220; loss: 0.84; acc: 0.73
Batch: 240; loss: 1.1; acc: 0.7
Batch: 260; loss: 0.86; acc: 0.73
Batch: 280; loss: 0.78; acc: 0.8
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.96; acc: 0.67
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 1.12; acc: 0.7
Batch: 380; loss: 1.02; acc: 0.64
Batch: 400; loss: 1.29; acc: 0.62
Batch: 420; loss: 0.88; acc: 0.73
Batch: 440; loss: 0.98; acc: 0.7
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 0.91; acc: 0.73
Batch: 500; loss: 1.01; acc: 0.7
Batch: 520; loss: 1.01; acc: 0.69
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.9; acc: 0.73
Batch: 600; loss: 0.86; acc: 0.75
Batch: 620; loss: 0.9; acc: 0.69
Batch: 640; loss: 0.86; acc: 0.7
Batch: 660; loss: 0.87; acc: 0.64
Batch: 680; loss: 1.03; acc: 0.72
Batch: 700; loss: 0.97; acc: 0.66
Batch: 720; loss: 0.89; acc: 0.7
Batch: 740; loss: 0.79; acc: 0.77
Batch: 760; loss: 0.83; acc: 0.7
Batch: 780; loss: 0.73; acc: 0.73
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.91; acc: 0.67
Batch: 20; loss: 1.08; acc: 0.67
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.67; acc: 0.81
Val Epoch over. val_loss: 0.8510022218439989; val_accuracy: 0.7251194267515924 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.7
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.79; acc: 0.69
Batch: 60; loss: 1.07; acc: 0.66
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.72
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.7; acc: 0.75
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 1.06; acc: 0.7
Batch: 220; loss: 1.2; acc: 0.66
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 1.11; acc: 0.61
Batch: 280; loss: 0.7; acc: 0.69
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 0.62; acc: 0.78
Batch: 340; loss: 0.92; acc: 0.69
Batch: 360; loss: 0.73; acc: 0.78
Batch: 380; loss: 0.62; acc: 0.77
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.9; acc: 0.77
Batch: 480; loss: 0.74; acc: 0.72
Batch: 500; loss: 0.72; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.64
Batch: 540; loss: 1.0; acc: 0.69
Batch: 560; loss: 0.74; acc: 0.69
Batch: 580; loss: 0.71; acc: 0.77
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 1.04; acc: 0.64
Batch: 640; loss: 1.25; acc: 0.66
Batch: 660; loss: 0.74; acc: 0.75
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.78; acc: 0.73
Batch: 720; loss: 0.92; acc: 0.69
Batch: 740; loss: 1.28; acc: 0.62
Batch: 760; loss: 1.05; acc: 0.64
Batch: 780; loss: 0.79; acc: 0.77
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.91; acc: 0.66
Batch: 20; loss: 1.22; acc: 0.55
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 0.65; acc: 0.81
Val Epoch over. val_loss: 0.8202330413138031; val_accuracy: 0.7369625796178344 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.43; acc: 0.67
Batch: 20; loss: 0.59; acc: 0.78
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 1.15; acc: 0.7
Batch: 100; loss: 0.99; acc: 0.66
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.9; acc: 0.73
Batch: 160; loss: 0.76; acc: 0.77
Batch: 180; loss: 0.83; acc: 0.73
Batch: 200; loss: 0.81; acc: 0.78
Batch: 220; loss: 0.93; acc: 0.67
Batch: 240; loss: 0.83; acc: 0.77
Batch: 260; loss: 1.0; acc: 0.67
Batch: 280; loss: 0.86; acc: 0.72
Batch: 300; loss: 0.87; acc: 0.69
Batch: 320; loss: 0.72; acc: 0.7
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.85; acc: 0.73
Batch: 380; loss: 0.78; acc: 0.7
Batch: 400; loss: 0.87; acc: 0.75
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 1.15; acc: 0.69
Batch: 460; loss: 1.04; acc: 0.72
Batch: 480; loss: 0.76; acc: 0.81
Batch: 500; loss: 0.75; acc: 0.77
Batch: 520; loss: 0.81; acc: 0.77
Batch: 540; loss: 0.93; acc: 0.66
Batch: 560; loss: 0.98; acc: 0.66
Batch: 580; loss: 0.96; acc: 0.73
Batch: 600; loss: 0.66; acc: 0.78
Batch: 620; loss: 0.91; acc: 0.72
Batch: 640; loss: 0.84; acc: 0.72
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 1.04; acc: 0.73
Batch: 720; loss: 1.18; acc: 0.7
Batch: 740; loss: 1.17; acc: 0.64
Batch: 760; loss: 0.91; acc: 0.69
Batch: 780; loss: 0.85; acc: 0.77
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.92; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 0.68; acc: 0.72
Batch: 60; loss: 0.83; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.64; acc: 0.78
Val Epoch over. val_loss: 0.8361680687992437; val_accuracy: 0.7239251592356688 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.98; acc: 0.7
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.69
Batch: 120; loss: 0.95; acc: 0.72
Batch: 140; loss: 0.97; acc: 0.7
Batch: 160; loss: 0.93; acc: 0.73
Batch: 180; loss: 0.86; acc: 0.7
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 0.47; acc: 0.83
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.92; acc: 0.7
Batch: 280; loss: 0.74; acc: 0.73
Batch: 300; loss: 0.82; acc: 0.72
Batch: 320; loss: 0.91; acc: 0.75
Batch: 340; loss: 0.99; acc: 0.66
Batch: 360; loss: 1.45; acc: 0.58
Batch: 380; loss: 0.95; acc: 0.67
Batch: 400; loss: 0.84; acc: 0.72
Batch: 420; loss: 0.84; acc: 0.77
Batch: 440; loss: 0.83; acc: 0.73
Batch: 460; loss: 1.07; acc: 0.62
Batch: 480; loss: 1.24; acc: 0.62
Batch: 500; loss: 0.87; acc: 0.67
Batch: 520; loss: 0.78; acc: 0.78
Batch: 540; loss: 0.88; acc: 0.69
Batch: 560; loss: 0.84; acc: 0.72
Batch: 580; loss: 1.1; acc: 0.67
Batch: 600; loss: 1.1; acc: 0.67
Batch: 620; loss: 0.92; acc: 0.7
Batch: 640; loss: 1.12; acc: 0.59
Batch: 660; loss: 0.78; acc: 0.72
Batch: 680; loss: 0.92; acc: 0.69
Batch: 700; loss: 0.77; acc: 0.72
Batch: 720; loss: 0.89; acc: 0.77
Batch: 740; loss: 0.74; acc: 0.78
Batch: 760; loss: 0.89; acc: 0.67
Batch: 780; loss: 0.79; acc: 0.8
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.15; acc: 0.59
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.75
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 1.06; acc: 0.69
Batch: 140; loss: 0.62; acc: 0.83
Val Epoch over. val_loss: 0.8306054277404858; val_accuracy: 0.7310907643312102 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.8; acc: 0.67
Batch: 40; loss: 0.94; acc: 0.7
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.96; acc: 0.7
Batch: 160; loss: 1.0; acc: 0.7
Batch: 180; loss: 0.91; acc: 0.69
Batch: 200; loss: 0.94; acc: 0.66
Batch: 220; loss: 0.74; acc: 0.77
Batch: 240; loss: 0.74; acc: 0.7
Batch: 260; loss: 1.06; acc: 0.62
Batch: 280; loss: 0.71; acc: 0.78
Batch: 300; loss: 1.0; acc: 0.72
Batch: 320; loss: 0.87; acc: 0.78
Batch: 340; loss: 0.89; acc: 0.67
Batch: 360; loss: 0.95; acc: 0.7
Batch: 380; loss: 0.88; acc: 0.72
Batch: 400; loss: 0.93; acc: 0.66
Batch: 420; loss: 1.01; acc: 0.64
Batch: 440; loss: 0.98; acc: 0.67
Batch: 460; loss: 0.84; acc: 0.77
Batch: 480; loss: 1.2; acc: 0.56
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.67; acc: 0.75
Batch: 540; loss: 0.62; acc: 0.77
Batch: 560; loss: 0.81; acc: 0.78
Batch: 580; loss: 1.17; acc: 0.61
Batch: 600; loss: 0.68; acc: 0.75
Batch: 620; loss: 1.07; acc: 0.56
Batch: 640; loss: 0.92; acc: 0.72
Batch: 660; loss: 1.01; acc: 0.67
Batch: 680; loss: 0.66; acc: 0.78
Batch: 700; loss: 1.58; acc: 0.61
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.88; acc: 0.66
Batch: 760; loss: 0.79; acc: 0.73
Batch: 780; loss: 0.91; acc: 0.7
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.89; acc: 0.62
Batch: 20; loss: 1.2; acc: 0.58
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.85; acc: 0.73
Batch: 80; loss: 0.77; acc: 0.72
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.66; acc: 0.8
Val Epoch over. val_loss: 0.8375188607698792; val_accuracy: 0.7274084394904459 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 1.05; acc: 0.66
Batch: 60; loss: 0.84; acc: 0.7
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.7
Batch: 140; loss: 1.06; acc: 0.64
Batch: 160; loss: 0.8; acc: 0.72
Batch: 180; loss: 1.0; acc: 0.69
Batch: 200; loss: 0.73; acc: 0.78
Batch: 220; loss: 0.91; acc: 0.7
Batch: 240; loss: 0.84; acc: 0.8
Batch: 260; loss: 1.14; acc: 0.67
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 1.1; acc: 0.67
Batch: 320; loss: 0.97; acc: 0.72
Batch: 340; loss: 1.04; acc: 0.7
Batch: 360; loss: 0.93; acc: 0.66
Batch: 380; loss: 0.78; acc: 0.7
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.74; acc: 0.73
Batch: 460; loss: 0.91; acc: 0.77
Batch: 480; loss: 0.78; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.79; acc: 0.75
Batch: 540; loss: 0.93; acc: 0.7
Batch: 560; loss: 0.89; acc: 0.72
Batch: 580; loss: 0.91; acc: 0.73
Batch: 600; loss: 0.8; acc: 0.77
Batch: 620; loss: 1.21; acc: 0.66
Batch: 640; loss: 0.77; acc: 0.73
Batch: 660; loss: 1.11; acc: 0.7
Batch: 680; loss: 0.76; acc: 0.77
Batch: 700; loss: 0.98; acc: 0.73
Batch: 720; loss: 0.84; acc: 0.72
Batch: 740; loss: 0.84; acc: 0.72
Batch: 760; loss: 0.73; acc: 0.75
Batch: 780; loss: 1.05; acc: 0.62
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.58
Batch: 40; loss: 0.65; acc: 0.75
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.77
Batch: 140; loss: 0.66; acc: 0.83
Val Epoch over. val_loss: 0.8164253261438601; val_accuracy: 0.7391520700636943 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.99; acc: 0.59
Batch: 20; loss: 0.81; acc: 0.73
Batch: 40; loss: 0.82; acc: 0.73
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.77; acc: 0.7
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.72
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 1.03; acc: 0.73
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.8; acc: 0.72
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.74; acc: 0.77
Batch: 260; loss: 0.96; acc: 0.7
Batch: 280; loss: 0.92; acc: 0.69
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.72; acc: 0.77
Batch: 340; loss: 1.01; acc: 0.75
Batch: 360; loss: 0.79; acc: 0.7
Batch: 380; loss: 1.03; acc: 0.69
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.91; acc: 0.7
Batch: 440; loss: 0.82; acc: 0.7
Batch: 460; loss: 0.94; acc: 0.73
Batch: 480; loss: 1.08; acc: 0.61
Batch: 500; loss: 1.12; acc: 0.67
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.77; acc: 0.7
Batch: 560; loss: 1.02; acc: 0.72
Batch: 580; loss: 0.79; acc: 0.75
Batch: 600; loss: 0.75; acc: 0.67
Batch: 620; loss: 0.84; acc: 0.75
Batch: 640; loss: 0.87; acc: 0.64
Batch: 660; loss: 0.84; acc: 0.78
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.64; acc: 0.83
Batch: 720; loss: 0.83; acc: 0.75
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 1.03; acc: 0.72
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.86; acc: 0.69
Batch: 20; loss: 1.15; acc: 0.58
Batch: 40; loss: 0.63; acc: 0.73
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.59; acc: 0.81
Val Epoch over. val_loss: 0.8065397291426446; val_accuracy: 0.7409434713375797 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 0.83; acc: 0.64
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 1.04; acc: 0.62
Batch: 100; loss: 1.09; acc: 0.69
Batch: 120; loss: 1.0; acc: 0.7
Batch: 140; loss: 0.91; acc: 0.69
Batch: 160; loss: 0.8; acc: 0.73
Batch: 180; loss: 0.82; acc: 0.73
Batch: 200; loss: 1.03; acc: 0.7
Batch: 220; loss: 0.88; acc: 0.77
Batch: 240; loss: 0.85; acc: 0.75
Batch: 260; loss: 0.91; acc: 0.69
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.96; acc: 0.72
Batch: 320; loss: 0.81; acc: 0.75
Batch: 340; loss: 0.65; acc: 0.8
Batch: 360; loss: 0.86; acc: 0.7
Batch: 380; loss: 0.99; acc: 0.73
Batch: 400; loss: 1.14; acc: 0.75
Batch: 420; loss: 0.81; acc: 0.77
Batch: 440; loss: 1.41; acc: 0.64
Batch: 460; loss: 0.96; acc: 0.67
Batch: 480; loss: 0.73; acc: 0.72
Batch: 500; loss: 0.9; acc: 0.67
Batch: 520; loss: 0.9; acc: 0.73
Batch: 540; loss: 1.06; acc: 0.67
Batch: 560; loss: 0.79; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.72
Batch: 600; loss: 0.72; acc: 0.77
Batch: 620; loss: 0.79; acc: 0.66
Batch: 640; loss: 0.61; acc: 0.77
Batch: 660; loss: 0.77; acc: 0.77
Batch: 680; loss: 1.01; acc: 0.67
Batch: 700; loss: 1.09; acc: 0.64
Batch: 720; loss: 0.7; acc: 0.77
Batch: 740; loss: 0.98; acc: 0.61
Batch: 760; loss: 0.88; acc: 0.66
Batch: 780; loss: 0.8; acc: 0.75
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.66; acc: 0.72
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.59; acc: 0.8
Val Epoch over. val_loss: 0.8227087451014549; val_accuracy: 0.7342754777070064 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 1.15; acc: 0.62
Batch: 60; loss: 0.84; acc: 0.72
Batch: 80; loss: 1.16; acc: 0.58
Batch: 100; loss: 0.88; acc: 0.73
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.74; acc: 0.7
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.94; acc: 0.7
Batch: 200; loss: 0.77; acc: 0.77
Batch: 220; loss: 0.91; acc: 0.7
Batch: 240; loss: 0.86; acc: 0.72
Batch: 260; loss: 0.73; acc: 0.77
Batch: 280; loss: 0.9; acc: 0.72
Batch: 300; loss: 0.8; acc: 0.72
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 1.11; acc: 0.7
Batch: 360; loss: 0.76; acc: 0.8
Batch: 380; loss: 0.99; acc: 0.66
Batch: 400; loss: 1.01; acc: 0.67
Batch: 420; loss: 0.71; acc: 0.81
Batch: 440; loss: 0.88; acc: 0.72
Batch: 460; loss: 0.9; acc: 0.66
Batch: 480; loss: 0.71; acc: 0.77
Batch: 500; loss: 0.89; acc: 0.73
Batch: 520; loss: 0.85; acc: 0.7
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 0.89; acc: 0.77
Batch: 580; loss: 0.92; acc: 0.7
Batch: 600; loss: 0.81; acc: 0.69
Batch: 620; loss: 0.82; acc: 0.78
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 0.93; acc: 0.78
Batch: 680; loss: 1.01; acc: 0.73
Batch: 700; loss: 0.94; acc: 0.66
Batch: 720; loss: 0.86; acc: 0.72
Batch: 740; loss: 0.93; acc: 0.66
Batch: 760; loss: 0.68; acc: 0.8
Batch: 780; loss: 0.91; acc: 0.69
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.86; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.8011656208023145; val_accuracy: 0.7398487261146497 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.84; acc: 0.75
Batch: 20; loss: 0.8; acc: 0.72
Batch: 40; loss: 0.89; acc: 0.67
Batch: 60; loss: 1.04; acc: 0.64
Batch: 80; loss: 1.01; acc: 0.72
Batch: 100; loss: 1.21; acc: 0.69
Batch: 120; loss: 0.88; acc: 0.69
Batch: 140; loss: 0.85; acc: 0.67
Batch: 160; loss: 0.93; acc: 0.7
Batch: 180; loss: 0.88; acc: 0.75
Batch: 200; loss: 0.76; acc: 0.72
Batch: 220; loss: 0.88; acc: 0.72
Batch: 240; loss: 1.13; acc: 0.59
Batch: 260; loss: 0.86; acc: 0.75
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 1.25; acc: 0.66
Batch: 320; loss: 0.67; acc: 0.77
Batch: 340; loss: 0.78; acc: 0.77
Batch: 360; loss: 0.78; acc: 0.72
Batch: 380; loss: 0.92; acc: 0.66
Batch: 400; loss: 0.8; acc: 0.72
Batch: 420; loss: 1.0; acc: 0.69
Batch: 440; loss: 0.75; acc: 0.78
Batch: 460; loss: 0.72; acc: 0.86
Batch: 480; loss: 1.01; acc: 0.72
Batch: 500; loss: 0.73; acc: 0.77
Batch: 520; loss: 0.73; acc: 0.84
Batch: 540; loss: 1.03; acc: 0.66
Batch: 560; loss: 0.62; acc: 0.75
Batch: 580; loss: 1.16; acc: 0.67
Batch: 600; loss: 0.76; acc: 0.73
Batch: 620; loss: 0.85; acc: 0.72
Batch: 640; loss: 0.81; acc: 0.75
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.71; acc: 0.81
Batch: 700; loss: 1.27; acc: 0.67
Batch: 720; loss: 0.69; acc: 0.77
Batch: 740; loss: 0.83; acc: 0.77
Batch: 760; loss: 0.83; acc: 0.7
Batch: 780; loss: 1.07; acc: 0.67
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.72
Batch: 20; loss: 1.18; acc: 0.59
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.97; acc: 0.72
Batch: 140; loss: 0.62; acc: 0.8
Val Epoch over. val_loss: 0.8152333928900919; val_accuracy: 0.73546974522293 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.84; acc: 0.69
Batch: 40; loss: 1.23; acc: 0.61
Batch: 60; loss: 0.97; acc: 0.67
Batch: 80; loss: 0.77; acc: 0.72
Batch: 100; loss: 0.81; acc: 0.73
Batch: 120; loss: 0.86; acc: 0.64
Batch: 140; loss: 0.7; acc: 0.75
Batch: 160; loss: 0.95; acc: 0.7
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.77; acc: 0.73
Batch: 220; loss: 0.87; acc: 0.75
Batch: 240; loss: 0.85; acc: 0.64
Batch: 260; loss: 1.05; acc: 0.67
Batch: 280; loss: 0.81; acc: 0.7
Batch: 300; loss: 0.6; acc: 0.78
Batch: 320; loss: 0.94; acc: 0.73
Batch: 340; loss: 0.79; acc: 0.73
Batch: 360; loss: 0.74; acc: 0.77
Batch: 380; loss: 0.74; acc: 0.75
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.72; acc: 0.78
Batch: 460; loss: 0.93; acc: 0.69
Batch: 480; loss: 0.61; acc: 0.77
Batch: 500; loss: 1.05; acc: 0.61
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.88; acc: 0.72
Batch: 560; loss: 0.74; acc: 0.73
Batch: 580; loss: 1.05; acc: 0.64
Batch: 600; loss: 0.82; acc: 0.73
Batch: 620; loss: 0.89; acc: 0.7
Batch: 640; loss: 0.83; acc: 0.67
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.99; acc: 0.73
Batch: 700; loss: 0.75; acc: 0.67
Batch: 720; loss: 0.88; acc: 0.72
Batch: 740; loss: 0.81; acc: 0.7
Batch: 760; loss: 0.91; acc: 0.72
Batch: 780; loss: 0.95; acc: 0.7
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 1.13; acc: 0.66
Batch: 40; loss: 0.69; acc: 0.73
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.63; acc: 0.78
Val Epoch over. val_loss: 0.8108417251307494; val_accuracy: 0.7360668789808917 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 1.08; acc: 0.66
Batch: 40; loss: 0.7; acc: 0.77
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.99; acc: 0.72
Batch: 100; loss: 0.83; acc: 0.7
Batch: 120; loss: 0.82; acc: 0.69
Batch: 140; loss: 0.91; acc: 0.7
Batch: 160; loss: 0.85; acc: 0.7
Batch: 180; loss: 1.02; acc: 0.72
Batch: 200; loss: 0.88; acc: 0.73
Batch: 220; loss: 1.23; acc: 0.52
Batch: 240; loss: 0.85; acc: 0.73
Batch: 260; loss: 0.83; acc: 0.75
Batch: 280; loss: 0.94; acc: 0.75
Batch: 300; loss: 1.09; acc: 0.62
Batch: 320; loss: 0.91; acc: 0.72
Batch: 340; loss: 0.77; acc: 0.72
Batch: 360; loss: 0.77; acc: 0.77
Batch: 380; loss: 0.92; acc: 0.69
Batch: 400; loss: 0.87; acc: 0.72
Batch: 420; loss: 0.84; acc: 0.77
Batch: 440; loss: 0.8; acc: 0.69
Batch: 460; loss: 0.92; acc: 0.73
Batch: 480; loss: 0.6; acc: 0.78
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.63; acc: 0.77
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.96; acc: 0.7
Batch: 580; loss: 0.9; acc: 0.73
Batch: 600; loss: 0.89; acc: 0.78
Batch: 620; loss: 0.99; acc: 0.72
Batch: 640; loss: 0.89; acc: 0.72
Batch: 660; loss: 0.96; acc: 0.66
Batch: 680; loss: 0.8; acc: 0.72
Batch: 700; loss: 0.86; acc: 0.72
Batch: 720; loss: 0.81; acc: 0.73
Batch: 740; loss: 1.02; acc: 0.66
Batch: 760; loss: 1.42; acc: 0.61
Batch: 780; loss: 0.98; acc: 0.67
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.89; acc: 0.66
Batch: 20; loss: 1.17; acc: 0.62
Batch: 40; loss: 0.71; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 1.01; acc: 0.72
Batch: 140; loss: 0.66; acc: 0.83
Val Epoch over. val_loss: 0.8270296520867925; val_accuracy: 0.732484076433121 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.86; acc: 0.66
Batch: 60; loss: 0.78; acc: 0.75
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.66
Batch: 140; loss: 0.77; acc: 0.75
Batch: 160; loss: 0.77; acc: 0.83
Batch: 180; loss: 0.86; acc: 0.69
Batch: 200; loss: 0.9; acc: 0.75
Batch: 220; loss: 1.04; acc: 0.66
Batch: 240; loss: 0.84; acc: 0.78
Batch: 260; loss: 0.97; acc: 0.66
Batch: 280; loss: 1.0; acc: 0.66
Batch: 300; loss: 0.69; acc: 0.78
Batch: 320; loss: 0.75; acc: 0.77
Batch: 340; loss: 0.87; acc: 0.66
Batch: 360; loss: 0.92; acc: 0.69
Batch: 380; loss: 0.74; acc: 0.75
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 0.67; acc: 0.8
Batch: 440; loss: 0.7; acc: 0.67
Batch: 460; loss: 0.88; acc: 0.72
Batch: 480; loss: 1.03; acc: 0.61
Batch: 500; loss: 0.82; acc: 0.69
Batch: 520; loss: 1.09; acc: 0.66
Batch: 540; loss: 0.84; acc: 0.73
Batch: 560; loss: 0.88; acc: 0.66
Batch: 580; loss: 0.62; acc: 0.77
Batch: 600; loss: 1.03; acc: 0.66
Batch: 620; loss: 0.84; acc: 0.72
Batch: 640; loss: 0.86; acc: 0.69
Batch: 660; loss: 0.89; acc: 0.69
Batch: 680; loss: 0.7; acc: 0.77
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 0.88; acc: 0.66
Batch: 740; loss: 0.88; acc: 0.7
Batch: 760; loss: 1.18; acc: 0.66
Batch: 780; loss: 0.91; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.72
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.8039086823630485; val_accuracy: 0.7383558917197452 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.66
Batch: 40; loss: 0.75; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.72
Batch: 80; loss: 0.66; acc: 0.75
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 1.2; acc: 0.69
Batch: 160; loss: 0.82; acc: 0.73
Batch: 180; loss: 0.93; acc: 0.67
Batch: 200; loss: 0.97; acc: 0.72
Batch: 220; loss: 0.9; acc: 0.78
Batch: 240; loss: 1.04; acc: 0.67
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.9; acc: 0.7
Batch: 300; loss: 0.97; acc: 0.77
Batch: 320; loss: 0.92; acc: 0.72
Batch: 340; loss: 0.84; acc: 0.69
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.75
Batch: 400; loss: 1.07; acc: 0.62
Batch: 420; loss: 1.06; acc: 0.66
Batch: 440; loss: 0.74; acc: 0.75
Batch: 460; loss: 0.74; acc: 0.73
Batch: 480; loss: 0.88; acc: 0.7
Batch: 500; loss: 1.02; acc: 0.72
Batch: 520; loss: 0.98; acc: 0.69
Batch: 540; loss: 0.69; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.73
Batch: 600; loss: 0.92; acc: 0.7
Batch: 620; loss: 1.16; acc: 0.69
Batch: 640; loss: 0.76; acc: 0.7
Batch: 660; loss: 0.73; acc: 0.7
Batch: 680; loss: 0.82; acc: 0.69
Batch: 700; loss: 0.8; acc: 0.7
Batch: 720; loss: 0.87; acc: 0.72
Batch: 740; loss: 0.95; acc: 0.7
Batch: 760; loss: 0.99; acc: 0.69
Batch: 780; loss: 0.78; acc: 0.7
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.89; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.66; acc: 0.75
Batch: 60; loss: 0.84; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.99; acc: 0.72
Batch: 140; loss: 0.63; acc: 0.81
Val Epoch over. val_loss: 0.8041775441093809; val_accuracy: 0.7414410828025477 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.0; acc: 0.69
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 1.14; acc: 0.59
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.73
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.66
Batch: 140; loss: 0.91; acc: 0.7
Batch: 160; loss: 0.59; acc: 0.89
Batch: 180; loss: 1.01; acc: 0.67
Batch: 200; loss: 1.13; acc: 0.64
Batch: 220; loss: 0.9; acc: 0.67
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.95; acc: 0.7
Batch: 280; loss: 1.04; acc: 0.7
Batch: 300; loss: 1.11; acc: 0.69
Batch: 320; loss: 0.72; acc: 0.77
Batch: 340; loss: 0.87; acc: 0.72
Batch: 360; loss: 0.8; acc: 0.72
Batch: 380; loss: 1.12; acc: 0.69
Batch: 400; loss: 0.8; acc: 0.75
Batch: 420; loss: 1.05; acc: 0.72
Batch: 440; loss: 0.75; acc: 0.75
Batch: 460; loss: 0.93; acc: 0.67
Batch: 480; loss: 0.55; acc: 0.77
Batch: 500; loss: 0.95; acc: 0.66
Batch: 520; loss: 0.96; acc: 0.72
Batch: 540; loss: 0.66; acc: 0.8
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 1.14; acc: 0.55
Batch: 600; loss: 0.77; acc: 0.72
Batch: 620; loss: 0.83; acc: 0.73
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 1.03; acc: 0.64
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.95; acc: 0.7
Batch: 720; loss: 1.04; acc: 0.72
Batch: 740; loss: 1.09; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.67
Batch: 780; loss: 0.86; acc: 0.8
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.59; acc: 0.8
Val Epoch over. val_loss: 0.8014716225065244; val_accuracy: 0.7419386942675159 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.98; acc: 0.7
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 1.05; acc: 0.62
Batch: 60; loss: 0.92; acc: 0.64
Batch: 80; loss: 0.81; acc: 0.7
Batch: 100; loss: 0.75; acc: 0.81
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.84; acc: 0.72
Batch: 160; loss: 0.93; acc: 0.75
Batch: 180; loss: 0.77; acc: 0.77
Batch: 200; loss: 0.93; acc: 0.75
Batch: 220; loss: 1.05; acc: 0.66
Batch: 240; loss: 0.92; acc: 0.67
Batch: 260; loss: 0.81; acc: 0.73
Batch: 280; loss: 0.77; acc: 0.77
Batch: 300; loss: 0.9; acc: 0.72
Batch: 320; loss: 1.24; acc: 0.61
Batch: 340; loss: 0.87; acc: 0.62
Batch: 360; loss: 0.9; acc: 0.69
Batch: 380; loss: 0.75; acc: 0.73
Batch: 400; loss: 0.64; acc: 0.77
Batch: 420; loss: 0.59; acc: 0.78
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 1.03; acc: 0.66
Batch: 480; loss: 0.88; acc: 0.67
Batch: 500; loss: 0.84; acc: 0.73
Batch: 520; loss: 0.79; acc: 0.75
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.66; acc: 0.73
Batch: 580; loss: 1.25; acc: 0.61
Batch: 600; loss: 0.75; acc: 0.75
Batch: 620; loss: 1.04; acc: 0.7
Batch: 640; loss: 1.09; acc: 0.66
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.66; acc: 0.75
Batch: 700; loss: 0.86; acc: 0.78
Batch: 720; loss: 0.89; acc: 0.73
Batch: 740; loss: 1.23; acc: 0.56
Batch: 760; loss: 0.85; acc: 0.7
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.89; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.64; acc: 0.75
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.65; acc: 0.81
Val Epoch over. val_loss: 0.8118784600382398; val_accuracy: 0.7399482484076433 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.9; acc: 0.7
Batch: 60; loss: 1.01; acc: 0.7
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.72
Batch: 140; loss: 0.75; acc: 0.8
Batch: 160; loss: 0.88; acc: 0.77
Batch: 180; loss: 1.06; acc: 0.61
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.92; acc: 0.8
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.9; acc: 0.73
Batch: 280; loss: 0.79; acc: 0.75
Batch: 300; loss: 0.94; acc: 0.7
Batch: 320; loss: 0.79; acc: 0.72
Batch: 340; loss: 0.76; acc: 0.73
Batch: 360; loss: 0.83; acc: 0.62
Batch: 380; loss: 0.85; acc: 0.75
Batch: 400; loss: 0.82; acc: 0.7
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.87; acc: 0.72
Batch: 460; loss: 0.89; acc: 0.7
Batch: 480; loss: 1.1; acc: 0.64
Batch: 500; loss: 0.64; acc: 0.72
Batch: 520; loss: 0.76; acc: 0.73
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.79; acc: 0.7
Batch: 580; loss: 1.17; acc: 0.67
Batch: 600; loss: 0.95; acc: 0.66
Batch: 620; loss: 0.75; acc: 0.77
Batch: 640; loss: 0.89; acc: 0.69
Batch: 660; loss: 1.02; acc: 0.69
Batch: 680; loss: 0.96; acc: 0.69
Batch: 700; loss: 0.73; acc: 0.69
Batch: 720; loss: 0.91; acc: 0.77
Batch: 740; loss: 0.7; acc: 0.75
Batch: 760; loss: 0.8; acc: 0.72
Batch: 780; loss: 0.97; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.69
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.63; acc: 0.81
Val Epoch over. val_loss: 0.8040380944871599; val_accuracy: 0.738953025477707 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.86; acc: 0.66
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.56; acc: 0.78
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.72; acc: 0.8
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.99; acc: 0.72
Batch: 160; loss: 0.79; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.63; acc: 0.77
Batch: 220; loss: 1.02; acc: 0.62
Batch: 240; loss: 0.92; acc: 0.73
Batch: 260; loss: 0.79; acc: 0.78
Batch: 280; loss: 0.78; acc: 0.7
Batch: 300; loss: 0.79; acc: 0.72
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 0.64; acc: 0.8
Batch: 360; loss: 1.15; acc: 0.62
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 1.11; acc: 0.66
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.67; acc: 0.77
Batch: 460; loss: 0.76; acc: 0.66
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 0.91; acc: 0.77
Batch: 520; loss: 0.68; acc: 0.77
Batch: 540; loss: 0.72; acc: 0.72
Batch: 560; loss: 1.05; acc: 0.69
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 1.14; acc: 0.73
Batch: 620; loss: 1.15; acc: 0.67
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.72
Batch: 680; loss: 0.8; acc: 0.75
Batch: 700; loss: 0.73; acc: 0.77
Batch: 720; loss: 0.66; acc: 0.73
Batch: 740; loss: 0.9; acc: 0.7
Batch: 760; loss: 0.87; acc: 0.67
Batch: 780; loss: 0.85; acc: 0.69
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.61
Batch: 40; loss: 0.68; acc: 0.73
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.63; acc: 0.83
Val Epoch over. val_loss: 0.8030938454874003; val_accuracy: 0.7412420382165605 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.88; acc: 0.72
Batch: 20; loss: 1.05; acc: 0.66
Batch: 40; loss: 0.98; acc: 0.67
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.7
Batch: 120; loss: 1.04; acc: 0.7
Batch: 140; loss: 0.89; acc: 0.73
Batch: 160; loss: 0.86; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.78
Batch: 200; loss: 0.99; acc: 0.69
Batch: 220; loss: 0.83; acc: 0.75
Batch: 240; loss: 0.89; acc: 0.73
Batch: 260; loss: 1.02; acc: 0.73
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 1.04; acc: 0.62
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.92; acc: 0.67
Batch: 380; loss: 0.96; acc: 0.69
Batch: 400; loss: 0.72; acc: 0.75
Batch: 420; loss: 0.67; acc: 0.73
Batch: 440; loss: 0.7; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.96; acc: 0.61
Batch: 500; loss: 0.88; acc: 0.75
Batch: 520; loss: 0.83; acc: 0.73
Batch: 540; loss: 1.06; acc: 0.59
Batch: 560; loss: 0.74; acc: 0.78
Batch: 580; loss: 0.79; acc: 0.67
Batch: 600; loss: 0.8; acc: 0.7
Batch: 620; loss: 0.98; acc: 0.72
Batch: 640; loss: 0.84; acc: 0.72
Batch: 660; loss: 1.1; acc: 0.62
Batch: 680; loss: 0.92; acc: 0.66
Batch: 700; loss: 0.89; acc: 0.73
Batch: 720; loss: 0.82; acc: 0.7
Batch: 740; loss: 1.01; acc: 0.61
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.73; acc: 0.77
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.7993818010873855; val_accuracy: 0.7410429936305732 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.92; acc: 0.75
Batch: 20; loss: 0.96; acc: 0.66
Batch: 40; loss: 0.88; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.7
Batch: 80; loss: 1.3; acc: 0.58
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 0.83; acc: 0.77
Batch: 160; loss: 0.74; acc: 0.75
Batch: 180; loss: 0.91; acc: 0.72
Batch: 200; loss: 1.17; acc: 0.64
Batch: 220; loss: 0.85; acc: 0.73
Batch: 240; loss: 0.74; acc: 0.8
Batch: 260; loss: 0.78; acc: 0.69
Batch: 280; loss: 0.99; acc: 0.64
Batch: 300; loss: 0.81; acc: 0.75
Batch: 320; loss: 0.81; acc: 0.77
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.81; acc: 0.72
Batch: 400; loss: 1.0; acc: 0.72
Batch: 420; loss: 0.92; acc: 0.75
Batch: 440; loss: 0.97; acc: 0.66
Batch: 460; loss: 0.86; acc: 0.73
Batch: 480; loss: 0.84; acc: 0.81
Batch: 500; loss: 0.89; acc: 0.7
Batch: 520; loss: 0.68; acc: 0.77
Batch: 540; loss: 0.89; acc: 0.75
Batch: 560; loss: 1.01; acc: 0.67
Batch: 580; loss: 0.85; acc: 0.77
Batch: 600; loss: 0.68; acc: 0.75
Batch: 620; loss: 0.61; acc: 0.73
Batch: 640; loss: 0.81; acc: 0.72
Batch: 660; loss: 0.85; acc: 0.7
Batch: 680; loss: 0.83; acc: 0.7
Batch: 700; loss: 0.91; acc: 0.7
Batch: 720; loss: 0.95; acc: 0.67
Batch: 740; loss: 1.03; acc: 0.73
Batch: 760; loss: 0.83; acc: 0.75
Batch: 780; loss: 1.12; acc: 0.67
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.8010948769226196; val_accuracy: 0.7412420382165605 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.01; acc: 0.62
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.79; acc: 0.72
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 1.15; acc: 0.69
Batch: 160; loss: 0.83; acc: 0.75
Batch: 180; loss: 0.73; acc: 0.84
Batch: 200; loss: 0.82; acc: 0.64
Batch: 220; loss: 0.64; acc: 0.72
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 1.01; acc: 0.66
Batch: 320; loss: 1.04; acc: 0.69
Batch: 340; loss: 0.91; acc: 0.7
Batch: 360; loss: 0.85; acc: 0.73
Batch: 380; loss: 1.01; acc: 0.72
Batch: 400; loss: 1.11; acc: 0.62
Batch: 420; loss: 0.78; acc: 0.69
Batch: 440; loss: 0.94; acc: 0.7
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.93; acc: 0.7
Batch: 500; loss: 0.62; acc: 0.78
Batch: 520; loss: 0.96; acc: 0.73
Batch: 540; loss: 0.83; acc: 0.7
Batch: 560; loss: 0.75; acc: 0.77
Batch: 580; loss: 0.81; acc: 0.75
Batch: 600; loss: 0.87; acc: 0.67
Batch: 620; loss: 0.81; acc: 0.73
Batch: 640; loss: 1.2; acc: 0.73
Batch: 660; loss: 0.89; acc: 0.69
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.85; acc: 0.67
Batch: 720; loss: 1.02; acc: 0.72
Batch: 740; loss: 0.97; acc: 0.73
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.61
Batch: 40; loss: 0.65; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.83
Val Epoch over. val_loss: 0.8001720631957814; val_accuracy: 0.7438296178343949 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.9; acc: 0.72
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.95; acc: 0.72
Batch: 100; loss: 1.07; acc: 0.53
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.78; acc: 0.73
Batch: 160; loss: 0.96; acc: 0.67
Batch: 180; loss: 1.0; acc: 0.75
Batch: 200; loss: 0.9; acc: 0.72
Batch: 220; loss: 0.76; acc: 0.77
Batch: 240; loss: 0.81; acc: 0.77
Batch: 260; loss: 0.61; acc: 0.78
Batch: 280; loss: 0.74; acc: 0.78
Batch: 300; loss: 0.73; acc: 0.75
Batch: 320; loss: 0.82; acc: 0.78
Batch: 340; loss: 0.92; acc: 0.66
Batch: 360; loss: 0.89; acc: 0.69
Batch: 380; loss: 0.96; acc: 0.67
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.72
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.98; acc: 0.69
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 0.83; acc: 0.75
Batch: 540; loss: 0.78; acc: 0.66
Batch: 560; loss: 0.91; acc: 0.7
Batch: 580; loss: 0.87; acc: 0.72
Batch: 600; loss: 0.87; acc: 0.73
Batch: 620; loss: 0.93; acc: 0.72
Batch: 640; loss: 0.85; acc: 0.75
Batch: 660; loss: 0.79; acc: 0.81
Batch: 680; loss: 1.05; acc: 0.75
Batch: 700; loss: 0.73; acc: 0.77
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.78; acc: 0.73
Batch: 760; loss: 0.68; acc: 0.88
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.69; acc: 0.73
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.63; acc: 0.83
Val Epoch over. val_loss: 0.8053529355556343; val_accuracy: 0.7404458598726115 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.93; acc: 0.75
Batch: 20; loss: 0.89; acc: 0.64
Batch: 40; loss: 0.75; acc: 0.69
Batch: 60; loss: 1.12; acc: 0.53
Batch: 80; loss: 0.8; acc: 0.72
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.67; acc: 0.77
Batch: 160; loss: 0.92; acc: 0.7
Batch: 180; loss: 0.7; acc: 0.75
Batch: 200; loss: 0.85; acc: 0.73
Batch: 220; loss: 0.95; acc: 0.62
Batch: 240; loss: 1.03; acc: 0.66
Batch: 260; loss: 0.7; acc: 0.72
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.85; acc: 0.7
Batch: 320; loss: 0.76; acc: 0.77
Batch: 340; loss: 0.76; acc: 0.72
Batch: 360; loss: 0.84; acc: 0.75
Batch: 380; loss: 1.07; acc: 0.67
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.87; acc: 0.77
Batch: 440; loss: 0.72; acc: 0.72
Batch: 460; loss: 0.94; acc: 0.75
Batch: 480; loss: 0.88; acc: 0.66
Batch: 500; loss: 0.95; acc: 0.69
Batch: 520; loss: 0.93; acc: 0.72
Batch: 540; loss: 0.68; acc: 0.75
Batch: 560; loss: 0.89; acc: 0.7
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.81; acc: 0.72
Batch: 620; loss: 1.08; acc: 0.67
Batch: 640; loss: 1.04; acc: 0.59
Batch: 660; loss: 0.92; acc: 0.66
Batch: 680; loss: 0.79; acc: 0.73
Batch: 700; loss: 1.03; acc: 0.7
Batch: 720; loss: 0.81; acc: 0.78
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.96; acc: 0.72
Batch: 780; loss: 1.0; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 1.11; acc: 0.64
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.8000453435311652; val_accuracy: 0.7414410828025477 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 1.03; acc: 0.67
Batch: 80; loss: 1.4; acc: 0.56
Batch: 100; loss: 0.68; acc: 0.73
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.97; acc: 0.64
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.64; acc: 0.78
Batch: 200; loss: 0.73; acc: 0.73
Batch: 220; loss: 0.81; acc: 0.66
Batch: 240; loss: 0.79; acc: 0.72
Batch: 260; loss: 0.98; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.87; acc: 0.73
Batch: 340; loss: 0.81; acc: 0.73
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.92; acc: 0.67
Batch: 400; loss: 0.88; acc: 0.77
Batch: 420; loss: 0.99; acc: 0.7
Batch: 440; loss: 0.91; acc: 0.64
Batch: 460; loss: 0.84; acc: 0.69
Batch: 480; loss: 1.07; acc: 0.61
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.85; acc: 0.69
Batch: 540; loss: 1.05; acc: 0.62
Batch: 560; loss: 0.69; acc: 0.77
Batch: 580; loss: 1.07; acc: 0.64
Batch: 600; loss: 0.69; acc: 0.83
Batch: 620; loss: 0.83; acc: 0.7
Batch: 640; loss: 0.75; acc: 0.75
Batch: 660; loss: 1.17; acc: 0.64
Batch: 680; loss: 1.06; acc: 0.67
Batch: 700; loss: 0.82; acc: 0.77
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.78; acc: 0.75
Batch: 780; loss: 0.76; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.88; acc: 0.66
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 0.65; acc: 0.75
Batch: 60; loss: 0.82; acc: 0.78
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.62; acc: 0.81
Val Epoch over. val_loss: 0.8011785447597504; val_accuracy: 0.7411425159235668 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.06; acc: 0.62
Batch: 20; loss: 0.81; acc: 0.72
Batch: 40; loss: 1.16; acc: 0.69
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 1.07; acc: 0.66
Batch: 160; loss: 0.77; acc: 0.75
Batch: 180; loss: 0.95; acc: 0.77
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.74; acc: 0.75
Batch: 240; loss: 0.65; acc: 0.75
Batch: 260; loss: 0.83; acc: 0.66
Batch: 280; loss: 0.78; acc: 0.73
Batch: 300; loss: 0.7; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.92; acc: 0.75
Batch: 360; loss: 1.04; acc: 0.61
Batch: 380; loss: 0.84; acc: 0.73
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.89; acc: 0.78
Batch: 440; loss: 0.79; acc: 0.73
Batch: 460; loss: 0.75; acc: 0.72
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.73
Batch: 520; loss: 0.73; acc: 0.73
Batch: 540; loss: 1.08; acc: 0.67
Batch: 560; loss: 1.0; acc: 0.75
Batch: 580; loss: 1.06; acc: 0.67
Batch: 600; loss: 0.96; acc: 0.64
Batch: 620; loss: 1.03; acc: 0.72
Batch: 640; loss: 1.05; acc: 0.62
Batch: 660; loss: 0.85; acc: 0.72
Batch: 680; loss: 0.8; acc: 0.75
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 1.02; acc: 0.67
Batch: 740; loss: 0.99; acc: 0.7
Batch: 760; loss: 1.0; acc: 0.62
Batch: 780; loss: 0.65; acc: 0.78
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 1.14; acc: 0.61
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.8020181226882206; val_accuracy: 0.7397492038216561 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 1.05; acc: 0.62
Batch: 40; loss: 0.63; acc: 0.77
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 1.03; acc: 0.66
Batch: 160; loss: 0.88; acc: 0.73
Batch: 180; loss: 1.02; acc: 0.72
Batch: 200; loss: 0.71; acc: 0.75
Batch: 220; loss: 0.87; acc: 0.73
Batch: 240; loss: 1.3; acc: 0.69
Batch: 260; loss: 1.24; acc: 0.62
Batch: 280; loss: 1.18; acc: 0.61
Batch: 300; loss: 0.92; acc: 0.67
Batch: 320; loss: 0.86; acc: 0.62
Batch: 340; loss: 0.77; acc: 0.73
Batch: 360; loss: 1.05; acc: 0.75
Batch: 380; loss: 0.63; acc: 0.81
Batch: 400; loss: 1.04; acc: 0.66
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.9; acc: 0.75
Batch: 480; loss: 0.93; acc: 0.73
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.75; acc: 0.78
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.75; acc: 0.8
Batch: 580; loss: 1.03; acc: 0.67
Batch: 600; loss: 0.96; acc: 0.7
Batch: 620; loss: 1.32; acc: 0.62
Batch: 640; loss: 0.83; acc: 0.83
Batch: 660; loss: 0.82; acc: 0.75
Batch: 680; loss: 0.93; acc: 0.66
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.88; acc: 0.72
Batch: 740; loss: 0.74; acc: 0.78
Batch: 760; loss: 1.03; acc: 0.66
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.64
Batch: 20; loss: 1.09; acc: 0.64
Batch: 40; loss: 0.66; acc: 0.75
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.84
Val Epoch over. val_loss: 0.8021214965042794; val_accuracy: 0.7417396496815286 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 1.07; acc: 0.64
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 1.04; acc: 0.78
Batch: 80; loss: 0.84; acc: 0.72
Batch: 100; loss: 0.84; acc: 0.7
Batch: 120; loss: 0.85; acc: 0.7
Batch: 140; loss: 0.92; acc: 0.72
Batch: 160; loss: 0.93; acc: 0.77
Batch: 180; loss: 1.2; acc: 0.53
Batch: 200; loss: 0.89; acc: 0.72
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.76; acc: 0.77
Batch: 260; loss: 1.08; acc: 0.72
Batch: 280; loss: 1.0; acc: 0.69
Batch: 300; loss: 0.8; acc: 0.75
Batch: 320; loss: 0.67; acc: 0.75
Batch: 340; loss: 1.08; acc: 0.64
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 0.96; acc: 0.64
Batch: 400; loss: 0.89; acc: 0.75
Batch: 420; loss: 0.78; acc: 0.75
Batch: 440; loss: 1.08; acc: 0.59
Batch: 460; loss: 0.81; acc: 0.73
Batch: 480; loss: 1.08; acc: 0.64
Batch: 500; loss: 0.73; acc: 0.75
Batch: 520; loss: 0.77; acc: 0.7
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.79; acc: 0.75
Batch: 580; loss: 0.9; acc: 0.72
Batch: 600; loss: 1.0; acc: 0.73
Batch: 620; loss: 1.08; acc: 0.72
Batch: 640; loss: 0.86; acc: 0.7
Batch: 660; loss: 0.95; acc: 0.72
Batch: 680; loss: 0.86; acc: 0.73
Batch: 700; loss: 0.81; acc: 0.75
Batch: 720; loss: 0.8; acc: 0.69
Batch: 740; loss: 0.72; acc: 0.77
Batch: 760; loss: 0.9; acc: 0.69
Batch: 780; loss: 0.79; acc: 0.69
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.8
Val Epoch over. val_loss: 0.803513051218288; val_accuracy: 0.7404458598726115 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.7
Batch: 20; loss: 0.79; acc: 0.67
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.96; acc: 0.62
Batch: 80; loss: 0.89; acc: 0.72
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 1.06; acc: 0.66
Batch: 160; loss: 0.92; acc: 0.73
Batch: 180; loss: 0.88; acc: 0.73
Batch: 200; loss: 0.87; acc: 0.77
Batch: 220; loss: 0.96; acc: 0.67
Batch: 240; loss: 1.27; acc: 0.64
Batch: 260; loss: 0.93; acc: 0.72
Batch: 280; loss: 0.72; acc: 0.75
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.96; acc: 0.69
Batch: 340; loss: 1.06; acc: 0.75
Batch: 360; loss: 0.95; acc: 0.64
Batch: 380; loss: 0.73; acc: 0.8
Batch: 400; loss: 0.7; acc: 0.73
Batch: 420; loss: 0.96; acc: 0.66
Batch: 440; loss: 0.75; acc: 0.73
Batch: 460; loss: 0.86; acc: 0.69
Batch: 480; loss: 1.04; acc: 0.73
Batch: 500; loss: 1.1; acc: 0.62
Batch: 520; loss: 0.99; acc: 0.67
Batch: 540; loss: 0.79; acc: 0.72
Batch: 560; loss: 0.64; acc: 0.77
Batch: 580; loss: 0.81; acc: 0.69
Batch: 600; loss: 0.99; acc: 0.64
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.98; acc: 0.7
Batch: 680; loss: 0.8; acc: 0.69
Batch: 700; loss: 0.88; acc: 0.72
Batch: 720; loss: 0.92; acc: 0.72
Batch: 740; loss: 1.03; acc: 0.69
Batch: 760; loss: 0.92; acc: 0.72
Batch: 780; loss: 0.8; acc: 0.73
Train Epoch over. train_loss: 0.86; train_accuracy: 0.73 

Batch: 0; loss: 0.89; acc: 0.66
Batch: 20; loss: 1.13; acc: 0.59
Batch: 40; loss: 0.67; acc: 0.75
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.62; acc: 0.81
Val Epoch over. val_loss: 0.802987264409946; val_accuracy: 0.7387539808917197 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 0.94; acc: 0.67
Batch: 40; loss: 0.71; acc: 0.78
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 1.07; acc: 0.7
Batch: 100; loss: 0.9; acc: 0.69
Batch: 120; loss: 0.67; acc: 0.73
Batch: 140; loss: 1.12; acc: 0.69
Batch: 160; loss: 0.98; acc: 0.7
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.7
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 1.06; acc: 0.73
Batch: 280; loss: 0.96; acc: 0.67
Batch: 300; loss: 1.11; acc: 0.62
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.91; acc: 0.64
Batch: 360; loss: 0.84; acc: 0.72
Batch: 380; loss: 0.89; acc: 0.67
Batch: 400; loss: 0.61; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.73
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 1.19; acc: 0.61
Batch: 480; loss: 0.84; acc: 0.75
Batch: 500; loss: 0.62; acc: 0.78
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 0.85; acc: 0.72
Batch: 580; loss: 0.8; acc: 0.7
Batch: 600; loss: 0.86; acc: 0.75
Batch: 620; loss: 0.78; acc: 0.7
Batch: 640; loss: 0.85; acc: 0.73
Batch: 660; loss: 1.06; acc: 0.72
Batch: 680; loss: 0.97; acc: 0.67
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.75; acc: 0.75
Batch: 740; loss: 1.07; acc: 0.72
Batch: 760; loss: 0.94; acc: 0.72
Batch: 780; loss: 0.93; acc: 0.73
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.83
Val Epoch over. val_loss: 0.7990160130771102; val_accuracy: 0.7434315286624203 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 1.15; acc: 0.67
Batch: 40; loss: 1.04; acc: 0.59
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 0.9; acc: 0.69
Batch: 100; loss: 1.17; acc: 0.64
Batch: 120; loss: 0.96; acc: 0.7
Batch: 140; loss: 0.87; acc: 0.75
Batch: 160; loss: 0.82; acc: 0.7
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.9; acc: 0.73
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.71; acc: 0.72
Batch: 260; loss: 0.69; acc: 0.75
Batch: 280; loss: 0.64; acc: 0.78
Batch: 300; loss: 1.14; acc: 0.66
Batch: 320; loss: 0.84; acc: 0.72
Batch: 340; loss: 0.72; acc: 0.8
Batch: 360; loss: 0.89; acc: 0.72
Batch: 380; loss: 0.84; acc: 0.73
Batch: 400; loss: 0.79; acc: 0.72
Batch: 420; loss: 0.74; acc: 0.73
Batch: 440; loss: 1.01; acc: 0.69
Batch: 460; loss: 1.02; acc: 0.72
Batch: 480; loss: 1.01; acc: 0.69
Batch: 500; loss: 1.06; acc: 0.72
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.88; acc: 0.66
Batch: 560; loss: 0.82; acc: 0.69
Batch: 580; loss: 1.14; acc: 0.61
Batch: 600; loss: 0.78; acc: 0.73
Batch: 620; loss: 0.85; acc: 0.73
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.97; acc: 0.7
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.78; acc: 0.8
Batch: 720; loss: 0.86; acc: 0.77
Batch: 740; loss: 0.92; acc: 0.78
Batch: 760; loss: 0.72; acc: 0.73
Batch: 780; loss: 0.74; acc: 0.83
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.8009810762800229; val_accuracy: 0.7424363057324841 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.85; acc: 0.73
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.9; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.67
Batch: 120; loss: 0.89; acc: 0.69
Batch: 140; loss: 0.82; acc: 0.75
Batch: 160; loss: 1.22; acc: 0.62
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.87; acc: 0.69
Batch: 220; loss: 0.9; acc: 0.77
Batch: 240; loss: 0.97; acc: 0.66
Batch: 260; loss: 1.11; acc: 0.64
Batch: 280; loss: 0.79; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 1.09; acc: 0.67
Batch: 340; loss: 1.11; acc: 0.64
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.88; acc: 0.73
Batch: 400; loss: 0.88; acc: 0.73
Batch: 420; loss: 1.09; acc: 0.67
Batch: 440; loss: 0.82; acc: 0.73
Batch: 460; loss: 0.99; acc: 0.67
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 1.06; acc: 0.67
Batch: 540; loss: 1.0; acc: 0.7
Batch: 560; loss: 0.75; acc: 0.77
Batch: 580; loss: 0.68; acc: 0.78
Batch: 600; loss: 0.66; acc: 0.77
Batch: 620; loss: 1.05; acc: 0.66
Batch: 640; loss: 0.85; acc: 0.66
Batch: 660; loss: 0.77; acc: 0.73
Batch: 680; loss: 0.91; acc: 0.7
Batch: 700; loss: 0.6; acc: 0.75
Batch: 720; loss: 0.84; acc: 0.73
Batch: 740; loss: 0.88; acc: 0.73
Batch: 760; loss: 0.77; acc: 0.86
Batch: 780; loss: 0.8; acc: 0.73
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.7990801934224026; val_accuracy: 0.742734872611465 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.09; acc: 0.59
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.85; acc: 0.73
Batch: 60; loss: 0.79; acc: 0.69
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.91; acc: 0.7
Batch: 160; loss: 0.87; acc: 0.66
Batch: 180; loss: 0.91; acc: 0.66
Batch: 200; loss: 0.84; acc: 0.73
Batch: 220; loss: 1.01; acc: 0.69
Batch: 240; loss: 1.03; acc: 0.64
Batch: 260; loss: 0.92; acc: 0.69
Batch: 280; loss: 0.76; acc: 0.75
Batch: 300; loss: 0.92; acc: 0.67
Batch: 320; loss: 0.7; acc: 0.8
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 1.08; acc: 0.72
Batch: 380; loss: 0.72; acc: 0.69
Batch: 400; loss: 0.79; acc: 0.7
Batch: 420; loss: 1.08; acc: 0.62
Batch: 440; loss: 0.83; acc: 0.67
Batch: 460; loss: 0.73; acc: 0.73
Batch: 480; loss: 1.09; acc: 0.61
Batch: 500; loss: 0.81; acc: 0.67
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 0.77; acc: 0.77
Batch: 580; loss: 0.99; acc: 0.62
Batch: 600; loss: 0.98; acc: 0.7
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.88; acc: 0.73
Batch: 660; loss: 0.76; acc: 0.75
Batch: 680; loss: 1.09; acc: 0.7
Batch: 700; loss: 0.93; acc: 0.7
Batch: 720; loss: 0.92; acc: 0.67
Batch: 740; loss: 0.85; acc: 0.72
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.87; acc: 0.7
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.62; acc: 0.81
Val Epoch over. val_loss: 0.7996333328781614; val_accuracy: 0.7407444267515924 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.93; acc: 0.69
Batch: 20; loss: 1.02; acc: 0.66
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 1.03; acc: 0.61
Batch: 100; loss: 0.84; acc: 0.69
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.79; acc: 0.78
Batch: 160; loss: 0.94; acc: 0.67
Batch: 180; loss: 0.85; acc: 0.72
Batch: 200; loss: 0.7; acc: 0.78
Batch: 220; loss: 0.96; acc: 0.72
Batch: 240; loss: 0.62; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.77
Batch: 280; loss: 0.94; acc: 0.7
Batch: 300; loss: 0.88; acc: 0.75
Batch: 320; loss: 0.94; acc: 0.7
Batch: 340; loss: 0.91; acc: 0.75
Batch: 360; loss: 0.76; acc: 0.73
Batch: 380; loss: 0.85; acc: 0.75
Batch: 400; loss: 0.85; acc: 0.72
Batch: 420; loss: 0.64; acc: 0.75
Batch: 440; loss: 1.02; acc: 0.72
Batch: 460; loss: 0.93; acc: 0.7
Batch: 480; loss: 0.73; acc: 0.72
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.77; acc: 0.73
Batch: 560; loss: 0.89; acc: 0.66
Batch: 580; loss: 0.87; acc: 0.69
Batch: 600; loss: 0.65; acc: 0.78
Batch: 620; loss: 1.11; acc: 0.66
Batch: 640; loss: 0.98; acc: 0.72
Batch: 660; loss: 0.77; acc: 0.75
Batch: 680; loss: 0.77; acc: 0.73
Batch: 700; loss: 0.84; acc: 0.75
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.65; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.96; acc: 0.7
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.7
Batch: 20; loss: 1.09; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.81
Val Epoch over. val_loss: 0.7983710746856252; val_accuracy: 0.7431329617834395 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 0.86; acc: 0.77
Batch: 40; loss: 0.82; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.66
Batch: 80; loss: 0.81; acc: 0.75
Batch: 100; loss: 0.83; acc: 0.67
Batch: 120; loss: 0.61; acc: 0.77
Batch: 140; loss: 0.84; acc: 0.72
Batch: 160; loss: 1.04; acc: 0.67
Batch: 180; loss: 0.96; acc: 0.66
Batch: 200; loss: 0.85; acc: 0.67
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.84
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 1.13; acc: 0.67
Batch: 320; loss: 0.65; acc: 0.81
Batch: 340; loss: 0.56; acc: 0.8
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 1.08; acc: 0.61
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 1.2; acc: 0.67
Batch: 480; loss: 0.75; acc: 0.72
Batch: 500; loss: 0.89; acc: 0.8
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.76; acc: 0.75
Batch: 560; loss: 0.71; acc: 0.78
Batch: 580; loss: 0.97; acc: 0.69
Batch: 600; loss: 0.91; acc: 0.73
Batch: 620; loss: 0.92; acc: 0.73
Batch: 640; loss: 0.66; acc: 0.83
Batch: 660; loss: 0.56; acc: 0.78
Batch: 680; loss: 0.86; acc: 0.78
Batch: 700; loss: 0.83; acc: 0.69
Batch: 720; loss: 0.77; acc: 0.72
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.79; acc: 0.73
Batch: 780; loss: 0.94; acc: 0.77
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.11; acc: 0.62
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.84; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.8002936120625515; val_accuracy: 0.7417396496815286 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.07; acc: 0.69
Batch: 20; loss: 0.94; acc: 0.7
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.59
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.75
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 1.01; acc: 0.72
Batch: 160; loss: 1.1; acc: 0.67
Batch: 180; loss: 1.13; acc: 0.61
Batch: 200; loss: 0.86; acc: 0.75
Batch: 220; loss: 0.72; acc: 0.75
Batch: 240; loss: 0.9; acc: 0.69
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.94; acc: 0.7
Batch: 340; loss: 1.1; acc: 0.64
Batch: 360; loss: 0.76; acc: 0.75
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 0.72; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.73
Batch: 440; loss: 1.16; acc: 0.64
Batch: 460; loss: 0.91; acc: 0.7
Batch: 480; loss: 1.04; acc: 0.64
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.7; acc: 0.78
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.82; acc: 0.73
Batch: 580; loss: 0.94; acc: 0.72
Batch: 600; loss: 0.88; acc: 0.75
Batch: 620; loss: 0.85; acc: 0.72
Batch: 640; loss: 1.03; acc: 0.69
Batch: 660; loss: 0.88; acc: 0.72
Batch: 680; loss: 0.85; acc: 0.69
Batch: 700; loss: 0.69; acc: 0.83
Batch: 720; loss: 0.9; acc: 0.73
Batch: 740; loss: 0.79; acc: 0.7
Batch: 760; loss: 0.66; acc: 0.78
Batch: 780; loss: 0.88; acc: 0.75
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.66
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.7993881110173122; val_accuracy: 0.742734872611465 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.08; acc: 0.64
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.79; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.81; acc: 0.73
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 1.0; acc: 0.69
Batch: 200; loss: 0.94; acc: 0.73
Batch: 220; loss: 1.06; acc: 0.67
Batch: 240; loss: 1.22; acc: 0.7
Batch: 260; loss: 0.63; acc: 0.77
Batch: 280; loss: 1.14; acc: 0.67
Batch: 300; loss: 1.27; acc: 0.62
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.83; acc: 0.69
Batch: 360; loss: 1.23; acc: 0.61
Batch: 380; loss: 0.99; acc: 0.69
Batch: 400; loss: 0.86; acc: 0.78
Batch: 420; loss: 0.8; acc: 0.73
Batch: 440; loss: 0.84; acc: 0.77
Batch: 460; loss: 0.84; acc: 0.73
Batch: 480; loss: 0.92; acc: 0.69
Batch: 500; loss: 1.06; acc: 0.67
Batch: 520; loss: 0.85; acc: 0.69
Batch: 540; loss: 0.85; acc: 0.72
Batch: 560; loss: 1.06; acc: 0.67
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.97; acc: 0.67
Batch: 620; loss: 0.95; acc: 0.67
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 0.91; acc: 0.73
Batch: 680; loss: 0.94; acc: 0.67
Batch: 700; loss: 0.84; acc: 0.73
Batch: 720; loss: 0.92; acc: 0.72
Batch: 740; loss: 0.71; acc: 0.75
Batch: 760; loss: 0.97; acc: 0.7
Batch: 780; loss: 0.82; acc: 0.7
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.84
Val Epoch over. val_loss: 0.7995527020305585; val_accuracy: 0.7409434713375797 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.89; acc: 0.78
Batch: 20; loss: 0.82; acc: 0.75
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 1.07; acc: 0.7
Batch: 80; loss: 0.68; acc: 0.72
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.72
Batch: 140; loss: 0.84; acc: 0.75
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 0.61; acc: 0.81
Batch: 220; loss: 0.87; acc: 0.75
Batch: 240; loss: 0.92; acc: 0.73
Batch: 260; loss: 1.06; acc: 0.66
Batch: 280; loss: 1.12; acc: 0.67
Batch: 300; loss: 0.96; acc: 0.73
Batch: 320; loss: 0.65; acc: 0.78
Batch: 340; loss: 1.0; acc: 0.7
Batch: 360; loss: 1.12; acc: 0.67
Batch: 380; loss: 0.75; acc: 0.73
Batch: 400; loss: 0.79; acc: 0.77
Batch: 420; loss: 0.89; acc: 0.69
Batch: 440; loss: 0.69; acc: 0.77
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 0.94; acc: 0.73
Batch: 500; loss: 0.83; acc: 0.75
Batch: 520; loss: 0.87; acc: 0.7
Batch: 540; loss: 0.85; acc: 0.75
Batch: 560; loss: 0.93; acc: 0.7
Batch: 580; loss: 0.75; acc: 0.72
Batch: 600; loss: 0.72; acc: 0.8
Batch: 620; loss: 1.11; acc: 0.58
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.7; acc: 0.75
Batch: 680; loss: 0.71; acc: 0.78
Batch: 700; loss: 1.06; acc: 0.66
Batch: 720; loss: 0.85; acc: 0.66
Batch: 740; loss: 0.87; acc: 0.73
Batch: 760; loss: 0.8; acc: 0.77
Batch: 780; loss: 0.83; acc: 0.7
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.61
Batch: 40; loss: 0.67; acc: 0.73
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.84
Val Epoch over. val_loss: 0.7999075495513381; val_accuracy: 0.7418391719745223 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 1.08; acc: 0.67
Batch: 60; loss: 1.25; acc: 0.73
Batch: 80; loss: 0.91; acc: 0.7
Batch: 100; loss: 0.82; acc: 0.73
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 0.8; acc: 0.73
Batch: 160; loss: 0.63; acc: 0.78
Batch: 180; loss: 0.6; acc: 0.86
Batch: 200; loss: 0.87; acc: 0.72
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.9; acc: 0.7
Batch: 280; loss: 0.82; acc: 0.69
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.88; acc: 0.77
Batch: 380; loss: 1.2; acc: 0.64
Batch: 400; loss: 0.74; acc: 0.77
Batch: 420; loss: 0.88; acc: 0.75
Batch: 440; loss: 0.79; acc: 0.72
Batch: 460; loss: 0.91; acc: 0.67
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 0.68; acc: 0.77
Batch: 520; loss: 0.71; acc: 0.72
Batch: 540; loss: 0.87; acc: 0.77
Batch: 560; loss: 0.93; acc: 0.66
Batch: 580; loss: 0.85; acc: 0.7
Batch: 600; loss: 0.72; acc: 0.77
Batch: 620; loss: 1.16; acc: 0.66
Batch: 640; loss: 0.94; acc: 0.67
Batch: 660; loss: 0.94; acc: 0.69
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.82; acc: 0.78
Batch: 720; loss: 0.96; acc: 0.64
Batch: 740; loss: 0.92; acc: 0.75
Batch: 760; loss: 0.61; acc: 0.78
Batch: 780; loss: 1.04; acc: 0.62
Train Epoch over. train_loss: 0.85; train_accuracy: 0.73 

Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.73
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.7991918727850459; val_accuracy: 0.7409434713375797 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_75_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 4442598
elements in E: 4442600
fraction nonzero: 0.9999995498131725
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.32; acc: 0.11
Batch: 60; loss: 2.31; acc: 0.09
Batch: 80; loss: 2.31; acc: 0.05
Batch: 100; loss: 2.31; acc: 0.12
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.29; acc: 0.05
Batch: 200; loss: 2.29; acc: 0.12
Batch: 220; loss: 2.28; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.29; acc: 0.06
Batch: 280; loss: 2.3; acc: 0.02
Batch: 300; loss: 2.29; acc: 0.08
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.19
Batch: 380; loss: 2.28; acc: 0.08
Batch: 400; loss: 2.28; acc: 0.12
Batch: 420; loss: 2.28; acc: 0.17
Batch: 440; loss: 2.27; acc: 0.22
Batch: 460; loss: 2.27; acc: 0.25
Batch: 480; loss: 2.27; acc: 0.17
Batch: 500; loss: 2.27; acc: 0.25
Batch: 520; loss: 2.28; acc: 0.16
Batch: 540; loss: 2.27; acc: 0.28
Batch: 560; loss: 2.26; acc: 0.34
Batch: 580; loss: 2.26; acc: 0.17
Batch: 600; loss: 2.25; acc: 0.3
Batch: 620; loss: 2.25; acc: 0.36
Batch: 640; loss: 2.24; acc: 0.34
Batch: 660; loss: 2.24; acc: 0.34
Batch: 680; loss: 2.26; acc: 0.22
Batch: 700; loss: 2.22; acc: 0.38
Batch: 720; loss: 2.21; acc: 0.41
Batch: 740; loss: 2.21; acc: 0.31
Batch: 760; loss: 2.19; acc: 0.41
Batch: 780; loss: 2.19; acc: 0.38
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.18; acc: 0.41
Batch: 20; loss: 2.18; acc: 0.39
Batch: 40; loss: 2.12; acc: 0.52
Batch: 60; loss: 2.16; acc: 0.42
Batch: 80; loss: 2.17; acc: 0.39
Batch: 100; loss: 2.17; acc: 0.47
Batch: 120; loss: 2.18; acc: 0.38
Batch: 140; loss: 2.17; acc: 0.34
Val Epoch over. val_loss: 2.1795151461461546; val_accuracy: 0.3854498407643312 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.17; acc: 0.36
Batch: 20; loss: 2.16; acc: 0.42
Batch: 40; loss: 2.12; acc: 0.52
Batch: 60; loss: 2.07; acc: 0.41
Batch: 80; loss: 1.99; acc: 0.45
Batch: 100; loss: 1.93; acc: 0.5
Batch: 120; loss: 1.83; acc: 0.48
Batch: 140; loss: 1.71; acc: 0.45
Batch: 160; loss: 1.52; acc: 0.59
Batch: 180; loss: 1.52; acc: 0.64
Batch: 200; loss: 1.29; acc: 0.67
Batch: 220; loss: 1.26; acc: 0.64
Batch: 240; loss: 1.01; acc: 0.75
Batch: 260; loss: 0.8; acc: 0.84
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 0.97; acc: 0.72
Batch: 320; loss: 1.19; acc: 0.56
Batch: 340; loss: 1.13; acc: 0.61
Batch: 360; loss: 1.21; acc: 0.55
Batch: 380; loss: 0.85; acc: 0.72
Batch: 400; loss: 0.91; acc: 0.69
Batch: 420; loss: 0.7; acc: 0.77
Batch: 440; loss: 0.77; acc: 0.81
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 1.07; acc: 0.64
Batch: 500; loss: 0.83; acc: 0.72
Batch: 520; loss: 0.97; acc: 0.67
Batch: 540; loss: 0.96; acc: 0.73
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.99; acc: 0.62
Batch: 600; loss: 0.92; acc: 0.75
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.97; acc: 0.72
Batch: 660; loss: 0.74; acc: 0.75
Batch: 680; loss: 0.98; acc: 0.67
Batch: 700; loss: 0.7; acc: 0.78
Batch: 720; loss: 0.94; acc: 0.7
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 0.82; acc: 0.72
Batch: 780; loss: 0.95; acc: 0.58
Train Epoch over. train_loss: 1.17; train_accuracy: 0.64 

Batch: 0; loss: 1.18; acc: 0.56
Batch: 20; loss: 0.87; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.83; acc: 0.7
Batch: 80; loss: 0.74; acc: 0.72
Batch: 100; loss: 1.1; acc: 0.66
Batch: 120; loss: 1.25; acc: 0.55
Batch: 140; loss: 0.36; acc: 0.91
Val Epoch over. val_loss: 0.921627866234749; val_accuracy: 0.6945660828025477 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.67
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.92; acc: 0.72
Batch: 60; loss: 1.13; acc: 0.64
Batch: 80; loss: 0.95; acc: 0.61
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.89; acc: 0.72
Batch: 180; loss: 0.74; acc: 0.72
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.8; acc: 0.75
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.89; acc: 0.64
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 0.72; acc: 0.75
Batch: 340; loss: 0.98; acc: 0.7
Batch: 360; loss: 0.61; acc: 0.75
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.79; acc: 0.73
Batch: 420; loss: 0.72; acc: 0.81
Batch: 440; loss: 0.75; acc: 0.72
Batch: 460; loss: 0.8; acc: 0.7
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.78; acc: 0.7
Batch: 520; loss: 0.75; acc: 0.72
Batch: 540; loss: 0.78; acc: 0.72
Batch: 560; loss: 0.75; acc: 0.75
Batch: 580; loss: 0.86; acc: 0.75
Batch: 600; loss: 0.79; acc: 0.75
Batch: 620; loss: 0.9; acc: 0.73
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 1.07; acc: 0.69
Batch: 680; loss: 0.62; acc: 0.77
Batch: 700; loss: 0.73; acc: 0.75
Batch: 720; loss: 0.75; acc: 0.84
Batch: 740; loss: 0.88; acc: 0.73
Batch: 760; loss: 0.6; acc: 0.78
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 1.04; acc: 0.72
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 1.03; acc: 0.66
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.8659937275443107; val_accuracy: 0.7145700636942676 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 1.42; acc: 0.53
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.84; acc: 0.73
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.67
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.82; acc: 0.77
Batch: 200; loss: 0.63; acc: 0.84
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 1.07; acc: 0.64
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.7; acc: 0.75
Batch: 300; loss: 0.89; acc: 0.75
Batch: 320; loss: 0.88; acc: 0.72
Batch: 340; loss: 0.75; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.72
Batch: 380; loss: 0.81; acc: 0.75
Batch: 400; loss: 0.9; acc: 0.7
Batch: 420; loss: 0.99; acc: 0.69
Batch: 440; loss: 0.66; acc: 0.83
Batch: 460; loss: 0.66; acc: 0.81
Batch: 480; loss: 0.9; acc: 0.72
Batch: 500; loss: 1.05; acc: 0.61
Batch: 520; loss: 0.73; acc: 0.8
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.82; acc: 0.75
Batch: 580; loss: 0.67; acc: 0.78
Batch: 600; loss: 0.95; acc: 0.73
Batch: 620; loss: 0.61; acc: 0.81
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.57; acc: 0.86
Batch: 720; loss: 0.83; acc: 0.72
Batch: 740; loss: 0.82; acc: 0.8
Batch: 760; loss: 0.65; acc: 0.8
Batch: 780; loss: 0.84; acc: 0.75
Train Epoch over. train_loss: 0.8; train_accuracy: 0.75 

Batch: 0; loss: 0.95; acc: 0.66
Batch: 20; loss: 1.01; acc: 0.58
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.85; acc: 0.7
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.91
Val Epoch over. val_loss: 0.7997101976233683; val_accuracy: 0.7445262738853503 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.78
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 0.54; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.81; acc: 0.75
Batch: 180; loss: 1.2; acc: 0.67
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.83; acc: 0.77
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.7; acc: 0.73
Batch: 280; loss: 1.17; acc: 0.61
Batch: 300; loss: 0.75; acc: 0.73
Batch: 320; loss: 0.66; acc: 0.78
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.77
Batch: 380; loss: 0.87; acc: 0.67
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 0.89; acc: 0.7
Batch: 460; loss: 0.74; acc: 0.8
Batch: 480; loss: 0.85; acc: 0.75
Batch: 500; loss: 0.67; acc: 0.75
Batch: 520; loss: 1.15; acc: 0.66
Batch: 540; loss: 0.85; acc: 0.73
Batch: 560; loss: 0.62; acc: 0.77
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.78; acc: 0.77
Batch: 620; loss: 0.78; acc: 0.72
Batch: 640; loss: 0.57; acc: 0.8
Batch: 660; loss: 1.05; acc: 0.66
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.91; acc: 0.69
Batch: 720; loss: 0.75; acc: 0.8
Batch: 740; loss: 0.61; acc: 0.75
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.79; train_accuracy: 0.75 

Batch: 0; loss: 1.91; acc: 0.45
Batch: 20; loss: 1.55; acc: 0.62
Batch: 40; loss: 1.51; acc: 0.58
Batch: 60; loss: 1.87; acc: 0.61
Batch: 80; loss: 1.45; acc: 0.55
Batch: 100; loss: 1.8; acc: 0.58
Batch: 120; loss: 1.72; acc: 0.45
Batch: 140; loss: 1.06; acc: 0.62
Val Epoch over. val_loss: 1.6772228312340511; val_accuracy: 0.5675756369426752 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.55
Batch: 20; loss: 1.1; acc: 0.62
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.69; acc: 0.72
Batch: 100; loss: 0.84; acc: 0.7
Batch: 120; loss: 0.74; acc: 0.83
Batch: 140; loss: 0.8; acc: 0.77
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.77
Batch: 200; loss: 0.9; acc: 0.69
Batch: 220; loss: 1.09; acc: 0.59
Batch: 240; loss: 0.91; acc: 0.75
Batch: 260; loss: 1.04; acc: 0.7
Batch: 280; loss: 0.92; acc: 0.69
Batch: 300; loss: 0.93; acc: 0.67
Batch: 320; loss: 0.76; acc: 0.8
Batch: 340; loss: 0.89; acc: 0.73
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 1.01; acc: 0.73
Batch: 400; loss: 0.82; acc: 0.77
Batch: 420; loss: 0.78; acc: 0.7
Batch: 440; loss: 0.87; acc: 0.73
Batch: 460; loss: 0.69; acc: 0.75
Batch: 480; loss: 0.65; acc: 0.75
Batch: 500; loss: 0.83; acc: 0.73
Batch: 520; loss: 0.69; acc: 0.75
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.54; acc: 0.8
Batch: 580; loss: 0.98; acc: 0.73
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.89; acc: 0.75
Batch: 640; loss: 0.95; acc: 0.67
Batch: 660; loss: 0.71; acc: 0.78
Batch: 680; loss: 0.87; acc: 0.72
Batch: 700; loss: 0.73; acc: 0.72
Batch: 720; loss: 0.72; acc: 0.75
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.84; acc: 0.72
Batch: 780; loss: 0.79; acc: 0.77
Train Epoch over. train_loss: 0.79; train_accuracy: 0.75 

Batch: 0; loss: 1.0; acc: 0.69
Batch: 20; loss: 0.97; acc: 0.64
Batch: 40; loss: 0.68; acc: 0.73
Batch: 60; loss: 0.92; acc: 0.72
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 0.44; acc: 0.88
Val Epoch over. val_loss: 0.899534208569557; val_accuracy: 0.7161624203821656 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.62
Batch: 20; loss: 1.05; acc: 0.66
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 1.11; acc: 0.69
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 0.74; acc: 0.73
Batch: 140; loss: 0.83; acc: 0.7
Batch: 160; loss: 0.8; acc: 0.72
Batch: 180; loss: 0.87; acc: 0.73
Batch: 200; loss: 0.84; acc: 0.73
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.64; acc: 0.77
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.74; acc: 0.75
Batch: 300; loss: 0.93; acc: 0.72
Batch: 320; loss: 0.78; acc: 0.72
Batch: 340; loss: 0.75; acc: 0.73
Batch: 360; loss: 0.84; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.77
Batch: 400; loss: 0.88; acc: 0.69
Batch: 420; loss: 0.92; acc: 0.69
Batch: 440; loss: 1.01; acc: 0.72
Batch: 460; loss: 0.67; acc: 0.77
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.71; acc: 0.8
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.68; acc: 0.73
Batch: 560; loss: 0.79; acc: 0.66
Batch: 580; loss: 0.77; acc: 0.77
Batch: 600; loss: 0.79; acc: 0.77
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.9; acc: 0.73
Batch: 660; loss: 0.64; acc: 0.75
Batch: 680; loss: 0.94; acc: 0.7
Batch: 700; loss: 0.49; acc: 0.81
Batch: 720; loss: 0.79; acc: 0.78
Batch: 740; loss: 0.88; acc: 0.7
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.79; train_accuracy: 0.75 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 1.02; acc: 0.62
Batch: 40; loss: 0.5; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.73
Batch: 80; loss: 0.82; acc: 0.72
Batch: 100; loss: 0.87; acc: 0.7
Batch: 120; loss: 1.15; acc: 0.64
Batch: 140; loss: 0.48; acc: 0.8
Val Epoch over. val_loss: 0.8883023613197788; val_accuracy: 0.7200437898089171 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.66
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.99; acc: 0.7
Batch: 60; loss: 0.75; acc: 0.78
Batch: 80; loss: 0.8; acc: 0.73
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.85; acc: 0.7
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.72; acc: 0.77
Batch: 180; loss: 0.96; acc: 0.73
Batch: 200; loss: 0.83; acc: 0.73
Batch: 220; loss: 0.73; acc: 0.75
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.8; acc: 0.66
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.69; acc: 0.77
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.89; acc: 0.72
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.72
Batch: 500; loss: 0.7; acc: 0.73
Batch: 520; loss: 0.54; acc: 0.8
Batch: 540; loss: 0.66; acc: 0.8
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.91; acc: 0.73
Batch: 620; loss: 0.73; acc: 0.77
Batch: 640; loss: 0.86; acc: 0.72
Batch: 660; loss: 1.05; acc: 0.69
Batch: 680; loss: 0.67; acc: 0.73
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 1.31; acc: 0.64
Batch: 760; loss: 0.65; acc: 0.83
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.78; train_accuracy: 0.75 

Batch: 0; loss: 1.01; acc: 0.66
Batch: 20; loss: 1.3; acc: 0.62
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.94; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.77
Batch: 100; loss: 1.24; acc: 0.72
Batch: 120; loss: 1.27; acc: 0.69
Batch: 140; loss: 0.41; acc: 0.88
Val Epoch over. val_loss: 0.9441906561137764; val_accuracy: 0.7082006369426752 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.72
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.78
Batch: 60; loss: 1.17; acc: 0.64
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.74; acc: 0.8
Batch: 120; loss: 0.64; acc: 0.77
Batch: 140; loss: 0.8; acc: 0.73
Batch: 160; loss: 0.78; acc: 0.72
Batch: 180; loss: 0.85; acc: 0.73
Batch: 200; loss: 1.12; acc: 0.62
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.77; acc: 0.75
Batch: 260; loss: 1.18; acc: 0.64
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.62; acc: 0.8
Batch: 320; loss: 0.93; acc: 0.62
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.73
Batch: 380; loss: 0.97; acc: 0.7
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.69; acc: 0.77
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 1.04; acc: 0.64
Batch: 480; loss: 0.86; acc: 0.69
Batch: 500; loss: 0.68; acc: 0.8
Batch: 520; loss: 0.68; acc: 0.75
Batch: 540; loss: 0.75; acc: 0.8
Batch: 560; loss: 0.81; acc: 0.7
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.88; acc: 0.7
Batch: 620; loss: 0.64; acc: 0.8
Batch: 640; loss: 1.03; acc: 0.67
Batch: 660; loss: 0.77; acc: 0.72
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 1.0; acc: 0.69
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.8; acc: 0.75
Batch: 760; loss: 1.03; acc: 0.69
Batch: 780; loss: 1.11; acc: 0.66
Train Epoch over. train_loss: 0.78; train_accuracy: 0.75 

Batch: 0; loss: 1.12; acc: 0.55
Batch: 20; loss: 1.37; acc: 0.48
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.97; acc: 0.69
Batch: 80; loss: 0.87; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.69
Batch: 120; loss: 1.25; acc: 0.61
Batch: 140; loss: 0.51; acc: 0.8
Val Epoch over. val_loss: 1.0268956929635091; val_accuracy: 0.6686902866242038 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.72
Batch: 20; loss: 0.78; acc: 0.69
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.78
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 0.62; acc: 0.77
Batch: 180; loss: 0.9; acc: 0.7
Batch: 200; loss: 0.87; acc: 0.75
Batch: 220; loss: 0.91; acc: 0.73
Batch: 240; loss: 0.94; acc: 0.67
Batch: 260; loss: 0.71; acc: 0.75
Batch: 280; loss: 0.88; acc: 0.7
Batch: 300; loss: 0.55; acc: 0.81
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.92; acc: 0.64
Batch: 360; loss: 0.68; acc: 0.77
Batch: 380; loss: 0.81; acc: 0.75
Batch: 400; loss: 1.02; acc: 0.69
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.8; acc: 0.67
Batch: 460; loss: 1.13; acc: 0.66
Batch: 480; loss: 0.89; acc: 0.7
Batch: 500; loss: 1.01; acc: 0.69
Batch: 520; loss: 0.72; acc: 0.72
Batch: 540; loss: 0.79; acc: 0.77
Batch: 560; loss: 0.96; acc: 0.66
Batch: 580; loss: 0.75; acc: 0.78
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.75; acc: 0.67
Batch: 640; loss: 0.64; acc: 0.75
Batch: 660; loss: 0.83; acc: 0.73
Batch: 680; loss: 0.86; acc: 0.77
Batch: 700; loss: 0.92; acc: 0.77
Batch: 720; loss: 0.95; acc: 0.73
Batch: 740; loss: 0.96; acc: 0.66
Batch: 760; loss: 0.86; acc: 0.75
Batch: 780; loss: 0.93; acc: 0.69
Train Epoch over. train_loss: 0.79; train_accuracy: 0.75 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.84; acc: 0.7
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.83; acc: 0.73
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.7212744072364394; val_accuracy: 0.7707006369426752 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 0.78; acc: 0.7
Batch: 80; loss: 0.95; acc: 0.64
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.78
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.67; acc: 0.73
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.66; acc: 0.8
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 0.75; acc: 0.8
Batch: 380; loss: 0.7; acc: 0.77
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.85; acc: 0.73
Batch: 440; loss: 0.72; acc: 0.73
Batch: 460; loss: 0.76; acc: 0.75
Batch: 480; loss: 0.75; acc: 0.73
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.7; acc: 0.75
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.8
Batch: 600; loss: 0.72; acc: 0.8
Batch: 620; loss: 0.5; acc: 0.81
Batch: 640; loss: 0.84; acc: 0.72
Batch: 660; loss: 0.78; acc: 0.72
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.8
Batch: 760; loss: 0.62; acc: 0.8
Batch: 780; loss: 0.8; acc: 0.78
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.94
Val Epoch over. val_loss: 0.6966943108731773; val_accuracy: 0.7813495222929936 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.82; acc: 0.72
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.89; acc: 0.72
Batch: 60; loss: 0.79; acc: 0.7
Batch: 80; loss: 0.88; acc: 0.77
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.87; acc: 0.72
Batch: 160; loss: 0.92; acc: 0.73
Batch: 180; loss: 0.67; acc: 0.73
Batch: 200; loss: 0.72; acc: 0.77
Batch: 220; loss: 0.89; acc: 0.69
Batch: 240; loss: 0.71; acc: 0.77
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.61; acc: 0.84
Batch: 340; loss: 0.98; acc: 0.75
Batch: 360; loss: 1.37; acc: 0.58
Batch: 380; loss: 0.48; acc: 0.75
Batch: 400; loss: 0.81; acc: 0.66
Batch: 420; loss: 0.77; acc: 0.7
Batch: 440; loss: 0.92; acc: 0.72
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.81; acc: 0.73
Batch: 500; loss: 0.68; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.78
Batch: 540; loss: 0.86; acc: 0.7
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.93; acc: 0.7
Batch: 600; loss: 0.8; acc: 0.72
Batch: 620; loss: 0.81; acc: 0.67
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.81
Batch: 680; loss: 0.93; acc: 0.67
Batch: 700; loss: 0.57; acc: 0.8
Batch: 720; loss: 1.08; acc: 0.7
Batch: 740; loss: 0.68; acc: 0.8
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 0.86; acc: 0.72
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.69
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.87; acc: 0.72
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.7271882568954662; val_accuracy: 0.7724920382165605 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.77; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.92; acc: 0.72
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.68; acc: 0.75
Batch: 160; loss: 0.57; acc: 0.83
Batch: 180; loss: 0.76; acc: 0.77
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.81; acc: 0.7
Batch: 240; loss: 0.91; acc: 0.72
Batch: 260; loss: 0.79; acc: 0.7
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.78
Batch: 360; loss: 0.91; acc: 0.72
Batch: 380; loss: 0.87; acc: 0.72
Batch: 400; loss: 0.6; acc: 0.8
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.8; acc: 0.72
Batch: 480; loss: 0.96; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.8
Batch: 520; loss: 0.75; acc: 0.78
Batch: 540; loss: 0.71; acc: 0.73
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.77; acc: 0.77
Batch: 600; loss: 0.9; acc: 0.73
Batch: 620; loss: 0.9; acc: 0.75
Batch: 640; loss: 0.84; acc: 0.69
Batch: 660; loss: 1.04; acc: 0.72
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.86; acc: 0.72
Batch: 720; loss: 0.84; acc: 0.72
Batch: 740; loss: 0.66; acc: 0.84
Batch: 760; loss: 0.81; acc: 0.73
Batch: 780; loss: 0.73; acc: 0.75
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.93; acc: 0.69
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.8; acc: 0.75
Batch: 120; loss: 1.01; acc: 0.69
Batch: 140; loss: 0.39; acc: 0.92
Val Epoch over. val_loss: 0.764977390599099; val_accuracy: 0.7613455414012739 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.75
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.61; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.7
Batch: 120; loss: 0.95; acc: 0.66
Batch: 140; loss: 0.66; acc: 0.77
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.57; acc: 0.78
Batch: 200; loss: 0.96; acc: 0.73
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.85; acc: 0.73
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.8; acc: 0.77
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 1.07; acc: 0.66
Batch: 380; loss: 1.04; acc: 0.69
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.72; acc: 0.72
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.8
Batch: 520; loss: 0.93; acc: 0.75
Batch: 540; loss: 0.87; acc: 0.66
Batch: 560; loss: 0.82; acc: 0.75
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 1.03; acc: 0.69
Batch: 620; loss: 0.81; acc: 0.75
Batch: 640; loss: 0.81; acc: 0.72
Batch: 660; loss: 0.68; acc: 0.8
Batch: 680; loss: 0.81; acc: 0.77
Batch: 700; loss: 0.87; acc: 0.75
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.64; acc: 0.78
Batch: 780; loss: 0.98; acc: 0.72
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.81; acc: 0.67
Batch: 20; loss: 0.89; acc: 0.69
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.81; acc: 0.73
Batch: 120; loss: 0.98; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.94
Val Epoch over. val_loss: 0.7145672761330939; val_accuracy: 0.7753781847133758 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.72
Batch: 20; loss: 0.82; acc: 0.77
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.81; acc: 0.7
Batch: 80; loss: 0.92; acc: 0.73
Batch: 100; loss: 0.81; acc: 0.7
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.78; acc: 0.77
Batch: 160; loss: 0.6; acc: 0.8
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.95; acc: 0.69
Batch: 220; loss: 0.76; acc: 0.75
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.55; acc: 0.78
Batch: 300; loss: 0.89; acc: 0.69
Batch: 320; loss: 0.72; acc: 0.73
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.81; acc: 0.73
Batch: 380; loss: 0.86; acc: 0.75
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.81; acc: 0.77
Batch: 440; loss: 0.71; acc: 0.77
Batch: 460; loss: 0.69; acc: 0.77
Batch: 480; loss: 0.71; acc: 0.77
Batch: 500; loss: 0.78; acc: 0.72
Batch: 520; loss: 0.93; acc: 0.73
Batch: 540; loss: 0.62; acc: 0.8
Batch: 560; loss: 0.75; acc: 0.73
Batch: 580; loss: 0.85; acc: 0.72
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.77; acc: 0.81
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.66; acc: 0.77
Batch: 720; loss: 0.86; acc: 0.75
Batch: 740; loss: 0.88; acc: 0.77
Batch: 760; loss: 1.03; acc: 0.67
Batch: 780; loss: 0.81; acc: 0.72
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.79; acc: 0.7
Batch: 20; loss: 0.84; acc: 0.72
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.8
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 1.03; acc: 0.7
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.7261019375673525; val_accuracy: 0.7722929936305732 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.76; acc: 0.81
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 0.77; acc: 0.7
Batch: 60; loss: 0.85; acc: 0.72
Batch: 80; loss: 0.76; acc: 0.73
Batch: 100; loss: 0.81; acc: 0.75
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.82; acc: 0.73
Batch: 200; loss: 0.74; acc: 0.77
Batch: 220; loss: 0.75; acc: 0.78
Batch: 240; loss: 0.61; acc: 0.78
Batch: 260; loss: 0.75; acc: 0.77
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 0.73; acc: 0.75
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.78; acc: 0.73
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.8
Batch: 480; loss: 0.83; acc: 0.72
Batch: 500; loss: 0.86; acc: 0.72
Batch: 520; loss: 0.77; acc: 0.78
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.61; acc: 0.75
Batch: 580; loss: 0.99; acc: 0.72
Batch: 600; loss: 1.03; acc: 0.75
Batch: 620; loss: 0.73; acc: 0.77
Batch: 640; loss: 0.95; acc: 0.72
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.84; acc: 0.7
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.8
Batch: 740; loss: 0.91; acc: 0.72
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.85; acc: 0.72
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.77
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 1.05; acc: 0.67
Batch: 120; loss: 1.3; acc: 0.61
Batch: 140; loss: 0.41; acc: 0.83
Val Epoch over. val_loss: 0.8901074439477009; val_accuracy: 0.7213375796178344 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.81; acc: 0.7
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.69; acc: 0.72
Batch: 60; loss: 0.6; acc: 0.77
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.94; acc: 0.75
Batch: 140; loss: 0.75; acc: 0.78
Batch: 160; loss: 0.69; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.57; acc: 0.8
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.64; acc: 0.78
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.91; acc: 0.69
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 0.88; acc: 0.75
Batch: 340; loss: 0.68; acc: 0.8
Batch: 360; loss: 0.55; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.78
Batch: 400; loss: 0.88; acc: 0.75
Batch: 420; loss: 1.24; acc: 0.7
Batch: 440; loss: 0.91; acc: 0.83
Batch: 460; loss: 0.99; acc: 0.7
Batch: 480; loss: 0.71; acc: 0.72
Batch: 500; loss: 0.94; acc: 0.77
Batch: 520; loss: 0.74; acc: 0.77
Batch: 540; loss: 0.71; acc: 0.77
Batch: 560; loss: 0.79; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.73
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.91; acc: 0.78
Batch: 640; loss: 0.81; acc: 0.75
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.77
Batch: 700; loss: 0.72; acc: 0.75
Batch: 720; loss: 0.86; acc: 0.75
Batch: 740; loss: 0.96; acc: 0.67
Batch: 760; loss: 0.72; acc: 0.78
Batch: 780; loss: 1.14; acc: 0.67
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.69; acc: 0.75
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.72
Batch: 120; loss: 1.03; acc: 0.7
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.7312440418513717; val_accuracy: 0.7678144904458599 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.52; acc: 0.81
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.8
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.79; acc: 0.75
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.85; acc: 0.73
Batch: 200; loss: 0.78; acc: 0.75
Batch: 220; loss: 0.69; acc: 0.75
Batch: 240; loss: 0.72; acc: 0.78
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.89; acc: 0.77
Batch: 300; loss: 0.65; acc: 0.78
Batch: 320; loss: 0.72; acc: 0.77
Batch: 340; loss: 0.72; acc: 0.77
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 1.06; acc: 0.69
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.77; acc: 0.73
Batch: 440; loss: 0.65; acc: 0.77
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.89; acc: 0.75
Batch: 500; loss: 0.61; acc: 0.8
Batch: 520; loss: 0.73; acc: 0.81
Batch: 540; loss: 0.64; acc: 0.78
Batch: 560; loss: 0.82; acc: 0.7
Batch: 580; loss: 0.83; acc: 0.69
Batch: 600; loss: 0.66; acc: 0.81
Batch: 620; loss: 0.61; acc: 0.73
Batch: 640; loss: 0.4; acc: 0.84
Batch: 660; loss: 0.77; acc: 0.72
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 1.02; acc: 0.67
Batch: 720; loss: 0.83; acc: 0.73
Batch: 740; loss: 0.97; acc: 0.7
Batch: 760; loss: 0.71; acc: 0.77
Batch: 780; loss: 0.59; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.8; acc: 0.66
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.87; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 0.28; acc: 0.92
Val Epoch over. val_loss: 0.7402266203218205; val_accuracy: 0.7628383757961783 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 0.6; acc: 0.78
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 0.74; acc: 0.72
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.83; acc: 0.7
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.72
Batch: 200; loss: 0.6; acc: 0.88
Batch: 220; loss: 0.71; acc: 0.67
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.66; acc: 0.77
Batch: 340; loss: 0.8; acc: 0.7
Batch: 360; loss: 0.77; acc: 0.77
Batch: 380; loss: 0.78; acc: 0.77
Batch: 400; loss: 0.77; acc: 0.7
Batch: 420; loss: 0.68; acc: 0.75
Batch: 440; loss: 0.79; acc: 0.77
Batch: 460; loss: 0.93; acc: 0.72
Batch: 480; loss: 0.87; acc: 0.75
Batch: 500; loss: 0.73; acc: 0.88
Batch: 520; loss: 0.8; acc: 0.72
Batch: 540; loss: 0.89; acc: 0.72
Batch: 560; loss: 0.62; acc: 0.78
Batch: 580; loss: 0.8; acc: 0.72
Batch: 600; loss: 0.81; acc: 0.73
Batch: 620; loss: 0.78; acc: 0.75
Batch: 640; loss: 0.67; acc: 0.8
Batch: 660; loss: 0.99; acc: 0.67
Batch: 680; loss: 1.02; acc: 0.69
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.81
Batch: 740; loss: 0.78; acc: 0.77
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.73; acc: 0.7
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.7; acc: 0.72
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.95
Val Epoch over. val_loss: 0.6878586132435283; val_accuracy: 0.7840366242038217 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.86; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.93; acc: 0.64
Batch: 140; loss: 0.86; acc: 0.75
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.77; acc: 0.72
Batch: 200; loss: 0.89; acc: 0.7
Batch: 220; loss: 0.67; acc: 0.75
Batch: 240; loss: 1.1; acc: 0.64
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.88
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.99; acc: 0.7
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.82; acc: 0.78
Batch: 480; loss: 0.75; acc: 0.78
Batch: 500; loss: 0.85; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.85; acc: 0.78
Batch: 580; loss: 0.78; acc: 0.7
Batch: 600; loss: 0.83; acc: 0.72
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.66; acc: 0.78
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.75; acc: 0.81
Batch: 740; loss: 1.02; acc: 0.7
Batch: 760; loss: 0.88; acc: 0.75
Batch: 780; loss: 0.73; acc: 0.8
Train Epoch over. train_loss: 0.72; train_accuracy: 0.77 

Batch: 0; loss: 0.97; acc: 0.61
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.98; acc: 0.75
Batch: 80; loss: 0.72; acc: 0.73
Batch: 100; loss: 0.92; acc: 0.73
Batch: 120; loss: 1.04; acc: 0.69
Batch: 140; loss: 0.32; acc: 0.91
Val Epoch over. val_loss: 0.7878485031568321; val_accuracy: 0.7455214968152867 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.86; acc: 0.72
Batch: 20; loss: 0.68; acc: 0.81
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.8
Batch: 100; loss: 0.86; acc: 0.69
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.7; acc: 0.77
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 0.7; acc: 0.81
Batch: 220; loss: 0.72; acc: 0.77
Batch: 240; loss: 0.46; acc: 0.81
Batch: 260; loss: 0.86; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.71; acc: 0.84
Batch: 320; loss: 0.74; acc: 0.72
Batch: 340; loss: 0.74; acc: 0.73
Batch: 360; loss: 0.47; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.93; acc: 0.77
Batch: 440; loss: 0.67; acc: 0.78
Batch: 460; loss: 0.8; acc: 0.75
Batch: 480; loss: 0.69; acc: 0.73
Batch: 500; loss: 0.46; acc: 0.81
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 0.85; acc: 0.73
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.88; acc: 0.73
Batch: 620; loss: 0.74; acc: 0.78
Batch: 640; loss: 0.69; acc: 0.73
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.86; acc: 0.7
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.62; acc: 0.81
Batch: 740; loss: 0.53; acc: 0.8
Batch: 760; loss: 0.76; acc: 0.77
Batch: 780; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.67; acc: 0.73
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.78
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.6817594286362836; val_accuracy: 0.792296974522293 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.92; acc: 0.72
Batch: 140; loss: 0.66; acc: 0.78
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.75
Batch: 200; loss: 0.92; acc: 0.69
Batch: 220; loss: 0.61; acc: 0.8
Batch: 240; loss: 0.86; acc: 0.77
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.75
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.85; acc: 0.72
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.69; acc: 0.75
Batch: 380; loss: 0.75; acc: 0.73
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.58; acc: 0.75
Batch: 460; loss: 0.83; acc: 0.78
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.8; acc: 0.78
Batch: 520; loss: 0.74; acc: 0.78
Batch: 540; loss: 0.62; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.88
Batch: 580; loss: 0.64; acc: 0.73
Batch: 600; loss: 0.79; acc: 0.78
Batch: 620; loss: 0.74; acc: 0.72
Batch: 640; loss: 0.75; acc: 0.77
Batch: 660; loss: 0.72; acc: 0.78
Batch: 680; loss: 0.83; acc: 0.8
Batch: 700; loss: 0.87; acc: 0.73
Batch: 720; loss: 0.77; acc: 0.72
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.71; acc: 0.78
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.6809164976618093; val_accuracy: 0.7917993630573248 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.62; acc: 0.75
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.96; acc: 0.7
Batch: 240; loss: 0.88; acc: 0.7
Batch: 260; loss: 0.68; acc: 0.73
Batch: 280; loss: 0.64; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.81; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.9; acc: 0.73
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.78
Batch: 500; loss: 0.89; acc: 0.73
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.75
Batch: 560; loss: 0.67; acc: 0.75
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.99; acc: 0.73
Batch: 640; loss: 0.56; acc: 0.8
Batch: 660; loss: 0.77; acc: 0.77
Batch: 680; loss: 0.52; acc: 0.8
Batch: 700; loss: 0.44; acc: 0.83
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.87; acc: 0.77
Batch: 760; loss: 0.53; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.97
Val Epoch over. val_loss: 0.6654550362924102; val_accuracy: 0.8001592356687898 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.8; acc: 0.73
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.98; acc: 0.67
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.84; acc: 0.73
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.87; acc: 0.8
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.75; acc: 0.7
Batch: 220; loss: 0.74; acc: 0.73
Batch: 240; loss: 0.68; acc: 0.78
Batch: 260; loss: 0.78; acc: 0.7
Batch: 280; loss: 0.78; acc: 0.78
Batch: 300; loss: 0.66; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.79; acc: 0.72
Batch: 360; loss: 0.55; acc: 0.8
Batch: 380; loss: 0.78; acc: 0.75
Batch: 400; loss: 0.89; acc: 0.8
Batch: 420; loss: 0.56; acc: 0.78
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.49; acc: 0.81
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.64; acc: 0.72
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.64; acc: 0.72
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.78; acc: 0.66
Batch: 680; loss: 0.66; acc: 0.81
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.81
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.6668318328773899; val_accuracy: 0.7959792993630573 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.78; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.8; acc: 0.69
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.77; acc: 0.77
Batch: 160; loss: 0.54; acc: 0.81
Batch: 180; loss: 0.58; acc: 0.8
Batch: 200; loss: 0.6; acc: 0.78
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.99; acc: 0.64
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.75; acc: 0.7
Batch: 320; loss: 0.46; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.91; acc: 0.8
Batch: 400; loss: 0.98; acc: 0.66
Batch: 420; loss: 0.99; acc: 0.7
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.92; acc: 0.67
Batch: 480; loss: 0.85; acc: 0.7
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.65; acc: 0.78
Batch: 580; loss: 0.68; acc: 0.78
Batch: 600; loss: 0.79; acc: 0.77
Batch: 620; loss: 0.82; acc: 0.7
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.51; acc: 0.81
Batch: 740; loss: 0.84; acc: 0.69
Batch: 760; loss: 0.93; acc: 0.72
Batch: 780; loss: 0.98; acc: 0.7
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.73
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.75
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.6669932835420985; val_accuracy: 0.7954816878980892 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.73
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.8; acc: 0.75
Batch: 100; loss: 0.78; acc: 0.67
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.76; acc: 0.72
Batch: 160; loss: 0.85; acc: 0.72
Batch: 180; loss: 0.87; acc: 0.75
Batch: 200; loss: 0.87; acc: 0.77
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.8
Batch: 260; loss: 0.83; acc: 0.81
Batch: 280; loss: 0.87; acc: 0.69
Batch: 300; loss: 0.72; acc: 0.75
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.78; acc: 0.73
Batch: 360; loss: 0.63; acc: 0.75
Batch: 380; loss: 0.71; acc: 0.77
Batch: 400; loss: 0.74; acc: 0.7
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.81
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.95; acc: 0.72
Batch: 560; loss: 0.79; acc: 0.72
Batch: 580; loss: 0.76; acc: 0.77
Batch: 600; loss: 0.59; acc: 0.73
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.72; acc: 0.8
Batch: 660; loss: 1.0; acc: 0.69
Batch: 680; loss: 0.63; acc: 0.72
Batch: 700; loss: 0.62; acc: 0.81
Batch: 720; loss: 0.86; acc: 0.69
Batch: 740; loss: 0.74; acc: 0.78
Batch: 760; loss: 0.71; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.69; acc: 0.75
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.6600664227631441; val_accuracy: 0.7993630573248408 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.78; acc: 0.77
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.77
Batch: 160; loss: 0.69; acc: 0.77
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.84; acc: 0.78
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.82; acc: 0.72
Batch: 260; loss: 0.69; acc: 0.73
Batch: 280; loss: 1.05; acc: 0.72
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.87; acc: 0.72
Batch: 340; loss: 0.66; acc: 0.75
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.54; acc: 0.78
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.9; acc: 0.77
Batch: 460; loss: 0.96; acc: 0.73
Batch: 480; loss: 0.9; acc: 0.73
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.91; acc: 0.7
Batch: 540; loss: 0.82; acc: 0.73
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.74; acc: 0.73
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.77; acc: 0.75
Batch: 660; loss: 0.78; acc: 0.77
Batch: 680; loss: 0.71; acc: 0.83
Batch: 700; loss: 0.56; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.75
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.87; acc: 0.75
Batch: 780; loss: 1.01; acc: 0.69
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.7; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.83; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.6640837474423609; val_accuracy: 0.7975716560509554 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.64; acc: 0.73
Batch: 20; loss: 0.58; acc: 0.75
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.8; acc: 0.7
Batch: 160; loss: 0.77; acc: 0.73
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.59; acc: 0.78
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.73; acc: 0.77
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.88; acc: 0.77
Batch: 320; loss: 0.87; acc: 0.72
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.75; acc: 0.69
Batch: 380; loss: 0.76; acc: 0.7
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.8
Batch: 480; loss: 0.59; acc: 0.81
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.8
Batch: 540; loss: 0.76; acc: 0.73
Batch: 560; loss: 0.54; acc: 0.81
Batch: 580; loss: 0.7; acc: 0.77
Batch: 600; loss: 0.74; acc: 0.75
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.78; acc: 0.83
Batch: 660; loss: 0.76; acc: 0.69
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.64; acc: 0.81
Batch: 720; loss: 0.95; acc: 0.64
Batch: 740; loss: 0.57; acc: 0.8
Batch: 760; loss: 0.68; acc: 0.75
Batch: 780; loss: 0.67; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.6782802727761542; val_accuracy: 0.7911027070063694 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.65; acc: 0.73
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.61; acc: 0.73
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.73; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.78
Batch: 180; loss: 0.46; acc: 0.84
Batch: 200; loss: 0.81; acc: 0.73
Batch: 220; loss: 0.97; acc: 0.64
Batch: 240; loss: 0.58; acc: 0.78
Batch: 260; loss: 0.64; acc: 0.8
Batch: 280; loss: 0.93; acc: 0.64
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.78; acc: 0.72
Batch: 340; loss: 0.77; acc: 0.8
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.72
Batch: 420; loss: 0.91; acc: 0.69
Batch: 440; loss: 0.81; acc: 0.8
Batch: 460; loss: 0.71; acc: 0.78
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.75; acc: 0.77
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.65; acc: 0.8
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.77
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.6; acc: 0.77
Batch: 640; loss: 0.72; acc: 0.77
Batch: 660; loss: 0.82; acc: 0.67
Batch: 680; loss: 0.63; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.83
Batch: 720; loss: 0.64; acc: 0.81
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 0.81; acc: 0.78
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.69; acc: 0.72
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.75
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.6613370500932074; val_accuracy: 0.7945859872611465 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.7
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.8; acc: 0.73
Batch: 240; loss: 0.63; acc: 0.75
Batch: 260; loss: 0.6; acc: 0.8
Batch: 280; loss: 1.06; acc: 0.72
Batch: 300; loss: 0.83; acc: 0.77
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.79; acc: 0.77
Batch: 440; loss: 0.69; acc: 0.78
Batch: 460; loss: 0.69; acc: 0.75
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.9; acc: 0.73
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.88; acc: 0.72
Batch: 580; loss: 0.68; acc: 0.73
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.94; acc: 0.73
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.85; acc: 0.7
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.72; acc: 0.7
Batch: 740; loss: 0.64; acc: 0.78
Batch: 760; loss: 0.59; acc: 0.77
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.71; acc: 0.72
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.6699979664034145; val_accuracy: 0.790406050955414 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 0.64; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.93; acc: 0.73
Batch: 280; loss: 0.84; acc: 0.67
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 1.02; acc: 0.67
Batch: 340; loss: 0.76; acc: 0.73
Batch: 360; loss: 0.9; acc: 0.8
Batch: 380; loss: 0.91; acc: 0.77
Batch: 400; loss: 0.62; acc: 0.78
Batch: 420; loss: 0.75; acc: 0.72
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.59; acc: 0.78
Batch: 480; loss: 0.78; acc: 0.73
Batch: 500; loss: 0.97; acc: 0.7
Batch: 520; loss: 0.8; acc: 0.8
Batch: 540; loss: 0.63; acc: 0.8
Batch: 560; loss: 1.04; acc: 0.69
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.82; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.74; acc: 0.72
Batch: 680; loss: 0.64; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.83
Batch: 720; loss: 0.47; acc: 0.81
Batch: 740; loss: 0.78; acc: 0.72
Batch: 760; loss: 0.7; acc: 0.77
Batch: 780; loss: 0.77; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.75
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.65099166058431; val_accuracy: 0.799562101910828 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.64; acc: 0.73
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.52; acc: 0.8
Batch: 100; loss: 0.9; acc: 0.72
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.75
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.71; acc: 0.75
Batch: 220; loss: 0.74; acc: 0.73
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.47; acc: 0.81
Batch: 280; loss: 0.8; acc: 0.73
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.88; acc: 0.72
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.93; acc: 0.69
Batch: 440; loss: 0.65; acc: 0.8
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.87; acc: 0.77
Batch: 540; loss: 0.89; acc: 0.72
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.59; acc: 0.78
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.7; acc: 0.75
Batch: 640; loss: 0.85; acc: 0.77
Batch: 660; loss: 0.79; acc: 0.77
Batch: 680; loss: 0.9; acc: 0.69
Batch: 700; loss: 0.67; acc: 0.78
Batch: 720; loss: 0.94; acc: 0.72
Batch: 740; loss: 0.64; acc: 0.77
Batch: 760; loss: 0.96; acc: 0.72
Batch: 780; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6496942400173017; val_accuracy: 0.7994625796178344 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.72
Batch: 40; loss: 0.54; acc: 0.77
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.89; acc: 0.75
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.77
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.9; acc: 0.78
Batch: 280; loss: 0.87; acc: 0.73
Batch: 300; loss: 0.8; acc: 0.7
Batch: 320; loss: 0.79; acc: 0.75
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.67; acc: 0.78
Batch: 380; loss: 0.9; acc: 0.73
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.76; acc: 0.78
Batch: 500; loss: 0.71; acc: 0.8
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.75
Batch: 600; loss: 0.65; acc: 0.75
Batch: 620; loss: 0.39; acc: 0.84
Batch: 640; loss: 0.89; acc: 0.72
Batch: 660; loss: 0.84; acc: 0.77
Batch: 680; loss: 0.8; acc: 0.73
Batch: 700; loss: 0.82; acc: 0.69
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.79; acc: 0.77
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6495795062013493; val_accuracy: 0.7997611464968153 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.65; acc: 0.81
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.83; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.44; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.77
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.69; acc: 0.77
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.82; acc: 0.81
Batch: 320; loss: 0.87; acc: 0.75
Batch: 340; loss: 0.76; acc: 0.69
Batch: 360; loss: 0.78; acc: 0.73
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.81
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.88; acc: 0.77
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.71; acc: 0.84
Batch: 520; loss: 0.83; acc: 0.7
Batch: 540; loss: 0.53; acc: 0.89
Batch: 560; loss: 0.71; acc: 0.81
Batch: 580; loss: 0.78; acc: 0.72
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.72; acc: 0.8
Batch: 640; loss: 0.68; acc: 0.75
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 0.87; acc: 0.75
Batch: 700; loss: 0.81; acc: 0.72
Batch: 720; loss: 0.65; acc: 0.83
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.83; acc: 0.73
Batch: 780; loss: 0.66; acc: 0.69
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.75
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.73
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.6525165259268633; val_accuracy: 0.7965764331210191 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.69
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.72; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.74; acc: 0.75
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.92; acc: 0.77
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.74; acc: 0.73
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.5; acc: 0.81
Batch: 300; loss: 0.58; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.78
Batch: 360; loss: 0.78; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.7
Batch: 420; loss: 0.86; acc: 0.75
Batch: 440; loss: 0.45; acc: 0.81
Batch: 460; loss: 0.92; acc: 0.7
Batch: 480; loss: 0.69; acc: 0.81
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.69; acc: 0.75
Batch: 540; loss: 0.85; acc: 0.8
Batch: 560; loss: 0.96; acc: 0.7
Batch: 580; loss: 1.1; acc: 0.64
Batch: 600; loss: 0.85; acc: 0.75
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.69; acc: 0.78
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.75; acc: 0.75
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.7; acc: 0.81
Batch: 760; loss: 0.67; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.97
Val Epoch over. val_loss: 0.6513243164795979; val_accuracy: 0.7974721337579618 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.94; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.64; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.81; acc: 0.73
Batch: 180; loss: 0.53; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.61; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.75
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.7; acc: 0.78
Batch: 340; loss: 0.69; acc: 0.8
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.79; acc: 0.77
Batch: 400; loss: 0.87; acc: 0.72
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.7; acc: 0.77
Batch: 460; loss: 0.62; acc: 0.77
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.57; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.8
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.41; acc: 0.94
Batch: 620; loss: 0.6; acc: 0.77
Batch: 640; loss: 0.64; acc: 0.77
Batch: 660; loss: 0.63; acc: 0.84
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.52; acc: 0.8
Batch: 720; loss: 0.67; acc: 0.77
Batch: 740; loss: 0.83; acc: 0.8
Batch: 760; loss: 0.79; acc: 0.69
Batch: 780; loss: 0.68; acc: 0.7
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.6479093990508159; val_accuracy: 0.799562101910828 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 0.92; acc: 0.77
Batch: 140; loss: 0.79; acc: 0.77
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.73
Batch: 200; loss: 0.74; acc: 0.75
Batch: 220; loss: 0.82; acc: 0.7
Batch: 240; loss: 0.76; acc: 0.78
Batch: 260; loss: 0.66; acc: 0.78
Batch: 280; loss: 0.79; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 0.85; acc: 0.7
Batch: 400; loss: 1.04; acc: 0.7
Batch: 420; loss: 0.55; acc: 0.8
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.93; acc: 0.73
Batch: 480; loss: 0.72; acc: 0.75
Batch: 500; loss: 0.73; acc: 0.73
Batch: 520; loss: 0.84; acc: 0.72
Batch: 540; loss: 0.65; acc: 0.78
Batch: 560; loss: 0.79; acc: 0.72
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.52; acc: 0.78
Batch: 620; loss: 0.99; acc: 0.69
Batch: 640; loss: 0.6; acc: 0.77
Batch: 660; loss: 0.96; acc: 0.73
Batch: 680; loss: 0.73; acc: 0.77
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.72; acc: 0.75
Batch: 760; loss: 0.69; acc: 0.73
Batch: 780; loss: 0.79; acc: 0.73
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6489148058329418; val_accuracy: 0.7986664012738853 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.85; acc: 0.75
Batch: 40; loss: 0.6; acc: 0.75
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.73
Batch: 100; loss: 0.83; acc: 0.73
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.77; acc: 0.8
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.86; acc: 0.7
Batch: 260; loss: 0.73; acc: 0.75
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.67; acc: 0.78
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.74; acc: 0.77
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.75
Batch: 420; loss: 0.69; acc: 0.77
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.75; acc: 0.78
Batch: 480; loss: 0.7; acc: 0.81
Batch: 500; loss: 0.72; acc: 0.78
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.86; acc: 0.73
Batch: 560; loss: 0.84; acc: 0.67
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.78
Batch: 640; loss: 0.8; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.87; acc: 0.83
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.85; acc: 0.73
Batch: 780; loss: 0.87; acc: 0.72
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.98
Val Epoch over. val_loss: 0.6519388026872258; val_accuracy: 0.7980692675159236 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 0.92; acc: 0.8
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.65; acc: 0.78
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.78; acc: 0.77
Batch: 200; loss: 0.81; acc: 0.8
Batch: 220; loss: 0.91; acc: 0.73
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.81
Batch: 280; loss: 0.72; acc: 0.75
Batch: 300; loss: 1.05; acc: 0.67
Batch: 320; loss: 0.63; acc: 0.78
Batch: 340; loss: 0.7; acc: 0.8
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.8
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.77
Batch: 480; loss: 0.74; acc: 0.81
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 1.0; acc: 0.69
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.71; acc: 0.73
Batch: 620; loss: 0.93; acc: 0.7
Batch: 640; loss: 0.78; acc: 0.75
Batch: 660; loss: 0.83; acc: 0.73
Batch: 680; loss: 0.73; acc: 0.72
Batch: 700; loss: 0.53; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.78
Batch: 740; loss: 0.91; acc: 0.72
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 1.03; acc: 0.75
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.75
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.6494680042289624; val_accuracy: 0.8008558917197452 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.82; acc: 0.72
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 0.8; acc: 0.69
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.75; acc: 0.81
Batch: 240; loss: 0.68; acc: 0.77
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.73; acc: 0.72
Batch: 300; loss: 0.79; acc: 0.73
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.6; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.77; acc: 0.73
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.93; acc: 0.7
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.71; acc: 0.75
Batch: 520; loss: 0.54; acc: 0.81
Batch: 540; loss: 0.99; acc: 0.7
Batch: 560; loss: 0.78; acc: 0.83
Batch: 580; loss: 0.62; acc: 0.81
Batch: 600; loss: 0.75; acc: 0.81
Batch: 620; loss: 0.59; acc: 0.77
Batch: 640; loss: 0.68; acc: 0.75
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.53; acc: 0.8
Batch: 700; loss: 0.57; acc: 0.8
Batch: 720; loss: 0.84; acc: 0.78
Batch: 740; loss: 0.67; acc: 0.77
Batch: 760; loss: 0.51; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6495030864029173; val_accuracy: 0.7972730891719745 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.86; acc: 0.72
Batch: 60; loss: 0.93; acc: 0.72
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.75; acc: 0.73
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.75
Batch: 200; loss: 0.9; acc: 0.75
Batch: 220; loss: 0.76; acc: 0.83
Batch: 240; loss: 0.68; acc: 0.81
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.71; acc: 0.72
Batch: 300; loss: 0.62; acc: 0.77
Batch: 320; loss: 0.6; acc: 0.78
Batch: 340; loss: 0.75; acc: 0.73
Batch: 360; loss: 1.09; acc: 0.7
Batch: 380; loss: 0.88; acc: 0.77
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.73
Batch: 440; loss: 0.65; acc: 0.81
Batch: 460; loss: 0.46; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.62; acc: 0.78
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.86; acc: 0.78
Batch: 600; loss: 0.75; acc: 0.73
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.66; acc: 0.73
Batch: 680; loss: 0.77; acc: 0.78
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.77; acc: 0.83
Batch: 740; loss: 0.72; acc: 0.78
Batch: 760; loss: 0.79; acc: 0.78
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6471388921806007; val_accuracy: 0.8000597133757962 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.57; acc: 0.77
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.58; acc: 0.78
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.75
Batch: 180; loss: 0.75; acc: 0.73
Batch: 200; loss: 0.79; acc: 0.75
Batch: 220; loss: 0.67; acc: 0.83
Batch: 240; loss: 0.58; acc: 0.81
Batch: 260; loss: 0.92; acc: 0.72
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.83; acc: 0.75
Batch: 320; loss: 0.67; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.73
Batch: 360; loss: 0.76; acc: 0.75
Batch: 380; loss: 1.0; acc: 0.72
Batch: 400; loss: 0.87; acc: 0.73
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.65; acc: 0.78
Batch: 500; loss: 0.58; acc: 0.8
Batch: 520; loss: 0.72; acc: 0.7
Batch: 540; loss: 0.67; acc: 0.78
Batch: 560; loss: 0.61; acc: 0.77
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.68; acc: 0.8
Batch: 620; loss: 0.76; acc: 0.75
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.6; acc: 0.78
Batch: 740; loss: 0.42; acc: 0.81
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.72; acc: 0.77
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.6476149849451271; val_accuracy: 0.8009554140127388 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.69
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.73
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.75
Batch: 220; loss: 0.67; acc: 0.73
Batch: 240; loss: 0.82; acc: 0.72
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.79; acc: 0.78
Batch: 320; loss: 0.71; acc: 0.77
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.69; acc: 0.83
Batch: 420; loss: 0.79; acc: 0.73
Batch: 440; loss: 0.77; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.77
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.88; acc: 0.77
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.52; acc: 0.78
Batch: 640; loss: 0.66; acc: 0.8
Batch: 660; loss: 0.76; acc: 0.7
Batch: 680; loss: 0.84; acc: 0.77
Batch: 700; loss: 1.01; acc: 0.7
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.63; acc: 0.8
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.48; acc: 0.81
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6472318280654349; val_accuracy: 0.7992635350318471 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.89; acc: 0.66
Batch: 40; loss: 1.0; acc: 0.73
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.91; acc: 0.67
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.77; acc: 0.73
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.71; acc: 0.81
Batch: 260; loss: 0.69; acc: 0.78
Batch: 280; loss: 0.73; acc: 0.73
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.63; acc: 0.8
Batch: 360; loss: 0.63; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.8
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.77
Batch: 500; loss: 0.7; acc: 0.78
Batch: 520; loss: 0.78; acc: 0.77
Batch: 540; loss: 0.54; acc: 0.8
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.8; acc: 0.73
Batch: 620; loss: 0.58; acc: 0.75
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.75; acc: 0.72
Batch: 680; loss: 0.45; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.74; acc: 0.75
Batch: 740; loss: 0.78; acc: 0.83
Batch: 760; loss: 0.62; acc: 0.78
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6475807893428074; val_accuracy: 0.7992635350318471 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.78; acc: 0.83
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.51; acc: 0.8
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.8; acc: 0.73
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.72; acc: 0.75
Batch: 160; loss: 0.84; acc: 0.78
Batch: 180; loss: 0.68; acc: 0.78
Batch: 200; loss: 1.2; acc: 0.77
Batch: 220; loss: 0.99; acc: 0.72
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.71; acc: 0.75
Batch: 300; loss: 0.72; acc: 0.77
Batch: 320; loss: 0.9; acc: 0.75
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.78
Batch: 380; loss: 0.57; acc: 0.77
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.76; acc: 0.81
Batch: 440; loss: 0.71; acc: 0.77
Batch: 460; loss: 0.55; acc: 0.78
Batch: 480; loss: 0.77; acc: 0.73
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.54; acc: 0.84
Batch: 540; loss: 0.72; acc: 0.75
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.83
Batch: 600; loss: 0.83; acc: 0.73
Batch: 620; loss: 0.79; acc: 0.83
Batch: 640; loss: 0.44; acc: 0.81
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.96; acc: 0.8
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.68; acc: 0.86
Batch: 760; loss: 1.05; acc: 0.69
Batch: 780; loss: 0.73; acc: 0.69
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.75
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.6478549782068107; val_accuracy: 0.7993630573248408 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.93; acc: 0.67
Batch: 80; loss: 0.77; acc: 0.77
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.73; acc: 0.73
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.76; acc: 0.73
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.98; acc: 0.73
Batch: 220; loss: 0.59; acc: 0.78
Batch: 240; loss: 0.56; acc: 0.8
Batch: 260; loss: 0.64; acc: 0.7
Batch: 280; loss: 0.81; acc: 0.7
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.73
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.81; acc: 0.75
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.68; acc: 0.78
Batch: 440; loss: 0.67; acc: 0.77
Batch: 460; loss: 0.75; acc: 0.8
Batch: 480; loss: 0.92; acc: 0.69
Batch: 500; loss: 0.89; acc: 0.69
Batch: 520; loss: 0.72; acc: 0.78
Batch: 540; loss: 0.64; acc: 0.73
Batch: 560; loss: 0.48; acc: 0.81
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.94; acc: 0.77
Batch: 620; loss: 0.7; acc: 0.83
Batch: 640; loss: 0.75; acc: 0.73
Batch: 660; loss: 0.66; acc: 0.84
Batch: 680; loss: 0.7; acc: 0.83
Batch: 700; loss: 0.75; acc: 0.77
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.97; acc: 0.72
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.81; acc: 0.72
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.81
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.6470185579957476; val_accuracy: 0.8002587579617835 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.76; acc: 0.78
Batch: 20; loss: 0.65; acc: 0.84
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 0.78; acc: 0.72
Batch: 80; loss: 0.88; acc: 0.7
Batch: 100; loss: 0.73; acc: 0.72
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.69; acc: 0.78
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.54; acc: 0.81
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.64; acc: 0.78
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.79; acc: 0.8
Batch: 280; loss: 0.66; acc: 0.78
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.91; acc: 0.7
Batch: 380; loss: 0.67; acc: 0.73
Batch: 400; loss: 0.65; acc: 0.8
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 0.75; acc: 0.67
Batch: 460; loss: 1.06; acc: 0.66
Batch: 480; loss: 0.67; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.73
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 1.01; acc: 0.67
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.78
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.57; acc: 0.78
Batch: 660; loss: 0.68; acc: 0.8
Batch: 680; loss: 0.79; acc: 0.73
Batch: 700; loss: 0.66; acc: 0.78
Batch: 720; loss: 0.75; acc: 0.78
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.8
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.6475824817160892; val_accuracy: 0.8002587579617835 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.71; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.61; acc: 0.78
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.86; acc: 0.78
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.94; acc: 0.75
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.8; acc: 0.73
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.72; acc: 0.78
Batch: 260; loss: 0.67; acc: 0.73
Batch: 280; loss: 0.68; acc: 0.84
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.82; acc: 0.7
Batch: 360; loss: 0.88; acc: 0.7
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.64; acc: 0.77
Batch: 420; loss: 0.67; acc: 0.8
Batch: 440; loss: 0.84; acc: 0.73
Batch: 460; loss: 0.65; acc: 0.78
Batch: 480; loss: 0.67; acc: 0.8
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.87; acc: 0.8
Batch: 540; loss: 0.7; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.68; acc: 0.73
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.77; acc: 0.75
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 1.03; acc: 0.72
Batch: 680; loss: 0.64; acc: 0.83
Batch: 700; loss: 0.8; acc: 0.81
Batch: 720; loss: 0.77; acc: 0.72
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.65; acc: 0.81
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.65; acc: 0.8
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.6466312877311828; val_accuracy: 0.8002587579617835 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.82; acc: 0.73
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.83; acc: 0.7
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.88; acc: 0.72
Batch: 180; loss: 0.65; acc: 0.78
Batch: 200; loss: 0.53; acc: 0.8
Batch: 220; loss: 0.8; acc: 0.72
Batch: 240; loss: 0.86; acc: 0.77
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.95; acc: 0.72
Batch: 300; loss: 0.78; acc: 0.73
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.45; acc: 0.81
Batch: 360; loss: 0.66; acc: 0.75
Batch: 380; loss: 0.64; acc: 0.84
Batch: 400; loss: 0.76; acc: 0.73
Batch: 420; loss: 0.65; acc: 0.73
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.8
Batch: 480; loss: 0.79; acc: 0.72
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 1.15; acc: 0.7
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.77
Batch: 580; loss: 0.77; acc: 0.77
Batch: 600; loss: 0.78; acc: 0.8
Batch: 620; loss: 0.68; acc: 0.83
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.7
Batch: 740; loss: 0.77; acc: 0.75
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.49; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.49; acc: 0.81
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.6466113016673714; val_accuracy: 0.8003582802547771 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 1.01; acc: 0.7
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.91; acc: 0.7
Batch: 160; loss: 0.83; acc: 0.73
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 0.8; acc: 0.72
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.65; acc: 0.77
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.75; acc: 0.77
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.69; acc: 0.75
Batch: 380; loss: 0.67; acc: 0.77
Batch: 400; loss: 0.74; acc: 0.73
Batch: 420; loss: 0.94; acc: 0.77
Batch: 440; loss: 0.7; acc: 0.75
Batch: 460; loss: 0.83; acc: 0.72
Batch: 480; loss: 0.77; acc: 0.78
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.9; acc: 0.67
Batch: 540; loss: 0.59; acc: 0.77
Batch: 560; loss: 0.6; acc: 0.73
Batch: 580; loss: 0.65; acc: 0.78
Batch: 600; loss: 0.82; acc: 0.75
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.74; acc: 0.75
Batch: 660; loss: 0.65; acc: 0.78
Batch: 680; loss: 0.6; acc: 0.78
Batch: 700; loss: 0.62; acc: 0.81
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.64; acc: 0.77
Batch: 760; loss: 0.97; acc: 0.72
Batch: 780; loss: 0.7; acc: 0.75
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.49; acc: 0.8
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.6471873321540796; val_accuracy: 0.8002587579617835 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 5553248
elements in E: 5553250
fraction nonzero: 0.999999639850538
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.16
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.03
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.29; acc: 0.12
Batch: 180; loss: 2.27; acc: 0.14
Batch: 200; loss: 2.28; acc: 0.08
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.26; acc: 0.17
Batch: 260; loss: 2.28; acc: 0.09
Batch: 280; loss: 2.26; acc: 0.22
Batch: 300; loss: 2.27; acc: 0.17
Batch: 320; loss: 2.27; acc: 0.16
Batch: 340; loss: 2.27; acc: 0.14
Batch: 360; loss: 2.26; acc: 0.23
Batch: 380; loss: 2.25; acc: 0.27
Batch: 400; loss: 2.25; acc: 0.22
Batch: 420; loss: 2.25; acc: 0.2
Batch: 440; loss: 2.24; acc: 0.22
Batch: 460; loss: 2.2; acc: 0.36
Batch: 480; loss: 2.22; acc: 0.25
Batch: 500; loss: 2.21; acc: 0.33
Batch: 520; loss: 2.17; acc: 0.27
Batch: 540; loss: 2.15; acc: 0.36
Batch: 560; loss: 2.14; acc: 0.28
Batch: 580; loss: 2.1; acc: 0.36
Batch: 600; loss: 2.09; acc: 0.36
Batch: 620; loss: 2.04; acc: 0.42
Batch: 640; loss: 2.02; acc: 0.31
Batch: 660; loss: 1.88; acc: 0.47
Batch: 680; loss: 1.92; acc: 0.38
Batch: 700; loss: 1.79; acc: 0.52
Batch: 720; loss: 1.6; acc: 0.53
Batch: 740; loss: 1.6; acc: 0.53
Batch: 760; loss: 1.63; acc: 0.48
Batch: 780; loss: 1.35; acc: 0.59
Train Epoch over. train_loss: 2.14; train_accuracy: 0.25 

Batch: 0; loss: 1.31; acc: 0.62
Batch: 20; loss: 1.44; acc: 0.41
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 1.24; acc: 0.56
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.34; acc: 0.64
Batch: 120; loss: 1.34; acc: 0.59
Batch: 140; loss: 1.25; acc: 0.53
Val Epoch over. val_loss: 1.333292895441602; val_accuracy: 0.5700636942675159 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.37; acc: 0.56
Batch: 20; loss: 1.41; acc: 0.53
Batch: 40; loss: 1.3; acc: 0.67
Batch: 60; loss: 1.26; acc: 0.58
Batch: 80; loss: 0.99; acc: 0.66
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 1.12; acc: 0.61
Batch: 140; loss: 1.18; acc: 0.58
Batch: 160; loss: 1.27; acc: 0.48
Batch: 180; loss: 0.95; acc: 0.64
Batch: 200; loss: 1.08; acc: 0.62
Batch: 220; loss: 0.66; acc: 0.75
Batch: 240; loss: 1.16; acc: 0.59
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.98; acc: 0.67
Batch: 300; loss: 1.14; acc: 0.59
Batch: 320; loss: 0.89; acc: 0.66
Batch: 340; loss: 0.87; acc: 0.72
Batch: 360; loss: 0.77; acc: 0.75
Batch: 380; loss: 1.01; acc: 0.7
Batch: 400; loss: 1.1; acc: 0.61
Batch: 420; loss: 0.82; acc: 0.73
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.78; acc: 0.84
Batch: 480; loss: 0.9; acc: 0.7
Batch: 500; loss: 0.76; acc: 0.7
Batch: 520; loss: 0.86; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.66
Batch: 560; loss: 0.94; acc: 0.7
Batch: 580; loss: 0.89; acc: 0.7
Batch: 600; loss: 0.75; acc: 0.78
Batch: 620; loss: 0.89; acc: 0.67
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 0.88; acc: 0.7
Batch: 680; loss: 0.92; acc: 0.73
Batch: 700; loss: 1.01; acc: 0.7
Batch: 720; loss: 0.86; acc: 0.72
Batch: 740; loss: 1.02; acc: 0.69
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.95; train_accuracy: 0.69 

Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.88; acc: 0.72
Batch: 80; loss: 0.63; acc: 0.73
Batch: 100; loss: 0.9; acc: 0.72
Batch: 120; loss: 1.09; acc: 0.59
Batch: 140; loss: 0.62; acc: 0.78
Val Epoch over. val_loss: 0.8937589562242958; val_accuracy: 0.7013335987261147 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.7
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.66; acc: 0.78
Batch: 60; loss: 1.34; acc: 0.55
Batch: 80; loss: 0.99; acc: 0.67
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 0.93; acc: 0.66
Batch: 140; loss: 0.62; acc: 0.75
Batch: 160; loss: 1.35; acc: 0.52
Batch: 180; loss: 0.8; acc: 0.72
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 0.61; acc: 0.77
Batch: 240; loss: 0.57; acc: 0.77
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.91; acc: 0.7
Batch: 300; loss: 0.79; acc: 0.72
Batch: 320; loss: 0.76; acc: 0.72
Batch: 340; loss: 0.88; acc: 0.78
Batch: 360; loss: 0.96; acc: 0.67
Batch: 380; loss: 0.66; acc: 0.81
Batch: 400; loss: 1.02; acc: 0.66
Batch: 420; loss: 0.92; acc: 0.73
Batch: 440; loss: 0.78; acc: 0.72
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.84; acc: 0.72
Batch: 500; loss: 0.88; acc: 0.67
Batch: 520; loss: 0.76; acc: 0.75
Batch: 540; loss: 0.88; acc: 0.77
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.78
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.75
Batch: 660; loss: 0.94; acc: 0.66
Batch: 680; loss: 0.72; acc: 0.78
Batch: 700; loss: 1.0; acc: 0.64
Batch: 720; loss: 0.88; acc: 0.75
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.72; acc: 0.67
Train Epoch over. train_loss: 0.8; train_accuracy: 0.74 

Batch: 0; loss: 1.14; acc: 0.61
Batch: 20; loss: 1.17; acc: 0.59
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 1.35; acc: 0.55
Batch: 80; loss: 1.01; acc: 0.77
Batch: 100; loss: 0.85; acc: 0.72
Batch: 120; loss: 1.07; acc: 0.62
Batch: 140; loss: 0.55; acc: 0.78
Val Epoch over. val_loss: 0.9672106644909853; val_accuracy: 0.6963574840764332 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.78
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.84; acc: 0.72
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.7
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.84; acc: 0.8
Batch: 180; loss: 0.93; acc: 0.75
Batch: 200; loss: 0.81; acc: 0.67
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.59; acc: 0.8
Batch: 280; loss: 0.88; acc: 0.75
Batch: 300; loss: 0.65; acc: 0.75
Batch: 320; loss: 0.62; acc: 0.73
Batch: 340; loss: 0.75; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.68; acc: 0.78
Batch: 400; loss: 0.54; acc: 0.81
Batch: 420; loss: 0.86; acc: 0.72
Batch: 440; loss: 0.72; acc: 0.69
Batch: 460; loss: 0.72; acc: 0.69
Batch: 480; loss: 0.89; acc: 0.7
Batch: 500; loss: 0.95; acc: 0.67
Batch: 520; loss: 0.68; acc: 0.83
Batch: 540; loss: 0.95; acc: 0.67
Batch: 560; loss: 0.9; acc: 0.72
Batch: 580; loss: 0.73; acc: 0.73
Batch: 600; loss: 0.78; acc: 0.75
Batch: 620; loss: 0.63; acc: 0.75
Batch: 640; loss: 0.67; acc: 0.75
Batch: 660; loss: 0.8; acc: 0.75
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.77
Batch: 720; loss: 0.79; acc: 0.77
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 1.19; acc: 0.58
Batch: 780; loss: 0.53; acc: 0.83
Train Epoch over. train_loss: 0.76; train_accuracy: 0.76 

Batch: 0; loss: 0.83; acc: 0.66
Batch: 20; loss: 1.01; acc: 0.61
Batch: 40; loss: 0.68; acc: 0.72
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 1.08; acc: 0.66
Batch: 120; loss: 1.3; acc: 0.56
Batch: 140; loss: 0.41; acc: 0.83
Val Epoch over. val_loss: 0.8779617701746096; val_accuracy: 0.7101910828025477 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.89; acc: 0.78
Batch: 80; loss: 0.83; acc: 0.8
Batch: 100; loss: 0.87; acc: 0.7
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.83; acc: 0.78
Batch: 160; loss: 0.91; acc: 0.75
Batch: 180; loss: 0.57; acc: 0.78
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.86; acc: 0.69
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.74; acc: 0.77
Batch: 280; loss: 0.82; acc: 0.75
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.91; acc: 0.75
Batch: 340; loss: 0.42; acc: 0.83
Batch: 360; loss: 0.5; acc: 0.8
Batch: 380; loss: 0.71; acc: 0.75
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.74; acc: 0.7
Batch: 440; loss: 0.76; acc: 0.77
Batch: 460; loss: 0.76; acc: 0.81
Batch: 480; loss: 0.8; acc: 0.77
Batch: 500; loss: 0.9; acc: 0.75
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.89; acc: 0.72
Batch: 560; loss: 0.84; acc: 0.69
Batch: 580; loss: 0.96; acc: 0.66
Batch: 600; loss: 0.53; acc: 0.77
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.9; acc: 0.73
Batch: 660; loss: 0.72; acc: 0.77
Batch: 680; loss: 0.61; acc: 0.78
Batch: 700; loss: 0.6; acc: 0.78
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.82; acc: 0.77
Batch: 780; loss: 0.74; acc: 0.8
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 1.21; acc: 0.55
Batch: 20; loss: 1.09; acc: 0.61
Batch: 40; loss: 0.82; acc: 0.69
Batch: 60; loss: 1.53; acc: 0.58
Batch: 80; loss: 1.01; acc: 0.67
Batch: 100; loss: 1.03; acc: 0.66
Batch: 120; loss: 1.27; acc: 0.55
Batch: 140; loss: 0.74; acc: 0.7
Val Epoch over. val_loss: 1.110980904785691; val_accuracy: 0.6421178343949044 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.59
Batch: 20; loss: 0.74; acc: 0.7
Batch: 40; loss: 0.57; acc: 0.73
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 1.1; acc: 0.66
Batch: 140; loss: 0.67; acc: 0.8
Batch: 160; loss: 0.91; acc: 0.73
Batch: 180; loss: 0.67; acc: 0.72
Batch: 200; loss: 0.69; acc: 0.77
Batch: 220; loss: 0.67; acc: 0.75
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.78
Batch: 280; loss: 0.96; acc: 0.73
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.77; acc: 0.75
Batch: 340; loss: 0.79; acc: 0.78
Batch: 360; loss: 0.83; acc: 0.72
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.86; acc: 0.64
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 0.75; acc: 0.7
Batch: 460; loss: 0.71; acc: 0.7
Batch: 480; loss: 0.54; acc: 0.81
Batch: 500; loss: 0.62; acc: 0.73
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.94; acc: 0.78
Batch: 560; loss: 0.8; acc: 0.73
Batch: 580; loss: 0.8; acc: 0.72
Batch: 600; loss: 0.6; acc: 0.78
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.78; acc: 0.77
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 0.64; acc: 0.83
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.77; acc: 0.75
Batch: 740; loss: 0.69; acc: 0.83
Batch: 760; loss: 0.57; acc: 0.8
Batch: 780; loss: 0.71; acc: 0.72
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.84; acc: 0.66
Batch: 20; loss: 0.88; acc: 0.66
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.95; acc: 0.73
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.89
Val Epoch over. val_loss: 0.7846451690242549; val_accuracy: 0.7581608280254777 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.86; acc: 0.7
Batch: 80; loss: 0.76; acc: 0.81
Batch: 100; loss: 0.84; acc: 0.69
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.81; acc: 0.75
Batch: 160; loss: 0.65; acc: 0.72
Batch: 180; loss: 0.78; acc: 0.73
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.71; acc: 0.77
Batch: 240; loss: 0.59; acc: 0.73
Batch: 260; loss: 0.71; acc: 0.73
Batch: 280; loss: 0.82; acc: 0.8
Batch: 300; loss: 0.75; acc: 0.7
Batch: 320; loss: 0.67; acc: 0.77
Batch: 340; loss: 1.02; acc: 0.73
Batch: 360; loss: 0.69; acc: 0.75
Batch: 380; loss: 0.73; acc: 0.75
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 1.02; acc: 0.66
Batch: 440; loss: 0.93; acc: 0.73
Batch: 460; loss: 0.9; acc: 0.78
Batch: 480; loss: 0.7; acc: 0.77
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.95; acc: 0.66
Batch: 540; loss: 0.68; acc: 0.83
Batch: 560; loss: 0.75; acc: 0.75
Batch: 580; loss: 0.8; acc: 0.78
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.73
Batch: 640; loss: 0.66; acc: 0.78
Batch: 660; loss: 0.95; acc: 0.72
Batch: 680; loss: 0.82; acc: 0.78
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.82; acc: 0.8
Batch: 760; loss: 0.77; acc: 0.75
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.93; acc: 0.7
Batch: 20; loss: 1.0; acc: 0.64
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.69
Batch: 80; loss: 0.68; acc: 0.73
Batch: 100; loss: 0.95; acc: 0.73
Batch: 120; loss: 1.08; acc: 0.67
Batch: 140; loss: 0.52; acc: 0.78
Val Epoch over. val_loss: 0.9017802095337278; val_accuracy: 0.7079020700636943 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.69
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.46; acc: 0.83
Batch: 60; loss: 0.66; acc: 0.73
Batch: 80; loss: 0.76; acc: 0.73
Batch: 100; loss: 0.76; acc: 0.73
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.87; acc: 0.72
Batch: 160; loss: 0.93; acc: 0.67
Batch: 180; loss: 0.68; acc: 0.77
Batch: 200; loss: 0.62; acc: 0.78
Batch: 220; loss: 1.08; acc: 0.69
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.76; acc: 0.7
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 1.1; acc: 0.67
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.81
Batch: 360; loss: 0.89; acc: 0.7
Batch: 380; loss: 0.95; acc: 0.7
Batch: 400; loss: 0.86; acc: 0.78
Batch: 420; loss: 0.6; acc: 0.8
Batch: 440; loss: 0.76; acc: 0.72
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.76; acc: 0.75
Batch: 500; loss: 1.04; acc: 0.69
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.96; acc: 0.72
Batch: 560; loss: 0.94; acc: 0.67
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 0.84; acc: 0.78
Batch: 640; loss: 0.92; acc: 0.69
Batch: 660; loss: 0.77; acc: 0.72
Batch: 680; loss: 0.8; acc: 0.72
Batch: 700; loss: 0.6; acc: 0.81
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.78; acc: 0.8
Batch: 760; loss: 0.76; acc: 0.83
Batch: 780; loss: 0.65; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.75; acc: 0.73
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.99; acc: 0.73
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 1.03; acc: 0.67
Batch: 140; loss: 0.35; acc: 0.88
Val Epoch over. val_loss: 0.7515589676465199; val_accuracy: 0.7563694267515924 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 1.15; acc: 0.66
Batch: 40; loss: 0.81; acc: 0.77
Batch: 60; loss: 0.92; acc: 0.67
Batch: 80; loss: 0.72; acc: 0.77
Batch: 100; loss: 0.79; acc: 0.7
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.85; acc: 0.75
Batch: 180; loss: 1.0; acc: 0.69
Batch: 200; loss: 0.76; acc: 0.78
Batch: 220; loss: 0.97; acc: 0.72
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 0.62; acc: 0.78
Batch: 300; loss: 0.84; acc: 0.73
Batch: 320; loss: 0.72; acc: 0.83
Batch: 340; loss: 0.87; acc: 0.78
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.84; acc: 0.72
Batch: 400; loss: 0.78; acc: 0.77
Batch: 420; loss: 0.8; acc: 0.72
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 1.04; acc: 0.72
Batch: 480; loss: 0.69; acc: 0.78
Batch: 500; loss: 0.73; acc: 0.75
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 0.93; acc: 0.77
Batch: 560; loss: 0.85; acc: 0.69
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.81; acc: 0.72
Batch: 620; loss: 0.98; acc: 0.67
Batch: 640; loss: 0.69; acc: 0.78
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.74; acc: 0.75
Batch: 720; loss: 0.69; acc: 0.77
Batch: 740; loss: 0.78; acc: 0.75
Batch: 760; loss: 0.66; acc: 0.73
Batch: 780; loss: 0.72; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.83; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.61
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 0.6; acc: 0.8
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 1.31; acc: 0.69
Batch: 140; loss: 0.63; acc: 0.8
Val Epoch over. val_loss: 0.8242219840265383; val_accuracy: 0.738953025477707 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.75
Batch: 20; loss: 1.06; acc: 0.62
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 1.2; acc: 0.66
Batch: 100; loss: 0.86; acc: 0.64
Batch: 120; loss: 0.68; acc: 0.73
Batch: 140; loss: 0.58; acc: 0.78
Batch: 160; loss: 0.85; acc: 0.78
Batch: 180; loss: 0.91; acc: 0.73
Batch: 200; loss: 0.72; acc: 0.75
Batch: 220; loss: 0.62; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.78
Batch: 260; loss: 0.77; acc: 0.73
Batch: 280; loss: 0.82; acc: 0.75
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.65; acc: 0.73
Batch: 360; loss: 0.8; acc: 0.83
Batch: 380; loss: 0.52; acc: 0.8
Batch: 400; loss: 0.58; acc: 0.78
Batch: 420; loss: 0.77; acc: 0.73
Batch: 440; loss: 0.51; acc: 0.8
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.78; acc: 0.83
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.8; acc: 0.73
Batch: 540; loss: 0.7; acc: 0.77
Batch: 560; loss: 0.77; acc: 0.78
Batch: 580; loss: 0.89; acc: 0.7
Batch: 600; loss: 0.78; acc: 0.83
Batch: 620; loss: 0.87; acc: 0.75
Batch: 640; loss: 0.67; acc: 0.72
Batch: 660; loss: 0.74; acc: 0.78
Batch: 680; loss: 0.8; acc: 0.73
Batch: 700; loss: 0.59; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.78
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.66; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.83; acc: 0.7
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.96; acc: 0.75
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.46; acc: 0.83
Val Epoch over. val_loss: 0.7490301798483369; val_accuracy: 0.7592555732484076 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.01; acc: 0.72
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.73; acc: 0.72
Batch: 60; loss: 0.52; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.73
Batch: 100; loss: 0.93; acc: 0.7
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 0.58; acc: 0.8
Batch: 180; loss: 0.68; acc: 0.73
Batch: 200; loss: 0.76; acc: 0.81
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.75; acc: 0.72
Batch: 260; loss: 0.67; acc: 0.81
Batch: 280; loss: 0.8; acc: 0.8
Batch: 300; loss: 0.81; acc: 0.72
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.51; acc: 0.77
Batch: 360; loss: 0.75; acc: 0.73
Batch: 380; loss: 0.82; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.74; acc: 0.77
Batch: 480; loss: 0.58; acc: 0.77
Batch: 500; loss: 0.56; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.7; acc: 0.78
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.85; acc: 0.78
Batch: 600; loss: 0.89; acc: 0.73
Batch: 620; loss: 0.79; acc: 0.78
Batch: 640; loss: 0.61; acc: 0.81
Batch: 660; loss: 0.67; acc: 0.84
Batch: 680; loss: 0.92; acc: 0.73
Batch: 700; loss: 0.86; acc: 0.81
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.61; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.77
Train Epoch over. train_loss: 0.67; train_accuracy: 0.79 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.96; acc: 0.7
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.97; acc: 0.72
Batch: 140; loss: 0.33; acc: 0.86
Val Epoch over. val_loss: 0.6375427686484756; val_accuracy: 0.7997611464968153 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.09; acc: 0.69
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.74; acc: 0.77
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.66; acc: 0.73
Batch: 140; loss: 0.67; acc: 0.67
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.52; acc: 0.8
Batch: 200; loss: 0.59; acc: 0.8
Batch: 220; loss: 0.65; acc: 0.75
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.8; acc: 0.67
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.91; acc: 0.67
Batch: 380; loss: 0.66; acc: 0.83
Batch: 400; loss: 0.85; acc: 0.77
Batch: 420; loss: 0.8; acc: 0.73
Batch: 440; loss: 0.71; acc: 0.84
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.8; acc: 0.73
Batch: 520; loss: 0.59; acc: 0.77
Batch: 540; loss: 0.81; acc: 0.75
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.73; acc: 0.84
Batch: 620; loss: 0.9; acc: 0.77
Batch: 640; loss: 0.78; acc: 0.78
Batch: 660; loss: 0.82; acc: 0.77
Batch: 680; loss: 0.58; acc: 0.81
Batch: 700; loss: 0.89; acc: 0.77
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.68; acc: 0.77
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.73; acc: 0.73
Train Epoch over. train_loss: 0.66; train_accuracy: 0.79 

Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.7
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.89
Val Epoch over. val_loss: 0.6020658781194383; val_accuracy: 0.8121019108280255 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.8
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.83; acc: 0.73
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 0.64; acc: 0.73
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.86; acc: 0.7
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.66; acc: 0.8
Batch: 320; loss: 0.94; acc: 0.7
Batch: 340; loss: 0.83; acc: 0.66
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.6; acc: 0.81
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.75; acc: 0.77
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.82; acc: 0.8
Batch: 580; loss: 0.84; acc: 0.7
Batch: 600; loss: 0.72; acc: 0.78
Batch: 620; loss: 0.7; acc: 0.7
Batch: 640; loss: 0.87; acc: 0.69
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.95; acc: 0.73
Batch: 700; loss: 0.72; acc: 0.78
Batch: 720; loss: 0.66; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.73
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.66; train_accuracy: 0.79 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.73; acc: 0.77
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 1.01; acc: 0.7
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.81; acc: 0.78
Batch: 120; loss: 1.01; acc: 0.67
Batch: 140; loss: 0.28; acc: 0.92
Val Epoch over. val_loss: 0.6307278879129203; val_accuracy: 0.8024482484076433 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.77
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.77
Batch: 160; loss: 0.6; acc: 0.8
Batch: 180; loss: 0.89; acc: 0.67
Batch: 200; loss: 0.78; acc: 0.7
Batch: 220; loss: 0.75; acc: 0.75
Batch: 240; loss: 0.6; acc: 0.81
Batch: 260; loss: 0.76; acc: 0.78
Batch: 280; loss: 0.58; acc: 0.78
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.88; acc: 0.75
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.7; acc: 0.8
Batch: 380; loss: 0.58; acc: 0.86
Batch: 400; loss: 0.82; acc: 0.7
Batch: 420; loss: 0.77; acc: 0.8
Batch: 440; loss: 0.62; acc: 0.81
Batch: 460; loss: 0.78; acc: 0.73
Batch: 480; loss: 0.86; acc: 0.75
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.57; acc: 0.78
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.91; acc: 0.61
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.75; acc: 0.78
Batch: 640; loss: 0.99; acc: 0.73
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.61; acc: 0.75
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.61; acc: 0.77
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.64; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.79 

Batch: 0; loss: 0.67; acc: 0.73
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.36; acc: 0.89
Val Epoch over. val_loss: 0.6680960110418356; val_accuracy: 0.7847332802547771 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.75
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.78
Batch: 160; loss: 0.52; acc: 0.78
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.73; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.71; acc: 0.72
Batch: 300; loss: 0.6; acc: 0.8
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.77; acc: 0.73
Batch: 420; loss: 0.62; acc: 0.8
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.81; acc: 0.69
Batch: 480; loss: 0.69; acc: 0.8
Batch: 500; loss: 0.75; acc: 0.7
Batch: 520; loss: 0.58; acc: 0.8
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.73; acc: 0.77
Batch: 580; loss: 0.83; acc: 0.72
Batch: 600; loss: 0.63; acc: 0.8
Batch: 620; loss: 0.77; acc: 0.77
Batch: 640; loss: 0.46; acc: 0.81
Batch: 660; loss: 0.81; acc: 0.73
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.69; acc: 0.73
Batch: 720; loss: 0.83; acc: 0.77
Batch: 740; loss: 0.82; acc: 0.72
Batch: 760; loss: 0.83; acc: 0.72
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.8 

Batch: 0; loss: 0.59; acc: 0.75
Batch: 20; loss: 0.89; acc: 0.73
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.95; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.5961554287725194; val_accuracy: 0.8132961783439491 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.83; acc: 0.72
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.72; acc: 0.78
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.9; acc: 0.72
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.81
Batch: 180; loss: 0.86; acc: 0.78
Batch: 200; loss: 0.8; acc: 0.8
Batch: 220; loss: 0.62; acc: 0.78
Batch: 240; loss: 0.73; acc: 0.77
Batch: 260; loss: 0.56; acc: 0.8
Batch: 280; loss: 0.86; acc: 0.75
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.92; acc: 0.7
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.7; acc: 0.78
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.75; acc: 0.83
Batch: 480; loss: 0.41; acc: 0.83
Batch: 500; loss: 0.73; acc: 0.81
Batch: 520; loss: 0.71; acc: 0.73
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.86; acc: 0.75
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 0.63; acc: 0.78
Batch: 640; loss: 0.57; acc: 0.8
Batch: 660; loss: 0.55; acc: 0.75
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.84; acc: 0.77
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.6; acc: 0.77
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.8; acc: 0.75
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 0.27; acc: 0.94
Val Epoch over. val_loss: 0.5803700734855263; val_accuracy: 0.8167794585987261 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.98; acc: 0.73
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 1.06; acc: 0.72
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.74; acc: 0.77
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.8; acc: 0.8
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.77
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.59; acc: 0.77
Batch: 300; loss: 0.75; acc: 0.78
Batch: 320; loss: 0.78; acc: 0.75
Batch: 340; loss: 0.82; acc: 0.72
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.5; acc: 0.8
Batch: 440; loss: 0.72; acc: 0.77
Batch: 460; loss: 0.89; acc: 0.75
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.56; acc: 0.78
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.89; acc: 0.75
Batch: 600; loss: 0.82; acc: 0.73
Batch: 620; loss: 0.69; acc: 0.81
Batch: 640; loss: 0.89; acc: 0.77
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.93; acc: 0.75
Batch: 700; loss: 0.8; acc: 0.67
Batch: 720; loss: 0.54; acc: 0.81
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.73
Batch: 780; loss: 0.91; acc: 0.78
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.63; acc: 0.75
Batch: 20; loss: 0.95; acc: 0.69
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.67
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.6261972157628672; val_accuracy: 0.8020501592356688 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 0.71; acc: 0.69
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.73
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.8
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.78
Batch: 200; loss: 0.53; acc: 0.78
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.59; acc: 0.78
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.78
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.68; acc: 0.81
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.83; acc: 0.75
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.9; acc: 0.73
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.42; acc: 0.81
Batch: 560; loss: 0.78; acc: 0.75
Batch: 580; loss: 0.58; acc: 0.77
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.8
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.76; acc: 0.75
Batch: 680; loss: 0.67; acc: 0.8
Batch: 700; loss: 0.68; acc: 0.75
Batch: 720; loss: 0.6; acc: 0.8
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.56; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.81
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.94; acc: 0.73
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.77
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.95
Val Epoch over. val_loss: 0.602871597382673; val_accuracy: 0.8095143312101911 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.72; acc: 0.7
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.96; acc: 0.75
Batch: 60; loss: 0.65; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.78
Batch: 120; loss: 0.62; acc: 0.73
Batch: 140; loss: 0.91; acc: 0.75
Batch: 160; loss: 0.69; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.89
Batch: 240; loss: 0.68; acc: 0.7
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.8; acc: 0.73
Batch: 300; loss: 0.78; acc: 0.7
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.43; acc: 0.83
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.54; acc: 0.78
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.83; acc: 0.75
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.89; acc: 0.73
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.55; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.78
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.78
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.9; acc: 0.69
Batch: 660; loss: 0.67; acc: 0.8
Batch: 680; loss: 0.62; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.8
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.54; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.8
Batch: 780; loss: 0.7; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.89; acc: 0.69
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.5577300145367908; val_accuracy: 0.8268312101910829 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.58; acc: 0.78
Batch: 40; loss: 0.36; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.73; acc: 0.77
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.7; acc: 0.77
Batch: 220; loss: 0.62; acc: 0.8
Batch: 240; loss: 0.71; acc: 0.83
Batch: 260; loss: 0.69; acc: 0.75
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.89; acc: 0.69
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.83; acc: 0.77
Batch: 420; loss: 0.87; acc: 0.73
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.73
Batch: 540; loss: 0.88; acc: 0.8
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.68; acc: 0.78
Batch: 620; loss: 0.6; acc: 0.77
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 0.61; acc: 0.78
Batch: 700; loss: 0.89; acc: 0.73
Batch: 720; loss: 0.84; acc: 0.72
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.64; acc: 0.8
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.78
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.5456124100905315; val_accuracy: 0.8323049363057324 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.77; acc: 0.73
Batch: 180; loss: 0.65; acc: 0.77
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.57; acc: 0.77
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.66; acc: 0.75
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.78; acc: 0.78
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.72; acc: 0.77
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.8
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 1.0; acc: 0.72
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.78; acc: 0.78
Batch: 660; loss: 0.85; acc: 0.69
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.75; acc: 0.75
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.534745362628797; val_accuracy: 0.834593949044586 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.81; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.77
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.59; acc: 0.78
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.86; acc: 0.73
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.83; acc: 0.77
Batch: 300; loss: 0.58; acc: 0.78
Batch: 320; loss: 0.54; acc: 0.8
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.79; acc: 0.66
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.61; acc: 0.77
Batch: 500; loss: 0.64; acc: 0.78
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.8
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.6; acc: 0.8
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.8
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.5335189925067744; val_accuracy: 0.8362858280254777 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.58; acc: 0.78
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.81
Batch: 220; loss: 0.94; acc: 0.77
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.59; acc: 0.8
Batch: 300; loss: 0.73; acc: 0.75
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.93; acc: 0.75
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.78
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.55; acc: 0.8
Batch: 480; loss: 0.52; acc: 0.81
Batch: 500; loss: 0.48; acc: 0.81
Batch: 520; loss: 0.36; acc: 0.83
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.63; acc: 0.75
Batch: 600; loss: 0.54; acc: 0.8
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.75; acc: 0.7
Batch: 660; loss: 0.62; acc: 0.77
Batch: 680; loss: 0.68; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.46; acc: 0.8
Batch: 740; loss: 0.64; acc: 0.81
Batch: 760; loss: 0.68; acc: 0.78
Batch: 780; loss: 0.69; acc: 0.77
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.5392950185734755; val_accuracy: 0.8388734076433121 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.98; acc: 0.73
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.6; acc: 0.8
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.53; acc: 0.8
Batch: 300; loss: 0.53; acc: 0.81
Batch: 320; loss: 0.91; acc: 0.72
Batch: 340; loss: 0.83; acc: 0.73
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.8; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.72; acc: 0.77
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.8
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.81
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.5197556012185516; val_accuracy: 0.8395700636942676 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.6; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.75
Batch: 160; loss: 0.62; acc: 0.8
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.66; acc: 0.81
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.77; acc: 0.66
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.63; acc: 0.77
Batch: 440; loss: 0.49; acc: 0.81
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.74; acc: 0.77
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.64; acc: 0.77
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.78
Batch: 580; loss: 0.49; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.8
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.49; acc: 0.8
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.8; acc: 0.73
Batch: 780; loss: 0.54; acc: 0.8
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.72
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.5195724592087375; val_accuracy: 0.8415605095541401 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.73; acc: 0.77
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.67; acc: 0.8
Batch: 160; loss: 0.49; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.8
Batch: 280; loss: 0.6; acc: 0.8
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.58; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.83; acc: 0.72
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.74; acc: 0.72
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.81
Batch: 760; loss: 0.51; acc: 0.8
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.69
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.5256219973230059; val_accuracy: 0.8393710191082803 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.8
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.7; acc: 0.75
Batch: 240; loss: 0.8; acc: 0.77
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.71; acc: 0.8
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.82; acc: 0.78
Batch: 440; loss: 0.6; acc: 0.8
Batch: 460; loss: 0.73; acc: 0.8
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.86; acc: 0.81
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.6; acc: 0.8
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.83; acc: 0.73
Batch: 740; loss: 0.68; acc: 0.81
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.6; acc: 0.78
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.89; acc: 0.7
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.5243664201657483; val_accuracy: 0.839171974522293 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.5; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.75
Batch: 200; loss: 0.54; acc: 0.8
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.56; acc: 0.77
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.57; acc: 0.78
Batch: 340; loss: 0.49; acc: 0.8
Batch: 360; loss: 0.47; acc: 0.81
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.72
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.99; acc: 0.73
Batch: 480; loss: 0.64; acc: 0.77
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.78; acc: 0.78
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.53; acc: 0.77
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.8
Batch: 620; loss: 0.85; acc: 0.72
Batch: 640; loss: 0.74; acc: 0.8
Batch: 660; loss: 0.55; acc: 0.78
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.78; acc: 0.75
Batch: 760; loss: 0.76; acc: 0.78
Batch: 780; loss: 0.65; acc: 0.78
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.5110372583008116; val_accuracy: 0.8436504777070064 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.68; acc: 0.77
Batch: 220; loss: 0.65; acc: 0.8
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.75; acc: 0.77
Batch: 300; loss: 0.5; acc: 0.81
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.85; acc: 0.67
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.69; acc: 0.8
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.84
Batch: 500; loss: 0.64; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.84; acc: 0.78
Batch: 600; loss: 0.72; acc: 0.78
Batch: 620; loss: 0.71; acc: 0.84
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.56; acc: 0.78
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.58; acc: 0.8
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.81; acc: 0.72
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.5147474236359262; val_accuracy: 0.8447452229299363 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.78; acc: 0.72
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.81
Batch: 180; loss: 0.78; acc: 0.81
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.78
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.7
Batch: 320; loss: 0.52; acc: 0.81
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.77
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.67; acc: 0.75
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.76; acc: 0.69
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.76; acc: 0.78
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.74; acc: 0.77
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.49; acc: 0.81
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.67
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.5188589128339367; val_accuracy: 0.839171974522293 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.77
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.63; acc: 0.8
Batch: 200; loss: 0.71; acc: 0.81
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.74; acc: 0.69
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 0.4; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.75
Batch: 340; loss: 0.62; acc: 0.78
Batch: 360; loss: 0.54; acc: 0.78
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.78
Batch: 420; loss: 0.5; acc: 0.8
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.79; acc: 0.8
Batch: 480; loss: 0.89; acc: 0.7
Batch: 500; loss: 0.67; acc: 0.73
Batch: 520; loss: 0.73; acc: 0.75
Batch: 540; loss: 0.59; acc: 0.78
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.75
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.72
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.7
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.5031572574642813; val_accuracy: 0.8458399681528662 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 1.0; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.77; acc: 0.75
Batch: 200; loss: 0.94; acc: 0.67
Batch: 220; loss: 0.81; acc: 0.8
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.65; acc: 0.8
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.7; acc: 0.81
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.83
Batch: 460; loss: 0.87; acc: 0.78
Batch: 480; loss: 0.75; acc: 0.77
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.8
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.64; acc: 0.88
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.5; acc: 0.81
Batch: 660; loss: 0.69; acc: 0.81
Batch: 680; loss: 0.79; acc: 0.75
Batch: 700; loss: 0.71; acc: 0.75
Batch: 720; loss: 0.49; acc: 0.84
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.71; acc: 0.8
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.82; acc: 0.72
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.9; acc: 0.69
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.5075089706546941; val_accuracy: 0.8449442675159236 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.55; acc: 0.8
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.8
Batch: 200; loss: 0.76; acc: 0.75
Batch: 220; loss: 0.31; acc: 0.86
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.58; acc: 0.8
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.41; acc: 0.83
Batch: 440; loss: 0.63; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.77
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.53; acc: 0.8
Batch: 620; loss: 0.5; acc: 0.81
Batch: 640; loss: 0.57; acc: 0.72
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.62; acc: 0.75
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.72; acc: 0.8
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.81
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.83; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.5040521461303067; val_accuracy: 0.8457404458598726 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.75
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 0.57; acc: 0.77
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.81
Batch: 240; loss: 0.86; acc: 0.77
Batch: 260; loss: 0.82; acc: 0.75
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.78
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.44; acc: 0.81
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.63; acc: 0.78
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.86; acc: 0.78
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.83
Batch: 740; loss: 0.75; acc: 0.8
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.85; acc: 0.67
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.99; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.5045314317296266; val_accuracy: 0.8463375796178344 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.85; acc: 0.73
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 0.69; acc: 0.75
Batch: 180; loss: 0.7; acc: 0.83
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.7; acc: 0.75
Batch: 260; loss: 0.54; acc: 0.75
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.66; acc: 0.77
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.7; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.61; acc: 0.81
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.64; acc: 0.77
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.76; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 1.03; acc: 0.8
Batch: 660; loss: 0.96; acc: 0.72
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.77
Batch: 720; loss: 0.63; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.57; acc: 0.8
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.96; acc: 0.69
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.5005683048515562; val_accuracy: 0.8482285031847133 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.63; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.75
Batch: 80; loss: 0.64; acc: 0.8
Batch: 100; loss: 0.54; acc: 0.78
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.74; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.8
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.63; acc: 0.77
Batch: 360; loss: 0.51; acc: 0.91
Batch: 380; loss: 0.58; acc: 0.81
Batch: 400; loss: 0.57; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.59; acc: 0.81
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.54; acc: 0.8
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.81
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.78
Batch: 720; loss: 0.59; acc: 0.77
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.7
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.93; acc: 0.67
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.5039273312517033; val_accuracy: 0.8469347133757962 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.76; acc: 0.69
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.78
Batch: 140; loss: 0.65; acc: 0.81
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.64; acc: 0.77
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.76; acc: 0.77
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.57; acc: 0.78
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.78
Batch: 460; loss: 0.6; acc: 0.8
Batch: 480; loss: 0.51; acc: 0.77
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.84; acc: 0.77
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.69; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.64; acc: 0.8
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.8
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.66; acc: 0.77
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.98; acc: 0.69
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.5022578462483777; val_accuracy: 0.8458399681528662 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.82; acc: 0.77
Batch: 300; loss: 0.35; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.8
Batch: 340; loss: 0.78; acc: 0.81
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.68; acc: 0.81
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.77
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.63; acc: 0.77
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.73; acc: 0.75
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.78; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.86; acc: 0.64
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.5111917966300514; val_accuracy: 0.8424562101910829 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.63; acc: 0.75
Batch: 20; loss: 0.88; acc: 0.73
Batch: 40; loss: 0.62; acc: 0.78
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.59; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.8
Batch: 160; loss: 0.76; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.84
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.76; acc: 0.81
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.48; acc: 0.81
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.63; acc: 0.81
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.8; acc: 0.8
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.77
Batch: 640; loss: 0.56; acc: 0.8
Batch: 660; loss: 0.61; acc: 0.8
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.61; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.75
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.82; acc: 0.67
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.5010473843972394; val_accuracy: 0.8468351910828026 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.69; acc: 0.78
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.83; acc: 0.7
Batch: 160; loss: 0.69; acc: 0.77
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.88; acc: 0.77
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.77; acc: 0.77
Batch: 280; loss: 0.74; acc: 0.8
Batch: 300; loss: 0.58; acc: 0.81
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.76; acc: 0.75
Batch: 380; loss: 0.64; acc: 0.73
Batch: 400; loss: 0.52; acc: 0.8
Batch: 420; loss: 0.89; acc: 0.77
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.78
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.59; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.73; acc: 0.75
Batch: 580; loss: 0.69; acc: 0.77
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.88; acc: 0.75
Batch: 640; loss: 0.49; acc: 0.81
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.68; acc: 0.75
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.85; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.5050850432769508; val_accuracy: 0.8471337579617835 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.67; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.53; acc: 0.78
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.87; acc: 0.67
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.73; acc: 0.75
Batch: 360; loss: 0.76; acc: 0.8
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.59; acc: 0.8
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.83
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.75; acc: 0.77
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.91; acc: 0.69
Batch: 600; loss: 0.52; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.75
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.72; acc: 0.77
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.83; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.5007432867197474; val_accuracy: 0.8482285031847133 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.77
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.46; acc: 0.8
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.45; acc: 0.81
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.51; acc: 0.81
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.8
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.66; acc: 0.77
Batch: 440; loss: 0.82; acc: 0.77
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.62; acc: 0.78
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.67; acc: 0.75
Batch: 540; loss: 0.59; acc: 0.8
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.41; acc: 0.81
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.86
Batch: 720; loss: 0.75; acc: 0.77
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.77; acc: 0.8
Batch: 780; loss: 0.62; acc: 0.75
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.83; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.49991582846565613; val_accuracy: 0.8474323248407644 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.81
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.45; acc: 0.81
Batch: 240; loss: 0.75; acc: 0.83
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.87; acc: 0.73
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.8
Batch: 380; loss: 0.81; acc: 0.8
Batch: 400; loss: 0.93; acc: 0.75
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.87; acc: 0.72
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.75; acc: 0.73
Batch: 560; loss: 0.77; acc: 0.8
Batch: 580; loss: 0.45; acc: 0.81
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.66; acc: 0.81
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.76; acc: 0.8
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.59; acc: 0.77
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.85; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.5008761523066053; val_accuracy: 0.8468351910828026 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.73
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.81
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.69; acc: 0.77
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.88; acc: 0.73
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.68; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.8
Batch: 520; loss: 0.42; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.81
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.65; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.8
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.78
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.67
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.95; acc: 0.67
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.5011782464897556; val_accuracy: 0.8476313694267515 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.86; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.78
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.73
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.81
Batch: 160; loss: 0.43; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 1.02; acc: 0.72
Batch: 220; loss: 0.59; acc: 0.75
Batch: 240; loss: 0.86; acc: 0.75
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.72; acc: 0.77
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.78
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.88; acc: 0.73
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.78
Batch: 620; loss: 0.57; acc: 0.78
Batch: 640; loss: 0.48; acc: 0.8
Batch: 660; loss: 0.6; acc: 0.81
Batch: 680; loss: 0.84; acc: 0.75
Batch: 700; loss: 0.75; acc: 0.75
Batch: 720; loss: 0.72; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.5003562858150263; val_accuracy: 0.8468351910828026 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.8
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.83
Batch: 100; loss: 0.32; acc: 0.97
Batch: 120; loss: 0.99; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.78; acc: 0.75
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.6; acc: 0.86
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.71; acc: 0.77
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.81
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.47; acc: 0.81
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.8
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.83; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.67
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.49954873560720187; val_accuracy: 0.8478304140127388 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.84
Batch: 160; loss: 0.62; acc: 0.8
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.54; acc: 0.8
Batch: 220; loss: 0.85; acc: 0.78
Batch: 240; loss: 0.71; acc: 0.77
Batch: 260; loss: 0.38; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.81
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.77; acc: 0.73
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.81
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.68; acc: 0.78
Batch: 460; loss: 0.56; acc: 0.78
Batch: 480; loss: 0.51; acc: 0.8
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.83
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.8
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.83; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.5038377583786181; val_accuracy: 0.8454418789808917 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.7; acc: 0.75
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.77; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.84
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.66; acc: 0.77
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.61; acc: 0.75
Batch: 440; loss: 0.53; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.78
Batch: 560; loss: 0.3; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.9; acc: 0.78
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.81
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.85; acc: 0.69
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.98; acc: 0.67
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.5015672745218702; val_accuracy: 0.8461385350318471 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.73; acc: 0.8
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.33; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.7
Batch: 180; loss: 0.5; acc: 0.8
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.63; acc: 0.89
Batch: 240; loss: 0.77; acc: 0.8
Batch: 260; loss: 0.88; acc: 0.73
Batch: 280; loss: 0.67; acc: 0.83
Batch: 300; loss: 0.73; acc: 0.75
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.61; acc: 0.78
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.81
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.66; acc: 0.77
Batch: 600; loss: 0.49; acc: 0.81
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.81
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.85; acc: 0.69
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.98; acc: 0.69
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.5003768516953584; val_accuracy: 0.8479299363057324 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.88
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.63; acc: 0.8
Batch: 200; loss: 0.49; acc: 0.78
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.63; acc: 0.77
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.95
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.73; acc: 0.77
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.7; acc: 0.77
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.63; acc: 0.8
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.8; acc: 0.7
Batch: 640; loss: 0.63; acc: 0.8
Batch: 660; loss: 0.65; acc: 0.8
Batch: 680; loss: 0.79; acc: 0.75
Batch: 700; loss: 0.6; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.77
Batch: 740; loss: 0.49; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.77
Batch: 780; loss: 0.5; acc: 0.8
Train Epoch over. train_loss: 0.55; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.84; acc: 0.67
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.5010465774566505; val_accuracy: 0.8487261146496815 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_125_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 6663898
elements in E: 6663900
fraction nonzero: 0.9999996998754483
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.12
Batch: 60; loss: 2.29; acc: 0.16
Batch: 80; loss: 2.28; acc: 0.19
Batch: 100; loss: 2.28; acc: 0.17
Batch: 120; loss: 2.29; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.08
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.29; acc: 0.06
Batch: 200; loss: 2.28; acc: 0.06
Batch: 220; loss: 2.28; acc: 0.14
Batch: 240; loss: 2.28; acc: 0.08
Batch: 260; loss: 2.28; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.08
Batch: 300; loss: 2.26; acc: 0.23
Batch: 320; loss: 2.26; acc: 0.2
Batch: 340; loss: 2.24; acc: 0.22
Batch: 360; loss: 2.22; acc: 0.36
Batch: 380; loss: 2.21; acc: 0.42
Batch: 400; loss: 2.21; acc: 0.33
Batch: 420; loss: 2.22; acc: 0.31
Batch: 440; loss: 2.19; acc: 0.33
Batch: 460; loss: 2.17; acc: 0.39
Batch: 480; loss: 2.12; acc: 0.44
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.0; acc: 0.39
Batch: 540; loss: 1.98; acc: 0.34
Batch: 560; loss: 1.86; acc: 0.44
Batch: 580; loss: 1.79; acc: 0.48
Batch: 600; loss: 1.46; acc: 0.61
Batch: 620; loss: 1.45; acc: 0.47
Batch: 640; loss: 1.15; acc: 0.7
Batch: 660; loss: 1.25; acc: 0.58
Batch: 680; loss: 1.05; acc: 0.69
Batch: 700; loss: 0.83; acc: 0.69
Batch: 720; loss: 1.39; acc: 0.53
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.9; acc: 0.67
Batch: 780; loss: 0.96; acc: 0.61
Train Epoch over. train_loss: 1.95; train_accuracy: 0.33 

Batch: 0; loss: 0.99; acc: 0.69
Batch: 20; loss: 1.25; acc: 0.64
Batch: 40; loss: 0.95; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.69
Batch: 80; loss: 1.13; acc: 0.66
Batch: 100; loss: 1.17; acc: 0.64
Batch: 120; loss: 1.52; acc: 0.58
Batch: 140; loss: 1.01; acc: 0.78
Val Epoch over. val_loss: 1.1841711284248693; val_accuracy: 0.6375398089171974 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.62; acc: 0.58
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 0.86; acc: 0.67
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 1.02; acc: 0.72
Batch: 120; loss: 0.94; acc: 0.66
Batch: 140; loss: 0.67; acc: 0.81
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 0.93; acc: 0.67
Batch: 200; loss: 0.8; acc: 0.7
Batch: 220; loss: 0.85; acc: 0.69
Batch: 240; loss: 0.86; acc: 0.75
Batch: 260; loss: 0.64; acc: 0.77
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.73; acc: 0.78
Batch: 320; loss: 0.99; acc: 0.62
Batch: 340; loss: 0.72; acc: 0.72
Batch: 360; loss: 0.91; acc: 0.77
Batch: 380; loss: 1.03; acc: 0.67
Batch: 400; loss: 0.62; acc: 0.8
Batch: 420; loss: 0.81; acc: 0.72
Batch: 440; loss: 0.75; acc: 0.73
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 0.66; acc: 0.78
Batch: 540; loss: 0.85; acc: 0.67
Batch: 560; loss: 0.51; acc: 0.88
Batch: 580; loss: 0.75; acc: 0.78
Batch: 600; loss: 0.89; acc: 0.75
Batch: 620; loss: 0.67; acc: 0.75
Batch: 640; loss: 0.92; acc: 0.72
Batch: 660; loss: 0.88; acc: 0.73
Batch: 680; loss: 1.11; acc: 0.59
Batch: 700; loss: 0.86; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.8
Batch: 740; loss: 0.61; acc: 0.77
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.79; train_accuracy: 0.74 

Batch: 0; loss: 1.12; acc: 0.59
Batch: 20; loss: 1.36; acc: 0.58
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 0.91; acc: 0.73
Batch: 100; loss: 0.9; acc: 0.77
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 0.83; acc: 0.67
Val Epoch over. val_loss: 0.9775134099137252; val_accuracy: 0.6896894904458599 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.78
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.79; acc: 0.72
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.81; acc: 0.78
Batch: 100; loss: 0.61; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.76; acc: 0.81
Batch: 160; loss: 0.87; acc: 0.69
Batch: 180; loss: 0.82; acc: 0.72
Batch: 200; loss: 0.85; acc: 0.72
Batch: 220; loss: 0.62; acc: 0.77
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.65; acc: 0.8
Batch: 280; loss: 0.68; acc: 0.77
Batch: 300; loss: 0.77; acc: 0.81
Batch: 320; loss: 0.76; acc: 0.75
Batch: 340; loss: 1.03; acc: 0.64
Batch: 360; loss: 0.66; acc: 0.77
Batch: 380; loss: 0.67; acc: 0.72
Batch: 400; loss: 0.88; acc: 0.67
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.93; acc: 0.72
Batch: 460; loss: 0.78; acc: 0.73
Batch: 480; loss: 0.62; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.8
Batch: 520; loss: 0.64; acc: 0.77
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.66; acc: 0.8
Batch: 600; loss: 0.87; acc: 0.69
Batch: 620; loss: 0.62; acc: 0.78
Batch: 640; loss: 0.74; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.77
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.72; acc: 0.75
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.83; acc: 0.75
Batch: 780; loss: 0.65; acc: 0.75
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.5781348067673908; val_accuracy: 0.8110071656050956 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 1.03; acc: 0.66
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.78; acc: 0.73
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.92; acc: 0.69
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.89; acc: 0.73
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.62; acc: 0.8
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.83
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.69; acc: 0.77
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.56; acc: 0.69
Batch: 380; loss: 0.81; acc: 0.81
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.76; acc: 0.73
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.76; acc: 0.75
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.79; acc: 0.75
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.56; acc: 0.78
Batch: 700; loss: 0.74; acc: 0.78
Batch: 720; loss: 0.6; acc: 0.78
Batch: 740; loss: 0.69; acc: 0.77
Batch: 760; loss: 0.58; acc: 0.83
Batch: 780; loss: 0.69; acc: 0.75
Train Epoch over. train_loss: 0.64; train_accuracy: 0.79 

Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.8
Batch: 100; loss: 0.7; acc: 0.77
Batch: 120; loss: 0.89; acc: 0.7
Batch: 140; loss: 0.45; acc: 0.84
Val Epoch over. val_loss: 0.7000193174477596; val_accuracy: 0.771297770700637 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.82; acc: 0.72
Batch: 60; loss: 0.65; acc: 0.77
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.83; acc: 0.8
Batch: 160; loss: 0.83; acc: 0.73
Batch: 180; loss: 0.65; acc: 0.8
Batch: 200; loss: 0.74; acc: 0.77
Batch: 220; loss: 0.36; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.72
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.57; acc: 0.77
Batch: 340; loss: 0.47; acc: 0.83
Batch: 360; loss: 0.93; acc: 0.67
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.82; acc: 0.72
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.82; acc: 0.7
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.81; acc: 0.73
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.84
Batch: 600; loss: 0.65; acc: 0.77
Batch: 620; loss: 0.81; acc: 0.75
Batch: 640; loss: 0.77; acc: 0.73
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.92; acc: 0.73
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.42; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.69; acc: 0.72
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 1.28; acc: 0.58
Batch: 20; loss: 1.56; acc: 0.58
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.77
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 0.71; acc: 0.8
Val Epoch over. val_loss: 0.9943249244598826; val_accuracy: 0.6896894904458599 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.21; acc: 0.66
Batch: 20; loss: 0.71; acc: 0.77
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.71; acc: 0.77
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.65; acc: 0.77
Batch: 200; loss: 0.98; acc: 0.77
Batch: 220; loss: 0.7; acc: 0.81
Batch: 240; loss: 0.62; acc: 0.78
Batch: 260; loss: 0.82; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.78
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.55; acc: 0.8
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.59; acc: 0.8
Batch: 440; loss: 0.39; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.72; acc: 0.77
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.83; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.58; acc: 0.77
Batch: 680; loss: 0.66; acc: 0.72
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.52; acc: 0.8
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.8 

Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.5123860215305522; val_accuracy: 0.8404657643312102 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.73
Batch: 120; loss: 0.76; acc: 0.72
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.85; acc: 0.75
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.67; acc: 0.8
Batch: 220; loss: 0.6; acc: 0.75
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.8; acc: 0.75
Batch: 280; loss: 0.67; acc: 0.78
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.52; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.8
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.76; acc: 0.78
Batch: 480; loss: 0.72; acc: 0.75
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.78
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.81; acc: 0.73
Batch: 620; loss: 0.63; acc: 0.78
Batch: 640; loss: 1.03; acc: 0.64
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.75; acc: 0.78
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.81; acc: 0.7
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 0.41; acc: 0.88
Val Epoch over. val_loss: 0.5945961258024167; val_accuracy: 0.8141918789808917 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.75; acc: 0.78
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.7; acc: 0.83
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.67; acc: 0.75
Batch: 160; loss: 1.09; acc: 0.7
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.95; acc: 0.67
Batch: 220; loss: 0.75; acc: 0.75
Batch: 240; loss: 0.66; acc: 0.75
Batch: 260; loss: 0.68; acc: 0.73
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.52; acc: 0.8
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.75; acc: 0.8
Batch: 400; loss: 0.6; acc: 0.8
Batch: 420; loss: 0.7; acc: 0.8
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.55; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.92; acc: 0.75
Batch: 540; loss: 0.71; acc: 0.77
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.63; acc: 0.75
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.96; acc: 0.72
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 0.74; acc: 0.75
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.87; acc: 0.75
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 1.16; acc: 0.58
Batch: 20; loss: 1.77; acc: 0.53
Batch: 40; loss: 0.76; acc: 0.66
Batch: 60; loss: 1.03; acc: 0.75
Batch: 80; loss: 0.81; acc: 0.69
Batch: 100; loss: 0.92; acc: 0.66
Batch: 120; loss: 1.44; acc: 0.62
Batch: 140; loss: 1.23; acc: 0.59
Val Epoch over. val_loss: 1.0567924991534774; val_accuracy: 0.6560509554140127 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.59
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.88; acc: 0.73
Batch: 60; loss: 0.76; acc: 0.72
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.75; acc: 0.78
Batch: 200; loss: 0.71; acc: 0.78
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.77
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.84; acc: 0.7
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.72; acc: 0.75
Batch: 360; loss: 0.51; acc: 0.77
Batch: 380; loss: 0.63; acc: 0.78
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.58; acc: 0.75
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.69; acc: 0.77
Batch: 520; loss: 0.55; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.63; acc: 0.77
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.78
Batch: 680; loss: 0.65; acc: 0.73
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.89; acc: 0.69
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.77; acc: 0.77
Batch: 20; loss: 0.81; acc: 0.66
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.98; acc: 0.75
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 0.52; acc: 0.77
Val Epoch over. val_loss: 0.6642804030020526; val_accuracy: 0.7833399681528662 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.8
Batch: 140; loss: 0.55; acc: 0.8
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.57; acc: 0.78
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.71; acc: 0.75
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.81
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.93; acc: 0.66
Batch: 420; loss: 0.75; acc: 0.75
Batch: 440; loss: 0.73; acc: 0.78
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.67; acc: 0.7
Batch: 580; loss: 0.66; acc: 0.77
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.82; acc: 0.67
Batch: 640; loss: 0.8; acc: 0.69
Batch: 660; loss: 0.6; acc: 0.8
Batch: 680; loss: 0.72; acc: 0.78
Batch: 700; loss: 0.88; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.56; acc: 0.75
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.67; acc: 0.72
Batch: 20; loss: 0.55; acc: 0.75
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.75
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.86
Val Epoch over. val_loss: 0.6159018312290216; val_accuracy: 0.8031449044585988 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.87; acc: 0.75
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.81
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.55; acc: 0.8
Batch: 480; loss: 0.92; acc: 0.81
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.78
Batch: 540; loss: 0.52; acc: 0.8
Batch: 560; loss: 0.6; acc: 0.78
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.7; acc: 0.78
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 1.07; acc: 0.69
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.5; acc: 0.8
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.4612280078184832; val_accuracy: 0.861265923566879 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.48; acc: 0.81
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.72; acc: 0.75
Batch: 160; loss: 0.66; acc: 0.78
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.58; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.8
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.83
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.59; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.69; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.93; acc: 0.72
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.47; acc: 0.8
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.44; acc: 0.83
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.63; acc: 0.77
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.97; acc: 0.75
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.4946375313647993; val_accuracy: 0.8480294585987261 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.61; acc: 0.8
Batch: 180; loss: 0.57; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.81
Batch: 220; loss: 0.6; acc: 0.8
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.65; acc: 0.83
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.73; acc: 0.8
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.65; acc: 0.78
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.68; acc: 0.83
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.59; acc: 0.75
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.47810718455132406; val_accuracy: 0.8541003184713376 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 0.65; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.7; acc: 0.78
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.8
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.73
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.83
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.59; acc: 0.78
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.83
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.4680647137248592; val_accuracy: 0.8578821656050956 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.56; acc: 0.83
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.75; acc: 0.72
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.8; acc: 0.8
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.8
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.81
Batch: 680; loss: 0.42; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.81
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.88
Val Epoch over. val_loss: 0.4780111031927121; val_accuracy: 0.8552945859872612 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.79; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.78; acc: 0.73
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.8
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.8
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.65; acc: 0.75
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.8
Batch: 440; loss: 0.65; acc: 0.89
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.7; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.8
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.6; acc: 0.84
Batch: 700; loss: 0.92; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.89; acc: 0.78
Batch: 780; loss: 0.67; acc: 0.78
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.4888741309475747; val_accuracy: 0.8476313694267515 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.81; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.68; acc: 0.84
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.49; acc: 0.81
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.62; acc: 0.78
Batch: 480; loss: 0.41; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.77
Batch: 540; loss: 0.65; acc: 0.8
Batch: 560; loss: 0.73; acc: 0.77
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.81
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.65; acc: 0.77
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.89; acc: 0.77
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.4654393609921644; val_accuracy: 0.8592754777070064 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.83
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.92
Batch: 320; loss: 0.51; acc: 0.81
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.74; acc: 0.75
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.61; acc: 0.8
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.77
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4607246672841394; val_accuracy: 0.8606687898089171 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.8
Batch: 220; loss: 0.63; acc: 0.77
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.81
Batch: 280; loss: 0.69; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.81
Batch: 320; loss: 0.65; acc: 0.81
Batch: 340; loss: 0.9; acc: 0.75
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.66; acc: 0.78
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.58; acc: 0.78
Batch: 540; loss: 0.38; acc: 0.83
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.95; acc: 0.78
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.31; acc: 0.91
Val Epoch over. val_loss: 0.4796772836499913; val_accuracy: 0.853702229299363 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.66; acc: 0.78
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 1.07; acc: 0.72
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.53; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.73; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.78
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.83
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.81
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.71; acc: 0.78
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.5331339887373007; val_accuracy: 0.837281050955414 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.77
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.8; acc: 0.73
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.81
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.81
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.7; acc: 0.81
Batch: 620; loss: 0.83; acc: 0.81
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.78
Batch: 680; loss: 0.78; acc: 0.73
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.59; acc: 0.81
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.92
Val Epoch over. val_loss: 0.44982567505472026; val_accuracy: 0.8652468152866242 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.71; acc: 0.77
Batch: 260; loss: 0.83; acc: 0.7
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.58; acc: 0.81
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.69; acc: 0.86
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.78
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.91; acc: 0.73
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.67; acc: 0.78
Batch: 640; loss: 1.06; acc: 0.75
Batch: 660; loss: 0.56; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.75; acc: 0.8
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 0.38; acc: 0.88
Val Epoch over. val_loss: 0.48430599177339273; val_accuracy: 0.8507165605095541 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.73; acc: 0.81
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.8; acc: 0.8
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.66; acc: 0.78
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.74; acc: 0.78
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.74; acc: 0.75
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.81
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.68; acc: 0.73
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.8
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.46523080063853295; val_accuracy: 0.8596735668789809 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.84; acc: 0.83
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.5; acc: 0.81
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.57; acc: 0.8
Batch: 280; loss: 0.64; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.73; acc: 0.7
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.7; acc: 0.77
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.66; acc: 0.77
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.75; acc: 0.8
Batch: 520; loss: 0.6; acc: 0.8
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.77
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.92; acc: 0.72
Batch: 140; loss: 0.23; acc: 0.91
Val Epoch over. val_loss: 0.46536320647236645; val_accuracy: 0.8585788216560509 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.72; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.73; acc: 0.8
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.67; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.46; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.54; acc: 0.77
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.4539323213753427; val_accuracy: 0.8622611464968153 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.81
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.65; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.91; acc: 0.67
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.83
Batch: 500; loss: 0.52; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 0.62; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.85; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.91
Val Epoch over. val_loss: 0.473521154492524; val_accuracy: 0.8599721337579618 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.65; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.63; acc: 0.78
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.58; acc: 0.78
Batch: 340; loss: 0.64; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.77
Batch: 380; loss: 0.6; acc: 0.78
Batch: 400; loss: 0.49; acc: 0.81
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.83
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.84; acc: 0.81
Batch: 600; loss: 0.65; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.84
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.71; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.4563096171351755; val_accuracy: 0.8644506369426752 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.84
Batch: 200; loss: 0.74; acc: 0.83
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.5; acc: 0.81
Batch: 260; loss: 0.77; acc: 0.8
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.82; acc: 0.81
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.8
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.67; acc: 0.83
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.76; acc: 0.72
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.47051195971145754; val_accuracy: 0.8595740445859873 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.83; acc: 0.69
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.77; acc: 0.81
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.34; acc: 0.95
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.34; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.69; acc: 0.77
Batch: 600; loss: 0.49; acc: 0.78
Batch: 620; loss: 0.55; acc: 0.81
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.68; acc: 0.73
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.46018089562844317; val_accuracy: 0.8606687898089171 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.78
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.81
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.47; acc: 0.83
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.72; acc: 0.72
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.73; acc: 0.75
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.61; acc: 0.78
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.68; acc: 0.77
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.27; acc: 0.88
Val Epoch over. val_loss: 0.47162557198743155; val_accuracy: 0.8565883757961783 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.8; acc: 0.73
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.75
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.81
Batch: 300; loss: 0.71; acc: 0.83
Batch: 320; loss: 0.59; acc: 0.78
Batch: 340; loss: 0.79; acc: 0.81
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.83
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.59; acc: 0.8
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.91
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.75
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.81
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.4458695431803442; val_accuracy: 0.8668391719745223 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.8
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.59; acc: 0.8
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.69; acc: 0.77
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.51; acc: 0.8
Batch: 400; loss: 0.35; acc: 0.84
Batch: 420; loss: 0.38; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.8; acc: 0.8
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.79; acc: 0.72
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.24; acc: 0.89
Val Epoch over. val_loss: 0.4480304515380768; val_accuracy: 0.8662420382165605 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.67; acc: 0.78
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.81
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.77; acc: 0.78
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.52; acc: 0.77
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.58; acc: 0.81
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.32; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.78
Batch: 700; loss: 0.6; acc: 0.8
Batch: 720; loss: 0.82; acc: 0.81
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.44676939042130853; val_accuracy: 0.8670382165605095 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.87; acc: 0.73
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.75; acc: 0.75
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.7; acc: 0.8
Batch: 380; loss: 0.68; acc: 0.8
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.74; acc: 0.8
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.58; acc: 0.77
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.53; acc: 0.8
Batch: 760; loss: 0.56; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.77
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.4450465870700824; val_accuracy: 0.865843949044586 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.73; acc: 0.77
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.67; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.83
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.51; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.4507395455222221; val_accuracy: 0.8656449044585988 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.83; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.73; acc: 0.91
Batch: 200; loss: 0.73; acc: 0.75
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.81
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.77
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.83
Batch: 740; loss: 0.62; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.81
Batch: 780; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.4459749410866172; val_accuracy: 0.8672372611464968 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.72; acc: 0.8
Batch: 180; loss: 0.61; acc: 0.84
Batch: 200; loss: 0.69; acc: 0.75
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.8
Batch: 300; loss: 0.6; acc: 0.77
Batch: 320; loss: 0.59; acc: 0.8
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.77
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.61; acc: 0.81
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.47; acc: 0.78
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.4488858137350933; val_accuracy: 0.8662420382165605 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.81
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.77; acc: 0.77
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.23; acc: 0.92
Val Epoch over. val_loss: 0.44592985396931883; val_accuracy: 0.8663415605095541 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.47; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.5; acc: 0.78
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.71; acc: 0.84
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.54; acc: 0.81
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.8
Batch: 520; loss: 0.64; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.77; acc: 0.75
Batch: 660; loss: 0.35; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.81
Batch: 740; loss: 0.94; acc: 0.75
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.74; acc: 0.8
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.4443169566476421; val_accuracy: 0.867734872611465 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.74; acc: 0.78
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.8
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.62; acc: 0.86
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.81
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.56; acc: 0.78
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.7; acc: 0.75
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.7; acc: 0.77
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.84
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.64; acc: 0.81
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.4472380158058397; val_accuracy: 0.8667396496815286 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.81
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.74; acc: 0.78
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.7; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.61; acc: 0.78
Batch: 360; loss: 0.66; acc: 0.77
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.63; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.75
Batch: 440; loss: 0.56; acc: 0.78
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.84
Batch: 660; loss: 0.85; acc: 0.78
Batch: 680; loss: 0.44; acc: 0.84
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.62; acc: 0.8
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.23; acc: 0.91
Val Epoch over. val_loss: 0.44589612466894135; val_accuracy: 0.8676353503184714 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.64; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.8
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.92
Batch: 520; loss: 0.62; acc: 0.77
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.41; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.75
Batch: 740; loss: 0.82; acc: 0.72
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.4454135027281038; val_accuracy: 0.8680334394904459 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.78; acc: 0.75
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 1.04; acc: 0.77
Batch: 240; loss: 0.55; acc: 0.8
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.57; acc: 0.81
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.72; acc: 0.81
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.61; acc: 0.73
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.61; acc: 0.78
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.4449724646130945; val_accuracy: 0.8680334394904459 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.8
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.84
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.49; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.76; acc: 0.83
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.78
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.63; acc: 0.78
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.83
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.74; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.4457383709158867; val_accuracy: 0.8669386942675159 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.8
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.74; acc: 0.75
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.8
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.63; acc: 0.73
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.86
Batch: 700; loss: 0.67; acc: 0.77
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.8
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.52; acc: 0.81
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.23; acc: 0.92
Val Epoch over. val_loss: 0.44785134124148424; val_accuracy: 0.865843949044586 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.81
Batch: 220; loss: 0.36; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.77; acc: 0.77
Batch: 360; loss: 0.63; acc: 0.81
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.83
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.78; acc: 0.77
Batch: 640; loss: 0.6; acc: 0.81
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.81
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.87; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.23; acc: 0.91
Val Epoch over. val_loss: 0.4458240530673106; val_accuracy: 0.8662420382165605 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.84; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.49; acc: 0.8
Batch: 240; loss: 0.65; acc: 0.73
Batch: 260; loss: 0.4; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.8
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.93; acc: 0.72
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.69; acc: 0.84
Batch: 480; loss: 0.49; acc: 0.83
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.25; acc: 0.97
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.51; acc: 0.81
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.44531104405214833; val_accuracy: 0.8674363057324841 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.63; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.73; acc: 0.8
Batch: 260; loss: 0.53; acc: 0.92
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.78; acc: 0.83
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.81
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.7; acc: 0.75
Batch: 460; loss: 0.4; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.77; acc: 0.78
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.65; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.83
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.76; acc: 0.73
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.44534859488344497; val_accuracy: 0.8671377388535032 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.78; acc: 0.78
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.73; acc: 0.8
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.64; acc: 0.83
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.68; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.77
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.44556934818340715; val_accuracy: 0.8681329617834395 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.57; acc: 0.78
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.81
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.44; acc: 0.81
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.59; acc: 0.75
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.66; acc: 0.75
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.63; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.69; acc: 0.78
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.4452244686852595; val_accuracy: 0.8676353503184714 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_150_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 7774548
elements in E: 7774550
fraction nonzero: 0.9999997427503843
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.16
Batch: 40; loss: 2.31; acc: 0.11
Batch: 60; loss: 2.31; acc: 0.05
Batch: 80; loss: 2.27; acc: 0.19
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.29; acc: 0.09
Batch: 140; loss: 2.28; acc: 0.14
Batch: 160; loss: 2.29; acc: 0.17
Batch: 180; loss: 2.28; acc: 0.11
Batch: 200; loss: 2.27; acc: 0.14
Batch: 220; loss: 2.27; acc: 0.17
Batch: 240; loss: 2.24; acc: 0.3
Batch: 260; loss: 2.27; acc: 0.19
Batch: 280; loss: 2.25; acc: 0.23
Batch: 300; loss: 2.24; acc: 0.33
Batch: 320; loss: 2.22; acc: 0.33
Batch: 340; loss: 2.23; acc: 0.27
Batch: 360; loss: 2.23; acc: 0.31
Batch: 380; loss: 2.2; acc: 0.39
Batch: 400; loss: 2.17; acc: 0.44
Batch: 420; loss: 2.14; acc: 0.44
Batch: 440; loss: 2.1; acc: 0.47
Batch: 460; loss: 2.12; acc: 0.42
Batch: 480; loss: 2.03; acc: 0.41
Batch: 500; loss: 1.95; acc: 0.39
Batch: 520; loss: 1.89; acc: 0.44
Batch: 540; loss: 1.63; acc: 0.53
Batch: 560; loss: 1.59; acc: 0.48
Batch: 580; loss: 1.2; acc: 0.69
Batch: 600; loss: 1.16; acc: 0.64
Batch: 620; loss: 1.16; acc: 0.62
Batch: 640; loss: 0.96; acc: 0.64
Batch: 660; loss: 0.82; acc: 0.75
Batch: 680; loss: 0.77; acc: 0.7
Batch: 700; loss: 0.88; acc: 0.75
Batch: 720; loss: 0.76; acc: 0.75
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 0.72; acc: 0.78
Batch: 780; loss: 0.76; acc: 0.72
Train Epoch over. train_loss: 1.84; train_accuracy: 0.38 

Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 0.75; acc: 0.7
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.77
Val Epoch over. val_loss: 0.7917313116371252; val_accuracy: 0.7333797770700637 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.83; acc: 0.67
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.91; acc: 0.72
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.95; acc: 0.58
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.78; acc: 0.77
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.81; acc: 0.78
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.56; acc: 0.81
Batch: 260; loss: 0.97; acc: 0.69
Batch: 280; loss: 0.78; acc: 0.75
Batch: 300; loss: 0.72; acc: 0.73
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.77
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.86; acc: 0.75
Batch: 400; loss: 1.0; acc: 0.62
Batch: 420; loss: 0.75; acc: 0.75
Batch: 440; loss: 0.75; acc: 0.77
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.85; acc: 0.67
Batch: 600; loss: 0.71; acc: 0.78
Batch: 620; loss: 0.76; acc: 0.69
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.56; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.89
Val Epoch over. val_loss: 0.5651480013587672; val_accuracy: 0.8180732484076433 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.8
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.99; acc: 0.64
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.8; acc: 0.73
Batch: 200; loss: 0.75; acc: 0.72
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.99; acc: 0.72
Batch: 280; loss: 0.67; acc: 0.78
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.94; acc: 0.72
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.81
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.81; acc: 0.73
Batch: 440; loss: 0.75; acc: 0.75
Batch: 460; loss: 0.63; acc: 0.78
Batch: 480; loss: 0.67; acc: 0.72
Batch: 500; loss: 0.5; acc: 0.81
Batch: 520; loss: 0.54; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.65; acc: 0.83
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 1.06; acc: 0.66
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.81; acc: 0.73
Batch: 700; loss: 0.52; acc: 0.81
Batch: 720; loss: 0.73; acc: 0.72
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.58; acc: 0.77
Batch: 780; loss: 0.58; acc: 0.81
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.68; acc: 0.75
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.81; acc: 0.72
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.5691560325539036; val_accuracy: 0.8258359872611465 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.68; acc: 0.77
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.57; acc: 0.78
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.83; acc: 0.77
Batch: 240; loss: 0.92; acc: 0.78
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.83; acc: 0.73
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.6; acc: 0.75
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.58; acc: 0.77
Batch: 400; loss: 0.68; acc: 0.83
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 0.62; acc: 0.77
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.77
Batch: 520; loss: 0.36; acc: 0.84
Batch: 540; loss: 0.45; acc: 0.81
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.95; acc: 0.73
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.84
Batch: 640; loss: 0.65; acc: 0.78
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.74; acc: 0.78
Batch: 760; loss: 0.73; acc: 0.73
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.87; acc: 0.72
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.55; acc: 0.8
Val Epoch over. val_loss: 0.7303333496971495; val_accuracy: 0.7762738853503185 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.75
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.82; acc: 0.81
Batch: 200; loss: 0.42; acc: 0.83
Batch: 220; loss: 0.9; acc: 0.73
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.73
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.76; acc: 0.69
Batch: 320; loss: 0.74; acc: 0.7
Batch: 340; loss: 0.92; acc: 0.7
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.66; acc: 0.72
Batch: 440; loss: 0.5; acc: 0.78
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.65; acc: 0.81
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.68; acc: 0.77
Batch: 600; loss: 0.5; acc: 0.8
Batch: 620; loss: 0.65; acc: 0.78
Batch: 640; loss: 1.03; acc: 0.72
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.58; acc: 0.77
Batch: 720; loss: 0.51; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.54; acc: 0.77
Batch: 780; loss: 0.55; acc: 0.8
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.67
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.97; acc: 0.69
Batch: 80; loss: 0.74; acc: 0.75
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.37; acc: 0.89
Val Epoch over. val_loss: 0.6799397802656624; val_accuracy: 0.7867237261146497 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.75
Batch: 60; loss: 0.66; acc: 0.72
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.78
Batch: 200; loss: 0.69; acc: 0.73
Batch: 220; loss: 0.62; acc: 0.8
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.75
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.8
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.8
Batch: 480; loss: 0.5; acc: 0.8
Batch: 500; loss: 0.46; acc: 0.83
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.7; acc: 0.77
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.56; acc: 0.78
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.83
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.65; acc: 0.78
Batch: 740; loss: 0.76; acc: 0.7
Batch: 760; loss: 0.59; acc: 0.81
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.82 

Batch: 0; loss: 0.54; acc: 0.78
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.75
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.5559841253954894; val_accuracy: 0.8179737261146497 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.78
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.83
Batch: 260; loss: 0.48; acc: 0.81
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.81
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.85; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.79; acc: 0.69
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.81; acc: 0.72
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.76; acc: 0.77
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.41; acc: 0.91
Val Epoch over. val_loss: 0.6334741838798401; val_accuracy: 0.7992635350318471 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.7; acc: 0.73
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.62; acc: 0.77
Batch: 120; loss: 0.85; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.77
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.6; acc: 0.81
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.8
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.84
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.83
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.68; acc: 0.72
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.54; acc: 0.8
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.9; acc: 0.77
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.65; acc: 0.8
Batch: 780; loss: 0.5; acc: 0.78
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.4918530240749857; val_accuracy: 0.845640923566879 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.84; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.9; acc: 0.73
Batch: 320; loss: 0.45; acc: 0.8
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.76; acc: 0.81
Batch: 400; loss: 0.96; acc: 0.7
Batch: 420; loss: 0.85; acc: 0.81
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.78; acc: 0.75
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.81; acc: 0.69
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.81
Batch: 580; loss: 0.94; acc: 0.72
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.83; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 0.49; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 1.01; acc: 0.66
Batch: 140; loss: 0.38; acc: 0.86
Val Epoch over. val_loss: 0.6896698152183727; val_accuracy: 0.7796576433121019 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.73
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.77
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.76; acc: 0.81
Batch: 240; loss: 0.57; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.81
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.64; acc: 0.72
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.79; acc: 0.73
Batch: 380; loss: 0.55; acc: 0.8
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.73
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.71; acc: 0.81
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.8
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.51; acc: 0.8
Batch: 780; loss: 0.4; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.82; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.61
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 1.13; acc: 0.69
Batch: 80; loss: 0.6; acc: 0.77
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 1.22; acc: 0.64
Batch: 140; loss: 0.56; acc: 0.8
Val Epoch over. val_loss: 0.929144861212202; val_accuracy: 0.714171974522293 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.77; acc: 0.73
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.59; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.31; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.52; acc: 0.78
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.38; acc: 0.81
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.63; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.8
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.27; acc: 0.89
Val Epoch over. val_loss: 0.4642457869022515; val_accuracy: 0.8551950636942676 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.8
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.83; acc: 0.77
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.81
Batch: 500; loss: 0.66; acc: 0.8
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.78; acc: 0.7
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.81
Batch: 780; loss: 0.44; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.45084067932359734; val_accuracy: 0.8599721337579618 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.84; acc: 0.81
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.8
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.45; acc: 0.81
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.55; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.81
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.61; acc: 0.77
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.8
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.8
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.84
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.61; acc: 0.77
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.75; acc: 0.75
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.460203581222683; val_accuracy: 0.8588773885350318 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.77
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.78
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.83
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.75
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.8
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.41602758892402525; val_accuracy: 0.8705214968152867 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.58; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.78
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.83
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.78
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.8
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.83
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.74; acc: 0.77
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.41265496331605184; val_accuracy: 0.8722133757961783 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.77
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.83
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.88
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.57; acc: 0.78
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.7; acc: 0.8
Batch: 720; loss: 0.63; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.4136786899843793; val_accuracy: 0.8709195859872612 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.8
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.6; acc: 0.78
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.81
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.52; acc: 0.89
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.81
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.62; acc: 0.77
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.81
Batch: 740; loss: 0.56; acc: 0.81
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.88
Val Epoch over. val_loss: 0.4453451979881639; val_accuracy: 0.8600716560509554 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.77
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.75
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.63; acc: 0.8
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.81; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.4951813291687115; val_accuracy: 0.8402667197452229 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.84
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.61; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.48; acc: 0.78
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.78
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.83
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.74; acc: 0.84
Batch: 740; loss: 0.41; acc: 0.81
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.8
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.75
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.4276055570715552; val_accuracy: 0.8641520700636943 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.8
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.81
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.42; acc: 0.81
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.84
Batch: 420; loss: 0.69; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.48; acc: 0.78
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.8
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.8
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.77
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.19; acc: 0.91
Val Epoch over. val_loss: 0.4105916106776827; val_accuracy: 0.8729100318471338 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.92
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.53; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.77
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.23; acc: 0.89
Val Epoch over. val_loss: 0.40786379865210526; val_accuracy: 0.8699243630573248 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.77
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.67; acc: 0.83
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.6; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.81
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.39867311701842933; val_accuracy: 0.8770899681528662 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.81
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.8
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.67; acc: 0.81
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.81
Batch: 520; loss: 0.6; acc: 0.8
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.83
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.52; acc: 0.8
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.42; acc: 0.81
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.40087930412049505; val_accuracy: 0.8756966560509554 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.74; acc: 0.75
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.73; acc: 0.8
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.61; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.83
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.3968377801453232; val_accuracy: 0.8773885350318471 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.78
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.81
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.81
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.8
Batch: 600; loss: 0.8; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.67; acc: 0.75
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.4005132183717315; val_accuracy: 0.8766918789808917 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.77
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.5; acc: 0.8
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.65; acc: 0.8
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.48; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.78
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.83
Batch: 740; loss: 0.49; acc: 0.8
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.89
Val Epoch over. val_loss: 0.4008483931801881; val_accuracy: 0.87390525477707 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.66; acc: 0.78
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.89
Val Epoch over. val_loss: 0.3959090594368376; val_accuracy: 0.8778861464968153 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.75
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.43; acc: 0.8
Batch: 180; loss: 0.5; acc: 0.86
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.8
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.81
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.7
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.66; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.89
Val Epoch over. val_loss: 0.4016105804568643; val_accuracy: 0.8771894904458599 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.78
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.59; acc: 0.75
Batch: 400; loss: 0.8; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.66; acc: 0.88
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.5; acc: 0.81
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.83
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.3941137599432544; val_accuracy: 0.8774880573248408 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.81
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.5; acc: 0.8
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.79; acc: 0.73
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.78
Batch: 620; loss: 0.61; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.8
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.72; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.3987265272884612; val_accuracy: 0.8770899681528662 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.4; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.8
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.69; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.67; acc: 0.77
Batch: 560; loss: 0.59; acc: 0.84
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.81
Batch: 640; loss: 0.67; acc: 0.8
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.89
Val Epoch over. val_loss: 0.39240572017848874; val_accuracy: 0.878781847133758 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.63; acc: 0.81
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.78
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.38; acc: 0.83
Batch: 400; loss: 0.5; acc: 0.81
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.64; acc: 0.78
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.8
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.46; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.86
Batch: 780; loss: 0.65; acc: 0.81
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.38979138742396785; val_accuracy: 0.8780851910828026 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.84
Batch: 200; loss: 0.63; acc: 0.81
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.78
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.76; acc: 0.8
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.83
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.56; acc: 0.77
Batch: 600; loss: 0.48; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.71; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.38963540985136275; val_accuracy: 0.8792794585987261 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.43; acc: 0.81
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.81
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.8
Batch: 560; loss: 0.48; acc: 0.81
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.83
Batch: 620; loss: 0.92; acc: 0.81
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.46; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.8
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.38805776170104933; val_accuracy: 0.8798765923566879 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.61; acc: 0.83
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.78; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.68; acc: 0.83
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.69; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.76; acc: 0.81
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.387134557933944; val_accuracy: 0.8798765923566879 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.48; acc: 0.83
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.77
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.3905799193367077; val_accuracy: 0.8772890127388535 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.83
Batch: 260; loss: 0.72; acc: 0.81
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.81
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.66; acc: 0.86
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.78; acc: 0.77
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.83
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.63; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.86
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.89
Val Epoch over. val_loss: 0.3880521557798052; val_accuracy: 0.878781847133758 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.6; acc: 0.77
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.63; acc: 0.77
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.76; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.6; acc: 0.8
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.78
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.3870870175824803; val_accuracy: 0.8796775477707006 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.81
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.68; acc: 0.78
Batch: 440; loss: 0.45; acc: 0.81
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.78
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.81
Batch: 720; loss: 0.43; acc: 0.83
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.38594109175881003; val_accuracy: 0.880672770700637 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.81
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.73
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.64; acc: 0.81
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.8
Batch: 460; loss: 0.47; acc: 0.81
Batch: 480; loss: 0.34; acc: 0.86
Batch: 500; loss: 0.34; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.8
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.8
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.89
Val Epoch over. val_loss: 0.3880679184084485; val_accuracy: 0.8797770700636943 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.83
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.37; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.3851779647124041; val_accuracy: 0.8805732484076433 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.87; acc: 0.81
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.97
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.81
Batch: 600; loss: 0.48; acc: 0.81
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.81
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.81
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.38271035994314084; val_accuracy: 0.8816679936305732 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.3; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.67; acc: 0.75
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.81
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.85; acc: 0.78
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.7; acc: 0.77
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.3830115497112274; val_accuracy: 0.8811703821656051 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.81
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.78
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.84
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.3822577487036681; val_accuracy: 0.8818670382165605 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.75
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.67; acc: 0.77
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.5; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.82; acc: 0.83
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.381750972834742; val_accuracy: 0.8817675159235668 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.41; acc: 0.81
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.7; acc: 0.84
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.61; acc: 0.83
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.38186509874026486; val_accuracy: 0.8820660828025477 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.8
Batch: 140; loss: 0.68; acc: 0.8
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.83
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.83
Batch: 720; loss: 0.49; acc: 0.75
Batch: 740; loss: 0.41; acc: 0.83
Batch: 760; loss: 0.59; acc: 0.78
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.89
Val Epoch over. val_loss: 0.3823103245086731; val_accuracy: 0.8801751592356688 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.4; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.41; acc: 0.83
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.78
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.48; acc: 0.8
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.38184493908267114; val_accuracy: 0.881468949044586 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.64; acc: 0.78
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.68; acc: 0.77
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.55; acc: 0.83
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.57; acc: 0.8
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.83
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.91
Val Epoch over. val_loss: 0.381457535087303; val_accuracy: 0.8831608280254777 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.8
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.81
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.78
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.58; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.57; acc: 0.78
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.3834463931193018; val_accuracy: 0.8798765923566879 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_175_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 8885198
elements in E: 8885200
fraction nonzero: 0.9999997749065862
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.08
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.31; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.06
Batch: 120; loss: 2.29; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.28; acc: 0.09
Batch: 200; loss: 2.27; acc: 0.14
Batch: 220; loss: 2.27; acc: 0.09
Batch: 240; loss: 2.27; acc: 0.06
Batch: 260; loss: 2.25; acc: 0.16
Batch: 280; loss: 2.26; acc: 0.25
Batch: 300; loss: 2.25; acc: 0.22
Batch: 320; loss: 2.24; acc: 0.33
Batch: 340; loss: 2.2; acc: 0.48
Batch: 360; loss: 2.2; acc: 0.42
Batch: 380; loss: 2.18; acc: 0.45
Batch: 400; loss: 2.19; acc: 0.36
Batch: 420; loss: 2.08; acc: 0.61
Batch: 440; loss: 2.1; acc: 0.41
Batch: 460; loss: 2.06; acc: 0.44
Batch: 480; loss: 1.79; acc: 0.66
Batch: 500; loss: 1.66; acc: 0.64
Batch: 520; loss: 1.61; acc: 0.56
Batch: 540; loss: 1.36; acc: 0.61
Batch: 560; loss: 1.09; acc: 0.64
Batch: 580; loss: 1.05; acc: 0.66
Batch: 600; loss: 0.92; acc: 0.69
Batch: 620; loss: 0.84; acc: 0.72
Batch: 640; loss: 0.99; acc: 0.64
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.7; acc: 0.78
Batch: 700; loss: 0.69; acc: 0.77
Batch: 720; loss: 0.72; acc: 0.75
Batch: 740; loss: 0.58; acc: 0.78
Batch: 760; loss: 0.65; acc: 0.77
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 1.75; train_accuracy: 0.42 

Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 1.04; acc: 0.64
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 1.0; acc: 0.72
Batch: 140; loss: 0.4; acc: 0.86
Val Epoch over. val_loss: 0.6748239981710531; val_accuracy: 0.7750796178343949 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.89; acc: 0.72
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.73
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.65; acc: 0.83
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.92; acc: 0.77
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.77
Batch: 260; loss: 0.55; acc: 0.78
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.72; acc: 0.83
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.83; acc: 0.72
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.71; acc: 0.75
Batch: 600; loss: 0.53; acc: 0.78
Batch: 620; loss: 0.77; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.89; acc: 0.77
Batch: 720; loss: 0.87; acc: 0.73
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.4862693021441721; val_accuracy: 0.8480294585987261 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.98; acc: 0.7
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.62; acc: 0.83
Batch: 340; loss: 0.75; acc: 0.83
Batch: 360; loss: 0.5; acc: 0.8
Batch: 380; loss: 0.36; acc: 0.84
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.6; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.77
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.81
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.62; acc: 0.81
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 0.51; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.47; acc: 0.81
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.7
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.83; acc: 0.73
Batch: 80; loss: 0.69; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.73
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.67; acc: 0.78
Val Epoch over. val_loss: 0.7942664731460013; val_accuracy: 0.7541799363057324 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.73
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.66; acc: 0.77
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.67; acc: 0.81
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.82; acc: 0.75
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.81
Batch: 700; loss: 0.68; acc: 0.8
Batch: 720; loss: 0.63; acc: 0.84
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.4432089005591004; val_accuracy: 0.8661425159235668 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.55; acc: 0.78
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.78
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.39; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.86
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.3; acc: 0.86
Batch: 500; loss: 0.67; acc: 0.81
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.56; acc: 0.77
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.7; acc: 0.83
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.44; acc: 0.81
Batch: 740; loss: 0.53; acc: 0.78
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.56; acc: 0.77
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.69
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.5487556519212237; val_accuracy: 0.828125 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.8
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.87; acc: 0.78
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.83
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.75
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.74; acc: 0.78
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.75; acc: 0.77
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.45796650145084233; val_accuracy: 0.8609673566878981 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.65; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.91
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.4; acc: 0.83
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.71; acc: 0.83
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.73; acc: 0.78
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.8
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.88
Val Epoch over. val_loss: 0.5171093705353463; val_accuracy: 0.8405652866242038 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.62; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.8
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.38894022199188827; val_accuracy: 0.8839570063694268 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.84
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.72; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.24; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.63; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.78
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.32; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.388914958449306; val_accuracy: 0.884952229299363 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.8
Batch: 260; loss: 0.49; acc: 0.81
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.4083268774353015; val_accuracy: 0.8749004777070064 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.85; acc: 0.83
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.59; acc: 0.88
Batch: 780; loss: 0.18; acc: 0.98
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.3256332266387666; val_accuracy: 0.9010748407643312 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.81
Batch: 300; loss: 0.36; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.86
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.57; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.38499197083863484; val_accuracy: 0.8831608280254777 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.77; acc: 0.83
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.85; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.83; acc: 0.83
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.3387064818458952; val_accuracy: 0.9000796178343949 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.56; acc: 0.78
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.51; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.62; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3280380517956178; val_accuracy: 0.9023686305732485 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.92
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.66; acc: 0.75
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.26; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.8
Batch: 460; loss: 0.43; acc: 0.83
Batch: 480; loss: 0.5; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.09; acc: 1.0
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.95
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3397274248444351; val_accuracy: 0.8998805732484076 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.83
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.84
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.61; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.35188619374849234; val_accuracy: 0.8900278662420382 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.53; acc: 0.83
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.84
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3396117115144137; val_accuracy: 0.8981886942675159 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.45; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.4105563310870699; val_accuracy: 0.8709195859872612 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.67; acc: 0.83
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.58; acc: 0.78
Batch: 520; loss: 0.54; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.61; acc: 0.83
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.81
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.33891131959048804; val_accuracy: 0.899781050955414 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.83
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.51; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.43; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.37893989777109427; val_accuracy: 0.8875398089171974 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.74; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3170729596049163; val_accuracy: 0.9054538216560509 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.41; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.32542167824639634; val_accuracy: 0.9019705414012739 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.321015354317085; val_accuracy: 0.902468152866242 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.83
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.64; acc: 0.83
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3167620373854212; val_accuracy: 0.9053542993630573 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3186985067311366; val_accuracy: 0.9070461783439491 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.62; acc: 0.81
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.61; acc: 0.88
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3217670180283155; val_accuracy: 0.9034633757961783 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.62; acc: 0.84
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.65; acc: 0.86
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.41; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.326474965828809; val_accuracy: 0.9012738853503185 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.61; acc: 0.83
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.65; acc: 0.78
Batch: 560; loss: 0.22; acc: 0.97
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.32451415899547803; val_accuracy: 0.9034633757961783 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.51; acc: 0.81
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.97
Batch: 360; loss: 0.5; acc: 0.89
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.317246740814417; val_accuracy: 0.9040605095541401 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.66; acc: 0.78
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.57; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3200816830774401; val_accuracy: 0.9029657643312102 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.81
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.83
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3107746281916169; val_accuracy: 0.9066480891719745 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.83
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.82; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.81
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3110569741363358; val_accuracy: 0.9067476114649682 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.65; acc: 0.81
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3129350988396034; val_accuracy: 0.9060509554140127 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.82; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.72; acc: 0.77
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.21; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3098395488634231; val_accuracy: 0.9075437898089171 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.22; acc: 0.97
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.77; acc: 0.81
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.44; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.3136965554610939; val_accuracy: 0.9054538216560509 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.57; acc: 0.89
Batch: 580; loss: 0.58; acc: 0.86
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3100090224034847; val_accuracy: 0.90625 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.52; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3112372231141777; val_accuracy: 0.9070461783439491 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.30969054837989957; val_accuracy: 0.9077428343949044 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.84; acc: 0.81
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.31000075811982913; val_accuracy: 0.9067476114649682 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.98
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.31068201885101904; val_accuracy: 0.9066480891719745 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3093882329097599; val_accuracy: 0.9076433121019108 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.83
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3089070088544469; val_accuracy: 0.9075437898089171 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.51; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.44; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.81
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3093391144351595; val_accuracy: 0.908140923566879 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.75; acc: 0.78
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.83
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.30923964054721176; val_accuracy: 0.9074442675159236 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.84
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.3088190760582116; val_accuracy: 0.9074442675159236 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.65; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.89
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.71; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.30865290058646233; val_accuracy: 0.9072452229299363 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.78
Batch: 740; loss: 0.59; acc: 0.83
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.30964414668595713; val_accuracy: 0.9067476114649682 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.83
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.30923355729052215; val_accuracy: 0.9069466560509554 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.8
Batch: 240; loss: 0.52; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.95
Batch: 460; loss: 0.41; acc: 0.94
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.64; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.30904543535060186; val_accuracy: 0.9070461783439491 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.65; acc: 0.78
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3086500538714752; val_accuracy: 0.9058519108280255 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 9329458
elements in E: 9329460
fraction nonzero: 0.9999997856253202
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.06
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.29; acc: 0.16
Batch: 60; loss: 2.29; acc: 0.09
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.06
Batch: 180; loss: 2.27; acc: 0.16
Batch: 200; loss: 2.27; acc: 0.25
Batch: 220; loss: 2.27; acc: 0.2
Batch: 240; loss: 2.26; acc: 0.31
Batch: 260; loss: 2.25; acc: 0.34
Batch: 280; loss: 2.23; acc: 0.44
Batch: 300; loss: 2.24; acc: 0.28
Batch: 320; loss: 2.2; acc: 0.33
Batch: 340; loss: 2.21; acc: 0.33
Batch: 360; loss: 2.16; acc: 0.34
Batch: 380; loss: 2.11; acc: 0.41
Batch: 400; loss: 2.1; acc: 0.42
Batch: 420; loss: 2.03; acc: 0.44
Batch: 440; loss: 1.92; acc: 0.5
Batch: 460; loss: 1.81; acc: 0.42
Batch: 480; loss: 1.59; acc: 0.56
Batch: 500; loss: 1.24; acc: 0.64
Batch: 520; loss: 1.17; acc: 0.61
Batch: 540; loss: 1.0; acc: 0.7
Batch: 560; loss: 1.13; acc: 0.64
Batch: 580; loss: 0.77; acc: 0.73
Batch: 600; loss: 0.91; acc: 0.69
Batch: 620; loss: 0.83; acc: 0.75
Batch: 640; loss: 0.86; acc: 0.69
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.91; acc: 0.72
Batch: 700; loss: 0.78; acc: 0.81
Batch: 720; loss: 0.87; acc: 0.72
Batch: 740; loss: 0.68; acc: 0.8
Batch: 760; loss: 0.55; acc: 0.78
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 1.69; train_accuracy: 0.44 

Batch: 0; loss: 0.9; acc: 0.7
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.75
Batch: 60; loss: 0.88; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.83
Batch: 100; loss: 0.81; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.77
Val Epoch over. val_loss: 0.8213035468083278; val_accuracy: 0.7325835987261147 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.73
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.7; acc: 0.7
Batch: 60; loss: 0.61; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.61; acc: 0.78
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.81
Batch: 160; loss: 0.69; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.72
Batch: 200; loss: 0.71; acc: 0.77
Batch: 220; loss: 0.42; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.73; acc: 0.77
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 0.54; acc: 0.8
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.8
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.88; acc: 0.73
Batch: 480; loss: 0.58; acc: 0.78
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.83
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.65; acc: 0.77
Batch: 660; loss: 0.61; acc: 0.8
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.78
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.81
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.86
Val Epoch over. val_loss: 0.4524291561572415; val_accuracy: 0.8509156050955414 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.57; acc: 0.8
Batch: 60; loss: 0.31; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.71; acc: 0.72
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.68; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.66; acc: 0.78
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.8
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.45; acc: 0.81
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.8
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.63; acc: 0.77
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.84 

Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.7
Batch: 140; loss: 0.48; acc: 0.8
Val Epoch over. val_loss: 0.5261897833389082; val_accuracy: 0.8309116242038217 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.64; acc: 0.8
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.81
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 0.42; acc: 0.81
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.75
Batch: 620; loss: 0.69; acc: 0.83
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.61; acc: 0.81
Batch: 740; loss: 0.42; acc: 0.81
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.72
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.78
Batch: 80; loss: 0.25; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.4944726768762443; val_accuracy: 0.8458399681528662 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.7; acc: 0.8
Batch: 180; loss: 0.35; acc: 0.84
Batch: 200; loss: 0.67; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.83
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.8
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.78
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.46; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.43; train_accuracy: 0.86 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3951079191248508; val_accuracy: 0.8791799363057324 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.77; acc: 0.75
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.76; acc: 0.77
Batch: 400; loss: 0.76; acc: 0.77
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.59; acc: 0.81
Batch: 500; loss: 0.26; acc: 0.97
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.78
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.84
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.86 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3755316206129493; val_accuracy: 0.8836584394904459 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.85; acc: 0.78
Batch: 380; loss: 0.53; acc: 0.8
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.58; acc: 0.78
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.55; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.53; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.77
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.41293668058837296; val_accuracy: 0.8714171974522293 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.75
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.84
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.52; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.46; acc: 0.81
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.77
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.66; acc: 0.77
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.78
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.3944866922535714; val_accuracy: 0.8789808917197452 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.55; acc: 0.8
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.84
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.81
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.73
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.88
Val Epoch over. val_loss: 0.5053133953149151; val_accuracy: 0.8407643312101911 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.83
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.38; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.77
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.77
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.64; acc: 0.78
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.7; acc: 0.81
Batch: 740; loss: 0.4; acc: 0.81
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.7; acc: 0.72
Batch: 20; loss: 0.91; acc: 0.69
Batch: 40; loss: 0.4; acc: 0.8
Batch: 60; loss: 1.01; acc: 0.7
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.49; acc: 0.8
Batch: 120; loss: 1.16; acc: 0.67
Batch: 140; loss: 0.43; acc: 0.81
Val Epoch over. val_loss: 0.7374903307219219; val_accuracy: 0.7555732484076433 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.4; acc: 0.8
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.81
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.81
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.83
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.2; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3459910029533562; val_accuracy: 0.8914211783439491 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.81
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.83
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.84
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.35230566615796394; val_accuracy: 0.8982882165605095 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.67; acc: 0.77
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.6; acc: 0.78
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.28; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.83
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.3434410861153511; val_accuracy: 0.8951035031847133 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.46; acc: 0.78
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.9; acc: 0.66
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.43650471490279885; val_accuracy: 0.856687898089172 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.86
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.58; acc: 0.8
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.86
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3938130247080402; val_accuracy: 0.8751990445859873 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.86
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.91
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 0.42; acc: 0.83
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.81
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.37775788574841374; val_accuracy: 0.87609474522293 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.68; acc: 0.77
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.8
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.81
Batch: 760; loss: 0.32; acc: 0.86
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.32047287835057375; val_accuracy: 0.9036624203821656 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.67; acc: 0.83
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.86
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.83
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.35894922276211394; val_accuracy: 0.8891321656050956 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.88; acc: 0.8
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.75
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3276492446945731; val_accuracy: 0.8980891719745223 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.83
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.58; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.78
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.87; acc: 0.7
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4888451841606456; val_accuracy: 0.8429538216560509 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.76; acc: 0.73
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.86
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.81
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3040432356013234; val_accuracy: 0.9056528662420382 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.31414311860871924; val_accuracy: 0.9048566878980892 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.81; acc: 0.83
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.86
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3084933887099385; val_accuracy: 0.9052547770700637 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.83
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.30572179738123706; val_accuracy: 0.9063495222929936 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.83
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3043349320958754; val_accuracy: 0.9061504777070064 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.55; acc: 0.8
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.48; acc: 0.81
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.30396589335457536; val_accuracy: 0.9069466560509554 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.84
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.30833146590620847; val_accuracy: 0.9046576433121019 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.55; acc: 0.83
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2970265906280393; val_accuracy: 0.9080414012738853 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.83
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.84
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.29593155803574117; val_accuracy: 0.908140923566879 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2998008445190017; val_accuracy: 0.9059514331210191 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.59; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2995135599308333; val_accuracy: 0.9064490445859873 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.84
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.47; acc: 0.8
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.29484931740221704; val_accuracy: 0.9096337579617835 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.83
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.81
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2962378339640274; val_accuracy: 0.908937101910828 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.78
Batch: 260; loss: 0.38; acc: 0.83
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.44; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.57; acc: 0.83
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.29402308381951536; val_accuracy: 0.9079418789808917 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.84
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.29282726765058603; val_accuracy: 0.9104299363057324 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.83
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.81
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.86
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.29510743206568585; val_accuracy: 0.9095342356687898 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.72; acc: 0.78
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.32; acc: 0.86
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.29304520126171174; val_accuracy: 0.9098328025477707 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.29127627933860584; val_accuracy: 0.9109275477707006 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.45; acc: 0.81
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.78
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.84
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.39; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.29210560163779625; val_accuracy: 0.9095342356687898 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.61; acc: 0.81
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.6; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.86
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.29489521613451325; val_accuracy: 0.9077428343949044 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.62; acc: 0.77
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.5; acc: 0.81
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.28; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.29388523476708467; val_accuracy: 0.9096337579617835 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.48; acc: 0.81
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.29075175848831036; val_accuracy: 0.9107285031847133 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.83
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2907783255740336; val_accuracy: 0.9106289808917197 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.29010269323446947; val_accuracy: 0.910828025477707 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.6; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.29084654861859455; val_accuracy: 0.9111265923566879 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.73; acc: 0.83
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.29097376158757576; val_accuracy: 0.9102308917197452 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.89
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.3; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.84
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.29061716287189227; val_accuracy: 0.9106289808917197 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.46; acc: 0.83
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.29043993024024994; val_accuracy: 0.9099323248407644 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.61; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2895165001083711; val_accuracy: 0.9111265923566879 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.84
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.55; acc: 0.81
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2905977603973477; val_accuracy: 0.9106289808917197 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_210_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 9773718
elements in E: 9773720
fraction nonzero: 0.9999997953696238
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.05
Batch: 40; loss: 2.31; acc: 0.03
Batch: 60; loss: 2.31; acc: 0.02
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.29; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.09
Batch: 160; loss: 2.28; acc: 0.12
Batch: 180; loss: 2.27; acc: 0.14
Batch: 200; loss: 2.27; acc: 0.08
Batch: 220; loss: 2.25; acc: 0.22
Batch: 240; loss: 2.25; acc: 0.17
Batch: 260; loss: 2.26; acc: 0.12
Batch: 280; loss: 2.24; acc: 0.19
Batch: 300; loss: 2.22; acc: 0.28
Batch: 320; loss: 2.21; acc: 0.28
Batch: 340; loss: 2.17; acc: 0.3
Batch: 360; loss: 2.1; acc: 0.39
Batch: 380; loss: 2.12; acc: 0.28
Batch: 400; loss: 2.01; acc: 0.42
Batch: 420; loss: 1.89; acc: 0.45
Batch: 440; loss: 1.75; acc: 0.44
Batch: 460; loss: 1.53; acc: 0.53
Batch: 480; loss: 1.53; acc: 0.47
Batch: 500; loss: 1.45; acc: 0.5
Batch: 520; loss: 1.39; acc: 0.55
Batch: 540; loss: 1.09; acc: 0.64
Batch: 560; loss: 0.82; acc: 0.8
Batch: 580; loss: 0.97; acc: 0.66
Batch: 600; loss: 0.76; acc: 0.73
Batch: 620; loss: 0.81; acc: 0.7
Batch: 640; loss: 0.91; acc: 0.62
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.69; acc: 0.75
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.63; acc: 0.8
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.78; acc: 0.7
Batch: 780; loss: 0.47; acc: 0.81
Train Epoch over. train_loss: 1.66; train_accuracy: 0.42 

Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.32; acc: 0.62
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.67
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.97; acc: 0.66
Batch: 120; loss: 1.17; acc: 0.72
Batch: 140; loss: 0.86; acc: 0.72
Val Epoch over. val_loss: 0.904843423017271; val_accuracy: 0.7202428343949044 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.7; acc: 0.78
Batch: 180; loss: 0.56; acc: 0.8
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.77
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.64; acc: 0.83
Batch: 560; loss: 0.79; acc: 0.8
Batch: 580; loss: 0.42; acc: 0.84
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.89; acc: 0.75
Batch: 680; loss: 0.27; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 1.03; acc: 0.7
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.55; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.72
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.33; acc: 0.83
Val Epoch over. val_loss: 0.460472922843353; val_accuracy: 0.8561902866242038 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.85; acc: 0.81
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.81
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.75; acc: 0.8
Batch: 640; loss: 0.57; acc: 0.8
Batch: 660; loss: 0.46; acc: 0.83
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.78; acc: 0.78
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.4382549403294636; val_accuracy: 0.865047770700637 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.68; acc: 0.8
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.8
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.59; acc: 0.8
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4007714162491689; val_accuracy: 0.8748009554140127 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.84
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.8
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.83
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.74; acc: 0.73
Batch: 20; loss: 0.67; acc: 0.73
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.78; acc: 0.78
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.49829102487890586; val_accuracy: 0.8403662420382165 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.72; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.39; acc: 0.84
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.97
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.63; acc: 0.84
Batch: 60; loss: 1.35; acc: 0.7
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.64; acc: 0.73
Val Epoch over. val_loss: 0.9201901558857815; val_accuracy: 0.7219347133757962 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.72
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.41; acc: 0.83
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.83
Batch: 360; loss: 0.49; acc: 0.8
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.77
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.4156682948303071; val_accuracy: 0.8645501592356688 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.58; acc: 0.78
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.57; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.58; acc: 0.77
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.43; acc: 0.8
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.39538156511677297; val_accuracy: 0.8802746815286624 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.8
Batch: 320; loss: 0.33; acc: 0.86
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.63; acc: 0.88
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.57; acc: 0.77
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4143469050811355; val_accuracy: 0.872312898089172 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.78
Batch: 160; loss: 0.39; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.8
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.4132790879648962; val_accuracy: 0.8690286624203821 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.81
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.57; acc: 0.83
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.48; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3314951382767243; val_accuracy: 0.8960987261146497 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3128385187429228; val_accuracy: 0.9032643312101911 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.58; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.89
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.8
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.59; acc: 0.81
Batch: 700; loss: 0.39; acc: 0.84
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.61; acc: 0.86
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2921443262677284; val_accuracy: 0.9114251592356688 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.83
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.56; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.31226644676866805; val_accuracy: 0.90515525477707 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.81
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.30128874662955096; val_accuracy: 0.9065485668789809 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.39; acc: 0.83
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3041799833203197; val_accuracy: 0.908937101910828 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.8
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.69; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.4; acc: 0.81
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3189154675906631; val_accuracy: 0.9008757961783439 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.69; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.81
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.84
Batch: 660; loss: 0.4; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.31208934150873474; val_accuracy: 0.9032643312101911 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.84
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.61; acc: 0.84
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.65; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.89
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.62; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.83
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.29988622285757854; val_accuracy: 0.9065485668789809 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.57; acc: 0.8
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.83
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.78
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3052511773766226; val_accuracy: 0.9061504777070064 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.26; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.46; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.28194422775487993; val_accuracy: 0.9149084394904459 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.98
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.58; acc: 0.83
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.81
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.28798794739280537; val_accuracy: 0.9110270700636943 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28078724365610225; val_accuracy: 0.9151074840764332 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.68; acc: 0.81
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.59; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2833813396371474; val_accuracy: 0.9155055732484076 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.86
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.81
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.47; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.28263991791161763; val_accuracy: 0.9129179936305732 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.47; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.28868936960864217; val_accuracy: 0.9087380573248408 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.71; acc: 0.78
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.84
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.86
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28862380122493025; val_accuracy: 0.9114251592356688 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.84
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.22; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28414755480684295; val_accuracy: 0.9104299363057324 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.86
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.94
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.84
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28047262186742133; val_accuracy: 0.9124203821656051 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28001092535674954; val_accuracy: 0.9145103503184714 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.277672188842942; val_accuracy: 0.9140127388535032 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2770363317363581; val_accuracy: 0.9153065286624203 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.81
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27919989995136385; val_accuracy: 0.9143113057324841 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2793448296179817; val_accuracy: 0.9129179936305732 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.55; acc: 0.8
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2799056733299972; val_accuracy: 0.9138136942675159 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.66; acc: 0.83
Batch: 60; loss: 0.26; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.86
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.8
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2779558671365498; val_accuracy: 0.9143113057324841 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.17; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.52; acc: 0.84
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2771702320522563; val_accuracy: 0.9139132165605095 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.83
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.86
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.59; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.28007404304518824; val_accuracy: 0.9134156050955414 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.86
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2789831998384303; val_accuracy: 0.9134156050955414 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.27809565383822293; val_accuracy: 0.9137141719745223 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.37; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.83
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2762039160937261; val_accuracy: 0.9143113057324841 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.84
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.86
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.27734740866217644; val_accuracy: 0.9136146496815286 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.84
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.27768076355954646; val_accuracy: 0.9140127388535032 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.43; acc: 0.83
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.27704645247216436; val_accuracy: 0.9128184713375797 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.83
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.97
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.78
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2776986710774671; val_accuracy: 0.9140127388535032 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.43; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2772682906146262; val_accuracy: 0.9139132165605095 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.65; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.53; acc: 0.78
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2762574578166767; val_accuracy: 0.9141122611464968 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.74; acc: 0.83
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.27686436301109135; val_accuracy: 0.9137141719745223 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.14; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.81
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.9 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.27797734621129216; val_accuracy: 0.9136146496815286 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.88
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.81
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2762779148093834; val_accuracy: 0.9142117834394905 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_220_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 10217978
elements in E: 10217980
fraction nonzero: 0.9999998042665967
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.16
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.16
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.16
Batch: 100; loss: 2.28; acc: 0.14
Batch: 120; loss: 2.28; acc: 0.08
Batch: 140; loss: 2.27; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.17
Batch: 180; loss: 2.27; acc: 0.12
Batch: 200; loss: 2.26; acc: 0.19
Batch: 220; loss: 2.24; acc: 0.19
Batch: 240; loss: 2.23; acc: 0.25
Batch: 260; loss: 2.21; acc: 0.3
Batch: 280; loss: 2.21; acc: 0.23
Batch: 300; loss: 2.16; acc: 0.36
Batch: 320; loss: 2.12; acc: 0.39
Batch: 340; loss: 2.0; acc: 0.39
Batch: 360; loss: 1.9; acc: 0.44
Batch: 380; loss: 1.6; acc: 0.5
Batch: 400; loss: 1.48; acc: 0.59
Batch: 420; loss: 1.27; acc: 0.61
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.79; acc: 0.69
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 1.05; acc: 0.69
Batch: 520; loss: 0.72; acc: 0.73
Batch: 540; loss: 1.01; acc: 0.7
Batch: 560; loss: 0.96; acc: 0.66
Batch: 580; loss: 0.74; acc: 0.75
Batch: 600; loss: 1.33; acc: 0.67
Batch: 620; loss: 0.57; acc: 0.81
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.92; acc: 0.73
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 1.52; train_accuracy: 0.49 

Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.88; acc: 0.7
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 0.47; acc: 0.84
Val Epoch over. val_loss: 0.6552516520023346; val_accuracy: 0.7864251592356688 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.78
Batch: 300; loss: 0.76; acc: 0.78
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.7; acc: 0.75
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.76; acc: 0.75
Batch: 460; loss: 0.37; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.82; acc: 0.77
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.46; acc: 0.81
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.4189825990017812; val_accuracy: 0.8718152866242038 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.51; acc: 0.92
Batch: 160; loss: 0.66; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 1.0; acc: 0.69
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.8
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.77
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.73; acc: 0.73
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.4055527568243112; val_accuracy: 0.8732085987261147 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.83
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.56; acc: 0.8
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.83
Batch: 420; loss: 0.43; acc: 0.81
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.81
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3773103304634428; val_accuracy: 0.8817675159235668 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.84
Batch: 180; loss: 0.89; acc: 0.78
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.62; acc: 0.78
Batch: 280; loss: 0.38; acc: 0.84
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.8
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 1.09; acc: 0.75
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.5654181129993148; val_accuracy: 0.833797770700637 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.77
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.86
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.81; acc: 0.81
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.59; acc: 0.86
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.92
Batch: 500; loss: 0.57; acc: 0.77
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.49; acc: 0.81
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.3983739091996934; val_accuracy: 0.8802746815286624 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 1.14; acc: 0.72
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.6; acc: 0.81
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.49; acc: 0.92
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.57; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.81
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.45; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.39; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3742727649629496; val_accuracy: 0.8869426751592356 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.63; acc: 0.8
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.5294893999957735; val_accuracy: 0.8430533439490446 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.77
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.1; acc: 1.0
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.61; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.72; acc: 0.72
Batch: 20; loss: 1.07; acc: 0.64
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 1.15; acc: 0.66
Batch: 140; loss: 0.68; acc: 0.81
Val Epoch over. val_loss: 0.8294578604637437; val_accuracy: 0.7480095541401274 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.73
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.64; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.81
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.23; acc: 0.95
Val Epoch over. val_loss: 0.38222065648645354; val_accuracy: 0.8845541401273885 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.81
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.78
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.83
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.69
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3690375549018763; val_accuracy: 0.8847531847133758 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.98
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.29235668791232594; val_accuracy: 0.9105294585987261 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.81
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.27; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3030312930702404; val_accuracy: 0.9050557324840764 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.54; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.29171389118311514; val_accuracy: 0.9130175159235668 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.86
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.83
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.29654956843917535; val_accuracy: 0.9090366242038217 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.46; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.83
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.28794176553844647; val_accuracy: 0.9114251592356688 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.8
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.8
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.2953128161940985; val_accuracy: 0.9077428343949044 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.59; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.86
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.28961439722567606; val_accuracy: 0.9125199044585988 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.76; acc: 0.78
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.4016134917356406; val_accuracy: 0.8738057324840764 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.46; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.29338931690925246; val_accuracy: 0.9095342356687898 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.84
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.57; acc: 0.8
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.27913732463671903; val_accuracy: 0.9158041401273885 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.55; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.55; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2741910820221825; val_accuracy: 0.9159036624203821 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.27350684276716725; val_accuracy: 0.917296974522293 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.98
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.86
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.84
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.46; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.27193038649619766; val_accuracy: 0.9182921974522293 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.83
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2716860741281965; val_accuracy: 0.9174960191082803 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.83
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.84
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27238690155516765; val_accuracy: 0.9174960191082803 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.64; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.84
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27127446960301915; val_accuracy: 0.9184912420382165 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.52; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27144741594411764; val_accuracy: 0.9192874203821656 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.57; acc: 0.89
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26851616830677744; val_accuracy: 0.9179936305732485 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26965512316318074; val_accuracy: 0.9176950636942676 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.89
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2680749237110281; val_accuracy: 0.9195859872611465 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2672592556922679; val_accuracy: 0.9184912420382165 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.24; acc: 0.88
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26642676476080707; val_accuracy: 0.9187898089171974 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26460708824882084; val_accuracy: 0.92078025477707 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.49; acc: 0.78
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26582818305131733; val_accuracy: 0.9185907643312102 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.83
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.91
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2661639977080427; val_accuracy: 0.9198845541401274 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.7; acc: 0.83
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.88
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.74; acc: 0.8
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.26526839936235147; val_accuracy: 0.919984076433121 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2669869989251635; val_accuracy: 0.9178941082802548 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2653219427937155; val_accuracy: 0.9200835987261147 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.88
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.86
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.26792973358255284; val_accuracy: 0.917296974522293 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.26338066677948474; val_accuracy: 0.9208797770700637 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.86
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.89
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.86
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.89
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.26356284239679384; val_accuracy: 0.9205812101910829 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.51; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2641596996053389; val_accuracy: 0.9212778662420382 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.26452686791871766; val_accuracy: 0.9208797770700637 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.81
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2638067237226067; val_accuracy: 0.9197850318471338 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.34; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.83
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2630890173612127; val_accuracy: 0.9215764331210191 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.26303847983574413; val_accuracy: 0.9212778662420382 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.81
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2632894894926791; val_accuracy: 0.9204816878980892 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.83
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.2; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2628246518504468; val_accuracy: 0.9208797770700637 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.46; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.81
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.91
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.26298636701076655; val_accuracy: 0.9222730891719745 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_230_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 10662238
elements in E: 10662240
fraction nonzero: 0.9999998124221552
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.33; acc: 0.05
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.06
Batch: 100; loss: 2.29; acc: 0.17
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.28; acc: 0.09
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.26; acc: 0.14
Batch: 200; loss: 2.27; acc: 0.19
Batch: 220; loss: 2.27; acc: 0.17
Batch: 240; loss: 2.25; acc: 0.22
Batch: 260; loss: 2.24; acc: 0.2
Batch: 280; loss: 2.24; acc: 0.22
Batch: 300; loss: 2.23; acc: 0.27
Batch: 320; loss: 2.21; acc: 0.2
Batch: 340; loss: 2.15; acc: 0.33
Batch: 360; loss: 2.17; acc: 0.2
Batch: 380; loss: 2.1; acc: 0.23
Batch: 400; loss: 2.02; acc: 0.34
Batch: 420; loss: 1.89; acc: 0.39
Batch: 440; loss: 1.81; acc: 0.38
Batch: 460; loss: 1.49; acc: 0.64
Batch: 480; loss: 1.41; acc: 0.61
Batch: 500; loss: 1.11; acc: 0.69
Batch: 520; loss: 1.25; acc: 0.53
Batch: 540; loss: 0.79; acc: 0.8
Batch: 560; loss: 1.01; acc: 0.66
Batch: 580; loss: 0.97; acc: 0.7
Batch: 600; loss: 0.79; acc: 0.69
Batch: 620; loss: 0.66; acc: 0.77
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.74; acc: 0.77
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.71; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.85; acc: 0.75
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 1.63; train_accuracy: 0.43 

Batch: 0; loss: 0.98; acc: 0.7
Batch: 20; loss: 0.96; acc: 0.64
Batch: 40; loss: 0.52; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.7
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.8368241812582988; val_accuracy: 0.7274084394904459 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.62
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.62; acc: 0.78
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.77
Batch: 240; loss: 0.71; acc: 0.7
Batch: 260; loss: 0.55; acc: 0.8
Batch: 280; loss: 0.42; acc: 0.81
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.78
Batch: 340; loss: 0.74; acc: 0.78
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.55; acc: 0.8
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.8
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.51; acc: 0.8
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.48; acc: 0.8
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.55; acc: 0.81
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.71; acc: 0.73
Batch: 20; loss: 0.72; acc: 0.7
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.56; acc: 0.78
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.83
Val Epoch over. val_loss: 0.6323561030588333; val_accuracy: 0.7936902866242038 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.73; acc: 0.7
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.81; acc: 0.8
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.65; acc: 0.77
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.54; acc: 0.88
Batch: 440; loss: 0.63; acc: 0.83
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.83
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.63; acc: 0.8
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.23; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.55; acc: 0.77
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.5105884105536589; val_accuracy: 0.8377786624203821 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.77
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.81
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.59; acc: 0.84
Batch: 400; loss: 0.75; acc: 0.83
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.49; acc: 0.81
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.69; acc: 0.75
Batch: 520; loss: 0.52; acc: 0.8
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.78
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.8
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.91; acc: 0.73
Train Epoch over. train_loss: 0.43; train_accuracy: 0.86 

Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.44949679945115073; val_accuracy: 0.8598726114649682 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.8
Batch: 40; loss: 0.72; acc: 0.7
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.8
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.8
Batch: 540; loss: 0.26; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.75
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.81
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.84
Val Epoch over. val_loss: 0.5083947069705672; val_accuracy: 0.8361863057324841 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.77
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.61; acc: 0.86
Batch: 280; loss: 0.56; acc: 0.8
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.83
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.4160485709453844; val_accuracy: 0.8694267515923567 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.72
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.52; acc: 0.8
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.27; acc: 0.86
Val Epoch over. val_loss: 0.4354384292367917; val_accuracy: 0.8619625796178344 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.64; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.57; acc: 0.8
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.83; acc: 0.75
Batch: 440; loss: 0.5; acc: 0.8
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.83
Batch: 640; loss: 0.62; acc: 0.73
Batch: 660; loss: 0.55; acc: 0.8
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.62; acc: 0.81
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.48; acc: 0.8
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.48893592269367475; val_accuracy: 0.8466361464968153 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.8
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.8
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.63; acc: 0.78
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.56; acc: 0.8
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.81
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.48; acc: 0.8
Batch: 760; loss: 0.56; acc: 0.78
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 1.26; acc: 0.64
Batch: 20; loss: 1.62; acc: 0.67
Batch: 40; loss: 1.51; acc: 0.64
Batch: 60; loss: 1.71; acc: 0.66
Batch: 80; loss: 1.2; acc: 0.72
Batch: 100; loss: 1.15; acc: 0.67
Batch: 120; loss: 1.43; acc: 0.62
Batch: 140; loss: 1.0; acc: 0.72
Val Epoch over. val_loss: 1.3975033824610863; val_accuracy: 0.6254976114649682 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.73
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.84
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.74; acc: 0.75
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.53; acc: 0.78
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 1.07; acc: 0.77
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 0.8; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.672078271485438; val_accuracy: 0.7924960191082803 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.05; acc: 0.75
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.84
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.97
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.48; acc: 0.86
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.53; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.92
Val Epoch over. val_loss: 0.3450718158559435; val_accuracy: 0.8934116242038217 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.98
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.32202919883428105; val_accuracy: 0.9023686305732485 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.81
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.88
Batch: 320; loss: 0.73; acc: 0.81
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.81
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.83
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.30972910271423637; val_accuracy: 0.9077428343949044 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.98
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.34; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.94
Val Epoch over. val_loss: 0.29452177846602573; val_accuracy: 0.9105294585987261 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.83
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.3133808355897095; val_accuracy: 0.9070461783439491 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.83
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.86
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.34257470650278077; val_accuracy: 0.8945063694267515 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.44; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.83
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.95
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.49; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.3391476582712049; val_accuracy: 0.8960987261146497 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.78
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.19; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.79; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.443523221358562; val_accuracy: 0.8601711783439491 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.64; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.65; acc: 0.8
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.36215636511422267; val_accuracy: 0.8852507961783439 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.45; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.84
Batch: 140; loss: 0.16; acc: 0.92
Val Epoch over. val_loss: 0.3247959087513814; val_accuracy: 0.9007762738853503 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.52; acc: 0.78
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3088591075769276; val_accuracy: 0.904359076433121 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.58; acc: 0.8
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.282959724640011; val_accuracy: 0.9145103503184714 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.48; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.2821480002088152; val_accuracy: 0.9152070063694268 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2899204948620432; val_accuracy: 0.9112261146496815 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.8
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.81
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.88
Batch: 720; loss: 0.64; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.29013296380449255; val_accuracy: 0.9133160828025477 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.6; acc: 0.81
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.28613286028811885; val_accuracy: 0.9145103503184714 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.86
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.86
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.1; acc: 1.0
Batch: 780; loss: 0.35; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.28522420589141784; val_accuracy: 0.9136146496815286 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.83
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2967188863237952; val_accuracy: 0.9083399681528662 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2855877533887222; val_accuracy: 0.9120222929936306 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.67; acc: 0.86
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.15; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.2853511047258878; val_accuracy: 0.9136146496815286 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.83
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.98
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.28005950648788436; val_accuracy: 0.9155055732484076 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.17; acc: 0.98
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.83
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.83
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.83
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2820695840344308; val_accuracy: 0.9138136942675159 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.28071027175541136; val_accuracy: 0.9139132165605095 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.8
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.83
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2795605486745288; val_accuracy: 0.9148089171974523 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.92
Batch: 440; loss: 0.58; acc: 0.88
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.84
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.28146001008475663; val_accuracy: 0.9130175159235668 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.48; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.27915777863970226; val_accuracy: 0.9138136942675159 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.66; acc: 0.78
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.89
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.28578067893625064; val_accuracy: 0.913515127388535 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2814917372670143; val_accuracy: 0.9150079617834395 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.8
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2829735173968373; val_accuracy: 0.9139132165605095 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.92
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.28207145396406486; val_accuracy: 0.9139132165605095 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.39; acc: 0.84
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28067247156694436; val_accuracy: 0.9132165605095541 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.81
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.58; acc: 0.81
Batch: 680; loss: 0.1; acc: 1.0
Batch: 700; loss: 0.59; acc: 0.86
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2787728637314526; val_accuracy: 0.9143113057324841 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.46; acc: 0.83
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.59; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.88
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.72; acc: 0.86
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27967088531916307; val_accuracy: 0.9153065286624203 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.66; acc: 0.86
Batch: 380; loss: 0.18; acc: 0.91
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.83
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27867744874897277; val_accuracy: 0.9163017515923567 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.21; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.83
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2810575709601117; val_accuracy: 0.914609872611465 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.83
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27958823370326097; val_accuracy: 0.9144108280254777 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.83
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.36; acc: 0.81
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2811509182641081; val_accuracy: 0.9123208598726115 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.81
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2798851241541516; val_accuracy: 0.9144108280254777 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.81
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.86
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.27863727723527104; val_accuracy: 0.915406050955414 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.84
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.86
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.81
Batch: 280; loss: 0.22; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2785808356466946; val_accuracy: 0.9152070063694268 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_240_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 11106498
elements in E: 11106500
fraction nonzero: 0.999999819925269
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.11
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.28; acc: 0.16
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.11
Batch: 120; loss: 2.27; acc: 0.14
Batch: 140; loss: 2.27; acc: 0.11
Batch: 160; loss: 2.26; acc: 0.14
Batch: 180; loss: 2.26; acc: 0.19
Batch: 200; loss: 2.26; acc: 0.28
Batch: 220; loss: 2.24; acc: 0.33
Batch: 240; loss: 2.24; acc: 0.31
Batch: 260; loss: 2.24; acc: 0.22
Batch: 280; loss: 2.2; acc: 0.34
Batch: 300; loss: 2.16; acc: 0.45
Batch: 320; loss: 2.13; acc: 0.36
Batch: 340; loss: 2.07; acc: 0.5
Batch: 360; loss: 2.02; acc: 0.41
Batch: 380; loss: 1.85; acc: 0.48
Batch: 400; loss: 1.56; acc: 0.56
Batch: 420; loss: 1.26; acc: 0.69
Batch: 440; loss: 1.13; acc: 0.61
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 0.92; acc: 0.75
Batch: 500; loss: 0.98; acc: 0.62
Batch: 520; loss: 0.76; acc: 0.78
Batch: 540; loss: 0.8; acc: 0.7
Batch: 560; loss: 0.69; acc: 0.77
Batch: 580; loss: 0.71; acc: 0.77
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.56; acc: 0.81
Batch: 660; loss: 0.7; acc: 0.83
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.77
Batch: 720; loss: 0.53; acc: 0.78
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.57; acc: 0.84
Train Epoch over. train_loss: 1.52; train_accuracy: 0.49 

Batch: 0; loss: 1.22; acc: 0.56
Batch: 20; loss: 1.37; acc: 0.53
Batch: 40; loss: 0.84; acc: 0.73
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 0.8; acc: 0.75
Batch: 100; loss: 1.2; acc: 0.56
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.5; acc: 0.8
Val Epoch over. val_loss: 1.0586910733751431; val_accuracy: 0.6455015923566879 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.87; acc: 0.69
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.59; acc: 0.78
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.66; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.92
Batch: 300; loss: 0.61; acc: 0.81
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.49; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.81
Batch: 460; loss: 0.54; acc: 0.8
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.78
Batch: 540; loss: 0.53; acc: 0.81
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.84
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.56; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.8
Batch: 740; loss: 0.43; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.94; acc: 0.69
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.75
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.69
Batch: 140; loss: 0.3; acc: 0.89
Val Epoch over. val_loss: 0.6212269781501429; val_accuracy: 0.7993630573248408 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.05; acc: 0.7
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.78
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.77
Batch: 260; loss: 0.37; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.52; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.81
Batch: 660; loss: 0.49; acc: 0.84
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.5510250254990948; val_accuracy: 0.8314092356687898 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.97
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.8
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.8
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.62; acc: 0.8
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.44; acc: 0.8
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3550489852382879; val_accuracy: 0.893312101910828 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.81
Batch: 240; loss: 0.66; acc: 0.78
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.83
Batch: 300; loss: 0.54; acc: 0.8
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.54; acc: 0.81
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.61; acc: 0.81
Batch: 460; loss: 0.3; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.64; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.59; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.23; acc: 0.92
Val Epoch over. val_loss: 0.5293445363166226; val_accuracy: 0.8379777070063694 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.78
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.84
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.78
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.78
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.8
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.75
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.23; acc: 0.91
Val Epoch over. val_loss: 0.45861124332733216; val_accuracy: 0.8630573248407644 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.8
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.56; acc: 0.86
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.89
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.98
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.8
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.4112994666122327; val_accuracy: 0.8672372611464968 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.83
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.3140560225316673; val_accuracy: 0.9057523885350318 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.83; acc: 0.83
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.78
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.83
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.83
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.83
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.35163825062240006; val_accuracy: 0.8863455414012739 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.84
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.69; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.81
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.77; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.86
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 1.14; acc: 0.62
Batch: 20; loss: 1.66; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.77
Batch: 60; loss: 1.25; acc: 0.73
Batch: 80; loss: 1.3; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.81
Val Epoch over. val_loss: 1.1522703700384516; val_accuracy: 0.7148686305732485 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.95; acc: 0.72
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.86
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.32137453712665354; val_accuracy: 0.9013734076433121 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.84
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.83
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.8
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.78; acc: 0.84
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2723474047461133; val_accuracy: 0.9187898089171974 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.77
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.12; acc: 1.0
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.27827669257760806; val_accuracy: 0.9166998407643312 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.11; acc: 1.0
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.83
Batch: 180; loss: 0.29; acc: 0.86
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.53; acc: 0.77
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.27999550350912056; val_accuracy: 0.9171974522292994 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.8
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.27110932074534666; val_accuracy: 0.9165007961783439 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.39; acc: 0.84
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3019003466625882; val_accuracy: 0.9092356687898089 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.83
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.67; acc: 0.83
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.36; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.77
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.33922533230606916; val_accuracy: 0.8909235668789809 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2902284451066309; val_accuracy: 0.9125199044585988 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.27746272395560695; val_accuracy: 0.9143113057324841 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.29; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.86
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.66; acc: 0.78
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.4805848328930557; val_accuracy: 0.8476313694267515 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.54; acc: 0.81
Batch: 360; loss: 0.3; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2845402421655169; val_accuracy: 0.9134156050955414 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.67; acc: 0.84
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.278593411728455; val_accuracy: 0.9166998407643312 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.42; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26372198319169365; val_accuracy: 0.9224721337579618 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.81
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2639279225544565; val_accuracy: 0.9224721337579618 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.86
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.49; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2716167820202317; val_accuracy: 0.9193869426751592 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.84
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2738878063525364; val_accuracy: 0.9188893312101911 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.09; acc: 1.0
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.52; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.27349357373395544; val_accuracy: 0.9192874203821656 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.98
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.83
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2681805596325048; val_accuracy: 0.9214769108280255 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.88
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26280400712209145; val_accuracy: 0.9239649681528662 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.84
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2636057587376066; val_accuracy: 0.921875 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.86
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.66; acc: 0.83
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2604946864733271; val_accuracy: 0.923765923566879 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.98
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.88
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2655428966898827; val_accuracy: 0.9230692675159236 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.83
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.98
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.8
Batch: 640; loss: 0.27; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26461248446232194; val_accuracy: 0.9222730891719745 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.21; acc: 0.89
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.49; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.26151216504680125; val_accuracy: 0.9231687898089171 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.26976020529771305; val_accuracy: 0.9198845541401274 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.41; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.26225794486369297; val_accuracy: 0.9224721337579618 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2652216801882549; val_accuracy: 0.9213773885350318 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.83
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.26111955779373264; val_accuracy: 0.9239649681528662 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.84
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2626821525916932; val_accuracy: 0.9235668789808917 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2621602632437542; val_accuracy: 0.9231687898089171 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.86
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2616384297039858; val_accuracy: 0.9232683121019108 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.5; acc: 0.86
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.98
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2614138400193992; val_accuracy: 0.9238654458598726 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.56; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.26143192798848364; val_accuracy: 0.9240644904458599 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2619613800553759; val_accuracy: 0.9235668789808917 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.26061994245477543; val_accuracy: 0.923765923566879 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.84
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.98
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.26184632249508694; val_accuracy: 0.9235668789808917 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.84
Batch: 720; loss: 0.2; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.26144352346468885; val_accuracy: 0.9243630573248408 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.67; acc: 0.88
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.88
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2612444607030814; val_accuracy: 0.9236664012738853 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.84
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.26309746503829956; val_accuracy: 0.9220740445859873 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2617161782209281; val_accuracy: 0.9232683121019108 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_250_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 11550758
elements in E: 11550760
fraction nonzero: 0.9999998268512201
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.08
Batch: 20; loss: 2.32; acc: 0.16
Batch: 40; loss: 2.31; acc: 0.05
Batch: 60; loss: 2.29; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.28; acc: 0.12
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.28; acc: 0.09
Batch: 200; loss: 2.25; acc: 0.17
Batch: 220; loss: 2.24; acc: 0.27
Batch: 240; loss: 2.25; acc: 0.27
Batch: 260; loss: 2.23; acc: 0.27
Batch: 280; loss: 2.23; acc: 0.28
Batch: 300; loss: 2.19; acc: 0.36
Batch: 320; loss: 2.15; acc: 0.42
Batch: 340; loss: 2.11; acc: 0.38
Batch: 360; loss: 1.98; acc: 0.55
Batch: 380; loss: 1.93; acc: 0.48
Batch: 400; loss: 1.75; acc: 0.52
Batch: 420; loss: 1.62; acc: 0.5
Batch: 440; loss: 1.21; acc: 0.64
Batch: 460; loss: 1.09; acc: 0.62
Batch: 480; loss: 1.16; acc: 0.64
Batch: 500; loss: 0.98; acc: 0.61
Batch: 520; loss: 0.81; acc: 0.75
Batch: 540; loss: 0.89; acc: 0.69
Batch: 560; loss: 0.75; acc: 0.7
Batch: 580; loss: 0.79; acc: 0.7
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.81; acc: 0.72
Batch: 640; loss: 0.64; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.75
Batch: 680; loss: 0.65; acc: 0.8
Batch: 700; loss: 0.6; acc: 0.77
Batch: 720; loss: 0.78; acc: 0.77
Batch: 740; loss: 0.83; acc: 0.78
Batch: 760; loss: 0.63; acc: 0.73
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 1.57; train_accuracy: 0.46 

Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.95; acc: 0.69
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.69
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.6263825515652918; val_accuracy: 0.8031449044585988 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.58; acc: 0.78
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.65; acc: 0.73
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.69; acc: 0.7
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.8
Batch: 280; loss: 0.48; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.78
Batch: 380; loss: 0.47; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.77
Batch: 420; loss: 0.43; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.64; acc: 0.8
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.8
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.43; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.78
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.64; acc: 0.75
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.48137549296685844; val_accuracy: 0.8454418789808917 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.81
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.89
Batch: 260; loss: 0.65; acc: 0.81
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.81
Batch: 620; loss: 0.83; acc: 0.78
Batch: 640; loss: 0.53; acc: 0.8
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.7; acc: 0.77
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.33; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.389826463547292; val_accuracy: 0.8803742038216561 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.73
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.6; acc: 0.78
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 0.55; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.73
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.83
Val Epoch over. val_loss: 0.8017736773961669; val_accuracy: 0.7491042993630573 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.84
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.432236965389768; val_accuracy: 0.8607683121019108 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.8
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.77
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.98
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.34003367678375; val_accuracy: 0.8915207006369427 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.3; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.61; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.77
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.8; acc: 0.8
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.52; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.44710847991666974; val_accuracy: 0.8621616242038217 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.83
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.67; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.84
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3285397975260665; val_accuracy: 0.8984872611464968 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.3757772624587557; val_accuracy: 0.8846536624203821 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.49; acc: 0.8
Batch: 360; loss: 0.4; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.45; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.52; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.78
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.45308603341602216; val_accuracy: 0.8618630573248408 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.18; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.86
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.2888446977943372; val_accuracy: 0.9145103503184714 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3024241052283223; val_accuracy: 0.9019705414012739 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.88
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.41; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.30243773193685874; val_accuracy: 0.9050557324840764 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.53; acc: 0.77
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.73; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.83
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.16; acc: 0.92
Val Epoch over. val_loss: 0.3221569798032569; val_accuracy: 0.9002786624203821 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 0.25; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.81
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.63; acc: 0.8
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.3008272315428895; val_accuracy: 0.9095342356687898 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.84
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.4; acc: 0.91
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2828470720749372; val_accuracy: 0.9153065286624203 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.2895258177712465; val_accuracy: 0.9129179936305732 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.55; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.97
Batch: 700; loss: 0.34; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.3032075652413687; val_accuracy: 0.9058519108280255 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.45; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.48; acc: 0.81
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.78; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.30118438217100824; val_accuracy: 0.908937101910828 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.78
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.56; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.30470124957193234; val_accuracy: 0.9095342356687898 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.13; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.7; acc: 0.88
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.84
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.272569301140726; val_accuracy: 0.92078025477707 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.84
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.48; acc: 0.89
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.53; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2823272744656368; val_accuracy: 0.9139132165605095 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.86
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.27032128294372254; val_accuracy: 0.9184912420382165 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.56; acc: 0.83
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2850606469971359; val_accuracy: 0.9148089171974523 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.88
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.09; acc: 1.0
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.86
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.27353635126618064; val_accuracy: 0.9179936305732485 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.86
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.88
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.272255106692671; val_accuracy: 0.919187898089172 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.5; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.08; acc: 1.0
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.27994207164664175; val_accuracy: 0.9163017515923567 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.86
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.89
Batch: 640; loss: 0.65; acc: 0.84
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.53; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.274907439472569; val_accuracy: 0.9179936305732485 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.98
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2741607768567884; val_accuracy: 0.9165007961783439 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.27200010872096014; val_accuracy: 0.9168988853503185 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2689592248647456; val_accuracy: 0.9176950636942676 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.83
Batch: 160; loss: 0.25; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.16; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2668064805874779; val_accuracy: 0.9200835987261147 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26673909713318394; val_accuracy: 0.919187898089172 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.88
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26637744303246974; val_accuracy: 0.9188893312101911 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.27014199585007254; val_accuracy: 0.9176950636942676 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.43; acc: 0.81
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.83
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2676269145101119; val_accuracy: 0.9188893312101911 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.43; acc: 0.83
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26860376924371265; val_accuracy: 0.9204816878980892 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.44; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26649191776278675; val_accuracy: 0.9188893312101911 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.09; acc: 1.0
Batch: 680; loss: 0.3; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.83
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26650100500340673; val_accuracy: 0.9202826433121019 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2672367265364926; val_accuracy: 0.9194864649681529 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.86
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26519470061561107; val_accuracy: 0.919187898089172 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.69; acc: 0.81
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.265852749869702; val_accuracy: 0.9184912420382165 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.265579044273135; val_accuracy: 0.919187898089172 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26497579418170225; val_accuracy: 0.919984076433121 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.86
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2656597755612082; val_accuracy: 0.9196855095541401 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.83
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.26584588420713784; val_accuracy: 0.9187898089171974 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.86
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.64; acc: 0.81
Batch: 760; loss: 0.45; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2653626715823723; val_accuracy: 0.9194864649681529 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.264896607512881; val_accuracy: 0.9201831210191083 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.86
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.84
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.86
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26522271206993964; val_accuracy: 0.9187898089171974 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.84
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.26506411663855717; val_accuracy: 0.9188893312101911 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_260_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 11995018
elements in E: 11995020
fraction nonzero: 0.999999833264138
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.12
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.29; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.14
Batch: 160; loss: 2.29; acc: 0.06
Batch: 180; loss: 2.27; acc: 0.2
Batch: 200; loss: 2.27; acc: 0.16
Batch: 220; loss: 2.24; acc: 0.39
Batch: 240; loss: 2.26; acc: 0.33
Batch: 260; loss: 2.21; acc: 0.41
Batch: 280; loss: 2.24; acc: 0.22
Batch: 300; loss: 2.18; acc: 0.38
Batch: 320; loss: 2.16; acc: 0.39
Batch: 340; loss: 2.14; acc: 0.39
Batch: 360; loss: 2.09; acc: 0.34
Batch: 380; loss: 2.13; acc: 0.27
Batch: 400; loss: 2.03; acc: 0.27
Batch: 420; loss: 2.0; acc: 0.28
Batch: 440; loss: 1.59; acc: 0.52
Batch: 460; loss: 1.48; acc: 0.58
Batch: 480; loss: 1.13; acc: 0.69
Batch: 500; loss: 1.11; acc: 0.61
Batch: 520; loss: 1.22; acc: 0.66
Batch: 540; loss: 0.89; acc: 0.73
Batch: 560; loss: 0.85; acc: 0.73
Batch: 580; loss: 0.96; acc: 0.75
Batch: 600; loss: 0.81; acc: 0.77
Batch: 620; loss: 1.07; acc: 0.64
Batch: 640; loss: 1.01; acc: 0.59
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.67; acc: 0.73
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 1.1; acc: 0.64
Batch: 780; loss: 0.81; acc: 0.69
Train Epoch over. train_loss: 1.65; train_accuracy: 0.44 

Batch: 0; loss: 0.74; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.62
Batch: 40; loss: 0.54; acc: 0.78
Batch: 60; loss: 0.88; acc: 0.73
Batch: 80; loss: 0.73; acc: 0.75
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 1.01; acc: 0.61
Batch: 140; loss: 0.79; acc: 0.69
Val Epoch over. val_loss: 0.7876042169370469; val_accuracy: 0.7300955414012739 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.57; acc: 0.77
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.63; acc: 0.77
Batch: 160; loss: 0.56; acc: 0.84
Batch: 180; loss: 0.66; acc: 0.75
Batch: 200; loss: 0.66; acc: 0.72
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.75
Batch: 300; loss: 0.95; acc: 0.72
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.71; acc: 0.78
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.64; acc: 0.77
Batch: 580; loss: 0.57; acc: 0.78
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.94
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.55; acc: 0.78
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.77
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.76; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Val Epoch over. val_loss: 0.444095965451116; val_accuracy: 0.8646496815286624 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.78
Batch: 80; loss: 0.73; acc: 0.77
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 1.22; acc: 0.72
Batch: 280; loss: 0.62; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.44; acc: 0.83
Batch: 500; loss: 0.85; acc: 0.72
Batch: 520; loss: 0.58; acc: 0.83
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.8
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.94; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.88
Val Epoch over. val_loss: 0.5069566394682903; val_accuracy: 0.8365843949044586 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.8
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.32; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.73; acc: 0.77
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.47; acc: 0.81
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.57; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.54; acc: 0.91
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.75
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.98; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.4277299572802653; val_accuracy: 0.8649482484076433 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.33; acc: 0.84
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.64; acc: 0.86
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.87; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3526902693282267; val_accuracy: 0.8932125796178344 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.62; acc: 0.78
Batch: 220; loss: 0.47; acc: 0.83
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.83
Batch: 320; loss: 0.34; acc: 0.84
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.83
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.83
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.88 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.37424630714449914; val_accuracy: 0.8809713375796179 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.88
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.98
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.33; acc: 0.83
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.79; acc: 0.78
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.26; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.86
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.34989933520080935; val_accuracy: 0.8884355095541401 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.77
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.86
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.3298002634743217; val_accuracy: 0.8961982484076433 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.51; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.85; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.47; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.89
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3508263153445189; val_accuracy: 0.8904259554140127 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.21; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.8
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.62; acc: 0.81
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.48150707259299647; val_accuracy: 0.8544984076433121 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.19; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.27021527053064603; val_accuracy: 0.917296974522293 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.84
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.27301330816973546; val_accuracy: 0.9178941082802548 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.83
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.72; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.28408182070703264; val_accuracy: 0.9118232484076433 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.86
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.26435736301028806; val_accuracy: 0.9175955414012739 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.43; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2823059321588771; val_accuracy: 0.9125199044585988 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.94
Val Epoch over. val_loss: 0.3211843144077404; val_accuracy: 0.9002786624203821 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.66; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.84
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.30189129952222676; val_accuracy: 0.9059514331210191 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.11; acc: 1.0
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.28596425345938675; val_accuracy: 0.9109275477707006 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.78; acc: 0.81
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.08; acc: 1.0
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 0.31; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.95
Val Epoch over. val_loss: 0.27625969806864004; val_accuracy: 0.9178941082802548 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.83
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.75; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.64; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.28850462809679617; val_accuracy: 0.9099323248407644 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.25826854700115837; val_accuracy: 0.9200835987261147 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.86
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2606839623041214; val_accuracy: 0.9178941082802548 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.44; acc: 0.83
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.25604761980331625; val_accuracy: 0.9200835987261147 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.61; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2605663421712104; val_accuracy: 0.9203821656050956 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.45; acc: 0.81
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.2644048636885965; val_accuracy: 0.9193869426751592 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.84
Batch: 180; loss: 0.2; acc: 0.89
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.25333639414636955; val_accuracy: 0.9228702229299363 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.25429740923035676; val_accuracy: 0.9232683121019108 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.97
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.83
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.33; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.25432469382597384; val_accuracy: 0.9244625796178344 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.84
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.35; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.25210377749553914; val_accuracy: 0.9243630573248408 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.88
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.88
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.25186506397784897; val_accuracy: 0.9238654458598726 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2522655009843741; val_accuracy: 0.9234673566878981 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.72; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.8
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2517981977219794; val_accuracy: 0.9228702229299363 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.38; acc: 0.83
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2502462078051962; val_accuracy: 0.923765923566879 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2504198375116488; val_accuracy: 0.9224721337579618 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.84
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2500481989448238; val_accuracy: 0.9242635350318471 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.94
Val Epoch over. val_loss: 0.2491203926171467; val_accuracy: 0.9239649681528662 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.84
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.84
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.86
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.52; acc: 0.88
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.25075245055423423; val_accuracy: 0.924562101910828 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.34; acc: 0.84
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2480004494357261; val_accuracy: 0.9244625796178344 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.98
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.28; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.88
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.24802410887305143; val_accuracy: 0.9238654458598726 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.66; acc: 0.88
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.15; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.86
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2494193298896407; val_accuracy: 0.9242635350318471 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.24816824310714272; val_accuracy: 0.9244625796178344 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.24718688509077025; val_accuracy: 0.9246616242038217 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.94
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.64; acc: 0.77
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.24801967041507647; val_accuracy: 0.9238654458598726 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.24; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.24781077668355528; val_accuracy: 0.9227707006369427 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.45; acc: 0.91
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2471879772414827; val_accuracy: 0.9246616242038217 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.24691226433037192; val_accuracy: 0.9246616242038217 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.88
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2475247182379103; val_accuracy: 0.9249601910828026 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.43; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2468458823622412; val_accuracy: 0.9248606687898089 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.19; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2474641327265721; val_accuracy: 0.9246616242038217 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.62; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.48; acc: 0.88
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.24658270275137226; val_accuracy: 0.9244625796178344 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_270_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 12439278
elements in E: 12439280
fraction nonzero: 0.9999998392189902
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.06
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.11
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.29; acc: 0.05
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.27; acc: 0.16
Batch: 180; loss: 2.27; acc: 0.2
Batch: 200; loss: 2.25; acc: 0.33
Batch: 220; loss: 2.25; acc: 0.3
Batch: 240; loss: 2.21; acc: 0.39
Batch: 260; loss: 2.19; acc: 0.39
Batch: 280; loss: 2.15; acc: 0.36
Batch: 300; loss: 2.11; acc: 0.38
Batch: 320; loss: 1.96; acc: 0.41
Batch: 340; loss: 1.71; acc: 0.56
Batch: 360; loss: 1.31; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.62
Batch: 400; loss: 0.92; acc: 0.78
Batch: 420; loss: 0.98; acc: 0.69
Batch: 440; loss: 0.96; acc: 0.66
Batch: 460; loss: 0.84; acc: 0.75
Batch: 480; loss: 0.92; acc: 0.7
Batch: 500; loss: 0.76; acc: 0.75
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.76; acc: 0.73
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.62; acc: 0.84
Batch: 640; loss: 0.65; acc: 0.8
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.78
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 1.39; train_accuracy: 0.54 

Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.9; acc: 0.78
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.7
Batch: 140; loss: 0.45; acc: 0.86
Val Epoch over. val_loss: 0.5153535980327874; val_accuracy: 0.8395700636942676 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 1.24; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.63; acc: 0.78
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.7; acc: 0.8
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.56; acc: 0.86
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.45; acc: 0.86
Batch: 760; loss: 0.52; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.41; acc: 0.88
Val Epoch over. val_loss: 0.4821711225304634; val_accuracy: 0.8579816878980892 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.8; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.86
Batch: 140; loss: 0.64; acc: 0.84
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.39; acc: 0.83
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.8
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.84
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 1.26; acc: 0.66
Batch: 20; loss: 1.96; acc: 0.53
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.95; acc: 0.81
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 1.01; acc: 0.72
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 0.84; acc: 0.66
Val Epoch over. val_loss: 1.0288632652562135; val_accuracy: 0.7131767515923567 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.22; acc: 0.67
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.57; acc: 0.83
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.37; acc: 0.84
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.4; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 1.02; acc: 0.75
Batch: 40; loss: 0.2; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.76; acc: 0.84
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.91
Val Epoch over. val_loss: 0.5203969829306481; val_accuracy: 0.8443471337579618 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.89; acc: 0.75
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.52; acc: 0.88
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.8
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.3759106957608727; val_accuracy: 0.8821656050955414 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.69; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.83
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.83; acc: 0.78
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.4994240751027302; val_accuracy: 0.8518113057324841 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.75
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.81
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.31908297225548204; val_accuracy: 0.9030652866242038 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.88
Batch: 60; loss: 0.67; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.72
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.34105143240492813; val_accuracy: 0.8927149681528662 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.86
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.12; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.16; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.32639523453203734; val_accuracy: 0.8974920382165605 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.6; acc: 0.83
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.89
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.75
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3216957238259589; val_accuracy: 0.898984872611465 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.84
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.5; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2317961590589991; val_accuracy: 0.9274482484076433 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2423398401232282; val_accuracy: 0.9255573248407644 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2863188786966026; val_accuracy: 0.9114251592356688 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.98
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.53; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.24645695517397231; val_accuracy: 0.9227707006369427 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.88
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2583717896015781; val_accuracy: 0.9168988853503185 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.07; acc: 1.0
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.88
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2722658156209691; val_accuracy: 0.9158041401273885 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2377787076743545; val_accuracy: 0.9265525477707006 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22701387320923958; val_accuracy: 0.9276472929936306 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.261217397821557; val_accuracy: 0.919984076433121 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.84
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.91
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.24817308033727536; val_accuracy: 0.9234673566878981 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.89
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.83
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.21888348958484685; val_accuracy: 0.9340167197452229 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2209671597666801; val_accuracy: 0.9336186305732485 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.68; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.34; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.21907803520655175; val_accuracy: 0.9315286624203821 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.98
Batch: 260; loss: 0.68; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.89
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.83
Batch: 600; loss: 0.28; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2318492944168437; val_accuracy: 0.9290406050955414 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.97
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.63; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.23548744561945556; val_accuracy: 0.9276472929936306 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.224204569343169; val_accuracy: 0.9313296178343949 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.34; acc: 0.84
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.83
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.53; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.07; acc: 1.0
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.22077205122276478; val_accuracy: 0.932921974522293 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.07; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22017431989976555; val_accuracy: 0.9326234076433121 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.45; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.86
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.07; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.89
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.22981700846913514; val_accuracy: 0.9313296178343949 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.89
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.21747254115191234; val_accuracy: 0.9349124203821656 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2153634463146234; val_accuracy: 0.9365047770700637 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.215832326917132; val_accuracy: 0.934812898089172 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21663426987494633; val_accuracy: 0.9361066878980892 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.46; acc: 0.83
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21746310591697693; val_accuracy: 0.9349124203821656 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.97
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.84
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2165556851845638; val_accuracy: 0.9357085987261147 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.88
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21637671075429127; val_accuracy: 0.9361066878980892 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2156081977923205; val_accuracy: 0.9349124203821656 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21606485251408475; val_accuracy: 0.9349124203821656 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21759519673836458; val_accuracy: 0.9354100318471338 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21695101811627673; val_accuracy: 0.9344148089171974 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.8
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2164110081970312; val_accuracy: 0.935609076433121 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.94
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.12; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2150197778917422; val_accuracy: 0.93640525477707 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21465065894992488; val_accuracy: 0.9365047770700637 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21511010090067129; val_accuracy: 0.935609076433121 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.91
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.86
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.97
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21484457302814836; val_accuracy: 0.9363057324840764 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.89
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21394653675282838; val_accuracy: 0.93640525477707 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.86
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21551890182457153; val_accuracy: 0.9353105095541401 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.21638523834715984; val_accuracy: 0.9371019108280255 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.88
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2152902757286266; val_accuracy: 0.9367038216560509 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2143698113074728; val_accuracy: 0.9366042993630573 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_280_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 12883538
elements in E: 12883540
fraction nonzero: 0.999999844763163
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.06
Batch: 20; loss: 2.32; acc: 0.03
Batch: 40; loss: 2.31; acc: 0.03
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.09
Batch: 160; loss: 2.27; acc: 0.16
Batch: 180; loss: 2.26; acc: 0.23
Batch: 200; loss: 2.25; acc: 0.36
Batch: 220; loss: 2.25; acc: 0.2
Batch: 240; loss: 2.23; acc: 0.34
Batch: 260; loss: 2.19; acc: 0.45
Batch: 280; loss: 2.18; acc: 0.41
Batch: 300; loss: 2.08; acc: 0.53
Batch: 320; loss: 2.04; acc: 0.44
Batch: 340; loss: 1.92; acc: 0.52
Batch: 360; loss: 1.68; acc: 0.53
Batch: 380; loss: 1.42; acc: 0.55
Batch: 400; loss: 1.21; acc: 0.64
Batch: 420; loss: 1.14; acc: 0.64
Batch: 440; loss: 0.89; acc: 0.73
Batch: 460; loss: 0.71; acc: 0.81
Batch: 480; loss: 0.9; acc: 0.66
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.67; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.78
Batch: 580; loss: 0.76; acc: 0.78
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.67; acc: 0.72
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.81
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.78
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.48; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.8
Train Epoch over. train_loss: 1.41; train_accuracy: 0.53 

Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.31; acc: 0.91
Val Epoch over. val_loss: 0.4686912152987377; val_accuracy: 0.8561902866242038 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.81
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.78
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.67; acc: 0.83
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.83
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.43; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.81
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.83
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.55; acc: 0.8
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.49173377852910644; val_accuracy: 0.8474323248407644 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.84
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.86
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.83
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.84
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.87; acc: 0.75
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.4; acc: 0.81
Val Epoch over. val_loss: 0.7130110155624948; val_accuracy: 0.7835390127388535 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.77
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.83
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.71; acc: 0.84
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.4507440152061973; val_accuracy: 0.8600716560509554 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.53; acc: 0.83
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.74; acc: 0.8
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.84
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.5562800566197201; val_accuracy: 0.8234474522292994 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.69; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.86
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.53; acc: 0.88
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.48; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.65; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.49; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.4119353993635648; val_accuracy: 0.8664410828025477 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.86
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.81
Batch: 760; loss: 0.34; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.77
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.3960143099450002; val_accuracy: 0.8801751592356688 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.21; acc: 0.98
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.71; acc: 0.62
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.82; acc: 0.84
Batch: 80; loss: 0.64; acc: 0.77
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 1.2; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.91
Val Epoch over. val_loss: 0.5792966833824564; val_accuracy: 0.8221536624203821 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.6; acc: 0.78
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.74; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.30877875090594503; val_accuracy: 0.9038614649681529 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.41; acc: 0.92
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.11; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.3297859550851166; val_accuracy: 0.8982882165605095 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.81
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.29730480282929295; val_accuracy: 0.9123208598726115 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.86
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2717686159310827; val_accuracy: 0.9166998407643312 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.2670107721618027; val_accuracy: 0.9176950636942676 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.47; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.53; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.86
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2524162436936312; val_accuracy: 0.9242635350318471 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.58; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2694893607478233; val_accuracy: 0.9182921974522293 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.2; acc: 0.98
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.83
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.24830607481443198; val_accuracy: 0.9253582802547771 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3019672798313153; val_accuracy: 0.9091361464968153 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.08; acc: 1.0
Batch: 300; loss: 0.43; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.53; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.24142297841371244; val_accuracy: 0.9271496815286624 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.57; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.84
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2989623424164049; val_accuracy: 0.9047571656050956 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.91
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.27; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2408595762814686; val_accuracy: 0.929140127388535 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.23279030839349055; val_accuracy: 0.9307324840764332 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.27; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.23098744750972006; val_accuracy: 0.9339171974522293 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.26; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.23246345986985856; val_accuracy: 0.932921974522293 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.84
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2330052108521674; val_accuracy: 0.9320262738853503 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.23807179083110422; val_accuracy: 0.9314291401273885 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.81
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.49; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.33; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.77
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2318207148438806; val_accuracy: 0.9320262738853503 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.86
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2324702884930714; val_accuracy: 0.9328224522292994 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.23203549557810377; val_accuracy: 0.9328224522292994 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.88
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.23113584376064836; val_accuracy: 0.9324243630573248 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.2291829201161482; val_accuracy: 0.9326234076433121 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.22870100076031533; val_accuracy: 0.9330214968152867 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.98
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.5; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2277668668965625; val_accuracy: 0.9346138535031847 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.56; acc: 0.8
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22833212052181268; val_accuracy: 0.9334195859872612 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22888549327091046; val_accuracy: 0.933718152866242 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.6; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.84
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2319255585123779; val_accuracy: 0.9322253184713376 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.42; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2305139418525301; val_accuracy: 0.9330214968152867 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.47; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2288856994669149; val_accuracy: 0.9335191082802548 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.23072371650843104; val_accuracy: 0.9332205414012739 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.09; acc: 1.0
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22799139232582347; val_accuracy: 0.9343152866242038 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.12; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.13; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22911443494877237; val_accuracy: 0.9339171974522293 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.68; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22741359301433442; val_accuracy: 0.9334195859872612 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.48; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22830242193808223; val_accuracy: 0.9342157643312102 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.8; acc: 0.88
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22860159748678754; val_accuracy: 0.9342157643312102 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22848766899792253; val_accuracy: 0.9346138535031847 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.42; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.22758414173961444; val_accuracy: 0.9334195859872612 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.22782535291021797; val_accuracy: 0.933718152866242 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22897201933109076; val_accuracy: 0.9332205414012739 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.22906044661808925; val_accuracy: 0.9340167197452229 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.63; acc: 0.83
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.22708447097213405; val_accuracy: 0.9340167197452229 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.2291485204533407; val_accuracy: 0.9340167197452229 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_290_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 13327798
elements in E: 13327800
fraction nonzero: 0.9999998499377242
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.03
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.11
Batch: 60; loss: 2.29; acc: 0.08
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.28; acc: 0.17
Batch: 140; loss: 2.27; acc: 0.12
Batch: 160; loss: 2.26; acc: 0.23
Batch: 180; loss: 2.25; acc: 0.31
Batch: 200; loss: 2.24; acc: 0.34
Batch: 220; loss: 2.2; acc: 0.42
Batch: 240; loss: 2.2; acc: 0.31
Batch: 260; loss: 2.14; acc: 0.44
Batch: 280; loss: 2.07; acc: 0.39
Batch: 300; loss: 1.93; acc: 0.39
Batch: 320; loss: 1.73; acc: 0.64
Batch: 340; loss: 1.56; acc: 0.55
Batch: 360; loss: 1.22; acc: 0.58
Batch: 380; loss: 1.06; acc: 0.69
Batch: 400; loss: 1.11; acc: 0.61
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.84; acc: 0.73
Batch: 460; loss: 0.9; acc: 0.72
Batch: 480; loss: 0.85; acc: 0.72
Batch: 500; loss: 0.57; acc: 0.78
Batch: 520; loss: 0.87; acc: 0.67
Batch: 540; loss: 0.76; acc: 0.72
Batch: 560; loss: 0.94; acc: 0.69
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.57; acc: 0.8
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.8
Batch: 660; loss: 1.0; acc: 0.7
Batch: 680; loss: 0.62; acc: 0.78
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.8
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.78
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 0.71; acc: 0.8
Batch: 20; loss: 1.0; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.63; acc: 0.77
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.86
Val Epoch over. val_loss: 0.5876162021782747; val_accuracy: 0.8215565286624203 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.77
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.57; acc: 0.78
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.83
Batch: 100; loss: 0.42; acc: 0.81
Batch: 120; loss: 1.09; acc: 0.64
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.54; acc: 0.81
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.81
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.63; acc: 0.8
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.6; acc: 0.83
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.8
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.84
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3628356998228723; val_accuracy: 0.8871417197452229 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.49; acc: 0.83
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.83
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.5461445384344478; val_accuracy: 0.832703025477707 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.78
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.83
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.88
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.81; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.79; acc: 0.75
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.85; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.72
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.608635384375882; val_accuracy: 0.8121019108280255 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.86; acc: 0.83
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3890583244193891; val_accuracy: 0.8679339171974523 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.81
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.79; acc: 0.8
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.36432670782895604; val_accuracy: 0.8815684713375797 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.81
Batch: 20; loss: 0.26; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.34; acc: 0.88
Val Epoch over. val_loss: 0.5168353448248213; val_accuracy: 0.8329020700636943 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.45; acc: 0.81
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.39; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.37; acc: 0.86
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.1; acc: 1.0
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.84
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.84; acc: 0.78
Batch: 140; loss: 0.22; acc: 0.88
Val Epoch over. val_loss: 0.4752705044996966; val_accuracy: 0.8458399681528662 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.86
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.1; acc: 1.0
Batch: 780; loss: 0.47; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.28107547209521005; val_accuracy: 0.910031847133758 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.81
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.522198006890382; val_accuracy: 0.8366839171974523 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.86
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.52; acc: 0.81
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.84
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.22719707258140584; val_accuracy: 0.9293391719745223 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.24780820055040204; val_accuracy: 0.9221735668789809 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.25; acc: 0.98
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21705760138856758; val_accuracy: 0.9338176751592356 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.17; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.83
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.38; acc: 0.84
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.07; acc: 1.0
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.225084225949683; val_accuracy: 0.9300358280254777 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.45; acc: 0.92
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.84
Batch: 260; loss: 0.27; acc: 0.86
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.81
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2165679719868549; val_accuracy: 0.931827229299363 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.84
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.24394898054895886; val_accuracy: 0.924562101910828 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.88
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.21266787286824101; val_accuracy: 0.9360071656050956 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.89
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.2137139762283131; val_accuracy: 0.935609076433121 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.81
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.22091502183751696; val_accuracy: 0.9339171974522293 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.95
Val Epoch over. val_loss: 0.24906562826341125; val_accuracy: 0.9216759554140127 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.32; acc: 0.86
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.21140557500254956; val_accuracy: 0.9363057324840764 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.2096414969178142; val_accuracy: 0.9358081210191083 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2119515556664129; val_accuracy: 0.9386942675159236 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.07; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.61; acc: 0.88
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.20944715898104344; val_accuracy: 0.9378980891719745 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.12; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.55; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.86
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2393867978944805; val_accuracy: 0.9276472929936306 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.07; acc: 1.0
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.84
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.88
Batch: 640; loss: 0.56; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.20734298860974562; val_accuracy: 0.9371019108280255 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.88
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.20888239826864688; val_accuracy: 0.9362062101910829 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.58; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2051962502538019; val_accuracy: 0.9396894904458599 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.86
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.21011314804481854; val_accuracy: 0.9376990445859873 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2061227174582565; val_accuracy: 0.9386942675159236 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.88
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.83
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.20344125154979859; val_accuracy: 0.9375 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.84
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.08; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.2049942876147047; val_accuracy: 0.9382961783439491 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.20580076228139127; val_accuracy: 0.9382961783439491 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2032292219009369; val_accuracy: 0.9382961783439491 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.88
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.20372440982135429; val_accuracy: 0.9372014331210191 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2052318216271841; val_accuracy: 0.9378980891719745 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.92
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.86
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.89
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.20417568143338535; val_accuracy: 0.9375 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.6; acc: 0.86
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2047541511191684; val_accuracy: 0.9387937898089171 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.58; acc: 0.94
Batch: 360; loss: 0.09; acc: 1.0
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2036010156130525; val_accuracy: 0.9394904458598726 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.33; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.91
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.84
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20317414975398854; val_accuracy: 0.9380971337579618 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.09; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20254674310659526; val_accuracy: 0.9389928343949044 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.09; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.20310893809055067; val_accuracy: 0.9380971337579618 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2028266901067298; val_accuracy: 0.9399880573248408 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.15; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.20352992663385383; val_accuracy: 0.9391918789808917 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.5; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20320759041555178; val_accuracy: 0.9384952229299363 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.07; acc: 1.0
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20303223213880875; val_accuracy: 0.9390923566878981 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.8
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2037145037287075; val_accuracy: 0.9391918789808917 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2033055511178674; val_accuracy: 0.9390923566878981 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.15; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20299462880939245; val_accuracy: 0.9386942675159236 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.20303983797740405; val_accuracy: 0.9392914012738853 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_300_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 15549098
elements in E: 15549100
fraction nonzero: 0.9999998713751921
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.12
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.14
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.06
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.27; acc: 0.14
Batch: 160; loss: 2.26; acc: 0.2
Batch: 180; loss: 2.24; acc: 0.33
Batch: 200; loss: 2.24; acc: 0.23
Batch: 220; loss: 2.21; acc: 0.31
Batch: 240; loss: 2.16; acc: 0.34
Batch: 260; loss: 2.12; acc: 0.41
Batch: 280; loss: 2.01; acc: 0.36
Batch: 300; loss: 1.75; acc: 0.58
Batch: 320; loss: 1.54; acc: 0.58
Batch: 340; loss: 1.07; acc: 0.75
Batch: 360; loss: 1.16; acc: 0.59
Batch: 380; loss: 0.77; acc: 0.83
Batch: 400; loss: 0.73; acc: 0.78
Batch: 420; loss: 0.75; acc: 0.72
Batch: 440; loss: 0.64; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.53; acc: 0.78
Batch: 540; loss: 0.69; acc: 0.78
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.6; acc: 0.75
Batch: 640; loss: 0.55; acc: 0.77
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.81
Batch: 700; loss: 0.86; acc: 0.73
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.54; acc: 0.8
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 1.29; train_accuracy: 0.56 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.88
Val Epoch over. val_loss: 0.5251579941458003; val_accuracy: 0.830812101910828 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.75
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.8
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.61; acc: 0.8
Batch: 220; loss: 0.63; acc: 0.78
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.17; acc: 0.98
Batch: 340; loss: 0.42; acc: 0.83
Batch: 360; loss: 0.43; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.42; acc: 0.81
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.57; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.28920808097549305; val_accuracy: 0.9101313694267515 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.81
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.97
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.46; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.29612117180019426; val_accuracy: 0.9079418789808917 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.08; acc: 1.0
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.09; acc: 1.0
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3880773639887761; val_accuracy: 0.884156050955414 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.84; acc: 0.84
Batch: 100; loss: 0.89; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.95
Val Epoch over. val_loss: 0.6097361456816364; val_accuracy: 0.8354896496815286 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.8
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.62; acc: 0.8
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.75
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.8
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3316143194723661; val_accuracy: 0.896297770700637 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.56; acc: 0.83
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.36168723486980814; val_accuracy: 0.8877388535031847 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.81
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.7; acc: 0.86
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.25160845838914253; val_accuracy: 0.9241640127388535 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.42; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.31433299435361933; val_accuracy: 0.9054538216560509 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.84
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.22160665251931566; val_accuracy: 0.934812898089172 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.88
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.89
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.21670218224927879; val_accuracy: 0.9367038216560509 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.98
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19804912160156637; val_accuracy: 0.9391918789808917 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.53; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.23114613865970807; val_accuracy: 0.9312300955414012 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.68; acc: 0.8
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.86
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.3145712777782398; val_accuracy: 0.8967953821656051 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.37; acc: 0.83
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.16; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.23; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.19906709658776878; val_accuracy: 0.9416799363057324 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.84
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.19546380520436415; val_accuracy: 0.9427746815286624 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.08; acc: 1.0
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.41; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.2689914024749379; val_accuracy: 0.9219745222929936 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.22030782312819153; val_accuracy: 0.9344148089171974 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.48; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.41; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23067867720297947; val_accuracy: 0.9319267515923567 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19410740256689157; val_accuracy: 0.943172770700637 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.98
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1875649244305055; val_accuracy: 0.9456608280254777 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.19363771345774838; val_accuracy: 0.9423765923566879 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19858851267179106; val_accuracy: 0.9406847133757962 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.07; acc: 1.0
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.06; acc: 1.0
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.86
Batch: 580; loss: 0.1; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18860459916151254; val_accuracy: 0.9451632165605095 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.47; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.19; acc: 0.91
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19112815359120916; val_accuracy: 0.9435708598726115 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.89
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.91
Batch: 640; loss: 0.06; acc: 1.0
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1857252191206452; val_accuracy: 0.9470541401273885 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19399342004945325; val_accuracy: 0.9424761146496815 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.09; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1949081546656645; val_accuracy: 0.942078025477707 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1897812918492943; val_accuracy: 0.944765127388535 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.14; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.98
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.19155509660767903; val_accuracy: 0.9451632165605095 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18515537570046772; val_accuracy: 0.9461584394904459 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1867704219689035; val_accuracy: 0.9454617834394905 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.88
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18558171697577852; val_accuracy: 0.9446656050955414 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18541249594870646; val_accuracy: 0.9455613057324841 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18457939945588445; val_accuracy: 0.9455613057324841 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.86
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18611544605558086; val_accuracy: 0.9451632165605095 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18580980629772897; val_accuracy: 0.9455613057324841 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18566289299707503; val_accuracy: 0.9449641719745223 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18466821248838855; val_accuracy: 0.9454617834394905 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.91
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.53; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1868240022260672; val_accuracy: 0.9449641719745223 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.08; acc: 1.0
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18459647075290891; val_accuracy: 0.9459593949044586 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.97
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18456280755863827; val_accuracy: 0.9463574840764332 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18407694760496449; val_accuracy: 0.9462579617834395 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18437642392933748; val_accuracy: 0.9465565286624203 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1837940555374334; val_accuracy: 0.9461584394904459 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18404872938516034; val_accuracy: 0.9461584394904459 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.89
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.88
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.88
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1846668249957121; val_accuracy: 0.9460589171974523 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.92
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.07; acc: 1.0
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.184786664072875; val_accuracy: 0.9455613057324841 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.11; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18463452646781683; val_accuracy: 0.945859872611465 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.08; acc: 1.0
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.86
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1838302883040753; val_accuracy: 0.9467555732484076 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_350_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 17770398
elements in E: 17770400
fraction nonzero: 0.9999998874532932
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.17
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.29; acc: 0.16
Batch: 60; loss: 2.28; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.25; acc: 0.23
Batch: 120; loss: 2.25; acc: 0.33
Batch: 140; loss: 2.24; acc: 0.3
Batch: 160; loss: 2.22; acc: 0.3
Batch: 180; loss: 2.19; acc: 0.39
Batch: 200; loss: 2.11; acc: 0.44
Batch: 220; loss: 2.09; acc: 0.41
Batch: 240; loss: 1.9; acc: 0.52
Batch: 260; loss: 1.82; acc: 0.34
Batch: 280; loss: 1.58; acc: 0.47
Batch: 300; loss: 1.5; acc: 0.64
Batch: 320; loss: 1.02; acc: 0.66
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.69; acc: 0.75
Batch: 380; loss: 1.25; acc: 0.64
Batch: 400; loss: 0.94; acc: 0.7
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.77
Batch: 460; loss: 0.81; acc: 0.69
Batch: 480; loss: 0.42; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.77; acc: 0.78
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.86
Batch: 700; loss: 0.67; acc: 0.78
Batch: 720; loss: 0.39; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 1.17; train_accuracy: 0.62 

Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.59; acc: 0.55
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 1.15; acc: 0.7
Batch: 80; loss: 0.85; acc: 0.73
Batch: 100; loss: 1.23; acc: 0.67
Batch: 120; loss: 1.39; acc: 0.66
Batch: 140; loss: 0.48; acc: 0.86
Val Epoch over. val_loss: 0.987264033335789; val_accuracy: 0.7364649681528662 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.7
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.8
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.81
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.81; acc: 0.81
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.63; acc: 0.8
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.88
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.76; acc: 0.77
Batch: 20; loss: 0.72; acc: 0.75
Batch: 40; loss: 0.65; acc: 0.8
Batch: 60; loss: 1.12; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.79; acc: 0.7
Batch: 140; loss: 0.57; acc: 0.83
Val Epoch over. val_loss: 0.8540432956188347; val_accuracy: 0.7544785031847133 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.72
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.84
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.69; acc: 0.73
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.83
Batch: 60; loss: 1.14; acc: 0.67
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.76; acc: 0.84
Batch: 120; loss: 0.93; acc: 0.7
Batch: 140; loss: 0.5; acc: 0.8
Val Epoch over. val_loss: 0.7333235303117971; val_accuracy: 0.7719944267515924 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.83
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.81
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2234948602071993; val_accuracy: 0.9330214968152867 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.52; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.84
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.86
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.21727422140775973; val_accuracy: 0.9346138535031847 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.08; acc: 1.0
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.95
Val Epoch over. val_loss: 0.21269327582447392; val_accuracy: 0.9360071656050956 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.36; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.18739106482381274; val_accuracy: 0.9461584394904459 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.86
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19802056993268857; val_accuracy: 0.9421775477707006 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.54; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.18668600907371302; val_accuracy: 0.9430732484076433 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2885303523415213; val_accuracy: 0.9114251592356688 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.51; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16476087853502316; val_accuracy: 0.9502388535031847 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.89
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2175357992861681; val_accuracy: 0.9347133757961783 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.98
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1629548203793301; val_accuracy: 0.953125 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.37; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17264010145026407; val_accuracy: 0.9492436305732485 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17150222405696372; val_accuracy: 0.9494426751592356 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.22; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16171685625223597; val_accuracy: 0.9501393312101911 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.07; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.88
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.63; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.89
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.95
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16865565036987043; val_accuracy: 0.9486464968152867 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1586497037368975; val_accuracy: 0.9540207006369427 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.15; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.06; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.1954673905471328; val_accuracy: 0.9421775477707006 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.88
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.2354201706256836; val_accuracy: 0.9313296178343949 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.49; acc: 0.89
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.89
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15192304217037123; val_accuracy: 0.955812101910828 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16018063098097304; val_accuracy: 0.9539211783439491 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1529061271553966; val_accuracy: 0.9556130573248408 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16339852136506397; val_accuracy: 0.9537221337579618 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15380232097806446; val_accuracy: 0.9552149681528662 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.86
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.88
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1531515152780873; val_accuracy: 0.9566082802547771 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1535499247538436; val_accuracy: 0.9554140127388535 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15483474871440298; val_accuracy: 0.9553144904458599 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1561035961149984; val_accuracy: 0.9555135350318471 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.92
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.91
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16471427942442288; val_accuracy: 0.9525278662420382 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1504608350242399; val_accuracy: 0.9563097133757962 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1550423736167941; val_accuracy: 0.9557125796178344 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.16; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1504590202621214; val_accuracy: 0.9563097133757962 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15257588662444407; val_accuracy: 0.9554140127388535 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15375774455772842; val_accuracy: 0.9553144904458599 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15126148964853803; val_accuracy: 0.9556130573248408 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15188485661604603; val_accuracy: 0.9553144904458599 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.91
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.152275304173588; val_accuracy: 0.9557125796178344 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15242055179491923; val_accuracy: 0.9563097133757962 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.27; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14983889534120348; val_accuracy: 0.9563097133757962 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.91
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1520332205257598; val_accuracy: 0.9567078025477707 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15134973102694105; val_accuracy: 0.9557125796178344 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.91
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.89
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1508619657177834; val_accuracy: 0.9560111464968153 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14995130958261005; val_accuracy: 0.9574044585987261 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15073191457588203; val_accuracy: 0.9574044585987261 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14943997633115502; val_accuracy: 0.9576035031847133 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14974650823671348; val_accuracy: 0.9563097133757962 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.91
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.14; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15090517151602514; val_accuracy: 0.9565087579617835 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.89
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.34; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.92
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15007211346250432; val_accuracy: 0.9562101910828026 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14963322068760349; val_accuracy: 0.9564092356687898 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_400_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 19991698
elements in E: 19991700
fraction nonzero: 0.9999998999584828
Epoch 1 start
The current lr is: 1.0
/home/llang/thesis-intrinsic-dimension/logging_helper.py:44: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax1 = plt.subplots()
Batch: 0; loss: 2.31; acc: 0.11
Batch: 20; loss: 2.29; acc: 0.17
Batch: 40; loss: 2.29; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.14
Batch: 80; loss: 2.28; acc: 0.14
Batch: 100; loss: 2.27; acc: 0.17
Batch: 120; loss: 2.24; acc: 0.34
Batch: 140; loss: 2.2; acc: 0.41
Batch: 160; loss: 2.17; acc: 0.36
Batch: 180; loss: 2.1; acc: 0.39
Batch: 200; loss: 2.0; acc: 0.33
Batch: 220; loss: 1.68; acc: 0.55
Batch: 240; loss: 1.35; acc: 0.62
Batch: 260; loss: 0.92; acc: 0.7
Batch: 280; loss: 1.02; acc: 0.75
Batch: 300; loss: 0.71; acc: 0.77
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 0.65; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.68; acc: 0.75
Batch: 440; loss: 0.46; acc: 0.83
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 1.03; train_accuracy: 0.66 

Batch: 0; loss: 0.62; acc: 0.77
Batch: 20; loss: 0.9; acc: 0.69
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.74; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.97; acc: 0.72
Batch: 140; loss: 0.34; acc: 0.91
Val Epoch over. val_loss: 0.6031208684679809; val_accuracy: 0.8233479299363057 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.7
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.17; acc: 0.98
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.2549836208130334; val_accuracy: 0.9227707006369427 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.25; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.15; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.3008813162091052; val_accuracy: 0.9105294585987261 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.91
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.47; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2933659895922348; val_accuracy: 0.9095342356687898 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.4; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.30680300262133786; val_accuracy: 0.9021695859872612 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.24578629704607521; val_accuracy: 0.9285429936305732 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.78
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.25820500592279966; val_accuracy: 0.9188893312101911 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.94
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.24336033620557207; val_accuracy: 0.925656847133758 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.59; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.92
Val Epoch over. val_loss: 0.6197305748796766; val_accuracy: 0.8280254777070064 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.86
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.86
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.24891344368884896; val_accuracy: 0.9227707006369427 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1666184146977534; val_accuracy: 0.9501393312101911 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16603023245645937; val_accuracy: 0.9510350318471338 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1728769409571104; val_accuracy: 0.9508359872611465 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16169018788963177; val_accuracy: 0.9527269108280255 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.08; acc: 1.0
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.89
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.83
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.31858040311723756; val_accuracy: 0.9020700636942676 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.88
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.31; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.17122958941017366; val_accuracy: 0.9506369426751592 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.91
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.07; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.31; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.06; acc: 1.0
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.18065755586525437; val_accuracy: 0.9468550955414012 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.47; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.14; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.19196859889539183; val_accuracy: 0.9416799363057324 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.26; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.92
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.21044009403106134; val_accuracy: 0.9372014331210191 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.19477804546143598; val_accuracy: 0.9408837579617835 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.84
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16483346741812624; val_accuracy: 0.9516321656050956 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.95
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.91
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1572561005877841; val_accuracy: 0.9537221337579618 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1542218850130678; val_accuracy: 0.9541202229299363 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.91
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.158334997381754; val_accuracy: 0.9541202229299363 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.11; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.91
Batch: 540; loss: 0.1; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.15618279562064796; val_accuracy: 0.9536226114649682 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.91
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.25; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.15693416053749573; val_accuracy: 0.9535230891719745 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.16227691290769608; val_accuracy: 0.9530254777070064 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.08; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.94
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.15742601258502265; val_accuracy: 0.9534235668789809 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1576021414511143; val_accuracy: 0.9543192675159236 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.88
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.29; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1588525945071582; val_accuracy: 0.9546178343949044 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15331873596663687; val_accuracy: 0.9563097133757962 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.88
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1537579424252176; val_accuracy: 0.9575039808917197 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 0.98
Val Epoch over. val_loss: 0.15473558133242615; val_accuracy: 0.9555135350318471 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.89
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 0.98
Val Epoch over. val_loss: 0.1532164348680882; val_accuracy: 0.955015923566879 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1540041425306896; val_accuracy: 0.955015923566879 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.01; acc: 0.98
Val Epoch over. val_loss: 0.1539518289790032; val_accuracy: 0.9545183121019108 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.98
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1544025244108241; val_accuracy: 0.9545183121019108 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.15499800009427556; val_accuracy: 0.9544187898089171 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15244715756908722; val_accuracy: 0.9562101910828026 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.94
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.98
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.14; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.92
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1547788485505019; val_accuracy: 0.955812101910828 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.92
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.1543986312926385; val_accuracy: 0.9543192675159236 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15327902046880523; val_accuracy: 0.9553144904458599 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.15423209945060265; val_accuracy: 0.9544187898089171 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15320535961562282; val_accuracy: 0.9549164012738853 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.97
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15343832414431177; val_accuracy: 0.9547173566878981 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.07; acc: 1.0
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15278659158261718; val_accuracy: 0.9557125796178344 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15312246625922668; val_accuracy: 0.9553144904458599 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15382892397606068; val_accuracy: 0.9551154458598726 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.92
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15286132956671108; val_accuracy: 0.9563097133757962 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.01; acc: 0.98
Val Epoch over. val_loss: 0.15388325160476052; val_accuracy: 0.9553144904458599 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_450_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 22212998
elements in E: 22213000
fraction nonzero: 0.9999999099626345
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.3; acc: 0.03
Batch: 40; loss: 2.29; acc: 0.12
Batch: 60; loss: 2.29; acc: 0.03
Batch: 80; loss: 2.29; acc: 0.02
Batch: 100; loss: 2.26; acc: 0.17
Batch: 120; loss: 2.24; acc: 0.34
Batch: 140; loss: 2.22; acc: 0.36
Batch: 160; loss: 2.19; acc: 0.28
Batch: 180; loss: 2.13; acc: 0.36
Batch: 200; loss: 2.01; acc: 0.48
Batch: 220; loss: 1.61; acc: 0.59
Batch: 240; loss: 1.33; acc: 0.59
Batch: 260; loss: 1.25; acc: 0.58
Batch: 280; loss: 0.84; acc: 0.77
Batch: 300; loss: 0.81; acc: 0.72
Batch: 320; loss: 0.72; acc: 0.75
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.55; acc: 0.81
Batch: 380; loss: 0.6; acc: 0.78
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.78
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.43; acc: 0.83
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.81
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.91
Train Epoch over. train_loss: 1.02; train_accuracy: 0.67 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.31363283577048856; val_accuracy: 0.9017714968152867 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.52; acc: 0.83
Batch: 220; loss: 0.15; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.46; acc: 0.81
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.24581286294540022; val_accuracy: 0.9254578025477707 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.23; acc: 0.89
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2274120332566416; val_accuracy: 0.9298367834394905 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.20848780417233514; val_accuracy: 0.9392914012738853 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.55; acc: 0.84
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.58; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.4; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.89
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.19407283142209053; val_accuracy: 0.9432722929936306 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.2; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.27; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.24755116006371322; val_accuracy: 0.9240644904458599 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.22; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19410687367058105; val_accuracy: 0.9415804140127388 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.89
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.5; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.54; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.74; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.3592956242430362; val_accuracy: 0.8908240445859873 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.88
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 1.13; acc: 0.72
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 1.03; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.92
Val Epoch over. val_loss: 0.6862668879092879; val_accuracy: 0.8306130573248408 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2016138386954168; val_accuracy: 0.940187101910828 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.89
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13825374253236564; val_accuracy: 0.9590963375796179 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.91
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.88
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1350377125274034; val_accuracy: 0.9596934713375797 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.91
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1540700459651127; val_accuracy: 0.9528264331210191 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13876006041600067; val_accuracy: 0.9574044585987261 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13759070558912434; val_accuracy: 0.9583001592356688 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.86
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.17393570021981267; val_accuracy: 0.9487460191082803 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1346705238791598; val_accuracy: 0.9597929936305732 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.55; acc: 0.92
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13644875014188942; val_accuracy: 0.9601910828025477 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1536122681276434; val_accuracy: 0.9532245222929936 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.21124979025524132; val_accuracy: 0.9373009554140127 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.92
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1271227164680411; val_accuracy: 0.9610867834394905 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.23; acc: 0.89
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13013892374031102; val_accuracy: 0.9611863057324841 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.89
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12890831037027062; val_accuracy: 0.9600915605095541 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.91
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14029895994741065; val_accuracy: 0.9581011146496815 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.92
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12677903207624033; val_accuracy: 0.9606886942675159 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.92
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14117203607775602; val_accuracy: 0.9567078025477707 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12747718220588508; val_accuracy: 0.9616839171974523 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.56; acc: 0.92
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12587004071872704; val_accuracy: 0.9627786624203821 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.25; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1312568297455455; val_accuracy: 0.9608877388535032 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.25; acc: 0.97
Batch: 140; loss: 0.5; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1377611577415922; val_accuracy: 0.957703025477707 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12576815181049952; val_accuracy: 0.9619824840764332 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.5; acc: 0.92
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1251570508239945; val_accuracy: 0.9627786624203821 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12361928297408448; val_accuracy: 0.963077229299363 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12731845390976995; val_accuracy: 0.9602906050955414 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.89
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.53; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.89
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1241410539550766; val_accuracy: 0.9623805732484076 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12585781336684895; val_accuracy: 0.9609872611464968 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12381187428951643; val_accuracy: 0.9627786624203821 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.92
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12646793238344087; val_accuracy: 0.961484872611465 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.92
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1250509671083871; val_accuracy: 0.9642714968152867 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.03; acc: 0.97
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.92
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12495496905865563; val_accuracy: 0.9620820063694268 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.07; acc: 1.0
Batch: 600; loss: 0.15; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12326302424452867; val_accuracy: 0.9628781847133758 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1236021285698672; val_accuracy: 0.9636743630573248 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12443598864042456; val_accuracy: 0.9628781847133758 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.48; acc: 0.92
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.88
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12345276385023715; val_accuracy: 0.9623805732484076 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.06; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1230207262026846; val_accuracy: 0.963077229299363 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1236980302247462; val_accuracy: 0.9627786624203821 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.92
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12358973608322583; val_accuracy: 0.9637738853503185 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.19; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12334318174298402; val_accuracy: 0.963077229299363 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12428065413122724; val_accuracy: 0.9637738853503185 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12409494146941953; val_accuracy: 0.963077229299363 

plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_500_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
plots/subspace_training/lenet/2020-01-19 22:58:43/d_dim_XXXXX_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
/var/spool/slurm-llnl/slurmd/job4386324/slurm_script: line 26: --print_freq=20: command not found
