nonzero elements in E: 10262
elements in E: 2221300
fraction nonzero: 0.004619817224148021
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 5.45; acc: 0.2
Batch: 40; loss: 5.05; acc: 0.28
Batch: 60; loss: 5.32; acc: 0.25
Batch: 80; loss: 4.47; acc: 0.25
Batch: 100; loss: 5.35; acc: 0.23
Batch: 120; loss: 4.17; acc: 0.3
Batch: 140; loss: 5.16; acc: 0.3
Batch: 160; loss: 4.9; acc: 0.31
Batch: 180; loss: 4.3; acc: 0.3
Batch: 200; loss: 3.69; acc: 0.34
Batch: 220; loss: 5.62; acc: 0.27
Batch: 240; loss: 5.36; acc: 0.33
Batch: 260; loss: 4.99; acc: 0.22
Batch: 280; loss: 4.56; acc: 0.36
Batch: 300; loss: 4.65; acc: 0.34
Batch: 320; loss: 4.63; acc: 0.34
Batch: 340; loss: 4.08; acc: 0.38
Batch: 360; loss: 4.18; acc: 0.39
Batch: 380; loss: 4.24; acc: 0.45
Batch: 400; loss: 4.04; acc: 0.39
Batch: 420; loss: 4.53; acc: 0.39
Batch: 440; loss: 3.48; acc: 0.41
Batch: 460; loss: 4.05; acc: 0.39
Batch: 480; loss: 4.83; acc: 0.36
Batch: 500; loss: 4.46; acc: 0.3
Batch: 520; loss: 4.8; acc: 0.39
Batch: 540; loss: 4.94; acc: 0.27
Batch: 560; loss: 4.21; acc: 0.42
Batch: 580; loss: 3.72; acc: 0.38
Batch: 600; loss: 4.78; acc: 0.38
Batch: 620; loss: 5.38; acc: 0.39
Train Epoch over. train_loss: 4.75; train_accuracy: 0.33 

Batch: 0; loss: 3.66; acc: 0.41
Batch: 20; loss: 5.29; acc: 0.27
Batch: 40; loss: 4.48; acc: 0.44
Batch: 60; loss: 4.35; acc: 0.39
Batch: 80; loss: 4.48; acc: 0.34
Batch: 100; loss: 4.64; acc: 0.38
Batch: 120; loss: 4.08; acc: 0.45
Batch: 140; loss: 4.81; acc: 0.45
Val Epoch over. val_loss: 4.3494317774560045; val_accuracy: 0.38923168789808915 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 4.35; acc: 0.33
Batch: 20; loss: 5.23; acc: 0.31
Batch: 40; loss: 4.8; acc: 0.33
Batch: 60; loss: 3.82; acc: 0.38
Batch: 80; loss: 5.14; acc: 0.27
Batch: 100; loss: 4.08; acc: 0.47
Batch: 120; loss: 5.61; acc: 0.42
Batch: 140; loss: 3.96; acc: 0.47
Batch: 160; loss: 4.53; acc: 0.45
Batch: 180; loss: 4.99; acc: 0.3
Batch: 200; loss: 4.49; acc: 0.45
Batch: 220; loss: 3.3; acc: 0.44
Batch: 240; loss: 4.26; acc: 0.38
Batch: 260; loss: 4.53; acc: 0.34
Batch: 280; loss: 5.45; acc: 0.33
Batch: 300; loss: 4.46; acc: 0.31
Batch: 320; loss: 5.44; acc: 0.34
Batch: 340; loss: 4.6; acc: 0.42
Batch: 360; loss: 4.88; acc: 0.38
Batch: 380; loss: 3.49; acc: 0.55
Batch: 400; loss: 3.82; acc: 0.38
Batch: 420; loss: 5.11; acc: 0.33
Batch: 440; loss: 3.52; acc: 0.48
Batch: 460; loss: 4.0; acc: 0.45
Batch: 480; loss: 3.79; acc: 0.47
Batch: 500; loss: 5.57; acc: 0.41
Batch: 520; loss: 5.27; acc: 0.39
Batch: 540; loss: 3.37; acc: 0.47
Batch: 560; loss: 3.53; acc: 0.44
Batch: 580; loss: 4.66; acc: 0.28
Batch: 600; loss: 4.71; acc: 0.33
Batch: 620; loss: 4.2; acc: 0.31
Train Epoch over. train_loss: 4.48; train_accuracy: 0.39 

Batch: 0; loss: 4.31; acc: 0.36
Batch: 20; loss: 4.93; acc: 0.36
Batch: 40; loss: 4.55; acc: 0.48
Batch: 60; loss: 4.43; acc: 0.47
Batch: 80; loss: 4.78; acc: 0.33
Batch: 100; loss: 5.03; acc: 0.31
Batch: 120; loss: 4.69; acc: 0.41
Batch: 140; loss: 5.28; acc: 0.39
Val Epoch over. val_loss: 4.505714762742352; val_accuracy: 0.39152070063694266 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 4.13; acc: 0.39
Batch: 20; loss: 5.25; acc: 0.3
Batch: 40; loss: 5.23; acc: 0.31
Batch: 60; loss: 4.82; acc: 0.34
Batch: 80; loss: 5.01; acc: 0.3
Batch: 100; loss: 5.28; acc: 0.31
Batch: 120; loss: 5.3; acc: 0.33
Batch: 140; loss: 5.24; acc: 0.3
Batch: 160; loss: 4.22; acc: 0.31
Batch: 180; loss: 4.2; acc: 0.45
Batch: 200; loss: 3.58; acc: 0.5
Batch: 220; loss: 5.28; acc: 0.45
Batch: 240; loss: 5.53; acc: 0.38
Batch: 260; loss: 3.81; acc: 0.48
Batch: 280; loss: 4.21; acc: 0.41
Batch: 300; loss: 3.81; acc: 0.47
Batch: 320; loss: 3.84; acc: 0.44
Batch: 340; loss: 4.77; acc: 0.36
Batch: 360; loss: 4.74; acc: 0.41
Batch: 380; loss: 3.69; acc: 0.53
Batch: 400; loss: 6.2; acc: 0.28
Batch: 420; loss: 5.24; acc: 0.3
Batch: 440; loss: 3.58; acc: 0.44
Batch: 460; loss: 4.34; acc: 0.44
Batch: 480; loss: 5.54; acc: 0.41
Batch: 500; loss: 4.3; acc: 0.47
Batch: 520; loss: 4.06; acc: 0.41
Batch: 540; loss: 3.85; acc: 0.48
Batch: 560; loss: 4.27; acc: 0.33
Batch: 580; loss: 4.9; acc: 0.42
Batch: 600; loss: 3.59; acc: 0.44
Batch: 620; loss: 5.32; acc: 0.38
Train Epoch over. train_loss: 4.46; train_accuracy: 0.39 

Batch: 0; loss: 4.21; acc: 0.45
Batch: 20; loss: 4.67; acc: 0.39
Batch: 40; loss: 4.36; acc: 0.44
Batch: 60; loss: 4.45; acc: 0.44
Batch: 80; loss: 4.77; acc: 0.38
Batch: 100; loss: 4.24; acc: 0.48
Batch: 120; loss: 3.91; acc: 0.47
Batch: 140; loss: 5.36; acc: 0.34
Val Epoch over. val_loss: 4.455794177996885; val_accuracy: 0.41132563694267515 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 3.46; acc: 0.45
Batch: 20; loss: 5.32; acc: 0.36
Batch: 40; loss: 3.61; acc: 0.42
Batch: 60; loss: 3.95; acc: 0.5
Batch: 80; loss: 4.83; acc: 0.25
Batch: 100; loss: 5.51; acc: 0.39
Batch: 120; loss: 4.52; acc: 0.36
Batch: 140; loss: 5.07; acc: 0.31
Batch: 160; loss: 2.92; acc: 0.47
Batch: 180; loss: 4.73; acc: 0.38
Batch: 200; loss: 4.37; acc: 0.42
Batch: 220; loss: 5.17; acc: 0.3
Batch: 240; loss: 4.81; acc: 0.45
Batch: 260; loss: 3.95; acc: 0.36
Batch: 280; loss: 5.32; acc: 0.38
Batch: 300; loss: 4.39; acc: 0.38
Batch: 320; loss: 5.36; acc: 0.3
Batch: 340; loss: 4.27; acc: 0.41
Batch: 360; loss: 3.19; acc: 0.5
Batch: 380; loss: 4.73; acc: 0.38
Batch: 400; loss: 4.3; acc: 0.44
Batch: 420; loss: 4.35; acc: 0.36
Batch: 440; loss: 3.94; acc: 0.39
Batch: 460; loss: 5.17; acc: 0.36
Batch: 480; loss: 4.09; acc: 0.34
Batch: 500; loss: 3.91; acc: 0.39
Batch: 520; loss: 5.22; acc: 0.44
Batch: 540; loss: 4.49; acc: 0.33
Batch: 560; loss: 4.84; acc: 0.34
Batch: 580; loss: 3.22; acc: 0.42
Batch: 600; loss: 4.59; acc: 0.31
Batch: 620; loss: 4.49; acc: 0.36
Train Epoch over. train_loss: 4.44; train_accuracy: 0.39 

Batch: 0; loss: 3.87; acc: 0.38
Batch: 20; loss: 5.38; acc: 0.31
Batch: 40; loss: 4.45; acc: 0.45
Batch: 60; loss: 4.2; acc: 0.42
Batch: 80; loss: 4.97; acc: 0.31
Batch: 100; loss: 4.61; acc: 0.36
Batch: 120; loss: 4.55; acc: 0.44
Batch: 140; loss: 5.06; acc: 0.39
Val Epoch over. val_loss: 4.438031570167299; val_accuracy: 0.38753980891719747 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 3.69; acc: 0.47
Batch: 20; loss: 5.71; acc: 0.33
Batch: 40; loss: 5.85; acc: 0.28
Batch: 60; loss: 3.83; acc: 0.38
Batch: 80; loss: 3.82; acc: 0.34
Batch: 100; loss: 4.84; acc: 0.39
Batch: 120; loss: 4.32; acc: 0.47
Batch: 140; loss: 4.26; acc: 0.48
Batch: 160; loss: 4.19; acc: 0.42
Batch: 180; loss: 4.21; acc: 0.38
Batch: 200; loss: 4.75; acc: 0.47
Batch: 220; loss: 5.72; acc: 0.36
Batch: 240; loss: 5.43; acc: 0.3
Batch: 260; loss: 3.74; acc: 0.52
Batch: 280; loss: 4.03; acc: 0.39
Batch: 300; loss: 4.29; acc: 0.33
Batch: 320; loss: 5.38; acc: 0.38
Batch: 340; loss: 5.36; acc: 0.33
Batch: 360; loss: 4.37; acc: 0.47
Batch: 380; loss: 4.62; acc: 0.44
Batch: 400; loss: 3.08; acc: 0.42
Batch: 420; loss: 5.07; acc: 0.34
Batch: 440; loss: 4.48; acc: 0.44
Batch: 460; loss: 5.02; acc: 0.31
Batch: 480; loss: 3.39; acc: 0.41
Batch: 500; loss: 5.39; acc: 0.31
Batch: 520; loss: 3.74; acc: 0.53
Batch: 540; loss: 5.27; acc: 0.28
Batch: 560; loss: 4.31; acc: 0.38
Batch: 580; loss: 3.95; acc: 0.39
Batch: 600; loss: 4.55; acc: 0.41
Batch: 620; loss: 3.81; acc: 0.5
Train Epoch over. train_loss: 4.44; train_accuracy: 0.4 

Batch: 0; loss: 3.77; acc: 0.44
Batch: 20; loss: 5.91; acc: 0.25
Batch: 40; loss: 4.45; acc: 0.41
Batch: 60; loss: 3.81; acc: 0.42
Batch: 80; loss: 4.65; acc: 0.3
Batch: 100; loss: 5.42; acc: 0.31
Batch: 120; loss: 4.65; acc: 0.36
Batch: 140; loss: 5.14; acc: 0.34
Val Epoch over. val_loss: 4.663862869238398; val_accuracy: 0.3411624203821656 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 3.66; acc: 0.44
Batch: 20; loss: 5.9; acc: 0.33
Batch: 40; loss: 5.34; acc: 0.28
Batch: 60; loss: 5.09; acc: 0.36
Batch: 80; loss: 5.35; acc: 0.31
Batch: 100; loss: 4.19; acc: 0.36
Batch: 120; loss: 5.58; acc: 0.36
Batch: 140; loss: 4.5; acc: 0.39
Batch: 160; loss: 5.05; acc: 0.38
Batch: 180; loss: 3.38; acc: 0.55
Batch: 200; loss: 4.2; acc: 0.41
Batch: 220; loss: 3.54; acc: 0.44
Batch: 240; loss: 4.62; acc: 0.34
Batch: 260; loss: 5.88; acc: 0.31
Batch: 280; loss: 4.77; acc: 0.3
Batch: 300; loss: 3.55; acc: 0.39
Batch: 320; loss: 4.2; acc: 0.41
Batch: 340; loss: 5.08; acc: 0.36
Batch: 360; loss: 4.14; acc: 0.41
Batch: 380; loss: 3.96; acc: 0.41
Batch: 400; loss: 4.02; acc: 0.36
Batch: 420; loss: 3.7; acc: 0.39
Batch: 440; loss: 5.04; acc: 0.39
Batch: 460; loss: 4.19; acc: 0.47
Batch: 480; loss: 3.7; acc: 0.45
Batch: 500; loss: 4.24; acc: 0.3
Batch: 520; loss: 5.03; acc: 0.38
Batch: 540; loss: 5.68; acc: 0.33
Batch: 560; loss: 3.24; acc: 0.42
Batch: 580; loss: 3.84; acc: 0.44
Batch: 600; loss: 4.25; acc: 0.45
Batch: 620; loss: 4.36; acc: 0.45
Train Epoch over. train_loss: 4.43; train_accuracy: 0.39 

Batch: 0; loss: 4.43; acc: 0.33
Batch: 20; loss: 5.24; acc: 0.33
Batch: 40; loss: 4.88; acc: 0.45
Batch: 60; loss: 4.6; acc: 0.44
Batch: 80; loss: 5.83; acc: 0.27
Batch: 100; loss: 4.7; acc: 0.36
Batch: 120; loss: 5.13; acc: 0.45
Batch: 140; loss: 5.52; acc: 0.36
Val Epoch over. val_loss: 4.69909007685959; val_accuracy: 0.38923168789808915 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 3.96; acc: 0.45
Batch: 20; loss: 4.13; acc: 0.44
Batch: 40; loss: 4.75; acc: 0.44
Batch: 60; loss: 4.28; acc: 0.27
Batch: 80; loss: 3.82; acc: 0.42
Batch: 100; loss: 3.75; acc: 0.44
Batch: 120; loss: 5.67; acc: 0.3
Batch: 140; loss: 4.94; acc: 0.39
Batch: 160; loss: 4.73; acc: 0.31
Batch: 180; loss: 5.12; acc: 0.45
Batch: 200; loss: 4.09; acc: 0.44
Batch: 220; loss: 4.61; acc: 0.31
Batch: 240; loss: 5.02; acc: 0.33
Batch: 260; loss: 3.59; acc: 0.45
Batch: 280; loss: 4.2; acc: 0.36
Batch: 300; loss: 4.08; acc: 0.44
Batch: 320; loss: 4.44; acc: 0.44
Batch: 340; loss: 5.35; acc: 0.28
Batch: 360; loss: 3.82; acc: 0.47
Batch: 380; loss: 4.1; acc: 0.41
Batch: 400; loss: 3.69; acc: 0.39
Batch: 420; loss: 4.73; acc: 0.34
Batch: 440; loss: 4.82; acc: 0.34
Batch: 460; loss: 5.05; acc: 0.38
Batch: 480; loss: 5.27; acc: 0.44
Batch: 500; loss: 4.37; acc: 0.39
Batch: 520; loss: 4.98; acc: 0.34
Batch: 540; loss: 3.59; acc: 0.42
Batch: 560; loss: 4.2; acc: 0.41
Batch: 580; loss: 3.72; acc: 0.38
Batch: 600; loss: 4.05; acc: 0.47
Batch: 620; loss: 5.2; acc: 0.39
Train Epoch over. train_loss: 4.46; train_accuracy: 0.39 

Batch: 0; loss: 3.98; acc: 0.36
Batch: 20; loss: 6.1; acc: 0.27
Batch: 40; loss: 4.93; acc: 0.52
Batch: 60; loss: 4.88; acc: 0.41
Batch: 80; loss: 6.21; acc: 0.25
Batch: 100; loss: 5.18; acc: 0.33
Batch: 120; loss: 4.85; acc: 0.41
Batch: 140; loss: 6.02; acc: 0.27
Val Epoch over. val_loss: 4.810358635179556; val_accuracy: 0.3744028662420382 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 3.9; acc: 0.44
Batch: 20; loss: 5.11; acc: 0.3
Batch: 40; loss: 4.86; acc: 0.44
Batch: 60; loss: 3.95; acc: 0.44
Batch: 80; loss: 4.75; acc: 0.41
Batch: 100; loss: 4.53; acc: 0.36
Batch: 120; loss: 4.71; acc: 0.44
Batch: 140; loss: 4.22; acc: 0.31
Batch: 160; loss: 3.1; acc: 0.45
Batch: 180; loss: 4.43; acc: 0.38
Batch: 200; loss: 4.21; acc: 0.47
Batch: 220; loss: 4.55; acc: 0.38
Batch: 240; loss: 5.25; acc: 0.44
Batch: 260; loss: 4.95; acc: 0.47
Batch: 280; loss: 4.61; acc: 0.36
Batch: 300; loss: 3.49; acc: 0.45
Batch: 320; loss: 3.47; acc: 0.44
Batch: 340; loss: 3.78; acc: 0.36
Batch: 360; loss: 5.32; acc: 0.38
Batch: 380; loss: 2.89; acc: 0.55
Batch: 400; loss: 4.65; acc: 0.27
Batch: 420; loss: 4.78; acc: 0.47
Batch: 440; loss: 4.56; acc: 0.42
Batch: 460; loss: 5.06; acc: 0.33
Batch: 480; loss: 3.72; acc: 0.47
Batch: 500; loss: 5.28; acc: 0.34
Batch: 520; loss: 3.53; acc: 0.48
Batch: 540; loss: 4.78; acc: 0.33
Batch: 560; loss: 3.93; acc: 0.36
Batch: 580; loss: 2.54; acc: 0.59
Batch: 600; loss: 3.83; acc: 0.47
Batch: 620; loss: 6.14; acc: 0.25
Train Epoch over. train_loss: 4.47; train_accuracy: 0.4 

Batch: 0; loss: 4.74; acc: 0.34
Batch: 20; loss: 5.36; acc: 0.34
Batch: 40; loss: 4.95; acc: 0.47
Batch: 60; loss: 5.03; acc: 0.41
Batch: 80; loss: 5.97; acc: 0.34
Batch: 100; loss: 4.31; acc: 0.48
Batch: 120; loss: 4.72; acc: 0.36
Batch: 140; loss: 5.82; acc: 0.3
Val Epoch over. val_loss: 4.893165823760306; val_accuracy: 0.37490047770700635 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 4.1; acc: 0.45
Batch: 20; loss: 6.47; acc: 0.33
Batch: 40; loss: 3.4; acc: 0.44
Batch: 60; loss: 4.41; acc: 0.45
Batch: 80; loss: 4.23; acc: 0.39
Batch: 100; loss: 4.54; acc: 0.42
Batch: 120; loss: 5.01; acc: 0.27
Batch: 140; loss: 3.75; acc: 0.41
Batch: 160; loss: 4.53; acc: 0.33
Batch: 180; loss: 6.29; acc: 0.28
Batch: 200; loss: 4.33; acc: 0.36
Batch: 220; loss: 4.86; acc: 0.34
Batch: 240; loss: 5.1; acc: 0.23
Batch: 260; loss: 5.02; acc: 0.39
Batch: 280; loss: 5.98; acc: 0.23
Batch: 300; loss: 4.58; acc: 0.45
Batch: 320; loss: 4.27; acc: 0.42
Batch: 340; loss: 3.29; acc: 0.42
Batch: 360; loss: 4.88; acc: 0.42
Batch: 380; loss: 4.63; acc: 0.39
Batch: 400; loss: 4.72; acc: 0.44
Batch: 420; loss: 3.77; acc: 0.44
Batch: 440; loss: 5.22; acc: 0.33
Batch: 460; loss: 4.03; acc: 0.38
Batch: 480; loss: 3.58; acc: 0.52
Batch: 500; loss: 4.15; acc: 0.44
Batch: 520; loss: 5.27; acc: 0.33
Batch: 540; loss: 4.53; acc: 0.41
Batch: 560; loss: 4.74; acc: 0.44
Batch: 580; loss: 3.98; acc: 0.39
Batch: 600; loss: 4.14; acc: 0.34
Batch: 620; loss: 5.2; acc: 0.41
Train Epoch over. train_loss: 4.46; train_accuracy: 0.39 

Batch: 0; loss: 4.24; acc: 0.41
Batch: 20; loss: 4.86; acc: 0.38
Batch: 40; loss: 4.57; acc: 0.44
Batch: 60; loss: 4.72; acc: 0.33
Batch: 80; loss: 5.12; acc: 0.38
Batch: 100; loss: 4.39; acc: 0.41
Batch: 120; loss: 4.07; acc: 0.47
Batch: 140; loss: 5.34; acc: 0.27
Val Epoch over. val_loss: 4.49063005265157; val_accuracy: 0.3958996815286624 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 4.3; acc: 0.42
Batch: 20; loss: 4.88; acc: 0.38
Batch: 40; loss: 3.3; acc: 0.5
Batch: 60; loss: 4.26; acc: 0.44
Batch: 80; loss: 3.89; acc: 0.48
Batch: 100; loss: 5.24; acc: 0.22
Batch: 120; loss: 3.01; acc: 0.48
Batch: 140; loss: 5.23; acc: 0.38
Batch: 160; loss: 5.48; acc: 0.34
Batch: 180; loss: 5.14; acc: 0.3
Batch: 200; loss: 4.12; acc: 0.36
Batch: 220; loss: 3.58; acc: 0.42
Batch: 240; loss: 4.6; acc: 0.44
Batch: 260; loss: 4.21; acc: 0.33
Batch: 280; loss: 4.95; acc: 0.41
Batch: 300; loss: 5.68; acc: 0.31
Batch: 320; loss: 3.71; acc: 0.42
Batch: 340; loss: 3.91; acc: 0.42
Batch: 360; loss: 3.93; acc: 0.47
Batch: 380; loss: 4.3; acc: 0.41
Batch: 400; loss: 3.54; acc: 0.47
Batch: 420; loss: 4.37; acc: 0.36
Batch: 440; loss: 3.29; acc: 0.41
Batch: 460; loss: 3.61; acc: 0.44
Batch: 480; loss: 4.18; acc: 0.39
Batch: 500; loss: 4.33; acc: 0.48
Batch: 520; loss: 3.93; acc: 0.45
Batch: 540; loss: 4.28; acc: 0.48
Batch: 560; loss: 4.21; acc: 0.38
Batch: 580; loss: 5.62; acc: 0.34
Batch: 600; loss: 4.46; acc: 0.42
Batch: 620; loss: 4.58; acc: 0.36
Train Epoch over. train_loss: 4.46; train_accuracy: 0.38 

Batch: 0; loss: 3.79; acc: 0.38
Batch: 20; loss: 5.86; acc: 0.27
Batch: 40; loss: 4.7; acc: 0.44
Batch: 60; loss: 4.39; acc: 0.39
Batch: 80; loss: 5.35; acc: 0.27
Batch: 100; loss: 4.63; acc: 0.38
Batch: 120; loss: 4.45; acc: 0.38
Batch: 140; loss: 5.24; acc: 0.34
Val Epoch over. val_loss: 4.525880620737744; val_accuracy: 0.37261146496815284 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 5.45; acc: 0.36
Batch: 20; loss: 3.29; acc: 0.41
Batch: 40; loss: 4.59; acc: 0.36
Batch: 60; loss: 3.18; acc: 0.47
Batch: 80; loss: 3.62; acc: 0.39
Batch: 100; loss: 4.3; acc: 0.34
Batch: 120; loss: 5.81; acc: 0.34
Batch: 140; loss: 3.51; acc: 0.47
Batch: 160; loss: 6.2; acc: 0.25
Batch: 180; loss: 5.79; acc: 0.28
Batch: 200; loss: 5.32; acc: 0.33
Batch: 220; loss: 5.37; acc: 0.34
Batch: 240; loss: 3.38; acc: 0.45
Batch: 260; loss: 3.11; acc: 0.48
Batch: 280; loss: 3.48; acc: 0.44
Batch: 300; loss: 3.61; acc: 0.39
Batch: 320; loss: 3.71; acc: 0.41
Batch: 340; loss: 5.55; acc: 0.31
Batch: 360; loss: 3.7; acc: 0.45
Batch: 380; loss: 4.43; acc: 0.39
Batch: 400; loss: 3.77; acc: 0.41
Batch: 420; loss: 3.31; acc: 0.41
Batch: 440; loss: 3.41; acc: 0.47
Batch: 460; loss: 4.6; acc: 0.44
Batch: 480; loss: 4.83; acc: 0.42
Batch: 500; loss: 4.65; acc: 0.39
Batch: 520; loss: 3.07; acc: 0.44
Batch: 540; loss: 5.76; acc: 0.38
Batch: 560; loss: 4.79; acc: 0.33
Batch: 580; loss: 3.62; acc: 0.45
Batch: 600; loss: 5.06; acc: 0.33
Batch: 620; loss: 3.6; acc: 0.38
Train Epoch over. train_loss: 4.15; train_accuracy: 0.4 

Batch: 0; loss: 3.87; acc: 0.36
Batch: 20; loss: 5.09; acc: 0.34
Batch: 40; loss: 4.3; acc: 0.48
Batch: 60; loss: 4.11; acc: 0.42
Batch: 80; loss: 5.11; acc: 0.3
Batch: 100; loss: 4.22; acc: 0.42
Batch: 120; loss: 3.94; acc: 0.42
Batch: 140; loss: 5.04; acc: 0.33
Val Epoch over. val_loss: 4.200218027564371; val_accuracy: 0.399781050955414 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.02; acc: 0.52
Batch: 20; loss: 4.74; acc: 0.34
Batch: 40; loss: 4.12; acc: 0.44
Batch: 60; loss: 4.21; acc: 0.34
Batch: 80; loss: 4.06; acc: 0.36
Batch: 100; loss: 4.43; acc: 0.31
Batch: 120; loss: 4.91; acc: 0.31
Batch: 140; loss: 3.83; acc: 0.42
Batch: 160; loss: 4.03; acc: 0.41
Batch: 180; loss: 4.48; acc: 0.36
Batch: 200; loss: 4.83; acc: 0.38
Batch: 220; loss: 3.97; acc: 0.48
Batch: 240; loss: 4.63; acc: 0.39
Batch: 260; loss: 3.69; acc: 0.33
Batch: 280; loss: 4.24; acc: 0.41
Batch: 300; loss: 4.86; acc: 0.34
Batch: 320; loss: 4.3; acc: 0.44
Batch: 340; loss: 5.61; acc: 0.33
Batch: 360; loss: 4.65; acc: 0.41
Batch: 380; loss: 5.81; acc: 0.33
Batch: 400; loss: 3.5; acc: 0.42
Batch: 420; loss: 4.41; acc: 0.39
Batch: 440; loss: 4.3; acc: 0.42
Batch: 460; loss: 4.08; acc: 0.39
Batch: 480; loss: 3.57; acc: 0.44
Batch: 500; loss: 4.65; acc: 0.28
Batch: 520; loss: 3.91; acc: 0.33
Batch: 540; loss: 3.85; acc: 0.45
Batch: 560; loss: 4.03; acc: 0.45
Batch: 580; loss: 4.35; acc: 0.38
Batch: 600; loss: 5.29; acc: 0.36
Batch: 620; loss: 5.28; acc: 0.34
Train Epoch over. train_loss: 4.14; train_accuracy: 0.4 

Batch: 0; loss: 3.88; acc: 0.33
Batch: 20; loss: 5.11; acc: 0.33
Batch: 40; loss: 4.3; acc: 0.42
Batch: 60; loss: 4.07; acc: 0.39
Batch: 80; loss: 5.12; acc: 0.3
Batch: 100; loss: 4.26; acc: 0.41
Batch: 120; loss: 4.04; acc: 0.42
Batch: 140; loss: 5.0; acc: 0.34
Val Epoch over. val_loss: 4.184033374118197; val_accuracy: 0.397093949044586 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.61; acc: 0.36
Batch: 20; loss: 3.54; acc: 0.38
Batch: 40; loss: 4.32; acc: 0.45
Batch: 60; loss: 3.46; acc: 0.36
Batch: 80; loss: 4.12; acc: 0.25
Batch: 100; loss: 4.44; acc: 0.42
Batch: 120; loss: 4.33; acc: 0.36
Batch: 140; loss: 4.58; acc: 0.27
Batch: 160; loss: 3.42; acc: 0.45
Batch: 180; loss: 2.98; acc: 0.45
Batch: 200; loss: 4.43; acc: 0.41
Batch: 220; loss: 4.34; acc: 0.45
Batch: 240; loss: 4.1; acc: 0.5
Batch: 260; loss: 3.72; acc: 0.44
Batch: 280; loss: 4.45; acc: 0.42
Batch: 300; loss: 4.8; acc: 0.36
Batch: 320; loss: 3.75; acc: 0.53
Batch: 340; loss: 4.41; acc: 0.34
Batch: 360; loss: 3.04; acc: 0.39
Batch: 380; loss: 3.8; acc: 0.39
Batch: 400; loss: 3.96; acc: 0.41
Batch: 420; loss: 4.25; acc: 0.44
Batch: 440; loss: 3.13; acc: 0.41
Batch: 460; loss: 4.03; acc: 0.44
Batch: 480; loss: 4.26; acc: 0.45
Batch: 500; loss: 3.8; acc: 0.42
Batch: 520; loss: 3.88; acc: 0.44
Batch: 540; loss: 3.81; acc: 0.47
Batch: 560; loss: 3.03; acc: 0.5
Batch: 580; loss: 3.42; acc: 0.42
Batch: 600; loss: 3.31; acc: 0.45
Batch: 620; loss: 3.27; acc: 0.45
Train Epoch over. train_loss: 4.14; train_accuracy: 0.4 

Batch: 0; loss: 3.96; acc: 0.36
Batch: 20; loss: 5.16; acc: 0.36
Batch: 40; loss: 4.15; acc: 0.45
Batch: 60; loss: 4.2; acc: 0.34
Batch: 80; loss: 5.04; acc: 0.31
Batch: 100; loss: 4.22; acc: 0.41
Batch: 120; loss: 3.94; acc: 0.41
Batch: 140; loss: 5.02; acc: 0.36
Val Epoch over. val_loss: 4.1856142829178244; val_accuracy: 0.397890127388535 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.15; acc: 0.39
Batch: 20; loss: 3.94; acc: 0.41
Batch: 40; loss: 5.27; acc: 0.33
Batch: 60; loss: 4.16; acc: 0.48
Batch: 80; loss: 4.63; acc: 0.36
Batch: 100; loss: 4.56; acc: 0.31
Batch: 120; loss: 4.03; acc: 0.36
Batch: 140; loss: 3.59; acc: 0.36
Batch: 160; loss: 4.13; acc: 0.42
Batch: 180; loss: 3.79; acc: 0.45
Batch: 200; loss: 3.6; acc: 0.34
Batch: 220; loss: 5.15; acc: 0.38
Batch: 240; loss: 2.84; acc: 0.48
Batch: 260; loss: 3.97; acc: 0.45
Batch: 280; loss: 3.82; acc: 0.48
Batch: 300; loss: 4.8; acc: 0.33
Batch: 320; loss: 4.78; acc: 0.36
Batch: 340; loss: 3.87; acc: 0.28
Batch: 360; loss: 3.69; acc: 0.38
Batch: 380; loss: 5.06; acc: 0.41
Batch: 400; loss: 4.5; acc: 0.38
Batch: 420; loss: 3.47; acc: 0.36
Batch: 440; loss: 3.53; acc: 0.42
Batch: 460; loss: 3.93; acc: 0.41
Batch: 480; loss: 4.89; acc: 0.36
Batch: 500; loss: 5.44; acc: 0.45
Batch: 520; loss: 3.71; acc: 0.42
Batch: 540; loss: 3.09; acc: 0.42
Batch: 560; loss: 4.53; acc: 0.41
Batch: 580; loss: 3.54; acc: 0.56
Batch: 600; loss: 4.15; acc: 0.34
Batch: 620; loss: 4.51; acc: 0.34
Train Epoch over. train_loss: 4.13; train_accuracy: 0.4 

Batch: 0; loss: 3.84; acc: 0.36
Batch: 20; loss: 5.09; acc: 0.33
Batch: 40; loss: 4.26; acc: 0.44
Batch: 60; loss: 4.26; acc: 0.33
Batch: 80; loss: 5.02; acc: 0.31
Batch: 100; loss: 4.05; acc: 0.41
Batch: 120; loss: 3.98; acc: 0.39
Batch: 140; loss: 4.94; acc: 0.36
Val Epoch over. val_loss: 4.184645337663638; val_accuracy: 0.39599920382165604 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.2; acc: 0.39
Batch: 20; loss: 4.18; acc: 0.34
Batch: 40; loss: 4.02; acc: 0.45
Batch: 60; loss: 4.26; acc: 0.38
Batch: 80; loss: 3.25; acc: 0.61
Batch: 100; loss: 5.62; acc: 0.31
Batch: 120; loss: 4.24; acc: 0.34
Batch: 140; loss: 3.7; acc: 0.34
Batch: 160; loss: 4.53; acc: 0.31
Batch: 180; loss: 3.71; acc: 0.41
Batch: 200; loss: 3.18; acc: 0.45
Batch: 220; loss: 3.89; acc: 0.41
Batch: 240; loss: 4.7; acc: 0.36
Batch: 260; loss: 3.55; acc: 0.48
Batch: 280; loss: 3.36; acc: 0.41
Batch: 300; loss: 5.39; acc: 0.28
Batch: 320; loss: 3.52; acc: 0.45
Batch: 340; loss: 3.62; acc: 0.36
Batch: 360; loss: 4.47; acc: 0.28
Batch: 380; loss: 3.39; acc: 0.42
Batch: 400; loss: 4.43; acc: 0.34
Batch: 420; loss: 3.84; acc: 0.38
Batch: 440; loss: 4.85; acc: 0.33
Batch: 460; loss: 4.02; acc: 0.42
Batch: 480; loss: 3.09; acc: 0.42
Batch: 500; loss: 3.48; acc: 0.48
Batch: 520; loss: 5.02; acc: 0.3
Batch: 540; loss: 3.89; acc: 0.47
Batch: 560; loss: 4.12; acc: 0.44
Batch: 580; loss: 3.3; acc: 0.52
Batch: 600; loss: 3.09; acc: 0.39
Batch: 620; loss: 4.27; acc: 0.34
Train Epoch over. train_loss: 4.13; train_accuracy: 0.4 

Batch: 0; loss: 3.82; acc: 0.36
Batch: 20; loss: 5.34; acc: 0.33
Batch: 40; loss: 4.24; acc: 0.42
Batch: 60; loss: 4.06; acc: 0.41
Batch: 80; loss: 5.07; acc: 0.3
Batch: 100; loss: 4.29; acc: 0.42
Batch: 120; loss: 4.05; acc: 0.41
Batch: 140; loss: 4.86; acc: 0.38
Val Epoch over. val_loss: 4.175213462987523; val_accuracy: 0.3929140127388535 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.97; acc: 0.45
Batch: 20; loss: 3.65; acc: 0.45
Batch: 40; loss: 3.29; acc: 0.47
Batch: 60; loss: 3.6; acc: 0.34
Batch: 80; loss: 3.46; acc: 0.41
Batch: 100; loss: 3.37; acc: 0.48
Batch: 120; loss: 3.65; acc: 0.36
Batch: 140; loss: 4.04; acc: 0.48
Batch: 160; loss: 4.93; acc: 0.27
Batch: 180; loss: 3.5; acc: 0.41
Batch: 200; loss: 5.35; acc: 0.34
Batch: 220; loss: 3.57; acc: 0.47
Batch: 240; loss: 4.26; acc: 0.41
Batch: 260; loss: 3.21; acc: 0.41
Batch: 280; loss: 4.49; acc: 0.38
Batch: 300; loss: 4.68; acc: 0.33
Batch: 320; loss: 4.61; acc: 0.28
Batch: 340; loss: 4.43; acc: 0.33
Batch: 360; loss: 5.81; acc: 0.33
Batch: 380; loss: 3.14; acc: 0.5
Batch: 400; loss: 4.04; acc: 0.38
Batch: 420; loss: 3.62; acc: 0.48
Batch: 440; loss: 3.83; acc: 0.45
Batch: 460; loss: 3.53; acc: 0.44
Batch: 480; loss: 4.16; acc: 0.39
Batch: 500; loss: 2.66; acc: 0.52
Batch: 520; loss: 3.14; acc: 0.5
Batch: 540; loss: 3.89; acc: 0.34
Batch: 560; loss: 3.88; acc: 0.42
Batch: 580; loss: 4.96; acc: 0.39
Batch: 600; loss: 3.85; acc: 0.45
Batch: 620; loss: 3.57; acc: 0.42
Train Epoch over. train_loss: 4.13; train_accuracy: 0.4 

Batch: 0; loss: 3.73; acc: 0.36
Batch: 20; loss: 5.28; acc: 0.3
Batch: 40; loss: 4.17; acc: 0.41
Batch: 60; loss: 4.21; acc: 0.39
Batch: 80; loss: 5.01; acc: 0.33
Batch: 100; loss: 4.2; acc: 0.42
Batch: 120; loss: 3.92; acc: 0.39
Batch: 140; loss: 4.96; acc: 0.34
Val Epoch over. val_loss: 4.158916578171359; val_accuracy: 0.3951035031847134 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 4.7; acc: 0.48
Batch: 20; loss: 3.92; acc: 0.39
Batch: 40; loss: 3.52; acc: 0.48
Batch: 60; loss: 4.61; acc: 0.27
Batch: 80; loss: 3.38; acc: 0.41
Batch: 100; loss: 3.29; acc: 0.47
Batch: 120; loss: 4.27; acc: 0.36
Batch: 140; loss: 3.17; acc: 0.44
Batch: 160; loss: 4.23; acc: 0.41
Batch: 180; loss: 3.92; acc: 0.33
Batch: 200; loss: 4.27; acc: 0.42
Batch: 220; loss: 5.24; acc: 0.36
Batch: 240; loss: 4.13; acc: 0.33
Batch: 260; loss: 4.83; acc: 0.33
Batch: 280; loss: 4.96; acc: 0.36
Batch: 300; loss: 4.41; acc: 0.48
Batch: 320; loss: 4.52; acc: 0.3
Batch: 340; loss: 3.37; acc: 0.5
Batch: 360; loss: 4.02; acc: 0.42
Batch: 380; loss: 4.63; acc: 0.41
Batch: 400; loss: 4.33; acc: 0.33
Batch: 420; loss: 3.15; acc: 0.53
Batch: 440; loss: 4.03; acc: 0.42
Batch: 460; loss: 4.7; acc: 0.36
Batch: 480; loss: 5.5; acc: 0.3
Batch: 500; loss: 3.26; acc: 0.44
Batch: 520; loss: 4.37; acc: 0.39
Batch: 540; loss: 3.74; acc: 0.42
Batch: 560; loss: 3.7; acc: 0.36
Batch: 580; loss: 5.26; acc: 0.3
Batch: 600; loss: 3.56; acc: 0.38
Batch: 620; loss: 4.3; acc: 0.39
Train Epoch over. train_loss: 4.13; train_accuracy: 0.4 

Batch: 0; loss: 3.79; acc: 0.38
Batch: 20; loss: 5.27; acc: 0.3
Batch: 40; loss: 4.19; acc: 0.42
Batch: 60; loss: 4.26; acc: 0.34
Batch: 80; loss: 5.04; acc: 0.31
Batch: 100; loss: 4.2; acc: 0.42
Batch: 120; loss: 3.92; acc: 0.38
Batch: 140; loss: 4.88; acc: 0.38
Val Epoch over. val_loss: 4.150628108887156; val_accuracy: 0.39341162420382164 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.23; acc: 0.39
Batch: 20; loss: 3.57; acc: 0.48
Batch: 40; loss: 4.36; acc: 0.42
Batch: 60; loss: 4.15; acc: 0.38
Batch: 80; loss: 2.94; acc: 0.5
Batch: 100; loss: 3.27; acc: 0.45
Batch: 120; loss: 4.82; acc: 0.3
Batch: 140; loss: 3.82; acc: 0.44
Batch: 160; loss: 4.59; acc: 0.41
Batch: 180; loss: 3.75; acc: 0.39
Batch: 200; loss: 3.82; acc: 0.45
Batch: 220; loss: 4.06; acc: 0.38
Batch: 240; loss: 3.36; acc: 0.47
Batch: 260; loss: 4.99; acc: 0.36
Batch: 280; loss: 3.14; acc: 0.47
Batch: 300; loss: 4.34; acc: 0.3
Batch: 320; loss: 5.23; acc: 0.36
Batch: 340; loss: 4.45; acc: 0.33
Batch: 360; loss: 3.66; acc: 0.45
Batch: 380; loss: 4.52; acc: 0.38
Batch: 400; loss: 3.82; acc: 0.47
Batch: 420; loss: 5.01; acc: 0.39
Batch: 440; loss: 2.98; acc: 0.48
Batch: 460; loss: 3.81; acc: 0.39
Batch: 480; loss: 3.89; acc: 0.39
Batch: 500; loss: 5.11; acc: 0.3
Batch: 520; loss: 3.83; acc: 0.38
Batch: 540; loss: 3.15; acc: 0.39
Batch: 560; loss: 3.66; acc: 0.39
Batch: 580; loss: 5.0; acc: 0.33
Batch: 600; loss: 3.4; acc: 0.47
Batch: 620; loss: 4.79; acc: 0.34
Train Epoch over. train_loss: 4.13; train_accuracy: 0.4 

Batch: 0; loss: 3.8; acc: 0.38
Batch: 20; loss: 5.31; acc: 0.28
Batch: 40; loss: 4.19; acc: 0.44
Batch: 60; loss: 4.34; acc: 0.33
Batch: 80; loss: 5.14; acc: 0.31
Batch: 100; loss: 3.96; acc: 0.44
Batch: 120; loss: 3.99; acc: 0.36
Batch: 140; loss: 4.83; acc: 0.36
Val Epoch over. val_loss: 4.162757123351857; val_accuracy: 0.40147292993630573 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.77; acc: 0.39
Batch: 20; loss: 4.25; acc: 0.44
Batch: 40; loss: 3.75; acc: 0.44
Batch: 60; loss: 3.88; acc: 0.41
Batch: 80; loss: 3.82; acc: 0.47
Batch: 100; loss: 5.4; acc: 0.33
Batch: 120; loss: 4.18; acc: 0.33
Batch: 140; loss: 4.58; acc: 0.34
Batch: 160; loss: 3.72; acc: 0.42
Batch: 180; loss: 4.3; acc: 0.31
Batch: 200; loss: 4.89; acc: 0.36
Batch: 220; loss: 4.68; acc: 0.38
Batch: 240; loss: 4.33; acc: 0.41
Batch: 260; loss: 4.47; acc: 0.34
Batch: 280; loss: 4.06; acc: 0.38
Batch: 300; loss: 3.58; acc: 0.39
Batch: 320; loss: 4.83; acc: 0.39
Batch: 340; loss: 3.42; acc: 0.47
Batch: 360; loss: 4.54; acc: 0.38
Batch: 380; loss: 3.22; acc: 0.48
Batch: 400; loss: 4.6; acc: 0.38
Batch: 420; loss: 3.98; acc: 0.45
Batch: 440; loss: 4.3; acc: 0.42
Batch: 460; loss: 3.15; acc: 0.42
Batch: 480; loss: 4.56; acc: 0.41
Batch: 500; loss: 4.54; acc: 0.36
Batch: 520; loss: 3.43; acc: 0.48
Batch: 540; loss: 3.78; acc: 0.47
Batch: 560; loss: 5.24; acc: 0.3
Batch: 580; loss: 4.08; acc: 0.45
Batch: 600; loss: 3.34; acc: 0.48
Batch: 620; loss: 4.96; acc: 0.41
Train Epoch over. train_loss: 4.13; train_accuracy: 0.4 

Batch: 0; loss: 3.63; acc: 0.36
Batch: 20; loss: 5.37; acc: 0.27
Batch: 40; loss: 4.2; acc: 0.42
Batch: 60; loss: 4.21; acc: 0.33
Batch: 80; loss: 5.08; acc: 0.31
Batch: 100; loss: 4.09; acc: 0.42
Batch: 120; loss: 3.93; acc: 0.38
Batch: 140; loss: 4.85; acc: 0.34
Val Epoch over. val_loss: 4.153178478502165; val_accuracy: 0.39241640127388533 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 5.51; acc: 0.34
Batch: 20; loss: 3.43; acc: 0.36
Batch: 40; loss: 5.1; acc: 0.31
Batch: 60; loss: 4.36; acc: 0.41
Batch: 80; loss: 5.3; acc: 0.38
Batch: 100; loss: 4.22; acc: 0.45
Batch: 120; loss: 3.3; acc: 0.5
Batch: 140; loss: 3.96; acc: 0.38
Batch: 160; loss: 4.61; acc: 0.36
Batch: 180; loss: 3.81; acc: 0.38
Batch: 200; loss: 3.66; acc: 0.47
Batch: 220; loss: 4.05; acc: 0.36
Batch: 240; loss: 4.24; acc: 0.38
Batch: 260; loss: 5.0; acc: 0.34
Batch: 280; loss: 4.09; acc: 0.41
Batch: 300; loss: 4.35; acc: 0.3
Batch: 320; loss: 4.53; acc: 0.31
Batch: 340; loss: 4.27; acc: 0.44
Batch: 360; loss: 3.32; acc: 0.45
Batch: 380; loss: 4.35; acc: 0.39
Batch: 400; loss: 4.33; acc: 0.36
Batch: 420; loss: 4.29; acc: 0.38
Batch: 440; loss: 3.62; acc: 0.41
Batch: 460; loss: 4.89; acc: 0.39
Batch: 480; loss: 4.31; acc: 0.47
Batch: 500; loss: 3.94; acc: 0.36
Batch: 520; loss: 4.13; acc: 0.39
Batch: 540; loss: 4.62; acc: 0.31
Batch: 560; loss: 3.8; acc: 0.39
Batch: 580; loss: 3.29; acc: 0.45
Batch: 600; loss: 4.92; acc: 0.27
Batch: 620; loss: 5.45; acc: 0.31
Train Epoch over. train_loss: 4.12; train_accuracy: 0.4 

Batch: 0; loss: 3.76; acc: 0.36
Batch: 20; loss: 5.46; acc: 0.28
Batch: 40; loss: 4.15; acc: 0.41
Batch: 60; loss: 4.15; acc: 0.36
Batch: 80; loss: 4.93; acc: 0.31
Batch: 100; loss: 4.16; acc: 0.41
Batch: 120; loss: 3.89; acc: 0.33
Batch: 140; loss: 4.77; acc: 0.38
Val Epoch over. val_loss: 4.165111074781722; val_accuracy: 0.3877388535031847 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.37; acc: 0.44
Batch: 20; loss: 3.35; acc: 0.38
Batch: 40; loss: 3.61; acc: 0.44
Batch: 60; loss: 4.67; acc: 0.38
Batch: 80; loss: 3.94; acc: 0.34
Batch: 100; loss: 3.97; acc: 0.38
Batch: 120; loss: 3.6; acc: 0.44
Batch: 140; loss: 3.65; acc: 0.42
Batch: 160; loss: 3.26; acc: 0.41
Batch: 180; loss: 2.93; acc: 0.47
Batch: 200; loss: 2.92; acc: 0.42
Batch: 220; loss: 3.87; acc: 0.44
Batch: 240; loss: 3.1; acc: 0.55
Batch: 260; loss: 3.74; acc: 0.47
Batch: 280; loss: 3.83; acc: 0.47
Batch: 300; loss: 3.13; acc: 0.45
Batch: 320; loss: 5.28; acc: 0.36
Batch: 340; loss: 3.86; acc: 0.34
Batch: 360; loss: 3.07; acc: 0.39
Batch: 380; loss: 4.21; acc: 0.31
Batch: 400; loss: 3.29; acc: 0.44
Batch: 420; loss: 4.12; acc: 0.36
Batch: 440; loss: 4.89; acc: 0.41
Batch: 460; loss: 4.31; acc: 0.42
Batch: 480; loss: 4.37; acc: 0.34
Batch: 500; loss: 4.01; acc: 0.39
Batch: 520; loss: 4.22; acc: 0.38
Batch: 540; loss: 4.33; acc: 0.39
Batch: 560; loss: 4.76; acc: 0.38
Batch: 580; loss: 4.66; acc: 0.39
Batch: 600; loss: 3.47; acc: 0.42
Batch: 620; loss: 4.8; acc: 0.34
Train Epoch over. train_loss: 4.11; train_accuracy: 0.39 

Batch: 0; loss: 3.75; acc: 0.36
Batch: 20; loss: 5.37; acc: 0.28
Batch: 40; loss: 4.14; acc: 0.42
Batch: 60; loss: 4.24; acc: 0.34
Batch: 80; loss: 5.01; acc: 0.31
Batch: 100; loss: 4.07; acc: 0.44
Batch: 120; loss: 3.91; acc: 0.38
Batch: 140; loss: 4.78; acc: 0.39
Val Epoch over. val_loss: 4.142500465842569; val_accuracy: 0.39460589171974525 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 3.55; acc: 0.39
Batch: 20; loss: 3.67; acc: 0.48
Batch: 40; loss: 4.17; acc: 0.34
Batch: 60; loss: 4.47; acc: 0.48
Batch: 80; loss: 4.09; acc: 0.45
Batch: 100; loss: 3.67; acc: 0.56
Batch: 120; loss: 4.07; acc: 0.45
Batch: 140; loss: 4.51; acc: 0.44
Batch: 160; loss: 4.42; acc: 0.48
Batch: 180; loss: 4.01; acc: 0.47
Batch: 200; loss: 3.47; acc: 0.48
Batch: 220; loss: 5.55; acc: 0.36
Batch: 240; loss: 4.12; acc: 0.38
Batch: 260; loss: 3.85; acc: 0.41
Batch: 280; loss: 4.31; acc: 0.39
Batch: 300; loss: 4.86; acc: 0.28
Batch: 320; loss: 3.24; acc: 0.48
Batch: 340; loss: 4.98; acc: 0.36
Batch: 360; loss: 4.71; acc: 0.34
Batch: 380; loss: 3.55; acc: 0.41
Batch: 400; loss: 4.27; acc: 0.31
Batch: 420; loss: 4.64; acc: 0.38
Batch: 440; loss: 5.28; acc: 0.38
Batch: 460; loss: 4.56; acc: 0.31
Batch: 480; loss: 3.26; acc: 0.44
Batch: 500; loss: 3.37; acc: 0.53
Batch: 520; loss: 3.55; acc: 0.34
Batch: 540; loss: 5.03; acc: 0.31
Batch: 560; loss: 4.61; acc: 0.44
Batch: 580; loss: 5.44; acc: 0.39
Batch: 600; loss: 5.17; acc: 0.33
Batch: 620; loss: 4.54; acc: 0.38
Train Epoch over. train_loss: 4.11; train_accuracy: 0.4 

Batch: 0; loss: 3.75; acc: 0.36
Batch: 20; loss: 5.39; acc: 0.28
Batch: 40; loss: 4.15; acc: 0.42
Batch: 60; loss: 4.23; acc: 0.33
Batch: 80; loss: 5.03; acc: 0.31
Batch: 100; loss: 4.07; acc: 0.44
Batch: 120; loss: 3.96; acc: 0.38
Batch: 140; loss: 4.77; acc: 0.39
Val Epoch over. val_loss: 4.141959289836276; val_accuracy: 0.3935111464968153 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.31; acc: 0.38
Batch: 20; loss: 3.09; acc: 0.42
Batch: 40; loss: 4.13; acc: 0.47
Batch: 60; loss: 4.12; acc: 0.41
Batch: 80; loss: 4.44; acc: 0.41
Batch: 100; loss: 3.23; acc: 0.39
Batch: 120; loss: 4.33; acc: 0.41
Batch: 140; loss: 3.96; acc: 0.44
Batch: 160; loss: 4.96; acc: 0.44
Batch: 180; loss: 3.91; acc: 0.39
Batch: 200; loss: 4.66; acc: 0.39
Batch: 220; loss: 4.77; acc: 0.28
Batch: 240; loss: 3.33; acc: 0.44
Batch: 260; loss: 3.83; acc: 0.44
Batch: 280; loss: 3.75; acc: 0.41
Batch: 300; loss: 4.51; acc: 0.34
Batch: 320; loss: 4.66; acc: 0.45
Batch: 340; loss: 4.54; acc: 0.31
Batch: 360; loss: 3.15; acc: 0.41
Batch: 380; loss: 3.63; acc: 0.39
Batch: 400; loss: 4.09; acc: 0.36
Batch: 420; loss: 3.88; acc: 0.52
Batch: 440; loss: 4.13; acc: 0.45
Batch: 460; loss: 3.66; acc: 0.41
Batch: 480; loss: 3.63; acc: 0.41
Batch: 500; loss: 3.78; acc: 0.41
Batch: 520; loss: 4.88; acc: 0.28
Batch: 540; loss: 3.89; acc: 0.45
Batch: 560; loss: 3.57; acc: 0.42
Batch: 580; loss: 4.4; acc: 0.41
Batch: 600; loss: 4.1; acc: 0.44
Batch: 620; loss: 3.57; acc: 0.42
Train Epoch over. train_loss: 4.11; train_accuracy: 0.4 

Batch: 0; loss: 3.73; acc: 0.38
Batch: 20; loss: 5.39; acc: 0.27
Batch: 40; loss: 4.13; acc: 0.44
Batch: 60; loss: 4.25; acc: 0.34
Batch: 80; loss: 5.02; acc: 0.31
Batch: 100; loss: 4.04; acc: 0.44
Batch: 120; loss: 3.93; acc: 0.38
Batch: 140; loss: 4.8; acc: 0.39
Val Epoch over. val_loss: 4.142774007882283; val_accuracy: 0.39490445859872614 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.21; acc: 0.39
Batch: 20; loss: 3.11; acc: 0.48
Batch: 40; loss: 4.68; acc: 0.34
Batch: 60; loss: 3.77; acc: 0.31
Batch: 80; loss: 4.41; acc: 0.41
Batch: 100; loss: 5.31; acc: 0.31
Batch: 120; loss: 3.82; acc: 0.45
Batch: 140; loss: 4.41; acc: 0.41
Batch: 160; loss: 2.68; acc: 0.55
Batch: 180; loss: 4.38; acc: 0.31
Batch: 200; loss: 4.66; acc: 0.34
Batch: 220; loss: 3.73; acc: 0.48
Batch: 240; loss: 4.28; acc: 0.31
Batch: 260; loss: 3.52; acc: 0.41
Batch: 280; loss: 4.47; acc: 0.3
Batch: 300; loss: 3.78; acc: 0.42
Batch: 320; loss: 4.23; acc: 0.41
Batch: 340; loss: 4.09; acc: 0.41
Batch: 360; loss: 4.75; acc: 0.31
Batch: 380; loss: 3.76; acc: 0.34
Batch: 400; loss: 4.05; acc: 0.34
Batch: 420; loss: 4.47; acc: 0.5
Batch: 440; loss: 3.65; acc: 0.36
Batch: 460; loss: 3.8; acc: 0.44
Batch: 480; loss: 3.47; acc: 0.47
Batch: 500; loss: 4.2; acc: 0.31
Batch: 520; loss: 3.78; acc: 0.41
Batch: 540; loss: 3.18; acc: 0.44
Batch: 560; loss: 4.2; acc: 0.44
Batch: 580; loss: 4.98; acc: 0.31
Batch: 600; loss: 4.38; acc: 0.39
Batch: 620; loss: 3.95; acc: 0.33
Train Epoch over. train_loss: 4.11; train_accuracy: 0.4 

Batch: 0; loss: 3.75; acc: 0.38
Batch: 20; loss: 5.38; acc: 0.3
Batch: 40; loss: 4.14; acc: 0.41
Batch: 60; loss: 4.25; acc: 0.34
Batch: 80; loss: 5.0; acc: 0.33
Batch: 100; loss: 4.08; acc: 0.42
Batch: 120; loss: 3.94; acc: 0.39
Batch: 140; loss: 4.77; acc: 0.39
Val Epoch over. val_loss: 4.146368263633388; val_accuracy: 0.3935111464968153 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.28; acc: 0.36
Batch: 20; loss: 3.84; acc: 0.38
Batch: 40; loss: 4.53; acc: 0.41
Batch: 60; loss: 4.57; acc: 0.48
Batch: 80; loss: 3.85; acc: 0.47
Batch: 100; loss: 4.58; acc: 0.31
Batch: 120; loss: 3.83; acc: 0.44
Batch: 140; loss: 3.75; acc: 0.5
Batch: 160; loss: 4.26; acc: 0.3
Batch: 180; loss: 3.62; acc: 0.33
Batch: 200; loss: 4.18; acc: 0.42
Batch: 220; loss: 4.12; acc: 0.47
Batch: 240; loss: 4.08; acc: 0.44
Batch: 260; loss: 3.4; acc: 0.52
Batch: 280; loss: 3.39; acc: 0.47
Batch: 300; loss: 4.38; acc: 0.42
Batch: 320; loss: 3.69; acc: 0.41
Batch: 340; loss: 3.75; acc: 0.38
Batch: 360; loss: 3.52; acc: 0.5
Batch: 380; loss: 4.2; acc: 0.31
Batch: 400; loss: 4.28; acc: 0.44
Batch: 420; loss: 4.14; acc: 0.41
Batch: 440; loss: 4.09; acc: 0.38
Batch: 460; loss: 4.07; acc: 0.41
Batch: 480; loss: 3.27; acc: 0.41
Batch: 500; loss: 3.43; acc: 0.36
Batch: 520; loss: 3.9; acc: 0.38
Batch: 540; loss: 3.93; acc: 0.34
Batch: 560; loss: 4.62; acc: 0.39
Batch: 580; loss: 3.74; acc: 0.44
Batch: 600; loss: 4.82; acc: 0.34
Batch: 620; loss: 3.85; acc: 0.45
Train Epoch over. train_loss: 4.11; train_accuracy: 0.4 

Batch: 0; loss: 3.72; acc: 0.38
Batch: 20; loss: 5.41; acc: 0.27
Batch: 40; loss: 4.11; acc: 0.44
Batch: 60; loss: 4.25; acc: 0.34
Batch: 80; loss: 5.02; acc: 0.31
Batch: 100; loss: 4.06; acc: 0.42
Batch: 120; loss: 3.93; acc: 0.39
Batch: 140; loss: 4.78; acc: 0.39
Val Epoch over. val_loss: 4.142280725157185; val_accuracy: 0.3938097133757962 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 5.07; acc: 0.33
Batch: 20; loss: 4.72; acc: 0.38
Batch: 40; loss: 4.42; acc: 0.44
Batch: 60; loss: 5.62; acc: 0.38
Batch: 80; loss: 3.85; acc: 0.44
Batch: 100; loss: 4.02; acc: 0.38
Batch: 120; loss: 4.03; acc: 0.39
Batch: 140; loss: 4.74; acc: 0.39
Batch: 160; loss: 3.31; acc: 0.53
Batch: 180; loss: 6.16; acc: 0.38
Batch: 200; loss: 3.65; acc: 0.34
Batch: 220; loss: 3.02; acc: 0.47
Batch: 240; loss: 4.15; acc: 0.45
Batch: 260; loss: 4.61; acc: 0.33
Batch: 280; loss: 3.35; acc: 0.41
Batch: 300; loss: 4.52; acc: 0.38
Batch: 320; loss: 3.89; acc: 0.34
Batch: 340; loss: 3.92; acc: 0.38
Batch: 360; loss: 3.59; acc: 0.41
Batch: 380; loss: 3.1; acc: 0.5
Batch: 400; loss: 3.31; acc: 0.45
Batch: 420; loss: 3.44; acc: 0.44
Batch: 440; loss: 3.67; acc: 0.38
Batch: 460; loss: 5.02; acc: 0.33
Batch: 480; loss: 3.48; acc: 0.45
Batch: 500; loss: 4.83; acc: 0.3
Batch: 520; loss: 4.78; acc: 0.36
Batch: 540; loss: 4.15; acc: 0.34
Batch: 560; loss: 4.03; acc: 0.42
Batch: 580; loss: 4.17; acc: 0.39
Batch: 600; loss: 3.2; acc: 0.38
Batch: 620; loss: 3.72; acc: 0.41
Train Epoch over. train_loss: 4.11; train_accuracy: 0.4 

Batch: 0; loss: 3.73; acc: 0.38
Batch: 20; loss: 5.38; acc: 0.27
Batch: 40; loss: 4.15; acc: 0.41
Batch: 60; loss: 4.25; acc: 0.34
Batch: 80; loss: 5.02; acc: 0.31
Batch: 100; loss: 4.08; acc: 0.44
Batch: 120; loss: 3.94; acc: 0.39
Batch: 140; loss: 4.79; acc: 0.39
Val Epoch over. val_loss: 4.143738045054636; val_accuracy: 0.3940087579617834 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.42; acc: 0.39
Batch: 20; loss: 4.44; acc: 0.33
Batch: 40; loss: 3.33; acc: 0.45
Batch: 60; loss: 3.64; acc: 0.45
Batch: 80; loss: 3.84; acc: 0.38
Batch: 100; loss: 4.45; acc: 0.36
Batch: 120; loss: 3.68; acc: 0.42
Batch: 140; loss: 3.83; acc: 0.38
Batch: 160; loss: 3.82; acc: 0.44
Batch: 180; loss: 4.04; acc: 0.39
Batch: 200; loss: 2.42; acc: 0.48
Batch: 220; loss: 4.45; acc: 0.33
Batch: 240; loss: 3.49; acc: 0.48
Batch: 260; loss: 3.59; acc: 0.42
Batch: 280; loss: 4.17; acc: 0.34
Batch: 300; loss: 5.3; acc: 0.31
Batch: 320; loss: 3.48; acc: 0.36
Batch: 340; loss: 4.99; acc: 0.27
Batch: 360; loss: 4.22; acc: 0.36
Batch: 380; loss: 4.1; acc: 0.41
Batch: 400; loss: 4.45; acc: 0.34
Batch: 420; loss: 3.61; acc: 0.42
Batch: 440; loss: 4.46; acc: 0.28
Batch: 460; loss: 3.67; acc: 0.41
Batch: 480; loss: 3.9; acc: 0.39
Batch: 500; loss: 3.88; acc: 0.42
Batch: 520; loss: 4.99; acc: 0.38
Batch: 540; loss: 3.01; acc: 0.45
Batch: 560; loss: 3.41; acc: 0.55
Batch: 580; loss: 4.52; acc: 0.36
Batch: 600; loss: 4.58; acc: 0.33
Batch: 620; loss: 2.58; acc: 0.5
Train Epoch over. train_loss: 4.11; train_accuracy: 0.4 

Batch: 0; loss: 3.73; acc: 0.38
Batch: 20; loss: 5.4; acc: 0.27
Batch: 40; loss: 4.13; acc: 0.41
Batch: 60; loss: 4.23; acc: 0.34
Batch: 80; loss: 5.01; acc: 0.31
Batch: 100; loss: 4.09; acc: 0.44
Batch: 120; loss: 3.94; acc: 0.39
Batch: 140; loss: 4.78; acc: 0.39
Val Epoch over. val_loss: 4.143055864200471; val_accuracy: 0.3939092356687898 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 5.16; acc: 0.33
Batch: 20; loss: 3.2; acc: 0.53
Batch: 40; loss: 3.41; acc: 0.36
Batch: 60; loss: 4.76; acc: 0.44
Batch: 80; loss: 4.47; acc: 0.44
Batch: 100; loss: 3.92; acc: 0.41
Batch: 120; loss: 3.88; acc: 0.47
Batch: 140; loss: 5.58; acc: 0.27
Batch: 160; loss: 4.57; acc: 0.33
Batch: 180; loss: 4.15; acc: 0.39
Batch: 200; loss: 3.0; acc: 0.38
Batch: 220; loss: 3.53; acc: 0.38
Batch: 240; loss: 2.73; acc: 0.52
Batch: 260; loss: 3.86; acc: 0.36
Batch: 280; loss: 5.07; acc: 0.31
Batch: 300; loss: 4.48; acc: 0.34
Batch: 320; loss: 4.07; acc: 0.53
Batch: 340; loss: 3.54; acc: 0.44
Batch: 360; loss: 4.86; acc: 0.36
Batch: 380; loss: 4.19; acc: 0.47
Batch: 400; loss: 5.73; acc: 0.33
Batch: 420; loss: 4.44; acc: 0.47
Batch: 440; loss: 3.11; acc: 0.42
Batch: 460; loss: 4.3; acc: 0.36
Batch: 480; loss: 3.78; acc: 0.38
Batch: 500; loss: 4.44; acc: 0.36
Batch: 520; loss: 4.38; acc: 0.45
Batch: 540; loss: 3.2; acc: 0.34
Batch: 560; loss: 4.6; acc: 0.44
Batch: 580; loss: 4.22; acc: 0.34
Batch: 600; loss: 3.99; acc: 0.5
Batch: 620; loss: 5.01; acc: 0.33
Train Epoch over. train_loss: 4.11; train_accuracy: 0.4 

Batch: 0; loss: 3.74; acc: 0.38
Batch: 20; loss: 5.39; acc: 0.3
Batch: 40; loss: 4.13; acc: 0.41
Batch: 60; loss: 4.24; acc: 0.34
Batch: 80; loss: 5.03; acc: 0.31
Batch: 100; loss: 4.07; acc: 0.44
Batch: 120; loss: 3.94; acc: 0.39
Batch: 140; loss: 4.78; acc: 0.39
Val Epoch over. val_loss: 4.143020298070969; val_accuracy: 0.39570063694267515 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.41; acc: 0.33
Batch: 20; loss: 5.24; acc: 0.3
Batch: 40; loss: 4.31; acc: 0.42
Batch: 60; loss: 4.25; acc: 0.44
Batch: 80; loss: 2.89; acc: 0.42
Batch: 100; loss: 4.06; acc: 0.34
Batch: 120; loss: 4.53; acc: 0.44
Batch: 140; loss: 4.78; acc: 0.36
Batch: 160; loss: 4.06; acc: 0.38
Batch: 180; loss: 4.11; acc: 0.42
Batch: 200; loss: 5.12; acc: 0.3
Batch: 220; loss: 4.86; acc: 0.34
Batch: 240; loss: 3.47; acc: 0.5
Batch: 260; loss: 4.05; acc: 0.42
Batch: 280; loss: 3.88; acc: 0.34
Batch: 300; loss: 4.28; acc: 0.31
Batch: 320; loss: 4.6; acc: 0.28
Batch: 340; loss: 3.33; acc: 0.39
Batch: 360; loss: 4.22; acc: 0.34
Batch: 380; loss: 2.91; acc: 0.47
Batch: 400; loss: 4.39; acc: 0.36
Batch: 420; loss: 4.05; acc: 0.45
Batch: 440; loss: 3.88; acc: 0.38
Batch: 460; loss: 4.15; acc: 0.33
Batch: 480; loss: 3.9; acc: 0.44
Batch: 500; loss: 3.22; acc: 0.45
Batch: 520; loss: 4.8; acc: 0.28
Batch: 540; loss: 5.52; acc: 0.31
Batch: 560; loss: 3.03; acc: 0.5
Batch: 580; loss: 3.89; acc: 0.42
Batch: 600; loss: 5.59; acc: 0.25
Batch: 620; loss: 4.18; acc: 0.34
Train Epoch over. train_loss: 4.11; train_accuracy: 0.4 

Batch: 0; loss: 3.75; acc: 0.38
Batch: 20; loss: 5.4; acc: 0.28
Batch: 40; loss: 4.13; acc: 0.44
Batch: 60; loss: 4.28; acc: 0.34
Batch: 80; loss: 5.02; acc: 0.31
Batch: 100; loss: 4.03; acc: 0.44
Batch: 120; loss: 3.92; acc: 0.39
Batch: 140; loss: 4.79; acc: 0.39
Val Epoch over. val_loss: 4.145614797901955; val_accuracy: 0.3940087579617834 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 4.23; acc: 0.45
Batch: 20; loss: 5.07; acc: 0.34
Batch: 40; loss: 4.23; acc: 0.39
Batch: 60; loss: 3.55; acc: 0.41
Batch: 80; loss: 4.68; acc: 0.34
Batch: 100; loss: 3.64; acc: 0.48
Batch: 120; loss: 4.57; acc: 0.3
Batch: 140; loss: 4.01; acc: 0.36
Batch: 160; loss: 4.38; acc: 0.39
Batch: 180; loss: 4.47; acc: 0.31
Batch: 200; loss: 3.22; acc: 0.38
Batch: 220; loss: 4.69; acc: 0.38
Batch: 240; loss: 3.93; acc: 0.5
Batch: 260; loss: 3.12; acc: 0.47
Batch: 280; loss: 3.6; acc: 0.47
Batch: 300; loss: 4.72; acc: 0.38
Batch: 320; loss: 3.96; acc: 0.3
Batch: 340; loss: 3.4; acc: 0.47
Batch: 360; loss: 4.26; acc: 0.33
Batch: 380; loss: 4.69; acc: 0.42
Batch: 400; loss: 4.67; acc: 0.31
Batch: 420; loss: 3.96; acc: 0.38
Batch: 440; loss: 3.56; acc: 0.47
Batch: 460; loss: 4.14; acc: 0.47
Batch: 480; loss: 3.97; acc: 0.47
Batch: 500; loss: 4.11; acc: 0.38
Batch: 520; loss: 4.6; acc: 0.39
Batch: 540; loss: 2.75; acc: 0.48
Batch: 560; loss: 3.92; acc: 0.45
Batch: 580; loss: 3.18; acc: 0.42
Batch: 600; loss: 3.88; acc: 0.45
Batch: 620; loss: 3.88; acc: 0.48
Train Epoch over. train_loss: 4.11; train_accuracy: 0.4 

Batch: 0; loss: 3.75; acc: 0.36
Batch: 20; loss: 5.41; acc: 0.28
Batch: 40; loss: 4.14; acc: 0.44
Batch: 60; loss: 4.27; acc: 0.34
Batch: 80; loss: 5.05; acc: 0.31
Batch: 100; loss: 4.02; acc: 0.44
Batch: 120; loss: 3.94; acc: 0.38
Batch: 140; loss: 4.79; acc: 0.39
Val Epoch over. val_loss: 4.145348852607095; val_accuracy: 0.3943073248407643 

plots/subspace_training/lenet/2020-01-10 13:15:25/d_dim_50_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 20957
elements in E: 4442600
fraction nonzero: 0.0047172826723090085
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 3.21; acc: 0.41
Batch: 40; loss: 3.95; acc: 0.23
Batch: 60; loss: 2.96; acc: 0.41
Batch: 80; loss: 2.65; acc: 0.39
Batch: 100; loss: 2.42; acc: 0.41
Batch: 120; loss: 3.62; acc: 0.39
Batch: 140; loss: 3.68; acc: 0.38
Batch: 160; loss: 2.98; acc: 0.5
Batch: 180; loss: 1.79; acc: 0.56
Batch: 200; loss: 2.42; acc: 0.48
Batch: 220; loss: 2.89; acc: 0.45
Batch: 240; loss: 2.8; acc: 0.53
Batch: 260; loss: 3.48; acc: 0.44
Batch: 280; loss: 2.63; acc: 0.34
Batch: 300; loss: 1.89; acc: 0.53
Batch: 320; loss: 2.75; acc: 0.52
Batch: 340; loss: 2.33; acc: 0.56
Batch: 360; loss: 2.17; acc: 0.52
Batch: 380; loss: 2.54; acc: 0.5
Batch: 400; loss: 2.93; acc: 0.47
Batch: 420; loss: 2.67; acc: 0.48
Batch: 440; loss: 2.76; acc: 0.47
Batch: 460; loss: 3.59; acc: 0.42
Batch: 480; loss: 2.03; acc: 0.53
Batch: 500; loss: 2.53; acc: 0.5
Batch: 520; loss: 2.5; acc: 0.55
Batch: 540; loss: 3.23; acc: 0.55
Batch: 560; loss: 2.15; acc: 0.61
Batch: 580; loss: 1.62; acc: 0.59
Batch: 600; loss: 2.33; acc: 0.56
Batch: 620; loss: 2.19; acc: 0.64
Train Epoch over. train_loss: 2.89; train_accuracy: 0.46 

Batch: 0; loss: 2.61; acc: 0.5
Batch: 20; loss: 3.1; acc: 0.44
Batch: 40; loss: 2.38; acc: 0.55
Batch: 60; loss: 2.5; acc: 0.5
Batch: 80; loss: 1.83; acc: 0.61
Batch: 100; loss: 2.45; acc: 0.52
Batch: 120; loss: 2.44; acc: 0.61
Batch: 140; loss: 3.56; acc: 0.38
Val Epoch over. val_loss: 2.3357538766921704; val_accuracy: 0.5367237261146497 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 2.4; acc: 0.47
Batch: 20; loss: 4.52; acc: 0.39
Batch: 40; loss: 2.62; acc: 0.52
Batch: 60; loss: 1.37; acc: 0.62
Batch: 80; loss: 2.54; acc: 0.45
Batch: 100; loss: 1.91; acc: 0.61
Batch: 120; loss: 1.68; acc: 0.69
Batch: 140; loss: 1.56; acc: 0.69
Batch: 160; loss: 4.32; acc: 0.42
Batch: 180; loss: 1.82; acc: 0.61
Batch: 200; loss: 2.42; acc: 0.53
Batch: 220; loss: 1.46; acc: 0.66
Batch: 240; loss: 2.33; acc: 0.56
Batch: 260; loss: 1.92; acc: 0.58
Batch: 280; loss: 4.26; acc: 0.44
Batch: 300; loss: 2.74; acc: 0.58
Batch: 320; loss: 3.01; acc: 0.47
Batch: 340; loss: 3.35; acc: 0.5
Batch: 360; loss: 2.56; acc: 0.58
Batch: 380; loss: 2.72; acc: 0.52
Batch: 400; loss: 2.59; acc: 0.48
Batch: 420; loss: 2.44; acc: 0.56
Batch: 440; loss: 2.34; acc: 0.55
Batch: 460; loss: 3.22; acc: 0.45
Batch: 480; loss: 1.78; acc: 0.66
Batch: 500; loss: 2.03; acc: 0.56
Batch: 520; loss: 1.67; acc: 0.56
Batch: 540; loss: 1.91; acc: 0.52
Batch: 560; loss: 2.5; acc: 0.62
Batch: 580; loss: 1.23; acc: 0.69
Batch: 600; loss: 2.26; acc: 0.61
Batch: 620; loss: 3.23; acc: 0.52
Train Epoch over. train_loss: 2.44; train_accuracy: 0.53 

Batch: 0; loss: 2.5; acc: 0.55
Batch: 20; loss: 3.14; acc: 0.53
Batch: 40; loss: 2.16; acc: 0.64
Batch: 60; loss: 3.0; acc: 0.47
Batch: 80; loss: 1.88; acc: 0.58
Batch: 100; loss: 2.07; acc: 0.55
Batch: 120; loss: 2.61; acc: 0.55
Batch: 140; loss: 4.08; acc: 0.42
Val Epoch over. val_loss: 2.4347080144153277; val_accuracy: 0.5428941082802548 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 1.71; acc: 0.69
Batch: 20; loss: 1.91; acc: 0.56
Batch: 40; loss: 2.62; acc: 0.5
Batch: 60; loss: 3.44; acc: 0.5
Batch: 80; loss: 2.04; acc: 0.59
Batch: 100; loss: 2.2; acc: 0.55
Batch: 120; loss: 2.11; acc: 0.58
Batch: 140; loss: 2.05; acc: 0.59
Batch: 160; loss: 2.45; acc: 0.52
Batch: 180; loss: 1.81; acc: 0.64
Batch: 200; loss: 2.49; acc: 0.58
Batch: 220; loss: 3.42; acc: 0.47
Batch: 240; loss: 1.45; acc: 0.7
Batch: 260; loss: 2.95; acc: 0.44
Batch: 280; loss: 2.7; acc: 0.52
Batch: 300; loss: 1.8; acc: 0.61
Batch: 320; loss: 3.14; acc: 0.48
Batch: 340; loss: 1.8; acc: 0.56
Batch: 360; loss: 2.29; acc: 0.64
Batch: 380; loss: 1.81; acc: 0.75
Batch: 400; loss: 2.0; acc: 0.52
Batch: 420; loss: 2.91; acc: 0.45
Batch: 440; loss: 1.75; acc: 0.64
Batch: 460; loss: 3.06; acc: 0.45
Batch: 480; loss: 4.81; acc: 0.33
Batch: 500; loss: 2.12; acc: 0.55
Batch: 520; loss: 2.64; acc: 0.52
Batch: 540; loss: 1.89; acc: 0.64
Batch: 560; loss: 2.21; acc: 0.55
Batch: 580; loss: 2.04; acc: 0.69
Batch: 600; loss: 1.56; acc: 0.61
Batch: 620; loss: 2.06; acc: 0.64
Train Epoch over. train_loss: 2.4; train_accuracy: 0.55 

Batch: 0; loss: 4.0; acc: 0.42
Batch: 20; loss: 5.07; acc: 0.41
Batch: 40; loss: 3.61; acc: 0.44
Batch: 60; loss: 4.5; acc: 0.39
Batch: 80; loss: 3.25; acc: 0.53
Batch: 100; loss: 3.62; acc: 0.47
Batch: 120; loss: 3.76; acc: 0.44
Batch: 140; loss: 4.44; acc: 0.41
Val Epoch over. val_loss: 3.9684004981047027; val_accuracy: 0.4297372611464968 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 3.44; acc: 0.44
Batch: 20; loss: 2.98; acc: 0.47
Batch: 40; loss: 1.85; acc: 0.64
Batch: 60; loss: 2.44; acc: 0.56
Batch: 80; loss: 2.32; acc: 0.45
Batch: 100; loss: 1.86; acc: 0.61
Batch: 120; loss: 1.99; acc: 0.66
Batch: 140; loss: 2.57; acc: 0.55
Batch: 160; loss: 1.7; acc: 0.61
Batch: 180; loss: 2.37; acc: 0.42
Batch: 200; loss: 1.41; acc: 0.72
Batch: 220; loss: 2.55; acc: 0.47
Batch: 240; loss: 2.4; acc: 0.48
Batch: 260; loss: 1.58; acc: 0.69
Batch: 280; loss: 2.53; acc: 0.48
Batch: 300; loss: 1.84; acc: 0.59
Batch: 320; loss: 2.31; acc: 0.47
Batch: 340; loss: 2.72; acc: 0.55
Batch: 360; loss: 3.11; acc: 0.41
Batch: 380; loss: 1.95; acc: 0.61
Batch: 400; loss: 2.16; acc: 0.5
Batch: 420; loss: 1.61; acc: 0.67
Batch: 440; loss: 2.78; acc: 0.45
Batch: 460; loss: 2.08; acc: 0.59
Batch: 480; loss: 2.19; acc: 0.56
Batch: 500; loss: 2.68; acc: 0.56
Batch: 520; loss: 2.04; acc: 0.59
Batch: 540; loss: 1.52; acc: 0.73
Batch: 560; loss: 2.55; acc: 0.52
Batch: 580; loss: 1.95; acc: 0.55
Batch: 600; loss: 2.38; acc: 0.56
Batch: 620; loss: 1.59; acc: 0.72
Train Epoch over. train_loss: 2.4; train_accuracy: 0.55 

Batch: 0; loss: 2.76; acc: 0.47
Batch: 20; loss: 4.35; acc: 0.41
Batch: 40; loss: 2.0; acc: 0.5
Batch: 60; loss: 3.17; acc: 0.47
Batch: 80; loss: 2.44; acc: 0.59
Batch: 100; loss: 2.91; acc: 0.5
Batch: 120; loss: 3.39; acc: 0.5
Batch: 140; loss: 3.54; acc: 0.42
Val Epoch over. val_loss: 2.9803524913301893; val_accuracy: 0.48974920382165604 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 2.89; acc: 0.52
Batch: 20; loss: 2.48; acc: 0.53
Batch: 40; loss: 2.48; acc: 0.58
Batch: 60; loss: 2.5; acc: 0.52
Batch: 80; loss: 1.87; acc: 0.64
Batch: 100; loss: 2.39; acc: 0.58
Batch: 120; loss: 2.12; acc: 0.55
Batch: 140; loss: 1.96; acc: 0.58
Batch: 160; loss: 2.32; acc: 0.64
Batch: 180; loss: 1.62; acc: 0.61
Batch: 200; loss: 2.06; acc: 0.58
Batch: 220; loss: 2.76; acc: 0.52
Batch: 240; loss: 2.47; acc: 0.48
Batch: 260; loss: 1.98; acc: 0.56
Batch: 280; loss: 2.33; acc: 0.47
Batch: 300; loss: 2.32; acc: 0.52
Batch: 320; loss: 2.62; acc: 0.5
Batch: 340; loss: 2.1; acc: 0.59
Batch: 360; loss: 2.45; acc: 0.56
Batch: 380; loss: 1.95; acc: 0.61
Batch: 400; loss: 1.73; acc: 0.55
Batch: 420; loss: 1.65; acc: 0.56
Batch: 440; loss: 3.16; acc: 0.5
Batch: 460; loss: 2.38; acc: 0.45
Batch: 480; loss: 2.44; acc: 0.53
Batch: 500; loss: 2.65; acc: 0.48
Batch: 520; loss: 1.96; acc: 0.56
Batch: 540; loss: 2.77; acc: 0.41
Batch: 560; loss: 1.7; acc: 0.66
Batch: 580; loss: 2.27; acc: 0.61
Batch: 600; loss: 2.66; acc: 0.55
Batch: 620; loss: 1.77; acc: 0.64
Train Epoch over. train_loss: 2.37; train_accuracy: 0.56 

Batch: 0; loss: 2.2; acc: 0.59
Batch: 20; loss: 3.52; acc: 0.41
Batch: 40; loss: 2.26; acc: 0.48
Batch: 60; loss: 2.68; acc: 0.44
Batch: 80; loss: 2.52; acc: 0.55
Batch: 100; loss: 2.75; acc: 0.48
Batch: 120; loss: 3.2; acc: 0.45
Batch: 140; loss: 3.39; acc: 0.39
Val Epoch over. val_loss: 2.65451786358645; val_accuracy: 0.4922372611464968 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 2.79; acc: 0.52
Batch: 20; loss: 2.73; acc: 0.47
Batch: 40; loss: 2.72; acc: 0.48
Batch: 60; loss: 2.85; acc: 0.55
Batch: 80; loss: 2.95; acc: 0.55
Batch: 100; loss: 1.97; acc: 0.58
Batch: 120; loss: 2.67; acc: 0.5
Batch: 140; loss: 2.61; acc: 0.41
Batch: 160; loss: 3.38; acc: 0.41
Batch: 180; loss: 2.52; acc: 0.5
Batch: 200; loss: 2.2; acc: 0.56
Batch: 220; loss: 1.65; acc: 0.66
Batch: 240; loss: 2.97; acc: 0.42
Batch: 260; loss: 2.78; acc: 0.53
Batch: 280; loss: 1.76; acc: 0.64
Batch: 300; loss: 1.55; acc: 0.67
Batch: 320; loss: 3.29; acc: 0.44
Batch: 340; loss: 2.5; acc: 0.53
Batch: 360; loss: 1.73; acc: 0.64
Batch: 380; loss: 2.13; acc: 0.58
Batch: 400; loss: 2.02; acc: 0.59
Batch: 420; loss: 2.19; acc: 0.61
Batch: 440; loss: 2.39; acc: 0.55
Batch: 460; loss: 1.98; acc: 0.66
Batch: 480; loss: 2.3; acc: 0.56
Batch: 500; loss: 1.68; acc: 0.56
Batch: 520; loss: 2.43; acc: 0.53
Batch: 540; loss: 1.38; acc: 0.64
Batch: 560; loss: 2.29; acc: 0.61
Batch: 580; loss: 2.81; acc: 0.56
Batch: 600; loss: 1.72; acc: 0.59
Batch: 620; loss: 3.86; acc: 0.45
Train Epoch over. train_loss: 2.38; train_accuracy: 0.56 

Batch: 0; loss: 1.92; acc: 0.56
Batch: 20; loss: 2.67; acc: 0.47
Batch: 40; loss: 1.93; acc: 0.48
Batch: 60; loss: 2.45; acc: 0.5
Batch: 80; loss: 1.81; acc: 0.52
Batch: 100; loss: 2.04; acc: 0.58
Batch: 120; loss: 2.61; acc: 0.55
Batch: 140; loss: 3.47; acc: 0.47
Val Epoch over. val_loss: 2.3385190963745117; val_accuracy: 0.5329418789808917 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 2.45; acc: 0.56
Batch: 20; loss: 2.08; acc: 0.52
Batch: 40; loss: 2.38; acc: 0.62
Batch: 60; loss: 2.81; acc: 0.48
Batch: 80; loss: 2.59; acc: 0.56
Batch: 100; loss: 1.73; acc: 0.66
Batch: 120; loss: 3.15; acc: 0.42
Batch: 140; loss: 2.8; acc: 0.53
Batch: 160; loss: 2.43; acc: 0.5
Batch: 180; loss: 2.4; acc: 0.53
Batch: 200; loss: 2.04; acc: 0.56
Batch: 220; loss: 1.92; acc: 0.48
Batch: 240; loss: 4.44; acc: 0.38
Batch: 260; loss: 1.21; acc: 0.66
Batch: 280; loss: 2.19; acc: 0.61
Batch: 300; loss: 2.02; acc: 0.62
Batch: 320; loss: 2.01; acc: 0.59
Batch: 340; loss: 1.92; acc: 0.59
Batch: 360; loss: 2.76; acc: 0.53
Batch: 380; loss: 2.15; acc: 0.58
Batch: 400; loss: 1.81; acc: 0.58
Batch: 420; loss: 2.02; acc: 0.62
Batch: 440; loss: 2.24; acc: 0.56
Batch: 460; loss: 2.55; acc: 0.61
Batch: 480; loss: 2.02; acc: 0.61
Batch: 500; loss: 2.35; acc: 0.5
Batch: 520; loss: 2.95; acc: 0.42
Batch: 540; loss: 2.2; acc: 0.48
Batch: 560; loss: 1.92; acc: 0.64
Batch: 580; loss: 2.32; acc: 0.53
Batch: 600; loss: 2.35; acc: 0.59
Batch: 620; loss: 2.22; acc: 0.5
Train Epoch over. train_loss: 2.38; train_accuracy: 0.55 

Batch: 0; loss: 1.72; acc: 0.66
Batch: 20; loss: 2.58; acc: 0.53
Batch: 40; loss: 1.7; acc: 0.61
Batch: 60; loss: 2.16; acc: 0.56
Batch: 80; loss: 1.76; acc: 0.56
Batch: 100; loss: 1.99; acc: 0.56
Batch: 120; loss: 2.42; acc: 0.56
Batch: 140; loss: 3.4; acc: 0.5
Val Epoch over. val_loss: 2.1140628430494077; val_accuracy: 0.5869824840764332 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 2.21; acc: 0.53
Batch: 20; loss: 2.5; acc: 0.53
Batch: 40; loss: 2.55; acc: 0.55
Batch: 60; loss: 2.53; acc: 0.64
Batch: 80; loss: 2.53; acc: 0.55
Batch: 100; loss: 2.11; acc: 0.64
Batch: 120; loss: 1.97; acc: 0.61
Batch: 140; loss: 1.88; acc: 0.53
Batch: 160; loss: 2.04; acc: 0.56
Batch: 180; loss: 2.32; acc: 0.53
Batch: 200; loss: 2.72; acc: 0.48
Batch: 220; loss: 1.99; acc: 0.61
Batch: 240; loss: 1.86; acc: 0.58
Batch: 260; loss: 2.27; acc: 0.53
Batch: 280; loss: 2.86; acc: 0.44
Batch: 300; loss: 1.62; acc: 0.64
Batch: 320; loss: 1.75; acc: 0.66
Batch: 340; loss: 3.61; acc: 0.47
Batch: 360; loss: 3.71; acc: 0.38
Batch: 380; loss: 2.83; acc: 0.48
Batch: 400; loss: 2.38; acc: 0.62
Batch: 420; loss: 2.11; acc: 0.64
Batch: 440; loss: 2.1; acc: 0.61
Batch: 460; loss: 3.18; acc: 0.58
Batch: 480; loss: 2.11; acc: 0.56
Batch: 500; loss: 2.21; acc: 0.59
Batch: 520; loss: 1.4; acc: 0.69
Batch: 540; loss: 3.79; acc: 0.39
Batch: 560; loss: 2.63; acc: 0.56
Batch: 580; loss: 1.99; acc: 0.59
Batch: 600; loss: 1.32; acc: 0.7
Batch: 620; loss: 2.97; acc: 0.53
Train Epoch over. train_loss: 2.42; train_accuracy: 0.56 

Batch: 0; loss: 1.78; acc: 0.64
Batch: 20; loss: 2.54; acc: 0.48
Batch: 40; loss: 1.49; acc: 0.69
Batch: 60; loss: 2.14; acc: 0.53
Batch: 80; loss: 2.14; acc: 0.56
Batch: 100; loss: 2.55; acc: 0.53
Batch: 120; loss: 2.31; acc: 0.58
Batch: 140; loss: 3.21; acc: 0.48
Val Epoch over. val_loss: 2.1934158251543714; val_accuracy: 0.5706608280254777 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 1.2; acc: 0.69
Batch: 20; loss: 3.29; acc: 0.53
Batch: 40; loss: 2.24; acc: 0.56
Batch: 60; loss: 2.72; acc: 0.5
Batch: 80; loss: 3.6; acc: 0.39
Batch: 100; loss: 2.53; acc: 0.5
Batch: 120; loss: 2.5; acc: 0.59
Batch: 140; loss: 3.63; acc: 0.44
Batch: 160; loss: 2.41; acc: 0.58
Batch: 180; loss: 2.03; acc: 0.59
Batch: 200; loss: 2.36; acc: 0.5
Batch: 220; loss: 2.89; acc: 0.58
Batch: 240; loss: 3.04; acc: 0.52
Batch: 260; loss: 1.68; acc: 0.67
Batch: 280; loss: 2.6; acc: 0.55
Batch: 300; loss: 2.0; acc: 0.61
Batch: 320; loss: 2.03; acc: 0.64
Batch: 340; loss: 1.55; acc: 0.61
Batch: 360; loss: 2.02; acc: 0.64
Batch: 380; loss: 1.72; acc: 0.64
Batch: 400; loss: 2.45; acc: 0.62
Batch: 420; loss: 2.83; acc: 0.55
Batch: 440; loss: 1.8; acc: 0.62
Batch: 460; loss: 2.38; acc: 0.53
Batch: 480; loss: 1.07; acc: 0.72
Batch: 500; loss: 2.42; acc: 0.59
Batch: 520; loss: 2.7; acc: 0.52
Batch: 540; loss: 2.03; acc: 0.55
Batch: 560; loss: 1.64; acc: 0.56
Batch: 580; loss: 2.64; acc: 0.52
Batch: 600; loss: 2.26; acc: 0.62
Batch: 620; loss: 3.16; acc: 0.48
Train Epoch over. train_loss: 2.39; train_accuracy: 0.56 

Batch: 0; loss: 1.92; acc: 0.62
Batch: 20; loss: 2.82; acc: 0.52
Batch: 40; loss: 1.62; acc: 0.64
Batch: 60; loss: 2.15; acc: 0.53
Batch: 80; loss: 2.51; acc: 0.53
Batch: 100; loss: 2.1; acc: 0.53
Batch: 120; loss: 2.7; acc: 0.58
Batch: 140; loss: 3.31; acc: 0.5
Val Epoch over. val_loss: 2.3050325156017473; val_accuracy: 0.560609076433121 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 1.58; acc: 0.67
Batch: 20; loss: 2.2; acc: 0.45
Batch: 40; loss: 2.99; acc: 0.5
Batch: 60; loss: 2.59; acc: 0.5
Batch: 80; loss: 1.91; acc: 0.48
Batch: 100; loss: 2.54; acc: 0.66
Batch: 120; loss: 2.24; acc: 0.58
Batch: 140; loss: 2.31; acc: 0.45
Batch: 160; loss: 2.83; acc: 0.53
Batch: 180; loss: 2.21; acc: 0.55
Batch: 200; loss: 3.37; acc: 0.41
Batch: 220; loss: 1.93; acc: 0.62
Batch: 240; loss: 2.48; acc: 0.56
Batch: 260; loss: 2.36; acc: 0.5
Batch: 280; loss: 3.15; acc: 0.45
Batch: 300; loss: 1.97; acc: 0.66
Batch: 320; loss: 2.3; acc: 0.59
Batch: 340; loss: 2.03; acc: 0.67
Batch: 360; loss: 2.12; acc: 0.64
Batch: 380; loss: 1.73; acc: 0.62
Batch: 400; loss: 1.85; acc: 0.58
Batch: 420; loss: 2.41; acc: 0.5
Batch: 440; loss: 2.3; acc: 0.61
Batch: 460; loss: 3.03; acc: 0.48
Batch: 480; loss: 3.2; acc: 0.47
Batch: 500; loss: 3.03; acc: 0.5
Batch: 520; loss: 2.14; acc: 0.62
Batch: 540; loss: 3.29; acc: 0.52
Batch: 560; loss: 2.91; acc: 0.42
Batch: 580; loss: 3.32; acc: 0.45
Batch: 600; loss: 2.25; acc: 0.58
Batch: 620; loss: 2.19; acc: 0.56
Train Epoch over. train_loss: 2.41; train_accuracy: 0.56 

Batch: 0; loss: 2.6; acc: 0.52
Batch: 20; loss: 3.9; acc: 0.41
Batch: 40; loss: 2.09; acc: 0.58
Batch: 60; loss: 3.13; acc: 0.45
Batch: 80; loss: 2.42; acc: 0.58
Batch: 100; loss: 3.03; acc: 0.55
Batch: 120; loss: 2.86; acc: 0.52
Batch: 140; loss: 3.31; acc: 0.52
Val Epoch over. val_loss: 2.896411067361285; val_accuracy: 0.4901472929936306 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.84; acc: 0.48
Batch: 20; loss: 2.33; acc: 0.47
Batch: 40; loss: 3.31; acc: 0.47
Batch: 60; loss: 1.81; acc: 0.61
Batch: 80; loss: 2.24; acc: 0.59
Batch: 100; loss: 2.33; acc: 0.55
Batch: 120; loss: 2.46; acc: 0.56
Batch: 140; loss: 1.68; acc: 0.62
Batch: 160; loss: 1.94; acc: 0.61
Batch: 180; loss: 1.66; acc: 0.56
Batch: 200; loss: 2.13; acc: 0.56
Batch: 220; loss: 2.13; acc: 0.55
Batch: 240; loss: 1.88; acc: 0.64
Batch: 260; loss: 2.66; acc: 0.56
Batch: 280; loss: 2.04; acc: 0.58
Batch: 300; loss: 1.52; acc: 0.67
Batch: 320; loss: 1.86; acc: 0.59
Batch: 340; loss: 1.88; acc: 0.64
Batch: 360; loss: 2.16; acc: 0.64
Batch: 380; loss: 2.54; acc: 0.62
Batch: 400; loss: 1.79; acc: 0.67
Batch: 420; loss: 1.85; acc: 0.61
Batch: 440; loss: 1.99; acc: 0.55
Batch: 460; loss: 2.19; acc: 0.56
Batch: 480; loss: 1.69; acc: 0.62
Batch: 500; loss: 2.21; acc: 0.52
Batch: 520; loss: 2.86; acc: 0.56
Batch: 540; loss: 2.41; acc: 0.56
Batch: 560; loss: 2.16; acc: 0.52
Batch: 580; loss: 1.36; acc: 0.64
Batch: 600; loss: 2.4; acc: 0.58
Batch: 620; loss: 1.69; acc: 0.59
Train Epoch over. train_loss: 1.91; train_accuracy: 0.61 

Batch: 0; loss: 1.41; acc: 0.66
Batch: 20; loss: 2.4; acc: 0.59
Batch: 40; loss: 1.37; acc: 0.67
Batch: 60; loss: 1.98; acc: 0.61
Batch: 80; loss: 1.69; acc: 0.58
Batch: 100; loss: 1.9; acc: 0.61
Batch: 120; loss: 2.16; acc: 0.61
Batch: 140; loss: 3.29; acc: 0.52
Val Epoch over. val_loss: 1.8833373644549376; val_accuracy: 0.6164410828025477 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.42; acc: 0.72
Batch: 20; loss: 1.92; acc: 0.64
Batch: 40; loss: 1.93; acc: 0.59
Batch: 60; loss: 2.64; acc: 0.59
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.93; acc: 0.66
Batch: 120; loss: 2.99; acc: 0.59
Batch: 140; loss: 1.7; acc: 0.64
Batch: 160; loss: 2.19; acc: 0.48
Batch: 180; loss: 2.34; acc: 0.52
Batch: 200; loss: 2.07; acc: 0.53
Batch: 220; loss: 1.88; acc: 0.62
Batch: 240; loss: 1.89; acc: 0.59
Batch: 260; loss: 1.52; acc: 0.59
Batch: 280; loss: 2.19; acc: 0.62
Batch: 300; loss: 2.07; acc: 0.61
Batch: 320; loss: 1.53; acc: 0.7
Batch: 340; loss: 1.53; acc: 0.66
Batch: 360; loss: 1.19; acc: 0.73
Batch: 380; loss: 2.5; acc: 0.48
Batch: 400; loss: 1.58; acc: 0.61
Batch: 420; loss: 1.99; acc: 0.56
Batch: 440; loss: 1.22; acc: 0.73
Batch: 460; loss: 1.64; acc: 0.67
Batch: 480; loss: 2.02; acc: 0.58
Batch: 500; loss: 1.99; acc: 0.59
Batch: 520; loss: 1.72; acc: 0.67
Batch: 540; loss: 1.78; acc: 0.62
Batch: 560; loss: 1.52; acc: 0.75
Batch: 580; loss: 2.62; acc: 0.53
Batch: 600; loss: 1.9; acc: 0.59
Batch: 620; loss: 1.48; acc: 0.72
Train Epoch over. train_loss: 1.88; train_accuracy: 0.62 

Batch: 0; loss: 1.46; acc: 0.66
Batch: 20; loss: 2.29; acc: 0.58
Batch: 40; loss: 1.4; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.61
Batch: 80; loss: 1.73; acc: 0.58
Batch: 100; loss: 1.85; acc: 0.62
Batch: 120; loss: 2.18; acc: 0.62
Batch: 140; loss: 3.31; acc: 0.53
Val Epoch over. val_loss: 1.873584695682404; val_accuracy: 0.6155453821656051 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.08; acc: 0.7
Batch: 20; loss: 1.71; acc: 0.61
Batch: 40; loss: 1.59; acc: 0.64
Batch: 60; loss: 1.47; acc: 0.7
Batch: 80; loss: 1.72; acc: 0.62
Batch: 100; loss: 1.4; acc: 0.66
Batch: 120; loss: 1.96; acc: 0.62
Batch: 140; loss: 1.91; acc: 0.62
Batch: 160; loss: 1.42; acc: 0.77
Batch: 180; loss: 1.62; acc: 0.67
Batch: 200; loss: 2.12; acc: 0.62
Batch: 220; loss: 1.66; acc: 0.61
Batch: 240; loss: 2.04; acc: 0.56
Batch: 260; loss: 1.87; acc: 0.66
Batch: 280; loss: 1.91; acc: 0.58
Batch: 300; loss: 1.36; acc: 0.61
Batch: 320; loss: 2.74; acc: 0.52
Batch: 340; loss: 2.33; acc: 0.59
Batch: 360; loss: 1.8; acc: 0.75
Batch: 380; loss: 1.96; acc: 0.67
Batch: 400; loss: 1.3; acc: 0.67
Batch: 420; loss: 2.33; acc: 0.67
Batch: 440; loss: 1.75; acc: 0.59
Batch: 460; loss: 1.42; acc: 0.69
Batch: 480; loss: 1.67; acc: 0.66
Batch: 500; loss: 2.01; acc: 0.62
Batch: 520; loss: 1.46; acc: 0.66
Batch: 540; loss: 1.83; acc: 0.66
Batch: 560; loss: 2.54; acc: 0.61
Batch: 580; loss: 1.82; acc: 0.62
Batch: 600; loss: 2.03; acc: 0.61
Batch: 620; loss: 1.84; acc: 0.62
Train Epoch over. train_loss: 1.87; train_accuracy: 0.63 

Batch: 0; loss: 1.43; acc: 0.66
Batch: 20; loss: 2.16; acc: 0.56
Batch: 40; loss: 1.34; acc: 0.69
Batch: 60; loss: 2.01; acc: 0.62
Batch: 80; loss: 1.77; acc: 0.55
Batch: 100; loss: 1.78; acc: 0.62
Batch: 120; loss: 2.15; acc: 0.62
Batch: 140; loss: 3.29; acc: 0.53
Val Epoch over. val_loss: 1.8726438177619011; val_accuracy: 0.6196257961783439 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.22; acc: 0.72
Batch: 20; loss: 2.19; acc: 0.66
Batch: 40; loss: 1.5; acc: 0.67
Batch: 60; loss: 1.71; acc: 0.58
Batch: 80; loss: 1.52; acc: 0.72
Batch: 100; loss: 2.28; acc: 0.58
Batch: 120; loss: 1.94; acc: 0.56
Batch: 140; loss: 1.39; acc: 0.61
Batch: 160; loss: 2.18; acc: 0.62
Batch: 180; loss: 1.48; acc: 0.66
Batch: 200; loss: 1.43; acc: 0.7
Batch: 220; loss: 2.03; acc: 0.61
Batch: 240; loss: 1.43; acc: 0.75
Batch: 260; loss: 1.37; acc: 0.62
Batch: 280; loss: 1.31; acc: 0.69
Batch: 300; loss: 1.89; acc: 0.59
Batch: 320; loss: 2.13; acc: 0.61
Batch: 340; loss: 2.01; acc: 0.52
Batch: 360; loss: 1.82; acc: 0.58
Batch: 380; loss: 1.9; acc: 0.61
Batch: 400; loss: 1.9; acc: 0.61
Batch: 420; loss: 2.64; acc: 0.52
Batch: 440; loss: 1.99; acc: 0.58
Batch: 460; loss: 2.13; acc: 0.59
Batch: 480; loss: 1.73; acc: 0.72
Batch: 500; loss: 1.86; acc: 0.62
Batch: 520; loss: 2.06; acc: 0.64
Batch: 540; loss: 1.6; acc: 0.73
Batch: 560; loss: 1.47; acc: 0.69
Batch: 580; loss: 2.02; acc: 0.59
Batch: 600; loss: 2.65; acc: 0.59
Batch: 620; loss: 1.99; acc: 0.64
Train Epoch over. train_loss: 1.87; train_accuracy: 0.63 

Batch: 0; loss: 1.36; acc: 0.69
Batch: 20; loss: 2.31; acc: 0.58
Batch: 40; loss: 1.38; acc: 0.64
Batch: 60; loss: 1.98; acc: 0.59
Batch: 80; loss: 1.85; acc: 0.56
Batch: 100; loss: 1.74; acc: 0.61
Batch: 120; loss: 2.25; acc: 0.59
Batch: 140; loss: 3.2; acc: 0.53
Val Epoch over. val_loss: 1.861827395524189; val_accuracy: 0.62390525477707 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.79; acc: 0.56
Batch: 20; loss: 1.34; acc: 0.67
Batch: 40; loss: 1.38; acc: 0.8
Batch: 60; loss: 1.91; acc: 0.62
Batch: 80; loss: 1.58; acc: 0.7
Batch: 100; loss: 2.21; acc: 0.58
Batch: 120; loss: 2.25; acc: 0.66
Batch: 140; loss: 1.91; acc: 0.56
Batch: 160; loss: 2.36; acc: 0.55
Batch: 180; loss: 1.69; acc: 0.62
Batch: 200; loss: 2.04; acc: 0.61
Batch: 220; loss: 1.3; acc: 0.73
Batch: 240; loss: 2.13; acc: 0.56
Batch: 260; loss: 2.59; acc: 0.48
Batch: 280; loss: 1.9; acc: 0.52
Batch: 300; loss: 2.33; acc: 0.61
Batch: 320; loss: 1.3; acc: 0.7
Batch: 340; loss: 2.1; acc: 0.62
Batch: 360; loss: 1.3; acc: 0.7
Batch: 380; loss: 2.08; acc: 0.64
Batch: 400; loss: 1.34; acc: 0.67
Batch: 420; loss: 1.53; acc: 0.7
Batch: 440; loss: 1.66; acc: 0.58
Batch: 460; loss: 1.56; acc: 0.73
Batch: 480; loss: 1.21; acc: 0.75
Batch: 500; loss: 2.29; acc: 0.52
Batch: 520; loss: 2.43; acc: 0.59
Batch: 540; loss: 1.72; acc: 0.69
Batch: 560; loss: 1.89; acc: 0.64
Batch: 580; loss: 2.05; acc: 0.61
Batch: 600; loss: 1.8; acc: 0.72
Batch: 620; loss: 2.38; acc: 0.53
Train Epoch over. train_loss: 1.87; train_accuracy: 0.63 

Batch: 0; loss: 1.38; acc: 0.67
Batch: 20; loss: 2.42; acc: 0.61
Batch: 40; loss: 1.4; acc: 0.67
Batch: 60; loss: 1.96; acc: 0.61
Batch: 80; loss: 1.8; acc: 0.59
Batch: 100; loss: 1.86; acc: 0.62
Batch: 120; loss: 2.21; acc: 0.66
Batch: 140; loss: 3.25; acc: 0.5
Val Epoch over. val_loss: 1.872291710726015; val_accuracy: 0.6227109872611465 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 3.01; acc: 0.58
Batch: 20; loss: 2.26; acc: 0.58
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 1.78; acc: 0.62
Batch: 80; loss: 1.79; acc: 0.55
Batch: 100; loss: 1.76; acc: 0.62
Batch: 120; loss: 2.23; acc: 0.69
Batch: 140; loss: 1.86; acc: 0.7
Batch: 160; loss: 1.86; acc: 0.67
Batch: 180; loss: 2.11; acc: 0.58
Batch: 200; loss: 2.86; acc: 0.53
Batch: 220; loss: 1.11; acc: 0.66
Batch: 240; loss: 1.34; acc: 0.69
Batch: 260; loss: 1.64; acc: 0.72
Batch: 280; loss: 1.68; acc: 0.69
Batch: 300; loss: 1.29; acc: 0.73
Batch: 320; loss: 2.16; acc: 0.55
Batch: 340; loss: 1.76; acc: 0.66
Batch: 360; loss: 2.57; acc: 0.44
Batch: 380; loss: 1.82; acc: 0.58
Batch: 400; loss: 1.51; acc: 0.72
Batch: 420; loss: 1.96; acc: 0.66
Batch: 440; loss: 1.96; acc: 0.62
Batch: 460; loss: 1.33; acc: 0.69
Batch: 480; loss: 1.48; acc: 0.72
Batch: 500; loss: 1.89; acc: 0.62
Batch: 520; loss: 1.19; acc: 0.66
Batch: 540; loss: 1.37; acc: 0.77
Batch: 560; loss: 2.31; acc: 0.53
Batch: 580; loss: 1.88; acc: 0.64
Batch: 600; loss: 1.99; acc: 0.59
Batch: 620; loss: 2.5; acc: 0.56
Train Epoch over. train_loss: 1.87; train_accuracy: 0.63 

Batch: 0; loss: 1.36; acc: 0.67
Batch: 20; loss: 2.07; acc: 0.58
Batch: 40; loss: 1.39; acc: 0.69
Batch: 60; loss: 1.98; acc: 0.62
Batch: 80; loss: 1.84; acc: 0.55
Batch: 100; loss: 1.68; acc: 0.66
Batch: 120; loss: 2.28; acc: 0.61
Batch: 140; loss: 3.31; acc: 0.52
Val Epoch over. val_loss: 1.8695104668854148; val_accuracy: 0.6254976114649682 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.9; acc: 0.55
Batch: 20; loss: 1.79; acc: 0.67
Batch: 40; loss: 2.16; acc: 0.56
Batch: 60; loss: 2.18; acc: 0.61
Batch: 80; loss: 1.16; acc: 0.69
Batch: 100; loss: 1.25; acc: 0.7
Batch: 120; loss: 1.44; acc: 0.7
Batch: 140; loss: 1.49; acc: 0.61
Batch: 160; loss: 1.28; acc: 0.66
Batch: 180; loss: 2.22; acc: 0.62
Batch: 200; loss: 1.97; acc: 0.73
Batch: 220; loss: 1.55; acc: 0.66
Batch: 240; loss: 1.6; acc: 0.7
Batch: 260; loss: 1.81; acc: 0.64
Batch: 280; loss: 2.65; acc: 0.53
Batch: 300; loss: 2.02; acc: 0.56
Batch: 320; loss: 1.87; acc: 0.58
Batch: 340; loss: 1.7; acc: 0.7
Batch: 360; loss: 2.85; acc: 0.56
Batch: 380; loss: 1.32; acc: 0.7
Batch: 400; loss: 1.93; acc: 0.56
Batch: 420; loss: 1.5; acc: 0.69
Batch: 440; loss: 1.61; acc: 0.67
Batch: 460; loss: 1.68; acc: 0.64
Batch: 480; loss: 2.17; acc: 0.53
Batch: 500; loss: 1.31; acc: 0.69
Batch: 520; loss: 2.15; acc: 0.7
Batch: 540; loss: 1.9; acc: 0.67
Batch: 560; loss: 2.46; acc: 0.61
Batch: 580; loss: 2.2; acc: 0.58
Batch: 600; loss: 0.85; acc: 0.83
Batch: 620; loss: 2.49; acc: 0.67
Train Epoch over. train_loss: 1.87; train_accuracy: 0.63 

Batch: 0; loss: 1.31; acc: 0.69
Batch: 20; loss: 2.25; acc: 0.59
Batch: 40; loss: 1.46; acc: 0.67
Batch: 60; loss: 2.0; acc: 0.64
Batch: 80; loss: 1.8; acc: 0.56
Batch: 100; loss: 1.67; acc: 0.62
Batch: 120; loss: 2.26; acc: 0.62
Batch: 140; loss: 3.21; acc: 0.53
Val Epoch over. val_loss: 1.8566296301829588; val_accuracy: 0.6249004777070064 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.08; acc: 0.62
Batch: 20; loss: 1.75; acc: 0.66
Batch: 40; loss: 2.23; acc: 0.61
Batch: 60; loss: 1.95; acc: 0.58
Batch: 80; loss: 1.01; acc: 0.72
Batch: 100; loss: 1.63; acc: 0.66
Batch: 120; loss: 2.33; acc: 0.61
Batch: 140; loss: 1.91; acc: 0.67
Batch: 160; loss: 1.3; acc: 0.67
Batch: 180; loss: 1.78; acc: 0.7
Batch: 200; loss: 1.5; acc: 0.69
Batch: 220; loss: 2.31; acc: 0.59
Batch: 240; loss: 1.64; acc: 0.56
Batch: 260; loss: 2.73; acc: 0.53
Batch: 280; loss: 1.29; acc: 0.67
Batch: 300; loss: 2.43; acc: 0.56
Batch: 320; loss: 2.24; acc: 0.59
Batch: 340; loss: 2.02; acc: 0.69
Batch: 360; loss: 1.84; acc: 0.67
Batch: 380; loss: 2.03; acc: 0.61
Batch: 400; loss: 2.37; acc: 0.59
Batch: 420; loss: 1.91; acc: 0.64
Batch: 440; loss: 1.31; acc: 0.66
Batch: 460; loss: 2.18; acc: 0.55
Batch: 480; loss: 1.91; acc: 0.66
Batch: 500; loss: 2.14; acc: 0.64
Batch: 520; loss: 2.05; acc: 0.62
Batch: 540; loss: 1.72; acc: 0.64
Batch: 560; loss: 1.76; acc: 0.62
Batch: 580; loss: 2.18; acc: 0.62
Batch: 600; loss: 1.1; acc: 0.73
Batch: 620; loss: 1.96; acc: 0.58
Train Epoch over. train_loss: 1.87; train_accuracy: 0.63 

Batch: 0; loss: 1.33; acc: 0.69
Batch: 20; loss: 2.28; acc: 0.58
Batch: 40; loss: 1.26; acc: 0.67
Batch: 60; loss: 2.02; acc: 0.59
Batch: 80; loss: 1.86; acc: 0.61
Batch: 100; loss: 1.73; acc: 0.62
Batch: 120; loss: 2.2; acc: 0.62
Batch: 140; loss: 3.18; acc: 0.52
Val Epoch over. val_loss: 1.8627045951831114; val_accuracy: 0.6242038216560509 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 2.3; acc: 0.66
Batch: 20; loss: 1.4; acc: 0.66
Batch: 40; loss: 2.09; acc: 0.58
Batch: 60; loss: 2.58; acc: 0.56
Batch: 80; loss: 2.39; acc: 0.61
Batch: 100; loss: 1.78; acc: 0.64
Batch: 120; loss: 1.85; acc: 0.61
Batch: 140; loss: 2.35; acc: 0.52
Batch: 160; loss: 2.93; acc: 0.55
Batch: 180; loss: 2.07; acc: 0.59
Batch: 200; loss: 1.85; acc: 0.61
Batch: 220; loss: 1.66; acc: 0.64
Batch: 240; loss: 2.27; acc: 0.66
Batch: 260; loss: 2.41; acc: 0.61
Batch: 280; loss: 1.91; acc: 0.64
Batch: 300; loss: 2.16; acc: 0.61
Batch: 320; loss: 1.57; acc: 0.61
Batch: 340; loss: 1.79; acc: 0.66
Batch: 360; loss: 2.26; acc: 0.56
Batch: 380; loss: 1.45; acc: 0.73
Batch: 400; loss: 1.46; acc: 0.7
Batch: 420; loss: 2.11; acc: 0.59
Batch: 440; loss: 1.01; acc: 0.72
Batch: 460; loss: 1.9; acc: 0.64
Batch: 480; loss: 1.93; acc: 0.67
Batch: 500; loss: 1.55; acc: 0.7
Batch: 520; loss: 1.01; acc: 0.7
Batch: 540; loss: 2.27; acc: 0.56
Batch: 560; loss: 1.95; acc: 0.64
Batch: 580; loss: 1.54; acc: 0.72
Batch: 600; loss: 1.67; acc: 0.75
Batch: 620; loss: 1.36; acc: 0.69
Train Epoch over. train_loss: 1.87; train_accuracy: 0.63 

Batch: 0; loss: 1.33; acc: 0.69
Batch: 20; loss: 2.23; acc: 0.59
Batch: 40; loss: 1.41; acc: 0.66
Batch: 60; loss: 1.94; acc: 0.61
Batch: 80; loss: 1.81; acc: 0.56
Batch: 100; loss: 1.68; acc: 0.66
Batch: 120; loss: 2.28; acc: 0.62
Batch: 140; loss: 3.22; acc: 0.53
Val Epoch over. val_loss: 1.8525680261812392; val_accuracy: 0.6258957006369427 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.89; acc: 0.58
Batch: 20; loss: 1.62; acc: 0.64
Batch: 40; loss: 2.21; acc: 0.56
Batch: 60; loss: 1.48; acc: 0.7
Batch: 80; loss: 1.66; acc: 0.69
Batch: 100; loss: 1.63; acc: 0.67
Batch: 120; loss: 1.76; acc: 0.62
Batch: 140; loss: 2.15; acc: 0.66
Batch: 160; loss: 2.42; acc: 0.62
Batch: 180; loss: 1.61; acc: 0.59
Batch: 200; loss: 1.26; acc: 0.75
Batch: 220; loss: 1.52; acc: 0.69
Batch: 240; loss: 2.03; acc: 0.66
Batch: 260; loss: 2.25; acc: 0.66
Batch: 280; loss: 2.73; acc: 0.48
Batch: 300; loss: 1.5; acc: 0.64
Batch: 320; loss: 1.6; acc: 0.7
Batch: 340; loss: 1.24; acc: 0.73
Batch: 360; loss: 1.88; acc: 0.58
Batch: 380; loss: 2.24; acc: 0.59
Batch: 400; loss: 1.95; acc: 0.61
Batch: 420; loss: 1.63; acc: 0.72
Batch: 440; loss: 1.37; acc: 0.7
Batch: 460; loss: 1.25; acc: 0.72
Batch: 480; loss: 1.62; acc: 0.64
Batch: 500; loss: 1.52; acc: 0.61
Batch: 520; loss: 2.22; acc: 0.67
Batch: 540; loss: 2.33; acc: 0.53
Batch: 560; loss: 2.05; acc: 0.62
Batch: 580; loss: 1.45; acc: 0.64
Batch: 600; loss: 2.28; acc: 0.64
Batch: 620; loss: 2.3; acc: 0.59
Train Epoch over. train_loss: 1.87; train_accuracy: 0.63 

Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 2.31; acc: 0.58
Batch: 40; loss: 1.45; acc: 0.64
Batch: 60; loss: 2.01; acc: 0.58
Batch: 80; loss: 1.83; acc: 0.58
Batch: 100; loss: 1.72; acc: 0.64
Batch: 120; loss: 2.28; acc: 0.61
Batch: 140; loss: 3.2; acc: 0.5
Val Epoch over. val_loss: 1.8553638834102897; val_accuracy: 0.6240047770700637 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.32; acc: 0.7
Batch: 20; loss: 1.74; acc: 0.56
Batch: 40; loss: 1.47; acc: 0.67
Batch: 60; loss: 2.58; acc: 0.61
Batch: 80; loss: 1.78; acc: 0.64
Batch: 100; loss: 2.51; acc: 0.59
Batch: 120; loss: 2.07; acc: 0.58
Batch: 140; loss: 2.09; acc: 0.61
Batch: 160; loss: 2.21; acc: 0.56
Batch: 180; loss: 1.74; acc: 0.61
Batch: 200; loss: 1.68; acc: 0.73
Batch: 220; loss: 1.6; acc: 0.58
Batch: 240; loss: 2.12; acc: 0.62
Batch: 260; loss: 1.76; acc: 0.67
Batch: 280; loss: 2.11; acc: 0.56
Batch: 300; loss: 1.83; acc: 0.61
Batch: 320; loss: 1.95; acc: 0.69
Batch: 340; loss: 1.71; acc: 0.67
Batch: 360; loss: 1.53; acc: 0.67
Batch: 380; loss: 2.08; acc: 0.56
Batch: 400; loss: 1.74; acc: 0.64
Batch: 420; loss: 2.03; acc: 0.61
Batch: 440; loss: 1.83; acc: 0.58
Batch: 460; loss: 1.19; acc: 0.7
Batch: 480; loss: 2.14; acc: 0.67
Batch: 500; loss: 2.42; acc: 0.58
Batch: 520; loss: 1.99; acc: 0.61
Batch: 540; loss: 1.95; acc: 0.56
Batch: 560; loss: 1.33; acc: 0.66
Batch: 580; loss: 1.71; acc: 0.67
Batch: 600; loss: 1.75; acc: 0.58
Batch: 620; loss: 1.73; acc: 0.66
Train Epoch over. train_loss: 1.85; train_accuracy: 0.63 

Batch: 0; loss: 1.33; acc: 0.7
Batch: 20; loss: 2.24; acc: 0.61
Batch: 40; loss: 1.42; acc: 0.64
Batch: 60; loss: 1.99; acc: 0.59
Batch: 80; loss: 1.83; acc: 0.56
Batch: 100; loss: 1.65; acc: 0.62
Batch: 120; loss: 2.23; acc: 0.62
Batch: 140; loss: 3.22; acc: 0.5
Val Epoch over. val_loss: 1.8408548835736172; val_accuracy: 0.6289808917197452 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.89; acc: 0.64
Batch: 20; loss: 1.65; acc: 0.73
Batch: 40; loss: 1.82; acc: 0.64
Batch: 60; loss: 1.81; acc: 0.55
Batch: 80; loss: 2.09; acc: 0.56
Batch: 100; loss: 1.48; acc: 0.66
Batch: 120; loss: 1.5; acc: 0.62
Batch: 140; loss: 3.07; acc: 0.5
Batch: 160; loss: 1.57; acc: 0.73
Batch: 180; loss: 1.83; acc: 0.64
Batch: 200; loss: 1.89; acc: 0.69
Batch: 220; loss: 2.3; acc: 0.59
Batch: 240; loss: 2.03; acc: 0.61
Batch: 260; loss: 1.29; acc: 0.7
Batch: 280; loss: 1.76; acc: 0.62
Batch: 300; loss: 1.78; acc: 0.59
Batch: 320; loss: 1.82; acc: 0.64
Batch: 340; loss: 1.76; acc: 0.67
Batch: 360; loss: 2.01; acc: 0.69
Batch: 380; loss: 1.78; acc: 0.66
Batch: 400; loss: 1.59; acc: 0.66
Batch: 420; loss: 1.98; acc: 0.61
Batch: 440; loss: 1.66; acc: 0.67
Batch: 460; loss: 1.23; acc: 0.73
Batch: 480; loss: 2.01; acc: 0.53
Batch: 500; loss: 1.23; acc: 0.69
Batch: 520; loss: 1.6; acc: 0.66
Batch: 540; loss: 2.18; acc: 0.58
Batch: 560; loss: 1.86; acc: 0.66
Batch: 580; loss: 1.6; acc: 0.69
Batch: 600; loss: 2.15; acc: 0.59
Batch: 620; loss: 2.53; acc: 0.61
Train Epoch over. train_loss: 1.84; train_accuracy: 0.63 

Batch: 0; loss: 1.34; acc: 0.7
Batch: 20; loss: 2.25; acc: 0.61
Batch: 40; loss: 1.41; acc: 0.67
Batch: 60; loss: 1.97; acc: 0.61
Batch: 80; loss: 1.82; acc: 0.59
Batch: 100; loss: 1.68; acc: 0.62
Batch: 120; loss: 2.21; acc: 0.62
Batch: 140; loss: 3.18; acc: 0.5
Val Epoch over. val_loss: 1.8416463820038327; val_accuracy: 0.6275875796178344 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.17; acc: 0.56
Batch: 20; loss: 1.29; acc: 0.72
Batch: 40; loss: 1.66; acc: 0.66
Batch: 60; loss: 1.66; acc: 0.67
Batch: 80; loss: 1.82; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.73
Batch: 120; loss: 1.8; acc: 0.59
Batch: 140; loss: 1.58; acc: 0.72
Batch: 160; loss: 2.97; acc: 0.56
Batch: 180; loss: 1.96; acc: 0.62
Batch: 200; loss: 1.56; acc: 0.62
Batch: 220; loss: 2.22; acc: 0.59
Batch: 240; loss: 1.14; acc: 0.77
Batch: 260; loss: 1.73; acc: 0.58
Batch: 280; loss: 1.63; acc: 0.59
Batch: 300; loss: 2.01; acc: 0.55
Batch: 320; loss: 1.42; acc: 0.69
Batch: 340; loss: 1.99; acc: 0.56
Batch: 360; loss: 1.9; acc: 0.66
Batch: 380; loss: 1.6; acc: 0.62
Batch: 400; loss: 1.96; acc: 0.67
Batch: 420; loss: 1.44; acc: 0.61
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.97; acc: 0.64
Batch: 480; loss: 1.49; acc: 0.66
Batch: 500; loss: 1.5; acc: 0.69
Batch: 520; loss: 2.07; acc: 0.64
Batch: 540; loss: 2.63; acc: 0.52
Batch: 560; loss: 1.3; acc: 0.67
Batch: 580; loss: 1.23; acc: 0.7
Batch: 600; loss: 1.95; acc: 0.61
Batch: 620; loss: 1.82; acc: 0.67
Train Epoch over. train_loss: 1.84; train_accuracy: 0.63 

Batch: 0; loss: 1.34; acc: 0.7
Batch: 20; loss: 2.23; acc: 0.61
Batch: 40; loss: 1.41; acc: 0.64
Batch: 60; loss: 1.97; acc: 0.61
Batch: 80; loss: 1.83; acc: 0.58
Batch: 100; loss: 1.64; acc: 0.62
Batch: 120; loss: 2.23; acc: 0.62
Batch: 140; loss: 3.22; acc: 0.5
Val Epoch over. val_loss: 1.8405126059890553; val_accuracy: 0.629578025477707 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.57; acc: 0.62
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 2.18; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.69
Batch: 80; loss: 1.54; acc: 0.69
Batch: 100; loss: 3.44; acc: 0.53
Batch: 120; loss: 1.6; acc: 0.64
Batch: 140; loss: 1.44; acc: 0.69
Batch: 160; loss: 1.97; acc: 0.64
Batch: 180; loss: 1.69; acc: 0.67
Batch: 200; loss: 1.98; acc: 0.62
Batch: 220; loss: 2.36; acc: 0.56
Batch: 240; loss: 1.8; acc: 0.61
Batch: 260; loss: 1.45; acc: 0.69
Batch: 280; loss: 1.98; acc: 0.59
Batch: 300; loss: 1.68; acc: 0.67
Batch: 320; loss: 1.38; acc: 0.7
Batch: 340; loss: 1.94; acc: 0.59
Batch: 360; loss: 1.97; acc: 0.59
Batch: 380; loss: 1.94; acc: 0.53
Batch: 400; loss: 2.08; acc: 0.64
Batch: 420; loss: 2.27; acc: 0.62
Batch: 440; loss: 1.32; acc: 0.67
Batch: 460; loss: 2.15; acc: 0.61
Batch: 480; loss: 1.17; acc: 0.73
Batch: 500; loss: 1.85; acc: 0.64
Batch: 520; loss: 1.51; acc: 0.59
Batch: 540; loss: 1.27; acc: 0.64
Batch: 560; loss: 2.4; acc: 0.64
Batch: 580; loss: 1.27; acc: 0.72
Batch: 600; loss: 2.36; acc: 0.47
Batch: 620; loss: 2.07; acc: 0.61
Train Epoch over. train_loss: 1.84; train_accuracy: 0.64 

Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 2.28; acc: 0.61
Batch: 40; loss: 1.4; acc: 0.64
Batch: 60; loss: 1.98; acc: 0.59
Batch: 80; loss: 1.83; acc: 0.59
Batch: 100; loss: 1.67; acc: 0.64
Batch: 120; loss: 2.22; acc: 0.62
Batch: 140; loss: 3.18; acc: 0.5
Val Epoch over. val_loss: 1.8419810962525143; val_accuracy: 0.6283837579617835 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.91; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.67
Batch: 40; loss: 1.88; acc: 0.64
Batch: 60; loss: 1.95; acc: 0.61
Batch: 80; loss: 1.92; acc: 0.61
Batch: 100; loss: 1.78; acc: 0.66
Batch: 120; loss: 1.6; acc: 0.67
Batch: 140; loss: 1.63; acc: 0.64
Batch: 160; loss: 1.68; acc: 0.62
Batch: 180; loss: 1.74; acc: 0.69
Batch: 200; loss: 1.87; acc: 0.66
Batch: 220; loss: 2.42; acc: 0.62
Batch: 240; loss: 1.77; acc: 0.66
Batch: 260; loss: 2.39; acc: 0.52
Batch: 280; loss: 1.14; acc: 0.77
Batch: 300; loss: 1.96; acc: 0.66
Batch: 320; loss: 1.63; acc: 0.72
Batch: 340; loss: 1.8; acc: 0.69
Batch: 360; loss: 1.57; acc: 0.7
Batch: 380; loss: 1.92; acc: 0.7
Batch: 400; loss: 1.99; acc: 0.66
Batch: 420; loss: 2.31; acc: 0.58
Batch: 440; loss: 1.87; acc: 0.59
Batch: 460; loss: 2.63; acc: 0.52
Batch: 480; loss: 2.54; acc: 0.66
Batch: 500; loss: 1.76; acc: 0.64
Batch: 520; loss: 2.27; acc: 0.55
Batch: 540; loss: 1.57; acc: 0.62
Batch: 560; loss: 1.97; acc: 0.61
Batch: 580; loss: 1.93; acc: 0.66
Batch: 600; loss: 1.71; acc: 0.61
Batch: 620; loss: 1.89; acc: 0.62
Train Epoch over. train_loss: 1.84; train_accuracy: 0.64 

Batch: 0; loss: 1.35; acc: 0.69
Batch: 20; loss: 2.24; acc: 0.61
Batch: 40; loss: 1.39; acc: 0.64
Batch: 60; loss: 1.97; acc: 0.59
Batch: 80; loss: 1.83; acc: 0.58
Batch: 100; loss: 1.65; acc: 0.62
Batch: 120; loss: 2.22; acc: 0.62
Batch: 140; loss: 3.2; acc: 0.5
Val Epoch over. val_loss: 1.8395620971728281; val_accuracy: 0.6293789808917197 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 2.61; acc: 0.56
Batch: 20; loss: 1.45; acc: 0.66
Batch: 40; loss: 1.57; acc: 0.67
Batch: 60; loss: 2.18; acc: 0.62
Batch: 80; loss: 1.49; acc: 0.66
Batch: 100; loss: 1.7; acc: 0.66
Batch: 120; loss: 1.66; acc: 0.7
Batch: 140; loss: 1.48; acc: 0.59
Batch: 160; loss: 1.62; acc: 0.62
Batch: 180; loss: 2.42; acc: 0.62
Batch: 200; loss: 1.28; acc: 0.61
Batch: 220; loss: 2.17; acc: 0.61
Batch: 240; loss: 1.81; acc: 0.59
Batch: 260; loss: 1.11; acc: 0.73
Batch: 280; loss: 1.47; acc: 0.67
Batch: 300; loss: 1.73; acc: 0.72
Batch: 320; loss: 1.82; acc: 0.62
Batch: 340; loss: 1.86; acc: 0.55
Batch: 360; loss: 1.96; acc: 0.64
Batch: 380; loss: 1.69; acc: 0.75
Batch: 400; loss: 1.5; acc: 0.69
Batch: 420; loss: 1.54; acc: 0.61
Batch: 440; loss: 2.16; acc: 0.59
Batch: 460; loss: 2.28; acc: 0.56
Batch: 480; loss: 1.23; acc: 0.7
Batch: 500; loss: 2.21; acc: 0.55
Batch: 520; loss: 1.7; acc: 0.64
Batch: 540; loss: 2.12; acc: 0.58
Batch: 560; loss: 2.01; acc: 0.56
Batch: 580; loss: 2.39; acc: 0.55
Batch: 600; loss: 1.23; acc: 0.64
Batch: 620; loss: 1.72; acc: 0.64
Train Epoch over. train_loss: 1.84; train_accuracy: 0.64 

Batch: 0; loss: 1.34; acc: 0.7
Batch: 20; loss: 2.27; acc: 0.58
Batch: 40; loss: 1.41; acc: 0.64
Batch: 60; loss: 1.96; acc: 0.59
Batch: 80; loss: 1.84; acc: 0.58
Batch: 100; loss: 1.67; acc: 0.66
Batch: 120; loss: 2.22; acc: 0.62
Batch: 140; loss: 3.18; acc: 0.5
Val Epoch over. val_loss: 1.8408798655127263; val_accuracy: 0.6282842356687898 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.9; acc: 0.62
Batch: 20; loss: 1.61; acc: 0.62
Batch: 40; loss: 1.8; acc: 0.66
Batch: 60; loss: 1.56; acc: 0.64
Batch: 80; loss: 1.22; acc: 0.7
Batch: 100; loss: 1.6; acc: 0.73
Batch: 120; loss: 1.53; acc: 0.61
Batch: 140; loss: 2.28; acc: 0.56
Batch: 160; loss: 1.46; acc: 0.7
Batch: 180; loss: 1.9; acc: 0.55
Batch: 200; loss: 2.12; acc: 0.56
Batch: 220; loss: 1.67; acc: 0.59
Batch: 240; loss: 1.53; acc: 0.69
Batch: 260; loss: 1.62; acc: 0.66
Batch: 280; loss: 1.63; acc: 0.61
Batch: 300; loss: 2.35; acc: 0.52
Batch: 320; loss: 1.33; acc: 0.73
Batch: 340; loss: 1.15; acc: 0.75
Batch: 360; loss: 2.19; acc: 0.52
Batch: 380; loss: 1.93; acc: 0.58
Batch: 400; loss: 2.27; acc: 0.62
Batch: 420; loss: 2.1; acc: 0.56
Batch: 440; loss: 2.18; acc: 0.61
Batch: 460; loss: 1.12; acc: 0.7
Batch: 480; loss: 2.18; acc: 0.61
Batch: 500; loss: 2.22; acc: 0.62
Batch: 520; loss: 2.15; acc: 0.61
Batch: 540; loss: 1.03; acc: 0.78
Batch: 560; loss: 1.51; acc: 0.66
Batch: 580; loss: 1.65; acc: 0.66
Batch: 600; loss: 2.09; acc: 0.59
Batch: 620; loss: 1.72; acc: 0.75
Train Epoch over. train_loss: 1.84; train_accuracy: 0.64 

Batch: 0; loss: 1.35; acc: 0.69
Batch: 20; loss: 2.24; acc: 0.61
Batch: 40; loss: 1.41; acc: 0.64
Batch: 60; loss: 1.96; acc: 0.59
Batch: 80; loss: 1.84; acc: 0.56
Batch: 100; loss: 1.65; acc: 0.64
Batch: 120; loss: 2.22; acc: 0.62
Batch: 140; loss: 3.2; acc: 0.5
Val Epoch over. val_loss: 1.8407438306292152; val_accuracy: 0.6291799363057324 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.94; acc: 0.66
Batch: 20; loss: 1.55; acc: 0.64
Batch: 40; loss: 1.76; acc: 0.59
Batch: 60; loss: 1.9; acc: 0.69
Batch: 80; loss: 1.71; acc: 0.66
Batch: 100; loss: 2.24; acc: 0.55
Batch: 120; loss: 2.46; acc: 0.55
Batch: 140; loss: 1.9; acc: 0.62
Batch: 160; loss: 1.32; acc: 0.73
Batch: 180; loss: 1.72; acc: 0.69
Batch: 200; loss: 1.75; acc: 0.62
Batch: 220; loss: 0.99; acc: 0.77
Batch: 240; loss: 1.33; acc: 0.77
Batch: 260; loss: 1.76; acc: 0.66
Batch: 280; loss: 1.44; acc: 0.62
Batch: 300; loss: 1.5; acc: 0.7
Batch: 320; loss: 2.02; acc: 0.64
Batch: 340; loss: 1.43; acc: 0.62
Batch: 360; loss: 1.27; acc: 0.69
Batch: 380; loss: 2.11; acc: 0.64
Batch: 400; loss: 1.66; acc: 0.66
Batch: 420; loss: 2.22; acc: 0.59
Batch: 440; loss: 1.57; acc: 0.7
Batch: 460; loss: 2.06; acc: 0.66
Batch: 480; loss: 1.46; acc: 0.72
Batch: 500; loss: 1.82; acc: 0.64
Batch: 520; loss: 1.87; acc: 0.66
Batch: 540; loss: 1.9; acc: 0.62
Batch: 560; loss: 1.45; acc: 0.66
Batch: 580; loss: 1.26; acc: 0.64
Batch: 600; loss: 1.28; acc: 0.73
Batch: 620; loss: 2.06; acc: 0.61
Train Epoch over. train_loss: 1.84; train_accuracy: 0.64 

Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 2.24; acc: 0.62
Batch: 40; loss: 1.4; acc: 0.62
Batch: 60; loss: 1.94; acc: 0.59
Batch: 80; loss: 1.85; acc: 0.58
Batch: 100; loss: 1.66; acc: 0.64
Batch: 120; loss: 2.22; acc: 0.62
Batch: 140; loss: 3.2; acc: 0.5
Val Epoch over. val_loss: 1.8414797441215272; val_accuracy: 0.6272890127388535 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.82; acc: 0.69
Batch: 20; loss: 1.51; acc: 0.64
Batch: 40; loss: 1.64; acc: 0.56
Batch: 60; loss: 1.94; acc: 0.62
Batch: 80; loss: 2.24; acc: 0.61
Batch: 100; loss: 2.3; acc: 0.69
Batch: 120; loss: 1.58; acc: 0.66
Batch: 140; loss: 1.38; acc: 0.72
Batch: 160; loss: 2.07; acc: 0.59
Batch: 180; loss: 1.53; acc: 0.69
Batch: 200; loss: 2.12; acc: 0.67
Batch: 220; loss: 2.04; acc: 0.62
Batch: 240; loss: 1.44; acc: 0.67
Batch: 260; loss: 1.89; acc: 0.69
Batch: 280; loss: 2.11; acc: 0.64
Batch: 300; loss: 1.84; acc: 0.62
Batch: 320; loss: 2.44; acc: 0.56
Batch: 340; loss: 1.82; acc: 0.66
Batch: 360; loss: 2.17; acc: 0.59
Batch: 380; loss: 1.44; acc: 0.69
Batch: 400; loss: 1.8; acc: 0.66
Batch: 420; loss: 1.54; acc: 0.66
Batch: 440; loss: 1.6; acc: 0.69
Batch: 460; loss: 1.91; acc: 0.62
Batch: 480; loss: 1.44; acc: 0.7
Batch: 500; loss: 2.47; acc: 0.56
Batch: 520; loss: 1.94; acc: 0.64
Batch: 540; loss: 2.03; acc: 0.64
Batch: 560; loss: 1.29; acc: 0.7
Batch: 580; loss: 1.54; acc: 0.64
Batch: 600; loss: 2.24; acc: 0.61
Batch: 620; loss: 2.29; acc: 0.55
Train Epoch over. train_loss: 1.84; train_accuracy: 0.64 

Batch: 0; loss: 1.34; acc: 0.72
Batch: 20; loss: 2.26; acc: 0.61
Batch: 40; loss: 1.4; acc: 0.64
Batch: 60; loss: 1.95; acc: 0.59
Batch: 80; loss: 1.85; acc: 0.59
Batch: 100; loss: 1.67; acc: 0.64
Batch: 120; loss: 2.23; acc: 0.62
Batch: 140; loss: 3.19; acc: 0.5
Val Epoch over. val_loss: 1.8407778944938806; val_accuracy: 0.6286823248407644 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.81; acc: 0.62
Batch: 20; loss: 2.51; acc: 0.55
Batch: 40; loss: 2.62; acc: 0.53
Batch: 60; loss: 1.73; acc: 0.62
Batch: 80; loss: 1.82; acc: 0.69
Batch: 100; loss: 2.47; acc: 0.58
Batch: 120; loss: 1.49; acc: 0.62
Batch: 140; loss: 1.79; acc: 0.62
Batch: 160; loss: 1.43; acc: 0.69
Batch: 180; loss: 1.22; acc: 0.69
Batch: 200; loss: 1.23; acc: 0.7
Batch: 220; loss: 2.37; acc: 0.64
Batch: 240; loss: 1.32; acc: 0.66
Batch: 260; loss: 2.02; acc: 0.62
Batch: 280; loss: 2.03; acc: 0.52
Batch: 300; loss: 1.74; acc: 0.61
Batch: 320; loss: 1.77; acc: 0.62
Batch: 340; loss: 1.24; acc: 0.64
Batch: 360; loss: 2.65; acc: 0.56
Batch: 380; loss: 1.93; acc: 0.61
Batch: 400; loss: 1.93; acc: 0.59
Batch: 420; loss: 2.18; acc: 0.56
Batch: 440; loss: 2.5; acc: 0.59
Batch: 460; loss: 1.95; acc: 0.69
Batch: 480; loss: 0.97; acc: 0.64
Batch: 500; loss: 1.67; acc: 0.66
Batch: 520; loss: 1.63; acc: 0.61
Batch: 540; loss: 2.05; acc: 0.58
Batch: 560; loss: 1.63; acc: 0.67
Batch: 580; loss: 1.2; acc: 0.64
Batch: 600; loss: 1.66; acc: 0.64
Batch: 620; loss: 1.59; acc: 0.64
Train Epoch over. train_loss: 1.84; train_accuracy: 0.64 

Batch: 0; loss: 1.35; acc: 0.7
Batch: 20; loss: 2.25; acc: 0.61
Batch: 40; loss: 1.39; acc: 0.62
Batch: 60; loss: 1.96; acc: 0.61
Batch: 80; loss: 1.85; acc: 0.59
Batch: 100; loss: 1.68; acc: 0.66
Batch: 120; loss: 2.22; acc: 0.62
Batch: 140; loss: 3.18; acc: 0.5
Val Epoch over. val_loss: 1.841248045301741; val_accuracy: 0.6277866242038217 

plots/subspace_training/lenet/2020-01-10 13:15:25/d_dim_100_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 42966
elements in E: 8885200
fraction nonzero: 0.004835681807950299
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.1; acc: 0.45
Batch: 40; loss: 2.22; acc: 0.39
Batch: 60; loss: 2.2; acc: 0.42
Batch: 80; loss: 1.64; acc: 0.61
Batch: 100; loss: 1.4; acc: 0.58
Batch: 120; loss: 1.17; acc: 0.64
Batch: 140; loss: 2.43; acc: 0.44
Batch: 160; loss: 1.37; acc: 0.58
Batch: 180; loss: 0.87; acc: 0.72
Batch: 200; loss: 1.33; acc: 0.61
Batch: 220; loss: 1.43; acc: 0.62
Batch: 240; loss: 1.79; acc: 0.55
Batch: 260; loss: 1.69; acc: 0.59
Batch: 280; loss: 1.52; acc: 0.62
Batch: 300; loss: 1.52; acc: 0.58
Batch: 320; loss: 1.35; acc: 0.66
Batch: 340; loss: 1.63; acc: 0.66
Batch: 360; loss: 1.42; acc: 0.66
Batch: 380; loss: 1.96; acc: 0.53
Batch: 400; loss: 1.55; acc: 0.59
Batch: 420; loss: 1.62; acc: 0.62
Batch: 440; loss: 1.17; acc: 0.69
Batch: 460; loss: 1.51; acc: 0.61
Batch: 480; loss: 1.0; acc: 0.72
Batch: 500; loss: 1.95; acc: 0.53
Batch: 520; loss: 1.83; acc: 0.53
Batch: 540; loss: 1.22; acc: 0.72
Batch: 560; loss: 1.16; acc: 0.7
Batch: 580; loss: 1.3; acc: 0.66
Batch: 600; loss: 0.9; acc: 0.78
Batch: 620; loss: 1.32; acc: 0.64
Train Epoch over. train_loss: 1.68; train_accuracy: 0.59 

Batch: 0; loss: 1.32; acc: 0.64
Batch: 20; loss: 2.22; acc: 0.52
Batch: 40; loss: 1.1; acc: 0.73
Batch: 60; loss: 0.88; acc: 0.78
Batch: 80; loss: 1.49; acc: 0.66
Batch: 100; loss: 1.3; acc: 0.7
Batch: 120; loss: 1.33; acc: 0.69
Batch: 140; loss: 1.8; acc: 0.61
Val Epoch over. val_loss: 1.4321215847495254; val_accuracy: 0.6617237261146497 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 1.35; acc: 0.69
Batch: 20; loss: 1.27; acc: 0.62
Batch: 40; loss: 1.97; acc: 0.53
Batch: 60; loss: 1.39; acc: 0.61
Batch: 80; loss: 1.14; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.75
Batch: 120; loss: 1.58; acc: 0.61
Batch: 140; loss: 1.34; acc: 0.67
Batch: 160; loss: 1.55; acc: 0.66
Batch: 180; loss: 1.54; acc: 0.67
Batch: 200; loss: 1.32; acc: 0.66
Batch: 220; loss: 1.09; acc: 0.7
Batch: 240; loss: 1.56; acc: 0.62
Batch: 260; loss: 1.22; acc: 0.66
Batch: 280; loss: 1.22; acc: 0.67
Batch: 300; loss: 1.81; acc: 0.64
Batch: 320; loss: 1.34; acc: 0.64
Batch: 340; loss: 1.46; acc: 0.66
Batch: 360; loss: 1.67; acc: 0.62
Batch: 380; loss: 1.13; acc: 0.62
Batch: 400; loss: 1.31; acc: 0.72
Batch: 420; loss: 1.66; acc: 0.59
Batch: 440; loss: 1.17; acc: 0.7
Batch: 460; loss: 1.93; acc: 0.61
Batch: 480; loss: 1.85; acc: 0.56
Batch: 500; loss: 1.88; acc: 0.58
Batch: 520; loss: 1.47; acc: 0.69
Batch: 540; loss: 0.97; acc: 0.73
Batch: 560; loss: 1.36; acc: 0.66
Batch: 580; loss: 1.13; acc: 0.69
Batch: 600; loss: 1.64; acc: 0.73
Batch: 620; loss: 1.13; acc: 0.73
Train Epoch over. train_loss: 1.37; train_accuracy: 0.67 

Batch: 0; loss: 1.26; acc: 0.72
Batch: 20; loss: 1.99; acc: 0.58
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.05; acc: 0.77
Batch: 100; loss: 1.64; acc: 0.64
Batch: 120; loss: 0.98; acc: 0.8
Batch: 140; loss: 1.35; acc: 0.61
Val Epoch over. val_loss: 1.3593902101941928; val_accuracy: 0.67078025477707 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 1.37; acc: 0.64
Batch: 20; loss: 1.59; acc: 0.59
Batch: 40; loss: 1.29; acc: 0.62
Batch: 60; loss: 1.31; acc: 0.73
Batch: 80; loss: 1.36; acc: 0.7
Batch: 100; loss: 1.39; acc: 0.62
Batch: 120; loss: 1.96; acc: 0.56
Batch: 140; loss: 1.36; acc: 0.67
Batch: 160; loss: 1.22; acc: 0.67
Batch: 180; loss: 1.69; acc: 0.61
Batch: 200; loss: 0.85; acc: 0.7
Batch: 220; loss: 2.27; acc: 0.61
Batch: 240; loss: 1.43; acc: 0.7
Batch: 260; loss: 1.03; acc: 0.77
Batch: 280; loss: 0.99; acc: 0.61
Batch: 300; loss: 1.43; acc: 0.66
Batch: 320; loss: 1.8; acc: 0.67
Batch: 340; loss: 1.3; acc: 0.69
Batch: 360; loss: 0.84; acc: 0.72
Batch: 380; loss: 1.08; acc: 0.69
Batch: 400; loss: 1.99; acc: 0.66
Batch: 420; loss: 1.29; acc: 0.66
Batch: 440; loss: 0.82; acc: 0.81
Batch: 460; loss: 1.54; acc: 0.7
Batch: 480; loss: 1.51; acc: 0.64
Batch: 500; loss: 1.18; acc: 0.72
Batch: 520; loss: 1.15; acc: 0.72
Batch: 540; loss: 1.21; acc: 0.75
Batch: 560; loss: 1.23; acc: 0.7
Batch: 580; loss: 1.37; acc: 0.69
Batch: 600; loss: 1.12; acc: 0.8
Batch: 620; loss: 1.49; acc: 0.72
Train Epoch over. train_loss: 1.31; train_accuracy: 0.69 

Batch: 0; loss: 1.14; acc: 0.72
Batch: 20; loss: 1.94; acc: 0.61
Batch: 40; loss: 1.01; acc: 0.78
Batch: 60; loss: 1.23; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 1.68; acc: 0.61
Batch: 120; loss: 0.94; acc: 0.73
Batch: 140; loss: 1.95; acc: 0.61
Val Epoch over. val_loss: 1.3131404513386404; val_accuracy: 0.6982484076433121 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 2.85; acc: 0.58
Batch: 40; loss: 0.94; acc: 0.81
Batch: 60; loss: 1.9; acc: 0.62
Batch: 80; loss: 1.22; acc: 0.77
Batch: 100; loss: 1.67; acc: 0.69
Batch: 120; loss: 1.28; acc: 0.73
Batch: 140; loss: 1.34; acc: 0.67
Batch: 160; loss: 1.31; acc: 0.69
Batch: 180; loss: 1.28; acc: 0.66
Batch: 200; loss: 0.97; acc: 0.73
Batch: 220; loss: 0.87; acc: 0.77
Batch: 240; loss: 1.01; acc: 0.83
Batch: 260; loss: 0.87; acc: 0.77
Batch: 280; loss: 1.3; acc: 0.72
Batch: 300; loss: 1.35; acc: 0.66
Batch: 320; loss: 1.69; acc: 0.62
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.63; acc: 0.66
Batch: 380; loss: 0.75; acc: 0.7
Batch: 400; loss: 1.21; acc: 0.64
Batch: 420; loss: 1.27; acc: 0.75
Batch: 440; loss: 1.6; acc: 0.64
Batch: 460; loss: 1.51; acc: 0.72
Batch: 480; loss: 1.14; acc: 0.69
Batch: 500; loss: 1.31; acc: 0.64
Batch: 520; loss: 1.32; acc: 0.67
Batch: 540; loss: 1.74; acc: 0.64
Batch: 560; loss: 1.33; acc: 0.72
Batch: 580; loss: 1.31; acc: 0.72
Batch: 600; loss: 1.3; acc: 0.64
Batch: 620; loss: 1.19; acc: 0.73
Train Epoch over. train_loss: 1.28; train_accuracy: 0.7 

Batch: 0; loss: 1.12; acc: 0.72
Batch: 20; loss: 2.29; acc: 0.55
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 1.35; acc: 0.75
Batch: 80; loss: 1.08; acc: 0.75
Batch: 100; loss: 1.67; acc: 0.66
Batch: 120; loss: 0.98; acc: 0.78
Batch: 140; loss: 1.99; acc: 0.58
Val Epoch over. val_loss: 1.3783493899995354; val_accuracy: 0.681031050955414 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.69
Batch: 40; loss: 1.57; acc: 0.66
Batch: 60; loss: 1.3; acc: 0.77
Batch: 80; loss: 1.23; acc: 0.77
Batch: 100; loss: 0.87; acc: 0.7
Batch: 120; loss: 1.08; acc: 0.8
Batch: 140; loss: 1.39; acc: 0.72
Batch: 160; loss: 1.48; acc: 0.7
Batch: 180; loss: 1.44; acc: 0.78
Batch: 200; loss: 1.1; acc: 0.75
Batch: 220; loss: 1.55; acc: 0.64
Batch: 240; loss: 1.58; acc: 0.67
Batch: 260; loss: 1.32; acc: 0.73
Batch: 280; loss: 1.55; acc: 0.61
Batch: 300; loss: 1.08; acc: 0.77
Batch: 320; loss: 1.6; acc: 0.66
Batch: 340; loss: 2.1; acc: 0.55
Batch: 360; loss: 0.97; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 1.51; acc: 0.72
Batch: 420; loss: 1.02; acc: 0.8
Batch: 440; loss: 1.21; acc: 0.73
Batch: 460; loss: 0.93; acc: 0.7
Batch: 480; loss: 1.34; acc: 0.7
Batch: 500; loss: 1.34; acc: 0.64
Batch: 520; loss: 1.22; acc: 0.78
Batch: 540; loss: 0.97; acc: 0.75
Batch: 560; loss: 1.66; acc: 0.72
Batch: 580; loss: 0.96; acc: 0.78
Batch: 600; loss: 1.72; acc: 0.69
Batch: 620; loss: 1.52; acc: 0.72
Train Epoch over. train_loss: 1.27; train_accuracy: 0.71 

Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 1.99; acc: 0.61
Batch: 40; loss: 0.8; acc: 0.75
Batch: 60; loss: 1.17; acc: 0.69
Batch: 80; loss: 1.57; acc: 0.61
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 2.21; acc: 0.53
Val Epoch over. val_loss: 1.2918481927388792; val_accuracy: 0.7035230891719745 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 1.7; acc: 0.64
Batch: 40; loss: 1.5; acc: 0.7
Batch: 60; loss: 1.61; acc: 0.64
Batch: 80; loss: 1.53; acc: 0.66
Batch: 100; loss: 1.11; acc: 0.61
Batch: 120; loss: 1.45; acc: 0.7
Batch: 140; loss: 1.23; acc: 0.64
Batch: 160; loss: 1.32; acc: 0.69
Batch: 180; loss: 1.06; acc: 0.81
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 1.1; acc: 0.67
Batch: 240; loss: 1.51; acc: 0.58
Batch: 260; loss: 1.51; acc: 0.7
Batch: 280; loss: 1.06; acc: 0.78
Batch: 300; loss: 1.18; acc: 0.72
Batch: 320; loss: 1.89; acc: 0.67
Batch: 340; loss: 1.06; acc: 0.67
Batch: 360; loss: 1.04; acc: 0.75
Batch: 380; loss: 1.51; acc: 0.69
Batch: 400; loss: 0.89; acc: 0.83
Batch: 420; loss: 0.75; acc: 0.8
Batch: 440; loss: 1.1; acc: 0.69
Batch: 460; loss: 1.86; acc: 0.66
Batch: 480; loss: 0.99; acc: 0.81
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 1.55; acc: 0.67
Batch: 540; loss: 1.02; acc: 0.75
Batch: 560; loss: 1.53; acc: 0.64
Batch: 580; loss: 1.45; acc: 0.69
Batch: 600; loss: 1.02; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.77
Train Epoch over. train_loss: 1.26; train_accuracy: 0.71 

Batch: 0; loss: 0.99; acc: 0.73
Batch: 20; loss: 2.32; acc: 0.58
Batch: 40; loss: 0.66; acc: 0.78
Batch: 60; loss: 1.39; acc: 0.69
Batch: 80; loss: 1.36; acc: 0.67
Batch: 100; loss: 1.65; acc: 0.69
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 2.2; acc: 0.56
Val Epoch over. val_loss: 1.351175806135129; val_accuracy: 0.6852109872611465 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.87; acc: 0.77
Batch: 20; loss: 1.69; acc: 0.59
Batch: 40; loss: 0.99; acc: 0.69
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 1.44; acc: 0.62
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.89; acc: 0.67
Batch: 140; loss: 1.36; acc: 0.69
Batch: 160; loss: 0.96; acc: 0.73
Batch: 180; loss: 1.68; acc: 0.66
Batch: 200; loss: 1.57; acc: 0.72
Batch: 220; loss: 1.39; acc: 0.61
Batch: 240; loss: 1.44; acc: 0.66
Batch: 260; loss: 0.62; acc: 0.8
Batch: 280; loss: 0.87; acc: 0.84
Batch: 300; loss: 1.0; acc: 0.72
Batch: 320; loss: 1.28; acc: 0.69
Batch: 340; loss: 1.23; acc: 0.77
Batch: 360; loss: 1.2; acc: 0.73
Batch: 380; loss: 1.24; acc: 0.78
Batch: 400; loss: 1.25; acc: 0.73
Batch: 420; loss: 1.55; acc: 0.62
Batch: 440; loss: 0.94; acc: 0.75
Batch: 460; loss: 1.96; acc: 0.59
Batch: 480; loss: 1.03; acc: 0.75
Batch: 500; loss: 1.14; acc: 0.66
Batch: 520; loss: 1.53; acc: 0.64
Batch: 540; loss: 0.94; acc: 0.81
Batch: 560; loss: 0.8; acc: 0.8
Batch: 580; loss: 0.98; acc: 0.75
Batch: 600; loss: 1.44; acc: 0.72
Batch: 620; loss: 1.57; acc: 0.7
Train Epoch over. train_loss: 1.27; train_accuracy: 0.71 

Batch: 0; loss: 0.93; acc: 0.73
Batch: 20; loss: 2.6; acc: 0.53
Batch: 40; loss: 0.99; acc: 0.73
Batch: 60; loss: 0.98; acc: 0.78
Batch: 80; loss: 1.61; acc: 0.61
Batch: 100; loss: 1.56; acc: 0.66
Batch: 120; loss: 0.9; acc: 0.8
Batch: 140; loss: 2.24; acc: 0.59
Val Epoch over. val_loss: 1.3458238319986184; val_accuracy: 0.6977507961783439 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 2.22; acc: 0.56
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 1.06; acc: 0.78
Batch: 60; loss: 1.12; acc: 0.75
Batch: 80; loss: 1.32; acc: 0.7
Batch: 100; loss: 1.96; acc: 0.62
Batch: 120; loss: 1.45; acc: 0.64
Batch: 140; loss: 1.23; acc: 0.69
Batch: 160; loss: 1.26; acc: 0.69
Batch: 180; loss: 1.65; acc: 0.58
Batch: 200; loss: 1.4; acc: 0.67
Batch: 220; loss: 1.19; acc: 0.7
Batch: 240; loss: 1.26; acc: 0.69
Batch: 260; loss: 0.92; acc: 0.78
Batch: 280; loss: 0.86; acc: 0.73
Batch: 300; loss: 0.8; acc: 0.73
Batch: 320; loss: 0.94; acc: 0.81
Batch: 340; loss: 1.07; acc: 0.75
Batch: 360; loss: 1.34; acc: 0.72
Batch: 380; loss: 1.02; acc: 0.7
Batch: 400; loss: 0.79; acc: 0.8
Batch: 420; loss: 1.36; acc: 0.72
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 1.31; acc: 0.73
Batch: 480; loss: 1.69; acc: 0.62
Batch: 500; loss: 1.96; acc: 0.59
Batch: 520; loss: 1.47; acc: 0.7
Batch: 540; loss: 2.14; acc: 0.59
Batch: 560; loss: 1.32; acc: 0.75
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 1.28; acc: 0.69
Batch: 620; loss: 1.5; acc: 0.66
Train Epoch over. train_loss: 1.26; train_accuracy: 0.71 

Batch: 0; loss: 1.0; acc: 0.8
Batch: 20; loss: 2.79; acc: 0.53
Batch: 40; loss: 0.62; acc: 0.86
Batch: 60; loss: 1.23; acc: 0.73
Batch: 80; loss: 1.71; acc: 0.66
Batch: 100; loss: 1.51; acc: 0.67
Batch: 120; loss: 0.84; acc: 0.83
Batch: 140; loss: 2.2; acc: 0.53
Val Epoch over. val_loss: 1.310063242532645; val_accuracy: 0.7063097133757962 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.75; acc: 0.8
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 1.56; acc: 0.69
Batch: 60; loss: 1.48; acc: 0.67
Batch: 80; loss: 0.93; acc: 0.73
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.06; acc: 0.8
Batch: 140; loss: 1.74; acc: 0.66
Batch: 160; loss: 1.73; acc: 0.61
Batch: 180; loss: 1.52; acc: 0.67
Batch: 200; loss: 1.75; acc: 0.69
Batch: 220; loss: 0.96; acc: 0.67
Batch: 240; loss: 2.07; acc: 0.59
Batch: 260; loss: 1.12; acc: 0.73
Batch: 280; loss: 2.05; acc: 0.58
Batch: 300; loss: 1.32; acc: 0.66
Batch: 320; loss: 1.04; acc: 0.72
Batch: 340; loss: 0.99; acc: 0.8
Batch: 360; loss: 1.3; acc: 0.7
Batch: 380; loss: 1.96; acc: 0.64
Batch: 400; loss: 1.4; acc: 0.72
Batch: 420; loss: 1.07; acc: 0.75
Batch: 440; loss: 1.48; acc: 0.64
Batch: 460; loss: 1.32; acc: 0.69
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 1.25; acc: 0.7
Batch: 520; loss: 1.1; acc: 0.77
Batch: 540; loss: 0.94; acc: 0.7
Batch: 560; loss: 1.39; acc: 0.8
Batch: 580; loss: 1.38; acc: 0.67
Batch: 600; loss: 1.7; acc: 0.69
Batch: 620; loss: 1.38; acc: 0.69
Train Epoch over. train_loss: 1.27; train_accuracy: 0.71 

Batch: 0; loss: 0.97; acc: 0.75
Batch: 20; loss: 2.22; acc: 0.58
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 1.19; acc: 0.77
Batch: 80; loss: 1.41; acc: 0.67
Batch: 100; loss: 1.24; acc: 0.77
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 2.42; acc: 0.5
Val Epoch over. val_loss: 1.3131261576133169; val_accuracy: 0.6969546178343949 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 1.39; acc: 0.69
Batch: 20; loss: 1.2; acc: 0.72
Batch: 40; loss: 1.42; acc: 0.69
Batch: 60; loss: 1.07; acc: 0.73
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 1.09; acc: 0.67
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 1.82; acc: 0.67
Batch: 160; loss: 1.31; acc: 0.67
Batch: 180; loss: 1.34; acc: 0.66
Batch: 200; loss: 1.51; acc: 0.73
Batch: 220; loss: 0.77; acc: 0.77
Batch: 240; loss: 1.08; acc: 0.7
Batch: 260; loss: 1.87; acc: 0.66
Batch: 280; loss: 1.56; acc: 0.67
Batch: 300; loss: 1.36; acc: 0.75
Batch: 320; loss: 1.17; acc: 0.72
Batch: 340; loss: 1.12; acc: 0.67
Batch: 360; loss: 1.1; acc: 0.72
Batch: 380; loss: 0.79; acc: 0.77
Batch: 400; loss: 1.26; acc: 0.7
Batch: 420; loss: 0.91; acc: 0.77
Batch: 440; loss: 1.55; acc: 0.66
Batch: 460; loss: 1.62; acc: 0.72
Batch: 480; loss: 1.4; acc: 0.69
Batch: 500; loss: 1.62; acc: 0.62
Batch: 520; loss: 2.11; acc: 0.59
Batch: 540; loss: 0.87; acc: 0.73
Batch: 560; loss: 1.13; acc: 0.73
Batch: 580; loss: 1.43; acc: 0.69
Batch: 600; loss: 1.36; acc: 0.78
Batch: 620; loss: 1.42; acc: 0.62
Train Epoch over. train_loss: 1.26; train_accuracy: 0.71 

Batch: 0; loss: 0.91; acc: 0.78
Batch: 20; loss: 2.99; acc: 0.52
Batch: 40; loss: 0.92; acc: 0.78
Batch: 60; loss: 1.25; acc: 0.77
Batch: 80; loss: 1.71; acc: 0.56
Batch: 100; loss: 1.57; acc: 0.67
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 2.56; acc: 0.53
Val Epoch over. val_loss: 1.3863439715591965; val_accuracy: 0.6845143312101911 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.76; acc: 0.67
Batch: 20; loss: 1.5; acc: 0.66
Batch: 40; loss: 1.1; acc: 0.8
Batch: 60; loss: 1.34; acc: 0.8
Batch: 80; loss: 1.27; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.47; acc: 0.7
Batch: 140; loss: 0.89; acc: 0.77
Batch: 160; loss: 1.34; acc: 0.77
Batch: 180; loss: 1.48; acc: 0.61
Batch: 200; loss: 1.15; acc: 0.72
Batch: 220; loss: 1.11; acc: 0.8
Batch: 240; loss: 0.57; acc: 0.8
Batch: 260; loss: 1.03; acc: 0.72
Batch: 280; loss: 1.06; acc: 0.75
Batch: 300; loss: 1.39; acc: 0.62
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 1.39; acc: 0.73
Batch: 360; loss: 0.81; acc: 0.78
Batch: 380; loss: 1.38; acc: 0.66
Batch: 400; loss: 0.9; acc: 0.77
Batch: 420; loss: 1.04; acc: 0.8
Batch: 440; loss: 1.11; acc: 0.78
Batch: 460; loss: 1.27; acc: 0.72
Batch: 480; loss: 0.88; acc: 0.72
Batch: 500; loss: 1.49; acc: 0.67
Batch: 520; loss: 1.18; acc: 0.73
Batch: 540; loss: 1.41; acc: 0.75
Batch: 560; loss: 1.21; acc: 0.72
Batch: 580; loss: 0.94; acc: 0.72
Batch: 600; loss: 1.09; acc: 0.78
Batch: 620; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 1.13; train_accuracy: 0.73 

Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 2.53; acc: 0.58
Batch: 40; loss: 0.76; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.77
Batch: 80; loss: 1.39; acc: 0.66
Batch: 100; loss: 1.34; acc: 0.7
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 2.12; acc: 0.56
Val Epoch over. val_loss: 1.1882468830248354; val_accuracy: 0.7218351910828026 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 0.82; acc: 0.72
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.77; acc: 0.62
Batch: 80; loss: 0.78; acc: 0.84
Batch: 100; loss: 1.32; acc: 0.72
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.95; acc: 0.83
Batch: 160; loss: 1.02; acc: 0.67
Batch: 180; loss: 1.01; acc: 0.72
Batch: 200; loss: 0.88; acc: 0.72
Batch: 220; loss: 1.17; acc: 0.7
Batch: 240; loss: 0.94; acc: 0.78
Batch: 260; loss: 1.06; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.73
Batch: 300; loss: 1.81; acc: 0.62
Batch: 320; loss: 1.17; acc: 0.66
Batch: 340; loss: 1.14; acc: 0.72
Batch: 360; loss: 0.98; acc: 0.72
Batch: 380; loss: 1.05; acc: 0.7
Batch: 400; loss: 1.27; acc: 0.66
Batch: 420; loss: 1.31; acc: 0.7
Batch: 440; loss: 0.65; acc: 0.81
Batch: 460; loss: 0.99; acc: 0.73
Batch: 480; loss: 1.43; acc: 0.62
Batch: 500; loss: 0.81; acc: 0.8
Batch: 520; loss: 1.25; acc: 0.73
Batch: 540; loss: 1.99; acc: 0.61
Batch: 560; loss: 1.75; acc: 0.64
Batch: 580; loss: 1.02; acc: 0.77
Batch: 600; loss: 1.13; acc: 0.8
Batch: 620; loss: 0.84; acc: 0.77
Train Epoch over. train_loss: 1.11; train_accuracy: 0.74 

Batch: 0; loss: 0.85; acc: 0.78
Batch: 20; loss: 2.48; acc: 0.56
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 1.07; acc: 0.77
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.31; acc: 0.72
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 2.13; acc: 0.56
Val Epoch over. val_loss: 1.1860903118066728; val_accuracy: 0.7229299363057324 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.17; acc: 0.69
Batch: 20; loss: 0.84; acc: 0.83
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.02; acc: 0.7
Batch: 80; loss: 1.07; acc: 0.73
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.28; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.72
Batch: 160; loss: 0.85; acc: 0.77
Batch: 180; loss: 0.82; acc: 0.81
Batch: 200; loss: 1.08; acc: 0.72
Batch: 220; loss: 1.14; acc: 0.73
Batch: 240; loss: 1.31; acc: 0.7
Batch: 260; loss: 1.1; acc: 0.64
Batch: 280; loss: 1.08; acc: 0.77
Batch: 300; loss: 1.0; acc: 0.75
Batch: 320; loss: 1.69; acc: 0.67
Batch: 340; loss: 1.19; acc: 0.7
Batch: 360; loss: 0.78; acc: 0.78
Batch: 380; loss: 1.28; acc: 0.73
Batch: 400; loss: 0.9; acc: 0.73
Batch: 420; loss: 1.28; acc: 0.75
Batch: 440; loss: 0.89; acc: 0.77
Batch: 460; loss: 0.84; acc: 0.83
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.77; acc: 0.75
Batch: 520; loss: 1.38; acc: 0.72
Batch: 540; loss: 0.97; acc: 0.75
Batch: 560; loss: 1.19; acc: 0.78
Batch: 580; loss: 1.29; acc: 0.7
Batch: 600; loss: 0.88; acc: 0.77
Batch: 620; loss: 1.5; acc: 0.72
Train Epoch over. train_loss: 1.1; train_accuracy: 0.74 

Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 2.37; acc: 0.59
Batch: 40; loss: 0.7; acc: 0.83
Batch: 60; loss: 1.08; acc: 0.77
Batch: 80; loss: 1.42; acc: 0.61
Batch: 100; loss: 1.32; acc: 0.72
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 2.13; acc: 0.59
Val Epoch over. val_loss: 1.1849890461393222; val_accuracy: 0.7236265923566879 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.91; acc: 0.73
Batch: 40; loss: 1.22; acc: 0.75
Batch: 60; loss: 0.89; acc: 0.69
Batch: 80; loss: 1.14; acc: 0.78
Batch: 100; loss: 1.19; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.73
Batch: 140; loss: 1.01; acc: 0.75
Batch: 160; loss: 1.19; acc: 0.69
Batch: 180; loss: 0.96; acc: 0.8
Batch: 200; loss: 1.09; acc: 0.7
Batch: 220; loss: 1.38; acc: 0.75
Batch: 240; loss: 1.11; acc: 0.77
Batch: 260; loss: 0.93; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.77
Batch: 300; loss: 1.12; acc: 0.72
Batch: 320; loss: 1.22; acc: 0.67
Batch: 340; loss: 0.94; acc: 0.77
Batch: 360; loss: 0.89; acc: 0.75
Batch: 380; loss: 1.16; acc: 0.7
Batch: 400; loss: 1.42; acc: 0.7
Batch: 420; loss: 0.89; acc: 0.7
Batch: 440; loss: 1.31; acc: 0.69
Batch: 460; loss: 1.07; acc: 0.77
Batch: 480; loss: 0.93; acc: 0.73
Batch: 500; loss: 0.97; acc: 0.75
Batch: 520; loss: 1.37; acc: 0.64
Batch: 540; loss: 0.86; acc: 0.81
Batch: 560; loss: 1.47; acc: 0.64
Batch: 580; loss: 1.08; acc: 0.75
Batch: 600; loss: 1.0; acc: 0.67
Batch: 620; loss: 1.52; acc: 0.66
Train Epoch over. train_loss: 1.11; train_accuracy: 0.74 

Batch: 0; loss: 0.84; acc: 0.77
Batch: 20; loss: 2.41; acc: 0.59
Batch: 40; loss: 0.72; acc: 0.84
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.46; acc: 0.59
Batch: 100; loss: 1.33; acc: 0.72
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 2.17; acc: 0.59
Val Epoch over. val_loss: 1.187742506622509; val_accuracy: 0.7249203821656051 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.19; acc: 0.73
Batch: 20; loss: 1.12; acc: 0.77
Batch: 40; loss: 0.8; acc: 0.78
Batch: 60; loss: 1.0; acc: 0.75
Batch: 80; loss: 1.12; acc: 0.66
Batch: 100; loss: 1.97; acc: 0.66
Batch: 120; loss: 1.09; acc: 0.77
Batch: 140; loss: 0.8; acc: 0.75
Batch: 160; loss: 1.34; acc: 0.67
Batch: 180; loss: 1.05; acc: 0.75
Batch: 200; loss: 1.41; acc: 0.67
Batch: 220; loss: 0.85; acc: 0.84
Batch: 240; loss: 1.46; acc: 0.64
Batch: 260; loss: 1.62; acc: 0.64
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 1.43; acc: 0.61
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.96; acc: 0.73
Batch: 360; loss: 1.02; acc: 0.75
Batch: 380; loss: 0.94; acc: 0.75
Batch: 400; loss: 1.28; acc: 0.69
Batch: 420; loss: 0.6; acc: 0.8
Batch: 440; loss: 1.0; acc: 0.77
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.98; acc: 0.73
Batch: 500; loss: 1.72; acc: 0.66
Batch: 520; loss: 1.68; acc: 0.66
Batch: 540; loss: 1.82; acc: 0.66
Batch: 560; loss: 1.49; acc: 0.66
Batch: 580; loss: 1.19; acc: 0.69
Batch: 600; loss: 1.12; acc: 0.78
Batch: 620; loss: 1.37; acc: 0.69
Train Epoch over. train_loss: 1.1; train_accuracy: 0.74 

Batch: 0; loss: 0.85; acc: 0.75
Batch: 20; loss: 2.47; acc: 0.58
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 1.09; acc: 0.75
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.36; acc: 0.7
Batch: 120; loss: 0.65; acc: 0.84
Batch: 140; loss: 2.13; acc: 0.59
Val Epoch over. val_loss: 1.1848179027912722; val_accuracy: 0.7237261146496815 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.15; acc: 0.75
Batch: 20; loss: 0.85; acc: 0.69
Batch: 40; loss: 0.8; acc: 0.8
Batch: 60; loss: 1.33; acc: 0.69
Batch: 80; loss: 0.66; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.72
Batch: 120; loss: 1.3; acc: 0.62
Batch: 140; loss: 1.25; acc: 0.72
Batch: 160; loss: 0.8; acc: 0.8
Batch: 180; loss: 0.95; acc: 0.73
Batch: 200; loss: 1.31; acc: 0.72
Batch: 220; loss: 0.54; acc: 0.83
Batch: 240; loss: 0.89; acc: 0.84
Batch: 260; loss: 1.0; acc: 0.77
Batch: 280; loss: 1.38; acc: 0.69
Batch: 300; loss: 1.47; acc: 0.67
Batch: 320; loss: 1.34; acc: 0.77
Batch: 340; loss: 1.33; acc: 0.7
Batch: 360; loss: 0.93; acc: 0.7
Batch: 380; loss: 1.28; acc: 0.67
Batch: 400; loss: 0.85; acc: 0.75
Batch: 420; loss: 1.03; acc: 0.72
Batch: 440; loss: 0.87; acc: 0.73
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.8; acc: 0.78
Batch: 500; loss: 0.88; acc: 0.73
Batch: 520; loss: 1.03; acc: 0.78
Batch: 540; loss: 0.92; acc: 0.78
Batch: 560; loss: 1.53; acc: 0.67
Batch: 580; loss: 1.15; acc: 0.75
Batch: 600; loss: 0.86; acc: 0.81
Batch: 620; loss: 1.35; acc: 0.77
Train Epoch over. train_loss: 1.1; train_accuracy: 0.74 

Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 2.42; acc: 0.58
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 1.03; acc: 0.77
Batch: 80; loss: 1.49; acc: 0.59
Batch: 100; loss: 1.33; acc: 0.72
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 2.21; acc: 0.56
Val Epoch over. val_loss: 1.1857161897762565; val_accuracy: 0.7231289808917197 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.02; acc: 0.7
Batch: 20; loss: 0.89; acc: 0.77
Batch: 40; loss: 1.09; acc: 0.67
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.07; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.8
Batch: 120; loss: 1.45; acc: 0.7
Batch: 140; loss: 1.19; acc: 0.77
Batch: 160; loss: 1.4; acc: 0.7
Batch: 180; loss: 0.93; acc: 0.8
Batch: 200; loss: 0.99; acc: 0.73
Batch: 220; loss: 1.16; acc: 0.78
Batch: 240; loss: 0.86; acc: 0.84
Batch: 260; loss: 1.54; acc: 0.64
Batch: 280; loss: 1.61; acc: 0.69
Batch: 300; loss: 0.83; acc: 0.77
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 1.09; acc: 0.8
Batch: 360; loss: 1.29; acc: 0.72
Batch: 380; loss: 0.87; acc: 0.8
Batch: 400; loss: 1.17; acc: 0.72
Batch: 420; loss: 1.31; acc: 0.69
Batch: 440; loss: 0.87; acc: 0.75
Batch: 460; loss: 1.04; acc: 0.75
Batch: 480; loss: 0.92; acc: 0.78
Batch: 500; loss: 0.79; acc: 0.81
Batch: 520; loss: 1.49; acc: 0.7
Batch: 540; loss: 1.81; acc: 0.72
Batch: 560; loss: 1.74; acc: 0.66
Batch: 580; loss: 1.36; acc: 0.59
Batch: 600; loss: 0.82; acc: 0.77
Batch: 620; loss: 1.22; acc: 0.78
Train Epoch over. train_loss: 1.1; train_accuracy: 0.74 

Batch: 0; loss: 0.85; acc: 0.73
Batch: 20; loss: 2.48; acc: 0.58
Batch: 40; loss: 0.75; acc: 0.86
Batch: 60; loss: 1.08; acc: 0.8
Batch: 80; loss: 1.5; acc: 0.61
Batch: 100; loss: 1.34; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 2.16; acc: 0.55
Val Epoch over. val_loss: 1.1942882302460398; val_accuracy: 0.7239251592356688 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.85; acc: 0.78
Batch: 20; loss: 1.29; acc: 0.67
Batch: 40; loss: 1.09; acc: 0.8
Batch: 60; loss: 1.24; acc: 0.75
Batch: 80; loss: 0.83; acc: 0.83
Batch: 100; loss: 0.96; acc: 0.81
Batch: 120; loss: 1.42; acc: 0.69
Batch: 140; loss: 0.91; acc: 0.77
Batch: 160; loss: 1.15; acc: 0.67
Batch: 180; loss: 0.91; acc: 0.78
Batch: 200; loss: 1.47; acc: 0.77
Batch: 220; loss: 0.84; acc: 0.73
Batch: 240; loss: 1.59; acc: 0.59
Batch: 260; loss: 1.33; acc: 0.69
Batch: 280; loss: 0.89; acc: 0.8
Batch: 300; loss: 1.6; acc: 0.67
Batch: 320; loss: 1.0; acc: 0.77
Batch: 340; loss: 1.12; acc: 0.7
Batch: 360; loss: 1.42; acc: 0.7
Batch: 380; loss: 1.74; acc: 0.61
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 1.31; acc: 0.73
Batch: 440; loss: 1.23; acc: 0.69
Batch: 460; loss: 1.74; acc: 0.62
Batch: 480; loss: 0.74; acc: 0.78
Batch: 500; loss: 1.3; acc: 0.73
Batch: 520; loss: 0.78; acc: 0.8
Batch: 540; loss: 0.97; acc: 0.73
Batch: 560; loss: 0.91; acc: 0.72
Batch: 580; loss: 1.59; acc: 0.7
Batch: 600; loss: 1.08; acc: 0.77
Batch: 620; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 1.1; train_accuracy: 0.74 

Batch: 0; loss: 0.87; acc: 0.75
Batch: 20; loss: 2.49; acc: 0.56
Batch: 40; loss: 0.8; acc: 0.84
Batch: 60; loss: 1.1; acc: 0.78
Batch: 80; loss: 1.43; acc: 0.61
Batch: 100; loss: 1.35; acc: 0.7
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 2.2; acc: 0.58
Val Epoch over. val_loss: 1.193113530897031; val_accuracy: 0.7221337579617835 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.95; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.78
Batch: 40; loss: 1.26; acc: 0.75
Batch: 60; loss: 1.22; acc: 0.69
Batch: 80; loss: 0.81; acc: 0.8
Batch: 100; loss: 0.95; acc: 0.81
Batch: 120; loss: 1.08; acc: 0.75
Batch: 140; loss: 1.64; acc: 0.61
Batch: 160; loss: 1.49; acc: 0.81
Batch: 180; loss: 1.23; acc: 0.73
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.84; acc: 0.81
Batch: 240; loss: 0.99; acc: 0.77
Batch: 260; loss: 1.5; acc: 0.69
Batch: 280; loss: 0.84; acc: 0.75
Batch: 300; loss: 1.09; acc: 0.75
Batch: 320; loss: 0.99; acc: 0.69
Batch: 340; loss: 1.06; acc: 0.78
Batch: 360; loss: 1.0; acc: 0.69
Batch: 380; loss: 0.76; acc: 0.78
Batch: 400; loss: 1.16; acc: 0.75
Batch: 420; loss: 1.26; acc: 0.73
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 1.06; acc: 0.77
Batch: 480; loss: 1.62; acc: 0.72
Batch: 500; loss: 0.95; acc: 0.78
Batch: 520; loss: 1.2; acc: 0.69
Batch: 540; loss: 0.98; acc: 0.78
Batch: 560; loss: 1.44; acc: 0.7
Batch: 580; loss: 1.15; acc: 0.72
Batch: 600; loss: 1.2; acc: 0.73
Batch: 620; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 1.1; train_accuracy: 0.74 

Batch: 0; loss: 0.84; acc: 0.75
Batch: 20; loss: 2.39; acc: 0.59
Batch: 40; loss: 0.76; acc: 0.84
Batch: 60; loss: 1.06; acc: 0.78
Batch: 80; loss: 1.46; acc: 0.59
Batch: 100; loss: 1.35; acc: 0.69
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 2.18; acc: 0.59
Val Epoch over. val_loss: 1.1889259179306637; val_accuracy: 0.7248208598726115 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 1.37; acc: 0.7
Batch: 20; loss: 1.4; acc: 0.73
Batch: 40; loss: 1.36; acc: 0.69
Batch: 60; loss: 1.22; acc: 0.7
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 1.11; acc: 0.73
Batch: 120; loss: 1.28; acc: 0.75
Batch: 140; loss: 0.82; acc: 0.78
Batch: 160; loss: 1.06; acc: 0.72
Batch: 180; loss: 1.34; acc: 0.59
Batch: 200; loss: 0.84; acc: 0.83
Batch: 220; loss: 1.33; acc: 0.75
Batch: 240; loss: 1.14; acc: 0.83
Batch: 260; loss: 1.09; acc: 0.8
Batch: 280; loss: 1.24; acc: 0.7
Batch: 300; loss: 1.31; acc: 0.67
Batch: 320; loss: 0.85; acc: 0.7
Batch: 340; loss: 0.68; acc: 0.8
Batch: 360; loss: 0.8; acc: 0.75
Batch: 380; loss: 1.68; acc: 0.61
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 0.87; acc: 0.81
Batch: 440; loss: 1.52; acc: 0.7
Batch: 460; loss: 1.06; acc: 0.72
Batch: 480; loss: 1.07; acc: 0.77
Batch: 500; loss: 0.98; acc: 0.69
Batch: 520; loss: 0.93; acc: 0.73
Batch: 540; loss: 1.43; acc: 0.73
Batch: 560; loss: 1.62; acc: 0.62
Batch: 580; loss: 0.84; acc: 0.77
Batch: 600; loss: 1.12; acc: 0.77
Batch: 620; loss: 1.02; acc: 0.77
Train Epoch over. train_loss: 1.1; train_accuracy: 0.74 

Batch: 0; loss: 0.83; acc: 0.77
Batch: 20; loss: 2.51; acc: 0.58
Batch: 40; loss: 0.79; acc: 0.81
Batch: 60; loss: 1.1; acc: 0.78
Batch: 80; loss: 1.46; acc: 0.61
Batch: 100; loss: 1.39; acc: 0.73
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 2.2; acc: 0.58
Val Epoch over. val_loss: 1.199198838821642; val_accuracy: 0.7246218152866242 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.83; acc: 0.8
Batch: 20; loss: 1.43; acc: 0.72
Batch: 40; loss: 0.77; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.67
Batch: 80; loss: 0.84; acc: 0.75
Batch: 100; loss: 1.19; acc: 0.75
Batch: 120; loss: 1.07; acc: 0.73
Batch: 140; loss: 0.85; acc: 0.78
Batch: 160; loss: 0.89; acc: 0.81
Batch: 180; loss: 0.94; acc: 0.77
Batch: 200; loss: 1.12; acc: 0.72
Batch: 220; loss: 0.95; acc: 0.73
Batch: 240; loss: 1.51; acc: 0.67
Batch: 260; loss: 0.92; acc: 0.8
Batch: 280; loss: 1.47; acc: 0.69
Batch: 300; loss: 1.2; acc: 0.67
Batch: 320; loss: 1.29; acc: 0.7
Batch: 340; loss: 0.95; acc: 0.73
Batch: 360; loss: 0.95; acc: 0.69
Batch: 380; loss: 0.86; acc: 0.8
Batch: 400; loss: 1.58; acc: 0.7
Batch: 420; loss: 1.01; acc: 0.69
Batch: 440; loss: 1.13; acc: 0.8
Batch: 460; loss: 0.81; acc: 0.81
Batch: 480; loss: 1.23; acc: 0.77
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 1.29; acc: 0.69
Batch: 540; loss: 1.26; acc: 0.69
Batch: 560; loss: 0.52; acc: 0.8
Batch: 580; loss: 1.36; acc: 0.73
Batch: 600; loss: 1.31; acc: 0.69
Batch: 620; loss: 1.65; acc: 0.69
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 2.45; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 1.08; acc: 0.8
Batch: 80; loss: 1.43; acc: 0.62
Batch: 100; loss: 1.37; acc: 0.7
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 2.14; acc: 0.58
Val Epoch over. val_loss: 1.1860004786852818; val_accuracy: 0.7251194267515924 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.88; acc: 0.75
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.75
Batch: 80; loss: 1.64; acc: 0.66
Batch: 100; loss: 1.25; acc: 0.7
Batch: 120; loss: 1.49; acc: 0.66
Batch: 140; loss: 1.26; acc: 0.73
Batch: 160; loss: 0.87; acc: 0.77
Batch: 180; loss: 0.98; acc: 0.73
Batch: 200; loss: 1.26; acc: 0.72
Batch: 220; loss: 1.41; acc: 0.69
Batch: 240; loss: 1.8; acc: 0.67
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 0.77; acc: 0.77
Batch: 300; loss: 1.4; acc: 0.75
Batch: 320; loss: 1.35; acc: 0.7
Batch: 340; loss: 1.04; acc: 0.7
Batch: 360; loss: 1.37; acc: 0.78
Batch: 380; loss: 0.77; acc: 0.81
Batch: 400; loss: 1.02; acc: 0.75
Batch: 420; loss: 1.15; acc: 0.75
Batch: 440; loss: 0.92; acc: 0.72
Batch: 460; loss: 1.27; acc: 0.8
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 1.08; acc: 0.72
Batch: 520; loss: 0.72; acc: 0.75
Batch: 540; loss: 1.0; acc: 0.78
Batch: 560; loss: 1.44; acc: 0.72
Batch: 580; loss: 1.5; acc: 0.73
Batch: 600; loss: 1.18; acc: 0.69
Batch: 620; loss: 1.19; acc: 0.75
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 2.45; acc: 0.59
Batch: 40; loss: 0.74; acc: 0.83
Batch: 60; loss: 1.09; acc: 0.8
Batch: 80; loss: 1.4; acc: 0.62
Batch: 100; loss: 1.37; acc: 0.72
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 2.13; acc: 0.59
Val Epoch over. val_loss: 1.1847065091133118; val_accuracy: 0.7250199044585988 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.46; acc: 0.67
Batch: 40; loss: 0.7; acc: 0.72
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 1.05; acc: 0.75
Batch: 120; loss: 1.31; acc: 0.73
Batch: 140; loss: 1.3; acc: 0.7
Batch: 160; loss: 1.72; acc: 0.72
Batch: 180; loss: 1.05; acc: 0.7
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 1.03; acc: 0.7
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 0.71; acc: 0.77
Batch: 280; loss: 1.5; acc: 0.64
Batch: 300; loss: 1.64; acc: 0.53
Batch: 320; loss: 0.63; acc: 0.78
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 1.16; acc: 0.72
Batch: 380; loss: 1.41; acc: 0.66
Batch: 400; loss: 1.11; acc: 0.69
Batch: 420; loss: 0.56; acc: 0.8
Batch: 440; loss: 0.79; acc: 0.86
Batch: 460; loss: 1.08; acc: 0.69
Batch: 480; loss: 1.36; acc: 0.73
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 1.21; acc: 0.75
Batch: 540; loss: 0.88; acc: 0.75
Batch: 560; loss: 0.9; acc: 0.77
Batch: 580; loss: 1.29; acc: 0.72
Batch: 600; loss: 0.76; acc: 0.77
Batch: 620; loss: 1.42; acc: 0.67
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 2.44; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.36; acc: 0.7
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 2.14; acc: 0.59
Val Epoch over. val_loss: 1.1844778634180688; val_accuracy: 0.7247213375796179 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.35; acc: 0.72
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 1.21; acc: 0.72
Batch: 60; loss: 1.38; acc: 0.64
Batch: 80; loss: 1.16; acc: 0.73
Batch: 100; loss: 1.82; acc: 0.72
Batch: 120; loss: 0.84; acc: 0.8
Batch: 140; loss: 0.99; acc: 0.78
Batch: 160; loss: 1.21; acc: 0.7
Batch: 180; loss: 0.77; acc: 0.78
Batch: 200; loss: 0.97; acc: 0.7
Batch: 220; loss: 1.28; acc: 0.7
Batch: 240; loss: 1.43; acc: 0.7
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.82; acc: 0.73
Batch: 300; loss: 1.0; acc: 0.73
Batch: 320; loss: 0.84; acc: 0.78
Batch: 340; loss: 1.01; acc: 0.73
Batch: 360; loss: 1.51; acc: 0.75
Batch: 380; loss: 1.31; acc: 0.75
Batch: 400; loss: 0.6; acc: 0.78
Batch: 420; loss: 0.96; acc: 0.8
Batch: 440; loss: 0.78; acc: 0.72
Batch: 460; loss: 1.1; acc: 0.78
Batch: 480; loss: 0.82; acc: 0.8
Batch: 500; loss: 1.02; acc: 0.77
Batch: 520; loss: 0.99; acc: 0.75
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 1.01; acc: 0.73
Batch: 580; loss: 0.93; acc: 0.72
Batch: 600; loss: 1.25; acc: 0.73
Batch: 620; loss: 1.59; acc: 0.67
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 2.45; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.42; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.72
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 2.15; acc: 0.58
Val Epoch over. val_loss: 1.1846727669998338; val_accuracy: 0.724422770700637 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.88; acc: 0.77
Batch: 20; loss: 1.18; acc: 0.73
Batch: 40; loss: 1.43; acc: 0.64
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 0.87; acc: 0.86
Batch: 100; loss: 1.35; acc: 0.72
Batch: 120; loss: 1.37; acc: 0.7
Batch: 140; loss: 0.87; acc: 0.72
Batch: 160; loss: 1.25; acc: 0.7
Batch: 180; loss: 0.45; acc: 0.92
Batch: 200; loss: 0.9; acc: 0.75
Batch: 220; loss: 1.05; acc: 0.78
Batch: 240; loss: 0.64; acc: 0.8
Batch: 260; loss: 1.31; acc: 0.66
Batch: 280; loss: 0.82; acc: 0.83
Batch: 300; loss: 1.09; acc: 0.77
Batch: 320; loss: 0.6; acc: 0.8
Batch: 340; loss: 0.72; acc: 0.75
Batch: 360; loss: 0.74; acc: 0.78
Batch: 380; loss: 1.06; acc: 0.73
Batch: 400; loss: 0.7; acc: 0.75
Batch: 420; loss: 1.03; acc: 0.72
Batch: 440; loss: 1.96; acc: 0.62
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 0.82; acc: 0.77
Batch: 500; loss: 1.02; acc: 0.7
Batch: 520; loss: 0.88; acc: 0.81
Batch: 540; loss: 1.15; acc: 0.75
Batch: 560; loss: 1.38; acc: 0.72
Batch: 580; loss: 1.91; acc: 0.59
Batch: 600; loss: 0.97; acc: 0.73
Batch: 620; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 2.44; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.36; acc: 0.7
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 2.14; acc: 0.58
Val Epoch over. val_loss: 1.1835285916829565; val_accuracy: 0.7250199044585988 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.87; acc: 0.67
Batch: 20; loss: 0.97; acc: 0.78
Batch: 40; loss: 1.47; acc: 0.7
Batch: 60; loss: 1.03; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.69
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 0.89; acc: 0.8
Batch: 160; loss: 1.38; acc: 0.72
Batch: 180; loss: 1.51; acc: 0.64
Batch: 200; loss: 1.19; acc: 0.7
Batch: 220; loss: 1.2; acc: 0.67
Batch: 240; loss: 0.75; acc: 0.77
Batch: 260; loss: 0.84; acc: 0.75
Batch: 280; loss: 0.93; acc: 0.83
Batch: 300; loss: 1.1; acc: 0.72
Batch: 320; loss: 0.95; acc: 0.78
Batch: 340; loss: 1.52; acc: 0.69
Batch: 360; loss: 0.98; acc: 0.7
Batch: 380; loss: 1.25; acc: 0.75
Batch: 400; loss: 0.71; acc: 0.8
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.67; acc: 0.77
Batch: 460; loss: 0.75; acc: 0.78
Batch: 480; loss: 1.35; acc: 0.67
Batch: 500; loss: 1.22; acc: 0.64
Batch: 520; loss: 1.39; acc: 0.7
Batch: 540; loss: 0.99; acc: 0.77
Batch: 560; loss: 1.77; acc: 0.69
Batch: 580; loss: 1.34; acc: 0.69
Batch: 600; loss: 0.99; acc: 0.73
Batch: 620; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 2.44; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.7
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 2.14; acc: 0.58
Val Epoch over. val_loss: 1.1832078801598518; val_accuracy: 0.7248208598726115 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.15; acc: 0.75
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.64; acc: 0.8
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 0.78; acc: 0.72
Batch: 100; loss: 0.71; acc: 0.81
Batch: 120; loss: 1.02; acc: 0.78
Batch: 140; loss: 1.06; acc: 0.7
Batch: 160; loss: 0.82; acc: 0.8
Batch: 180; loss: 1.38; acc: 0.7
Batch: 200; loss: 1.43; acc: 0.67
Batch: 220; loss: 1.12; acc: 0.77
Batch: 240; loss: 0.95; acc: 0.84
Batch: 260; loss: 0.72; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.8
Batch: 300; loss: 1.44; acc: 0.66
Batch: 320; loss: 1.14; acc: 0.72
Batch: 340; loss: 0.9; acc: 0.78
Batch: 360; loss: 1.06; acc: 0.77
Batch: 380; loss: 0.69; acc: 0.78
Batch: 400; loss: 1.27; acc: 0.67
Batch: 420; loss: 1.07; acc: 0.75
Batch: 440; loss: 1.38; acc: 0.73
Batch: 460; loss: 1.07; acc: 0.75
Batch: 480; loss: 0.92; acc: 0.8
Batch: 500; loss: 1.5; acc: 0.7
Batch: 520; loss: 1.25; acc: 0.7
Batch: 540; loss: 0.9; acc: 0.75
Batch: 560; loss: 0.9; acc: 0.78
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 1.06; acc: 0.75
Batch: 620; loss: 0.75; acc: 0.77
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 2.46; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 1.08; acc: 0.78
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.38; acc: 0.7
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 2.15; acc: 0.56
Val Epoch over. val_loss: 1.1842165587434343; val_accuracy: 0.7247213375796179 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 1.42; acc: 0.69
Batch: 20; loss: 0.86; acc: 0.7
Batch: 40; loss: 1.16; acc: 0.77
Batch: 60; loss: 1.26; acc: 0.75
Batch: 80; loss: 1.08; acc: 0.75
Batch: 100; loss: 1.26; acc: 0.73
Batch: 120; loss: 1.11; acc: 0.72
Batch: 140; loss: 1.25; acc: 0.61
Batch: 160; loss: 1.25; acc: 0.75
Batch: 180; loss: 1.16; acc: 0.73
Batch: 200; loss: 0.88; acc: 0.78
Batch: 220; loss: 0.65; acc: 0.78
Batch: 240; loss: 0.88; acc: 0.7
Batch: 260; loss: 1.63; acc: 0.66
Batch: 280; loss: 0.89; acc: 0.7
Batch: 300; loss: 0.68; acc: 0.83
Batch: 320; loss: 1.49; acc: 0.58
Batch: 340; loss: 0.69; acc: 0.83
Batch: 360; loss: 0.92; acc: 0.75
Batch: 380; loss: 1.3; acc: 0.72
Batch: 400; loss: 1.23; acc: 0.78
Batch: 420; loss: 1.16; acc: 0.69
Batch: 440; loss: 1.32; acc: 0.66
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 1.4; acc: 0.72
Batch: 500; loss: 0.96; acc: 0.77
Batch: 520; loss: 0.8; acc: 0.75
Batch: 540; loss: 0.78; acc: 0.8
Batch: 560; loss: 0.9; acc: 0.81
Batch: 580; loss: 0.96; acc: 0.77
Batch: 600; loss: 0.93; acc: 0.73
Batch: 620; loss: 0.9; acc: 0.72
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 2.46; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.83
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.72
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 2.15; acc: 0.56
Val Epoch over. val_loss: 1.184531459003497; val_accuracy: 0.7237261146496815 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.83
Batch: 40; loss: 1.33; acc: 0.69
Batch: 60; loss: 1.49; acc: 0.69
Batch: 80; loss: 1.12; acc: 0.69
Batch: 100; loss: 1.25; acc: 0.73
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.89; acc: 0.72
Batch: 160; loss: 0.62; acc: 0.84
Batch: 180; loss: 0.76; acc: 0.78
Batch: 200; loss: 1.18; acc: 0.8
Batch: 220; loss: 1.26; acc: 0.69
Batch: 240; loss: 1.17; acc: 0.73
Batch: 260; loss: 1.41; acc: 0.67
Batch: 280; loss: 1.12; acc: 0.73
Batch: 300; loss: 1.36; acc: 0.69
Batch: 320; loss: 1.52; acc: 0.66
Batch: 340; loss: 0.9; acc: 0.77
Batch: 360; loss: 0.95; acc: 0.73
Batch: 380; loss: 1.22; acc: 0.69
Batch: 400; loss: 0.77; acc: 0.81
Batch: 420; loss: 1.34; acc: 0.75
Batch: 440; loss: 0.73; acc: 0.81
Batch: 460; loss: 1.21; acc: 0.77
Batch: 480; loss: 0.86; acc: 0.77
Batch: 500; loss: 0.81; acc: 0.7
Batch: 520; loss: 1.11; acc: 0.77
Batch: 540; loss: 1.02; acc: 0.67
Batch: 560; loss: 0.81; acc: 0.78
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.99; acc: 0.69
Batch: 620; loss: 0.9; acc: 0.8
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 2.45; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 1.07; acc: 0.8
Batch: 80; loss: 1.41; acc: 0.64
Batch: 100; loss: 1.36; acc: 0.72
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 2.14; acc: 0.56
Val Epoch over. val_loss: 1.1837097491807997; val_accuracy: 0.7251194267515924 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.61; acc: 0.78
Batch: 20; loss: 1.28; acc: 0.75
Batch: 40; loss: 0.99; acc: 0.77
Batch: 60; loss: 1.17; acc: 0.77
Batch: 80; loss: 1.08; acc: 0.8
Batch: 100; loss: 1.49; acc: 0.72
Batch: 120; loss: 1.15; acc: 0.72
Batch: 140; loss: 0.95; acc: 0.7
Batch: 160; loss: 0.99; acc: 0.8
Batch: 180; loss: 0.9; acc: 0.8
Batch: 200; loss: 0.98; acc: 0.69
Batch: 220; loss: 1.69; acc: 0.66
Batch: 240; loss: 1.05; acc: 0.72
Batch: 260; loss: 0.99; acc: 0.8
Batch: 280; loss: 1.47; acc: 0.69
Batch: 300; loss: 1.18; acc: 0.66
Batch: 320; loss: 1.19; acc: 0.66
Batch: 340; loss: 0.81; acc: 0.8
Batch: 360; loss: 1.29; acc: 0.75
Batch: 380; loss: 1.49; acc: 0.7
Batch: 400; loss: 1.28; acc: 0.69
Batch: 420; loss: 0.76; acc: 0.78
Batch: 440; loss: 1.13; acc: 0.72
Batch: 460; loss: 0.86; acc: 0.78
Batch: 480; loss: 0.87; acc: 0.73
Batch: 500; loss: 0.91; acc: 0.78
Batch: 520; loss: 0.9; acc: 0.72
Batch: 540; loss: 1.49; acc: 0.73
Batch: 560; loss: 1.22; acc: 0.69
Batch: 580; loss: 1.09; acc: 0.77
Batch: 600; loss: 0.85; acc: 0.78
Batch: 620; loss: 1.25; acc: 0.72
Train Epoch over. train_loss: 1.09; train_accuracy: 0.74 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 2.46; acc: 0.59
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 1.07; acc: 0.78
Batch: 80; loss: 1.4; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.72
Batch: 120; loss: 0.59; acc: 0.84
Batch: 140; loss: 2.15; acc: 0.56
Val Epoch over. val_loss: 1.1845213546874418; val_accuracy: 0.7243232484076433 

plots/subspace_training/lenet/2020-01-10 13:15:25/d_dim_200_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 63506
elements in E: 13327800
fraction nonzero: 0.004764927444889629
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 1.84; acc: 0.36
Batch: 40; loss: 1.76; acc: 0.48
Batch: 60; loss: 1.51; acc: 0.5
Batch: 80; loss: 1.08; acc: 0.72
Batch: 100; loss: 0.76; acc: 0.7
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 1.24; acc: 0.69
Batch: 160; loss: 1.24; acc: 0.66
Batch: 180; loss: 1.06; acc: 0.73
Batch: 200; loss: 0.77; acc: 0.73
Batch: 220; loss: 1.05; acc: 0.7
Batch: 240; loss: 0.93; acc: 0.7
Batch: 260; loss: 1.36; acc: 0.64
Batch: 280; loss: 0.83; acc: 0.7
Batch: 300; loss: 1.1; acc: 0.77
Batch: 320; loss: 0.48; acc: 0.83
Batch: 340; loss: 0.83; acc: 0.78
Batch: 360; loss: 0.6; acc: 0.78
Batch: 380; loss: 1.17; acc: 0.72
Batch: 400; loss: 0.84; acc: 0.75
Batch: 420; loss: 1.06; acc: 0.73
Batch: 440; loss: 0.73; acc: 0.81
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.74; acc: 0.8
Batch: 500; loss: 0.97; acc: 0.78
Batch: 520; loss: 1.11; acc: 0.77
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 1.09; train_accuracy: 0.71 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 1.26; acc: 0.67
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.77
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 2.87; acc: 0.59
Val Epoch over. val_loss: 0.7835348779988137; val_accuracy: 0.7901074840764332 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 1.02; acc: 0.78
Batch: 40; loss: 0.89; acc: 0.72
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.6; acc: 0.84
Batch: 160; loss: 0.82; acc: 0.84
Batch: 180; loss: 0.84; acc: 0.72
Batch: 200; loss: 1.33; acc: 0.69
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.78; acc: 0.78
Batch: 260; loss: 0.78; acc: 0.8
Batch: 280; loss: 0.57; acc: 0.84
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 0.76; acc: 0.8
Batch: 340; loss: 0.96; acc: 0.77
Batch: 360; loss: 1.11; acc: 0.77
Batch: 380; loss: 1.13; acc: 0.72
Batch: 400; loss: 0.82; acc: 0.8
Batch: 420; loss: 0.84; acc: 0.75
Batch: 440; loss: 1.03; acc: 0.75
Batch: 460; loss: 0.54; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.83
Batch: 500; loss: 0.99; acc: 0.75
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.92; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.84
Batch: 600; loss: 0.94; acc: 0.77
Batch: 620; loss: 0.83; acc: 0.81
Train Epoch over. train_loss: 0.76; train_accuracy: 0.8 

Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 1.34; acc: 0.7
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.94; acc: 0.75
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 2.26; acc: 0.62
Val Epoch over. val_loss: 0.7488759272986916; val_accuracy: 0.7983678343949044 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 1.17; acc: 0.7
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 1.03; acc: 0.77
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.89; acc: 0.72
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 1.25; acc: 0.75
Batch: 240; loss: 0.82; acc: 0.84
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.93; acc: 0.78
Batch: 300; loss: 0.47; acc: 0.75
Batch: 320; loss: 0.8; acc: 0.8
Batch: 340; loss: 0.38; acc: 0.92
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.78; acc: 0.73
Batch: 400; loss: 0.67; acc: 0.83
Batch: 420; loss: 1.0; acc: 0.73
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.63; acc: 0.86
Batch: 480; loss: 1.06; acc: 0.81
Batch: 500; loss: 0.73; acc: 0.83
Batch: 520; loss: 0.81; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.78
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.83
Train Epoch over. train_loss: 0.74; train_accuracy: 0.8 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 1.41; acc: 0.62
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.84
Batch: 80; loss: 0.71; acc: 0.81
Batch: 100; loss: 1.11; acc: 0.8
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 2.18; acc: 0.67
Val Epoch over. val_loss: 0.7047238122126099; val_accuracy: 0.8223527070063694 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 1.67; acc: 0.64
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.92; acc: 0.72
Batch: 160; loss: 0.78; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.85; acc: 0.77
Batch: 320; loss: 1.27; acc: 0.7
Batch: 340; loss: 0.66; acc: 0.8
Batch: 360; loss: 0.76; acc: 0.78
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 1.43; acc: 0.77
Batch: 500; loss: 0.69; acc: 0.83
Batch: 520; loss: 0.82; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.81; acc: 0.83
Train Epoch over. train_loss: 0.72; train_accuracy: 0.81 

Batch: 0; loss: 0.58; acc: 0.86
Batch: 20; loss: 1.47; acc: 0.67
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.83
Batch: 100; loss: 1.07; acc: 0.77
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 2.07; acc: 0.67
Val Epoch over. val_loss: 0.701722781084905; val_accuracy: 0.8110071656050956 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.6; acc: 0.88
Batch: 20; loss: 0.96; acc: 0.78
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.4; acc: 0.81
Batch: 100; loss: 0.7; acc: 0.77
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.78; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.88
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.82; acc: 0.8
Batch: 240; loss: 0.86; acc: 0.75
Batch: 260; loss: 0.66; acc: 0.78
Batch: 280; loss: 1.03; acc: 0.78
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 1.06; acc: 0.78
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 1.11; acc: 0.83
Batch: 400; loss: 0.82; acc: 0.78
Batch: 420; loss: 0.55; acc: 0.86
Batch: 440; loss: 0.83; acc: 0.77
Batch: 460; loss: 0.62; acc: 0.8
Batch: 480; loss: 0.6; acc: 0.88
Batch: 500; loss: 1.14; acc: 0.73
Batch: 520; loss: 0.62; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.81; acc: 0.83
Batch: 600; loss: 0.73; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.69; train_accuracy: 0.82 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.66; acc: 0.8
Batch: 100; loss: 1.1; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 1.96; acc: 0.62
Val Epoch over. val_loss: 0.6563396435824169; val_accuracy: 0.8188694267515924 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.83; acc: 0.77
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.75; acc: 0.83
Batch: 160; loss: 0.88; acc: 0.81
Batch: 180; loss: 0.58; acc: 0.91
Batch: 200; loss: 0.83; acc: 0.72
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.89; acc: 0.73
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 0.84; acc: 0.75
Batch: 320; loss: 1.19; acc: 0.7
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.71; acc: 0.8
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.67; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.75; acc: 0.81
Batch: 520; loss: 0.95; acc: 0.75
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.8; acc: 0.78
Batch: 580; loss: 0.99; acc: 0.77
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.81; acc: 0.83
Train Epoch over. train_loss: 0.67; train_accuracy: 0.82 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 1.26; acc: 0.8
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 2.06; acc: 0.66
Val Epoch over. val_loss: 0.6594279253748572; val_accuracy: 0.8230493630573248 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.35; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.43; acc: 0.81
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.89; acc: 0.8
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 1.0; acc: 0.77
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.65; acc: 0.83
Batch: 220; loss: 1.31; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.8
Batch: 260; loss: 0.21; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.8
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.72; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.71; acc: 0.83
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.74; acc: 0.77
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.94; acc: 0.81
Batch: 480; loss: 0.52; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.78
Batch: 520; loss: 0.69; acc: 0.8
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.85; acc: 0.77
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.7; acc: 0.84
Train Epoch over. train_loss: 0.66; train_accuracy: 0.83 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 1.05; acc: 0.75
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.83; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 1.67; acc: 0.67
Val Epoch over. val_loss: 0.6509013305993596; val_accuracy: 0.8288216560509554 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.89; acc: 0.77
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.81
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 1.05; acc: 0.78
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.74; acc: 0.8
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.88; acc: 0.81
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.79; acc: 0.81
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 1.02; acc: 0.75
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.7; acc: 0.8
Batch: 620; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 1.04; acc: 0.73
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.8; acc: 0.75
Batch: 100; loss: 0.84; acc: 0.86
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 1.77; acc: 0.72
Val Epoch over. val_loss: 0.6508500510530107; val_accuracy: 0.8315087579617835 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.8
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.63; acc: 0.8
Batch: 160; loss: 0.62; acc: 0.88
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.72; acc: 0.84
Batch: 220; loss: 0.88; acc: 0.81
Batch: 240; loss: 1.08; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.8
Batch: 280; loss: 0.95; acc: 0.81
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.94; acc: 0.73
Batch: 380; loss: 0.95; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.81; acc: 0.8
Batch: 440; loss: 0.65; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.8
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.77; acc: 0.8
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.94; acc: 0.75
Batch: 600; loss: 0.88; acc: 0.84
Batch: 620; loss: 0.95; acc: 0.77
Train Epoch over. train_loss: 0.65; train_accuracy: 0.83 

Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.95; acc: 0.73
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 1.2; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 1.9; acc: 0.67
Val Epoch over. val_loss: 0.67157601304115; val_accuracy: 0.8295183121019108 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.63; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.97; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.92
Batch: 140; loss: 0.99; acc: 0.77
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.75; acc: 0.84
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.8
Batch: 300; loss: 0.56; acc: 0.78
Batch: 320; loss: 0.95; acc: 0.81
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.78; acc: 0.86
Batch: 440; loss: 0.99; acc: 0.8
Batch: 460; loss: 0.94; acc: 0.7
Batch: 480; loss: 0.86; acc: 0.84
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.52; acc: 0.8
Batch: 540; loss: 0.73; acc: 0.83
Batch: 560; loss: 0.71; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.96; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.83 

Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.89; acc: 0.81
Batch: 100; loss: 1.2; acc: 0.8
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 1.7; acc: 0.72
Val Epoch over. val_loss: 0.6997742605437139; val_accuracy: 0.8136942675159236 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.8; acc: 0.8
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.91; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.91; acc: 0.8
Batch: 180; loss: 0.63; acc: 0.83
Batch: 200; loss: 0.77; acc: 0.8
Batch: 220; loss: 0.96; acc: 0.86
Batch: 240; loss: 0.55; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.83
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.72; acc: 0.89
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.94; acc: 0.89
Batch: 560; loss: 0.68; acc: 0.83
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.53; train_accuracy: 0.86 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 1.71; acc: 0.73
Val Epoch over. val_loss: 0.5739260118478423; val_accuracy: 0.8500199044585988 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.65; acc: 0.78
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.65; acc: 0.78
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.89; acc: 0.8
Batch: 220; loss: 0.64; acc: 0.86
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.65; acc: 0.8
Batch: 300; loss: 0.89; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.75; acc: 0.83
Batch: 400; loss: 0.47; acc: 0.83
Batch: 420; loss: 0.66; acc: 0.78
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.59; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.72; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.83
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.68; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.86 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.96; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.91
Batch: 140; loss: 1.62; acc: 0.75
Val Epoch over. val_loss: 0.566633476478279; val_accuracy: 0.8517117834394905 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.63; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.48; acc: 0.83
Batch: 220; loss: 0.45; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.59; acc: 0.89
Batch: 340; loss: 0.93; acc: 0.81
Batch: 360; loss: 0.18; acc: 0.91
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.76; acc: 0.84
Train Epoch over. train_loss: 0.51; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.99; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 1.74; acc: 0.75
Val Epoch over. val_loss: 0.5749673836739959; val_accuracy: 0.8511146496815286 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.69; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.82; acc: 0.8
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.7; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.72; acc: 0.8
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.72; acc: 0.8
Train Epoch over. train_loss: 0.51; train_accuracy: 0.86 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.96; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 1.57; acc: 0.73
Val Epoch over. val_loss: 0.5659439635409671; val_accuracy: 0.8515127388535032 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.8; acc: 0.83
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.68; acc: 0.78
Batch: 160; loss: 1.06; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.78
Batch: 260; loss: 0.64; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.8; acc: 0.75
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.74; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.83
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.92; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.92
Batch: 140; loss: 1.59; acc: 0.73
Val Epoch over. val_loss: 0.5573624658641542; val_accuracy: 0.8543988853503185 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.89
Batch: 200; loss: 0.92; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.94; acc: 0.81
Batch: 280; loss: 0.82; acc: 0.83
Batch: 300; loss: 0.78; acc: 0.77
Batch: 320; loss: 0.83; acc: 0.86
Batch: 340; loss: 0.78; acc: 0.84
Batch: 360; loss: 0.7; acc: 0.86
Batch: 380; loss: 0.74; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.68; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.81
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.86
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.79; acc: 0.81
Batch: 580; loss: 0.98; acc: 0.78
Batch: 600; loss: 0.78; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.51; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.99; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 1.63; acc: 0.75
Val Epoch over. val_loss: 0.5623609714542225; val_accuracy: 0.8528065286624203 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 1.03; acc: 0.8
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.76; acc: 0.83
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.82; acc: 0.78
Batch: 500; loss: 0.76; acc: 0.8
Batch: 520; loss: 1.12; acc: 0.83
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.89; acc: 0.81
Batch: 580; loss: 0.55; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 1.04; acc: 0.78
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 1.67; acc: 0.73
Val Epoch over. val_loss: 0.5632870186381279; val_accuracy: 0.8560907643312102 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.86; acc: 0.81
Batch: 340; loss: 0.79; acc: 0.75
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.77; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.66; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.9; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 1.01; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.91
Batch: 140; loss: 1.59; acc: 0.72
Val Epoch over. val_loss: 0.5665508662914015; val_accuracy: 0.849422770700637 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.84; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.94; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.68; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.94
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 1.4; acc: 0.78
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.84
Batch: 500; loss: 0.61; acc: 0.86
Batch: 520; loss: 0.87; acc: 0.8
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 1.01; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 1.64; acc: 0.72
Val Epoch over. val_loss: 0.5589055915357201; val_accuracy: 0.854796974522293 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 1.04; acc: 0.81
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.62; acc: 0.83
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.89
Batch: 240; loss: 0.64; acc: 0.88
Batch: 260; loss: 0.7; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.83
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.91; acc: 0.78
Batch: 400; loss: 0.55; acc: 0.86
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.64; acc: 0.8
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.25; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.83
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.7; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.77
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.98; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.92
Batch: 140; loss: 1.51; acc: 0.73
Val Epoch over. val_loss: 0.5677101395692036; val_accuracy: 0.8511146496815286 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.23; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.95
Batch: 280; loss: 0.84; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.65; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.92
Batch: 140; loss: 1.57; acc: 0.72
Val Epoch over. val_loss: 0.5571833607402576; val_accuracy: 0.8545979299363057 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.63; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.81
Batch: 140; loss: 0.71; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.75; acc: 0.89
Batch: 220; loss: 0.87; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.67; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.54; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.91
Batch: 600; loss: 0.84; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.92
Batch: 140; loss: 1.6; acc: 0.72
Val Epoch over. val_loss: 0.5551903310475076; val_accuracy: 0.8548964968152867 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.74; acc: 0.83
Batch: 180; loss: 0.38; acc: 0.83
Batch: 200; loss: 0.8; acc: 0.8
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.88
Batch: 420; loss: 0.62; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.79; acc: 0.81
Batch: 540; loss: 0.81; acc: 0.8
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 1.59; acc: 0.73
Val Epoch over. val_loss: 0.5551511113810691; val_accuracy: 0.8551950636942676 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.91
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.79; acc: 0.81
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.64; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.55; acc: 0.88
Batch: 360; loss: 1.0; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.72; acc: 0.91
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.78; acc: 0.81
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.71; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 1.58; acc: 0.72
Val Epoch over. val_loss: 0.5553778330231928; val_accuracy: 0.854796974522293 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.78; acc: 0.83
Batch: 120; loss: 0.88; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.6; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.76; acc: 0.86
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.8
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.52; acc: 0.88
Batch: 480; loss: 0.6; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.51; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.71; acc: 0.8
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.99; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 1.59; acc: 0.72
Val Epoch over. val_loss: 0.5561486263373855; val_accuracy: 0.8541998407643312 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.91
Batch: 40; loss: 0.87; acc: 0.83
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.75; acc: 0.83
Batch: 200; loss: 0.7; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.7; acc: 0.88
Batch: 360; loss: 0.79; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.81
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.91; acc: 0.83
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.57; acc: 0.83
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.81; acc: 0.8
Batch: 600; loss: 0.71; acc: 0.83
Batch: 620; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.98; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 1.59; acc: 0.72
Val Epoch over. val_loss: 0.5547245689638102; val_accuracy: 0.8545979299363057 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.77; acc: 0.84
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.71; acc: 0.78
Batch: 540; loss: 0.56; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 1.59; acc: 0.72
Val Epoch over. val_loss: 0.5561380286220532; val_accuracy: 0.8539012738853503 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.55; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.56; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.78; acc: 0.83
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.74; acc: 0.83
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.34; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.81
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.99; acc: 0.83
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 1.58; acc: 0.73
Val Epoch over. val_loss: 0.5562684191450192; val_accuracy: 0.8541003184713376 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.92; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.91
Batch: 220; loss: 0.66; acc: 0.84
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.68; acc: 0.92
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.86
Batch: 520; loss: 0.72; acc: 0.86
Batch: 540; loss: 0.76; acc: 0.81
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.84
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.99; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 1.59; acc: 0.73
Val Epoch over. val_loss: 0.5548906317277319; val_accuracy: 0.8549960191082803 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.61; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.84
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.88; acc: 0.8
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.49; acc: 0.81
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.76; acc: 0.8
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.94
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.49; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 1.0; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 1.58; acc: 0.72
Val Epoch over. val_loss: 0.5556519980168646; val_accuracy: 0.8541998407643312 

plots/subspace_training/lenet/2020-01-10 13:15:25/d_dim_300_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 84460
elements in E: 17770400
fraction nonzero: 0.004752847431684149
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 1.89; acc: 0.31
Batch: 40; loss: 1.52; acc: 0.59
Batch: 60; loss: 1.3; acc: 0.55
Batch: 80; loss: 1.4; acc: 0.59
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.73
Batch: 140; loss: 1.18; acc: 0.67
Batch: 160; loss: 0.94; acc: 0.73
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.56; acc: 0.8
Batch: 220; loss: 0.83; acc: 0.8
Batch: 240; loss: 0.89; acc: 0.73
Batch: 260; loss: 0.99; acc: 0.77
Batch: 280; loss: 0.66; acc: 0.84
Batch: 300; loss: 0.87; acc: 0.75
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 1.16; acc: 0.73
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.67; acc: 0.86
Batch: 440; loss: 0.78; acc: 0.83
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 0.65; acc: 0.78
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.25; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.95; train_accuracy: 0.75 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 1.11; acc: 0.67
Batch: 40; loss: 0.55; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.82; acc: 0.77
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 1.63; acc: 0.66
Val Epoch over. val_loss: 0.6466444409956598; val_accuracy: 0.8261345541401274 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.69; acc: 0.83
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.88; acc: 0.83
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.71; acc: 0.8
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.84
Batch: 360; loss: 0.73; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.92
Batch: 400; loss: 0.91; acc: 0.81
Batch: 420; loss: 0.65; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.97; acc: 0.83
Batch: 520; loss: 0.43; acc: 0.83
Batch: 540; loss: 0.23; acc: 0.89
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.7; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.56; train_accuracy: 0.85 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.84
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.89
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 1.02; acc: 0.75
Val Epoch over. val_loss: 0.5388113199051019; val_accuracy: 0.8598726114649682 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.76; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.72; acc: 0.77
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.83; acc: 0.8
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.91; acc: 0.78
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.73; acc: 0.88
Batch: 480; loss: 0.83; acc: 0.8
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.69; acc: 0.81
Batch: 40; loss: 0.49; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.72; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.94; acc: 0.73
Val Epoch over. val_loss: 0.5315927398052944; val_accuracy: 0.8651472929936306 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.88; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.7; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.51; acc: 0.8
Batch: 260; loss: 0.28; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.23; acc: 0.89
Batch: 320; loss: 0.76; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.83
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.72; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.65; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.46; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.87 

Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.74; acc: 0.84
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.89; acc: 0.81
Val Epoch over. val_loss: 0.5172762144712886; val_accuracy: 0.8642515923566879 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.64; acc: 0.81
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.81; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.59; acc: 0.83
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.72; acc: 0.8
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.92
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.66; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.87 

Batch: 0; loss: 0.62; acc: 0.81
Batch: 20; loss: 1.01; acc: 0.75
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.96; acc: 0.88
Batch: 100; loss: 0.93; acc: 0.8
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 1.81; acc: 0.72
Val Epoch over. val_loss: 0.8401727416333119; val_accuracy: 0.7974721337579618 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.61; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.94; acc: 0.78
Batch: 180; loss: 0.52; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.71; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.99; acc: 0.81
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.8
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.7; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.93; acc: 0.78
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.74; acc: 0.8
Val Epoch over. val_loss: 0.5243812160696953; val_accuracy: 0.8558917197452229 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.51; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.92
Batch: 160; loss: 1.06; acc: 0.78
Batch: 180; loss: 0.61; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.62; acc: 0.84
Batch: 240; loss: 0.71; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.67; acc: 0.81
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.64; acc: 0.83
Batch: 480; loss: 0.52; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.75; acc: 0.75
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.74; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.67; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.66; acc: 0.81
Val Epoch over. val_loss: 0.41249003925710725; val_accuracy: 0.8915207006369427 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.15; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.6; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.91
Batch: 340; loss: 0.62; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.73; acc: 0.84
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.83
Train Epoch over. train_loss: 0.43; train_accuracy: 0.89 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.78
Batch: 40; loss: 0.27; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 0.54; acc: 0.84
Val Epoch over. val_loss: 0.4399316238749559; val_accuracy: 0.8827627388535032 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.48; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.81
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.63; acc: 0.83
Batch: 580; loss: 0.63; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.89 

Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.7; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.84
Val Epoch over. val_loss: 0.4247158667796357; val_accuracy: 0.8864450636942676 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.64; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.77; acc: 0.81
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.5; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.43717847524839604; val_accuracy: 0.8843550955414012 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.79; acc: 0.84
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.83
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.59; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.52; acc: 0.89
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.61; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.3468000539548837; val_accuracy: 0.9090366242038217 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.47; acc: 0.86
Val Epoch over. val_loss: 0.35028223565239813; val_accuracy: 0.9059514331210191 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.84
Val Epoch over. val_loss: 0.34086895492046504; val_accuracy: 0.9083399681528662 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.48; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.63; acc: 0.86
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.56; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.54; acc: 0.84
Val Epoch over. val_loss: 0.3390337415276819; val_accuracy: 0.9097332802547771 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.79; acc: 0.86
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.58; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.11; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.71; acc: 0.88
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.49; acc: 0.84
Val Epoch over. val_loss: 0.33351316023024785; val_accuracy: 0.9093351910828026 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.91
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.12; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.47; acc: 0.84
Val Epoch over. val_loss: 0.3422436591498791; val_accuracy: 0.9080414012738853 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.94
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.88
Batch: 360; loss: 0.7; acc: 0.88
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.49; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.51; acc: 0.84
Val Epoch over. val_loss: 0.33268640925929804; val_accuracy: 0.9118232484076433 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.33777168976842975; val_accuracy: 0.9086385350318471 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.5; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.88
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.2; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.52; acc: 0.94
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.84
Val Epoch over. val_loss: 0.335359524247373; val_accuracy: 0.9105294585987261 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.65; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.96; acc: 0.77
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.21; acc: 0.89
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.55; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.89
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.84
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.84
Val Epoch over. val_loss: 0.3334104706574777; val_accuracy: 0.9107285031847133 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.83
Val Epoch over. val_loss: 0.330292468071933; val_accuracy: 0.9110270700636943 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.52; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.84
Val Epoch over. val_loss: 0.32987171468461396; val_accuracy: 0.9107285031847133 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.94
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.69; acc: 0.86
Batch: 540; loss: 0.49; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.83
Val Epoch over. val_loss: 0.3294917001466083; val_accuracy: 0.9110270700636943 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.55; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.18; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.3294690885124313; val_accuracy: 0.9112261146496815 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.16; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.89
Batch: 440; loss: 0.55; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.33019394541432145; val_accuracy: 0.9113256369426752 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.23; acc: 0.97
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.57; acc: 0.89
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.94
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.83
Val Epoch over. val_loss: 0.3292765885022036; val_accuracy: 0.9113256369426752 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.25; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.45; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.55; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.84
Val Epoch over. val_loss: 0.3298705559532354; val_accuracy: 0.9116242038216561 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.86
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.72; acc: 0.86
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.83
Val Epoch over. val_loss: 0.3297415950163535; val_accuracy: 0.9116242038216561 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.32; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.89
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.329678221042179; val_accuracy: 0.9112261146496815 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.66; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.62; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.53; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.3299099201229727; val_accuracy: 0.9113256369426752 

plots/subspace_training/lenet/2020-01-10 13:15:25/d_dim_400_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 105993
elements in E: 22213000
fraction nonzero: 0.004771665241075046
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 1.85; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.56
Batch: 60; loss: 1.39; acc: 0.47
Batch: 80; loss: 0.94; acc: 0.66
Batch: 100; loss: 0.58; acc: 0.75
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 1.23; acc: 0.66
Batch: 160; loss: 1.07; acc: 0.75
Batch: 180; loss: 0.84; acc: 0.8
Batch: 200; loss: 0.54; acc: 0.8
Batch: 220; loss: 0.85; acc: 0.72
Batch: 240; loss: 0.92; acc: 0.73
Batch: 260; loss: 1.27; acc: 0.67
Batch: 280; loss: 0.76; acc: 0.77
Batch: 300; loss: 0.89; acc: 0.72
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.66; acc: 0.78
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.77; acc: 0.8
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.83
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.91; train_accuracy: 0.74 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.84
Batch: 100; loss: 1.05; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 1.14; acc: 0.73
Val Epoch over. val_loss: 0.5963947691355541; val_accuracy: 0.8299164012738853 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.49; acc: 0.81
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.75
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.79; acc: 0.73
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.7; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.96; acc: 0.81
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.56; acc: 0.8
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.57; acc: 0.86
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.59; acc: 0.75
Batch: 620; loss: 0.72; acc: 0.77
Train Epoch over. train_loss: 0.54; train_accuracy: 0.85 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.6; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.89; acc: 0.77
Val Epoch over. val_loss: 0.5075947550261856; val_accuracy: 0.8563893312101911 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.77
Batch: 200; loss: 0.62; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.81
Batch: 260; loss: 0.52; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.85; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 1.02; acc: 0.77
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.49; acc: 0.81
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.84; acc: 0.81
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.81
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 0.52; acc: 0.92
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 0.6424549297921976; val_accuracy: 0.8260350318471338 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.76; acc: 0.86
Batch: 80; loss: 0.88; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.94
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.8
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.52; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.27; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.83; acc: 0.78
Batch: 40; loss: 0.55; acc: 0.92
Batch: 60; loss: 0.78; acc: 0.81
Batch: 80; loss: 0.79; acc: 0.84
Batch: 100; loss: 0.93; acc: 0.8
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.93; acc: 0.8
Val Epoch over. val_loss: 0.605231413036395; val_accuracy: 0.8354896496815286 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.46; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.45; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.66; acc: 0.83
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.89
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.66; acc: 0.8
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.7; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.77; acc: 0.8
Val Epoch over. val_loss: 0.41938903885092704; val_accuracy: 0.8805732484076433 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.67; acc: 0.83
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.57; acc: 0.92
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.97; acc: 0.78
Val Epoch over. val_loss: 0.40104552486519907; val_accuracy: 0.887937898089172 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.46; acc: 0.81
Batch: 140; loss: 0.51; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.88
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.83; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.36; acc: 0.83
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.83
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.72; acc: 0.81
Val Epoch over. val_loss: 0.3863239397004152; val_accuracy: 0.895203025477707 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.47; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.65; acc: 0.86
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.94
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.63; acc: 0.83
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.85; acc: 0.81
Val Epoch over. val_loss: 0.38769873758410195; val_accuracy: 0.8929140127388535 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.82; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.92; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.73; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.84
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.83; acc: 0.81
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.17; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.21; acc: 0.89
Batch: 140; loss: 0.75; acc: 0.81
Val Epoch over. val_loss: 0.38811546255638646; val_accuracy: 0.8931130573248408 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.56; acc: 0.91
Batch: 60; loss: 0.69; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.91
Batch: 160; loss: 0.42; acc: 0.94
Batch: 180; loss: 0.56; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.11; acc: 0.94
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.11; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.82; acc: 0.78
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.24; acc: 0.97
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.74; acc: 0.81
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.76; acc: 0.81
Val Epoch over. val_loss: 0.3801010231588297; val_accuracy: 0.8928144904458599 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.65; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.67; acc: 0.86
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.76; acc: 0.8
Val Epoch over. val_loss: 0.3166441944469312; val_accuracy: 0.911922770700637 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.83
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.81; acc: 0.81
Val Epoch over. val_loss: 0.3112332151289199; val_accuracy: 0.9144108280254777 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.45; acc: 0.94
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.44; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.77; acc: 0.84
Val Epoch over. val_loss: 0.3097666937169755; val_accuracy: 0.9157046178343949 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.98
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.57; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.3044224074406988; val_accuracy: 0.9161027070063694 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.88
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.62; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.78; acc: 0.83
Val Epoch over. val_loss: 0.3053362904365655; val_accuracy: 0.9165007961783439 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.48; acc: 0.91
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.89
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.30484169788041693; val_accuracy: 0.9160031847133758 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.94
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.98
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.75; acc: 0.81
Val Epoch over. val_loss: 0.3071906790137291; val_accuracy: 0.9165007961783439 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.51; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.98
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.79; acc: 0.81
Val Epoch over. val_loss: 0.3090736461198254; val_accuracy: 0.9158041401273885 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.16; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.62; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.45; acc: 0.88
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.46; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.23; acc: 0.97
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.73; acc: 0.84
Val Epoch over. val_loss: 0.3026700116171958; val_accuracy: 0.9168988853503185 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.91
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.92
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.33; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.91
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.2; acc: 0.89
Batch: 620; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.8; acc: 0.83
Val Epoch over. val_loss: 0.3045329770464806; val_accuracy: 0.9179936305732485 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.48; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.78; acc: 0.81
Val Epoch over. val_loss: 0.3010610945190594; val_accuracy: 0.9181926751592356 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.81
Val Epoch over. val_loss: 0.3002216580092527; val_accuracy: 0.9171974522292994 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.81
Val Epoch over. val_loss: 0.30027047369130855; val_accuracy: 0.9182921974522293 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.81
Val Epoch over. val_loss: 0.3001567385853476; val_accuracy: 0.9186902866242038 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.78; acc: 0.81
Val Epoch over. val_loss: 0.30142534751990796; val_accuracy: 0.9175955414012739 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.30064813811687907; val_accuracy: 0.9176950636942676 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.52; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.43; acc: 0.89
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.21; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.81
Val Epoch over. val_loss: 0.3009086040554533; val_accuracy: 0.9183917197452229 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.89
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.81
Val Epoch over. val_loss: 0.3005358625179643; val_accuracy: 0.9179936305732485 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.81
Val Epoch over. val_loss: 0.3016682104415195; val_accuracy: 0.9181926751592356 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.36; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.84
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.77; acc: 0.81
Val Epoch over. val_loss: 0.3010439152835281; val_accuracy: 0.9177945859872612 

plots/subspace_training/lenet/2020-01-10 13:15:25/d_dim_500_lr_0.1_seed_1_epochs_30_batchsize_64
plots/subspace_training/lenet/2020-01-10 13:15:25/d_dim_XXXXX_lr_0.1_seed_1_epochs_30_batchsize_64
