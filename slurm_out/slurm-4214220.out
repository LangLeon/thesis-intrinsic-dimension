Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 5.0; acc: 0.23
Batch: 40; loss: 5.81; acc: 0.22
Batch: 60; loss: 5.56; acc: 0.14
Batch: 80; loss: 4.97; acc: 0.25
Batch: 100; loss: 3.92; acc: 0.3
Batch: 120; loss: 5.18; acc: 0.16
Batch: 140; loss: 5.54; acc: 0.25
Batch: 160; loss: 5.12; acc: 0.3
Batch: 180; loss: 4.1; acc: 0.31
Batch: 200; loss: 3.61; acc: 0.45
Batch: 220; loss: 5.43; acc: 0.2
Batch: 240; loss: 5.07; acc: 0.33
Batch: 260; loss: 5.4; acc: 0.25
Batch: 280; loss: 4.0; acc: 0.25
Batch: 300; loss: 4.16; acc: 0.38
Batch: 320; loss: 4.24; acc: 0.34
Batch: 340; loss: 3.86; acc: 0.31
Batch: 360; loss: 3.82; acc: 0.36
Batch: 380; loss: 4.78; acc: 0.38
Batch: 400; loss: 4.0; acc: 0.33
Batch: 420; loss: 4.43; acc: 0.25
Batch: 440; loss: 3.87; acc: 0.38
Batch: 460; loss: 4.89; acc: 0.38
Batch: 480; loss: 3.42; acc: 0.34
Batch: 500; loss: 5.13; acc: 0.31
Batch: 520; loss: 4.98; acc: 0.27
Batch: 540; loss: 3.72; acc: 0.22
Batch: 560; loss: 3.01; acc: 0.42
Batch: 580; loss: 3.98; acc: 0.36
Batch: 600; loss: 4.21; acc: 0.33
Batch: 620; loss: 4.75; acc: 0.31
Train Epoch over. train_loss: 4.7; train_accuracy: 0.3 

Batch: 0; loss: 4.09; acc: 0.28
Batch: 20; loss: 5.55; acc: 0.22
Batch: 40; loss: 4.69; acc: 0.22
Batch: 60; loss: 4.51; acc: 0.23
Batch: 80; loss: 4.12; acc: 0.34
Batch: 100; loss: 3.51; acc: 0.36
Batch: 120; loss: 3.39; acc: 0.44
Batch: 140; loss: 5.37; acc: 0.19
Val Epoch over. val_loss: 4.310841151863147; val_accuracy: 0.306031050955414 

Epoch 2 start
Batch: 0; loss: 3.62; acc: 0.39
Batch: 20; loss: 4.06; acc: 0.28
Batch: 40; loss: 4.21; acc: 0.25
Batch: 60; loss: 3.4; acc: 0.36
Batch: 80; loss: 4.55; acc: 0.33
Batch: 100; loss: 4.36; acc: 0.36
Batch: 120; loss: 4.88; acc: 0.36
Batch: 140; loss: 3.58; acc: 0.38
Batch: 160; loss: 3.99; acc: 0.3
Batch: 180; loss: 3.75; acc: 0.28
Batch: 200; loss: 4.78; acc: 0.34
Batch: 220; loss: 3.33; acc: 0.39
Batch: 240; loss: 3.88; acc: 0.52
Batch: 260; loss: 4.42; acc: 0.34
Batch: 280; loss: 4.11; acc: 0.33
Batch: 300; loss: 3.19; acc: 0.39
Batch: 320; loss: 3.34; acc: 0.44
Batch: 340; loss: 4.76; acc: 0.39
Batch: 360; loss: 4.16; acc: 0.36
Batch: 380; loss: 3.81; acc: 0.41
Batch: 400; loss: 3.34; acc: 0.39
Batch: 420; loss: 3.87; acc: 0.44
Batch: 440; loss: 3.99; acc: 0.41
Batch: 460; loss: 3.51; acc: 0.5
Batch: 480; loss: 5.62; acc: 0.31
Batch: 500; loss: 2.42; acc: 0.48
Batch: 520; loss: 3.74; acc: 0.34
Batch: 540; loss: 3.8; acc: 0.33
Batch: 560; loss: 4.05; acc: 0.39
Batch: 580; loss: 4.56; acc: 0.3
Batch: 600; loss: 4.47; acc: 0.39
Batch: 620; loss: 3.61; acc: 0.44
Train Epoch over. train_loss: 4.21; train_accuracy: 0.37 

Batch: 0; loss: 4.53; acc: 0.33
Batch: 20; loss: 5.54; acc: 0.36
Batch: 40; loss: 4.74; acc: 0.27
Batch: 60; loss: 3.4; acc: 0.36
Batch: 80; loss: 4.65; acc: 0.39
Batch: 100; loss: 4.69; acc: 0.33
Batch: 120; loss: 3.97; acc: 0.45
Batch: 140; loss: 6.3; acc: 0.2
Val Epoch over. val_loss: 4.441385741446428; val_accuracy: 0.3600716560509554 

Epoch 3 start
Batch: 0; loss: 3.94; acc: 0.5
Batch: 20; loss: 5.17; acc: 0.31
Batch: 40; loss: 4.71; acc: 0.39
Batch: 60; loss: 4.51; acc: 0.33
Batch: 80; loss: 4.91; acc: 0.39
Batch: 100; loss: 3.68; acc: 0.47
Batch: 120; loss: 4.51; acc: 0.33
Batch: 140; loss: 4.85; acc: 0.28
Batch: 160; loss: 4.1; acc: 0.36
Batch: 180; loss: 4.01; acc: 0.3
Batch: 200; loss: 4.72; acc: 0.31
Batch: 220; loss: 4.7; acc: 0.3
Batch: 240; loss: 4.26; acc: 0.41
Batch: 260; loss: 4.49; acc: 0.36
Batch: 280; loss: 4.2; acc: 0.33
Batch: 300; loss: 4.09; acc: 0.39
Batch: 320; loss: 4.34; acc: 0.38
Batch: 340; loss: 3.95; acc: 0.44
Batch: 360; loss: 3.66; acc: 0.45
Batch: 380; loss: 3.83; acc: 0.42
Batch: 400; loss: 3.76; acc: 0.45
Batch: 420; loss: 5.42; acc: 0.36
Batch: 440; loss: 3.23; acc: 0.41
Batch: 460; loss: 4.13; acc: 0.41
Batch: 480; loss: 6.44; acc: 0.31
Batch: 500; loss: 4.06; acc: 0.41
Batch: 520; loss: 4.19; acc: 0.42
Batch: 540; loss: 3.67; acc: 0.47
Batch: 560; loss: 4.64; acc: 0.31
Batch: 580; loss: 5.66; acc: 0.33
Batch: 600; loss: 4.34; acc: 0.34
Batch: 620; loss: 3.48; acc: 0.41
Train Epoch over. train_loss: 4.17; train_accuracy: 0.39 

Batch: 0; loss: 3.61; acc: 0.36
Batch: 20; loss: 5.07; acc: 0.33
Batch: 40; loss: 3.99; acc: 0.36
Batch: 60; loss: 3.31; acc: 0.45
Batch: 80; loss: 3.88; acc: 0.38
Batch: 100; loss: 3.93; acc: 0.36
Batch: 120; loss: 3.26; acc: 0.56
Batch: 140; loss: 5.39; acc: 0.28
Val Epoch over. val_loss: 4.025386945457216; val_accuracy: 0.39798964968152867 

Epoch 4 start
Batch: 0; loss: 3.75; acc: 0.34
Batch: 20; loss: 4.95; acc: 0.36
Batch: 40; loss: 3.84; acc: 0.38
Batch: 60; loss: 3.8; acc: 0.42
Batch: 80; loss: 3.58; acc: 0.44
Batch: 100; loss: 3.62; acc: 0.34
Batch: 120; loss: 4.63; acc: 0.28
Batch: 140; loss: 4.41; acc: 0.27
Batch: 160; loss: 3.02; acc: 0.41
Batch: 180; loss: 4.35; acc: 0.39
Batch: 200; loss: 4.43; acc: 0.47
Batch: 220; loss: 3.98; acc: 0.41
Batch: 240; loss: 3.55; acc: 0.5
Batch: 260; loss: 4.4; acc: 0.36
Batch: 280; loss: 3.71; acc: 0.38
Batch: 300; loss: 3.58; acc: 0.39
Batch: 320; loss: 5.62; acc: 0.31
Batch: 340; loss: 4.37; acc: 0.33
Batch: 360; loss: 4.1; acc: 0.38
Batch: 380; loss: 3.79; acc: 0.36
Batch: 400; loss: 5.06; acc: 0.3
Batch: 420; loss: 4.72; acc: 0.53
Batch: 440; loss: 5.03; acc: 0.3
Batch: 460; loss: 4.09; acc: 0.45
Batch: 480; loss: 3.18; acc: 0.44
Batch: 500; loss: 4.07; acc: 0.31
Batch: 520; loss: 2.71; acc: 0.47
Batch: 540; loss: 4.62; acc: 0.39
Batch: 560; loss: 3.75; acc: 0.44
Batch: 580; loss: 3.76; acc: 0.42
Batch: 600; loss: 4.74; acc: 0.38
Batch: 620; loss: 4.36; acc: 0.47
Train Epoch over. train_loss: 4.17; train_accuracy: 0.39 

Batch: 0; loss: 3.47; acc: 0.45
Batch: 20; loss: 5.31; acc: 0.3
Batch: 40; loss: 4.09; acc: 0.36
Batch: 60; loss: 3.83; acc: 0.3
Batch: 80; loss: 3.96; acc: 0.33
Batch: 100; loss: 4.08; acc: 0.33
Batch: 120; loss: 3.36; acc: 0.48
Batch: 140; loss: 5.63; acc: 0.22
Val Epoch over. val_loss: 4.1669336261263314; val_accuracy: 0.38554936305732485 

Epoch 5 start
Batch: 0; loss: 3.1; acc: 0.5
Batch: 20; loss: 4.4; acc: 0.41
Batch: 40; loss: 4.3; acc: 0.27
Batch: 60; loss: 4.18; acc: 0.34
Batch: 80; loss: 4.37; acc: 0.36
Batch: 100; loss: 3.94; acc: 0.39
Batch: 120; loss: 4.23; acc: 0.34
Batch: 140; loss: 4.39; acc: 0.42
Batch: 160; loss: 4.11; acc: 0.39
Batch: 180; loss: 3.46; acc: 0.55
Batch: 200; loss: 2.91; acc: 0.5
Batch: 220; loss: 4.68; acc: 0.36
Batch: 240; loss: 4.62; acc: 0.33
Batch: 260; loss: 3.93; acc: 0.41
Batch: 280; loss: 4.32; acc: 0.36
Batch: 300; loss: 4.52; acc: 0.38
Batch: 320; loss: 4.16; acc: 0.34
Batch: 340; loss: 5.19; acc: 0.34
Batch: 360; loss: 3.68; acc: 0.33
Batch: 380; loss: 4.61; acc: 0.45
Batch: 400; loss: 4.34; acc: 0.39
Batch: 420; loss: 4.2; acc: 0.41
Batch: 440; loss: 3.96; acc: 0.42
Batch: 460; loss: 3.6; acc: 0.44
Batch: 480; loss: 3.09; acc: 0.45
Batch: 500; loss: 5.26; acc: 0.34
Batch: 520; loss: 2.98; acc: 0.5
Batch: 540; loss: 4.21; acc: 0.34
Batch: 560; loss: 3.8; acc: 0.34
Batch: 580; loss: 3.47; acc: 0.41
Batch: 600; loss: 3.17; acc: 0.44
Batch: 620; loss: 3.87; acc: 0.45
Train Epoch over. train_loss: 4.19; train_accuracy: 0.39 

Batch: 0; loss: 3.77; acc: 0.41
Batch: 20; loss: 4.91; acc: 0.38
Batch: 40; loss: 3.93; acc: 0.41
Batch: 60; loss: 2.99; acc: 0.47
Batch: 80; loss: 4.35; acc: 0.41
Batch: 100; loss: 3.99; acc: 0.38
Batch: 120; loss: 3.42; acc: 0.52
Batch: 140; loss: 5.58; acc: 0.22
Val Epoch over. val_loss: 4.11851158415436; val_accuracy: 0.38057324840764334 

Epoch 6 start
Batch: 0; loss: 3.64; acc: 0.48
Batch: 20; loss: 4.78; acc: 0.31
Batch: 40; loss: 4.03; acc: 0.36
Batch: 60; loss: 4.52; acc: 0.45
Batch: 80; loss: 5.46; acc: 0.39
Batch: 100; loss: 2.84; acc: 0.56
Batch: 120; loss: 3.64; acc: 0.42
Batch: 140; loss: 5.58; acc: 0.34
Batch: 160; loss: 4.64; acc: 0.36
Batch: 180; loss: 4.25; acc: 0.39
Batch: 200; loss: 4.42; acc: 0.38
Batch: 220; loss: 3.02; acc: 0.47
Batch: 240; loss: 4.62; acc: 0.34
Batch: 260; loss: 4.23; acc: 0.33
Batch: 280; loss: 4.15; acc: 0.33
Batch: 300; loss: 4.36; acc: 0.38
Batch: 320; loss: 3.71; acc: 0.31
Batch: 340; loss: 4.08; acc: 0.42
Batch: 360; loss: 4.14; acc: 0.41
Batch: 380; loss: 4.53; acc: 0.38
Batch: 400; loss: 4.17; acc: 0.41
Batch: 420; loss: 3.37; acc: 0.47
Batch: 440; loss: 4.28; acc: 0.41
Batch: 460; loss: 4.22; acc: 0.36
Batch: 480; loss: 4.32; acc: 0.33
Batch: 500; loss: 3.25; acc: 0.52
Batch: 520; loss: 4.64; acc: 0.39
Batch: 540; loss: 3.85; acc: 0.41
Batch: 560; loss: 4.77; acc: 0.27
Batch: 580; loss: 4.52; acc: 0.36
Batch: 600; loss: 4.54; acc: 0.39
Batch: 620; loss: 4.2; acc: 0.44
Train Epoch over. train_loss: 4.17; train_accuracy: 0.39 

Batch: 0; loss: 3.9; acc: 0.38
Batch: 20; loss: 5.03; acc: 0.38
Batch: 40; loss: 3.94; acc: 0.3
Batch: 60; loss: 3.37; acc: 0.45
Batch: 80; loss: 4.1; acc: 0.39
Batch: 100; loss: 3.98; acc: 0.27
Batch: 120; loss: 3.3; acc: 0.53
Batch: 140; loss: 5.56; acc: 0.23
Val Epoch over. val_loss: 4.033596444281803; val_accuracy: 0.39729299363057324 

Epoch 7 start
Batch: 0; loss: 3.72; acc: 0.34
Batch: 20; loss: 3.06; acc: 0.45
Batch: 40; loss: 4.99; acc: 0.25
Batch: 60; loss: 3.61; acc: 0.34
Batch: 80; loss: 5.01; acc: 0.36
Batch: 100; loss: 3.91; acc: 0.47
Batch: 120; loss: 5.81; acc: 0.31
Batch: 140; loss: 3.67; acc: 0.42
Batch: 160; loss: 4.09; acc: 0.42
Batch: 180; loss: 4.87; acc: 0.33
Batch: 200; loss: 5.0; acc: 0.25
Batch: 220; loss: 4.78; acc: 0.39
Batch: 240; loss: 4.86; acc: 0.33
Batch: 260; loss: 4.03; acc: 0.41
Batch: 280; loss: 3.61; acc: 0.41
Batch: 300; loss: 4.2; acc: 0.38
Batch: 320; loss: 3.61; acc: 0.36
Batch: 340; loss: 3.67; acc: 0.42
Batch: 360; loss: 3.54; acc: 0.48
Batch: 380; loss: 3.69; acc: 0.33
Batch: 400; loss: 4.58; acc: 0.31
Batch: 420; loss: 4.14; acc: 0.39
Batch: 440; loss: 5.02; acc: 0.34
Batch: 460; loss: 4.05; acc: 0.39
Batch: 480; loss: 4.6; acc: 0.31
Batch: 500; loss: 4.0; acc: 0.39
Batch: 520; loss: 4.44; acc: 0.38
Batch: 540; loss: 4.57; acc: 0.36
Batch: 560; loss: 3.76; acc: 0.38
Batch: 580; loss: 3.57; acc: 0.39
Batch: 600; loss: 4.07; acc: 0.39
Batch: 620; loss: 4.86; acc: 0.41
Train Epoch over. train_loss: 4.15; train_accuracy: 0.39 

Batch: 0; loss: 4.08; acc: 0.34
Batch: 20; loss: 5.41; acc: 0.34
Batch: 40; loss: 4.19; acc: 0.28
Batch: 60; loss: 3.23; acc: 0.42
Batch: 80; loss: 4.2; acc: 0.41
Batch: 100; loss: 4.28; acc: 0.33
Batch: 120; loss: 3.39; acc: 0.48
Batch: 140; loss: 6.1; acc: 0.2
Val Epoch over. val_loss: 4.203385442685169; val_accuracy: 0.3756966560509554 

Epoch 8 start
Batch: 0; loss: 4.55; acc: 0.33
Batch: 20; loss: 3.8; acc: 0.36
Batch: 40; loss: 3.89; acc: 0.39
Batch: 60; loss: 3.48; acc: 0.42
Batch: 80; loss: 4.16; acc: 0.42
Batch: 100; loss: 4.58; acc: 0.34
Batch: 120; loss: 3.51; acc: 0.45
Batch: 140; loss: 4.14; acc: 0.39
Batch: 160; loss: 2.99; acc: 0.45
Batch: 180; loss: 4.33; acc: 0.3
Batch: 200; loss: 5.23; acc: 0.31
Batch: 220; loss: 4.29; acc: 0.38
Batch: 240; loss: 4.0; acc: 0.48
Batch: 260; loss: 3.93; acc: 0.36
Batch: 280; loss: 4.8; acc: 0.36
Batch: 300; loss: 2.99; acc: 0.45
Batch: 320; loss: 2.45; acc: 0.45
Batch: 340; loss: 3.96; acc: 0.42
Batch: 360; loss: 3.83; acc: 0.41
Batch: 380; loss: 2.78; acc: 0.55
Batch: 400; loss: 4.1; acc: 0.34
Batch: 420; loss: 3.15; acc: 0.45
Batch: 440; loss: 4.82; acc: 0.38
Batch: 460; loss: 4.07; acc: 0.34
Batch: 480; loss: 3.29; acc: 0.42
Batch: 500; loss: 3.48; acc: 0.38
Batch: 520; loss: 3.53; acc: 0.39
Batch: 540; loss: 6.21; acc: 0.3
Batch: 560; loss: 3.51; acc: 0.39
Batch: 580; loss: 4.92; acc: 0.39
Batch: 600; loss: 3.68; acc: 0.44
Batch: 620; loss: 4.95; acc: 0.33
Train Epoch over. train_loss: 4.16; train_accuracy: 0.39 

Batch: 0; loss: 4.49; acc: 0.34
Batch: 20; loss: 5.68; acc: 0.33
Batch: 40; loss: 4.5; acc: 0.31
Batch: 60; loss: 3.29; acc: 0.45
Batch: 80; loss: 4.65; acc: 0.38
Batch: 100; loss: 4.48; acc: 0.34
Batch: 120; loss: 3.69; acc: 0.53
Batch: 140; loss: 6.21; acc: 0.2
Val Epoch over. val_loss: 4.4272665127067805; val_accuracy: 0.3695262738853503 

Epoch 9 start
Batch: 0; loss: 3.25; acc: 0.44
Batch: 20; loss: 4.47; acc: 0.38
Batch: 40; loss: 3.83; acc: 0.47
Batch: 60; loss: 3.95; acc: 0.36
Batch: 80; loss: 3.85; acc: 0.34
Batch: 100; loss: 3.89; acc: 0.42
Batch: 120; loss: 3.88; acc: 0.27
Batch: 140; loss: 5.12; acc: 0.34
Batch: 160; loss: 4.47; acc: 0.34
Batch: 180; loss: 4.93; acc: 0.36
Batch: 200; loss: 4.0; acc: 0.45
Batch: 220; loss: 4.02; acc: 0.34
Batch: 240; loss: 4.2; acc: 0.34
Batch: 260; loss: 3.8; acc: 0.45
Batch: 280; loss: 4.2; acc: 0.39
Batch: 300; loss: 4.48; acc: 0.34
Batch: 320; loss: 4.11; acc: 0.36
Batch: 340; loss: 3.52; acc: 0.44
Batch: 360; loss: 4.16; acc: 0.39
Batch: 380; loss: 3.95; acc: 0.42
Batch: 400; loss: 5.14; acc: 0.38
Batch: 420; loss: 3.74; acc: 0.39
Batch: 440; loss: 5.12; acc: 0.33
Batch: 460; loss: 6.18; acc: 0.27
Batch: 480; loss: 4.03; acc: 0.44
Batch: 500; loss: 3.42; acc: 0.36
Batch: 520; loss: 4.82; acc: 0.3
Batch: 540; loss: 4.08; acc: 0.36
Batch: 560; loss: 3.69; acc: 0.47
Batch: 580; loss: 5.02; acc: 0.3
Batch: 600; loss: 4.69; acc: 0.36
Batch: 620; loss: 4.9; acc: 0.38
Train Epoch over. train_loss: 4.19; train_accuracy: 0.39 

Batch: 0; loss: 3.44; acc: 0.36
Batch: 20; loss: 4.46; acc: 0.42
Batch: 40; loss: 4.44; acc: 0.33
Batch: 60; loss: 3.39; acc: 0.45
Batch: 80; loss: 4.23; acc: 0.38
Batch: 100; loss: 4.15; acc: 0.36
Batch: 120; loss: 3.63; acc: 0.55
Batch: 140; loss: 5.51; acc: 0.3
Val Epoch over. val_loss: 4.106478575688259; val_accuracy: 0.39470541401273884 

Epoch 10 start
Batch: 0; loss: 4.42; acc: 0.39
Batch: 20; loss: 3.91; acc: 0.41
Batch: 40; loss: 3.69; acc: 0.42
Batch: 60; loss: 4.28; acc: 0.42
Batch: 80; loss: 3.0; acc: 0.41
Batch: 100; loss: 4.54; acc: 0.38
Batch: 120; loss: 3.49; acc: 0.47
Batch: 140; loss: 4.77; acc: 0.31
Batch: 160; loss: 3.78; acc: 0.45
Batch: 180; loss: 4.54; acc: 0.36
Batch: 200; loss: 4.99; acc: 0.3
Batch: 220; loss: 3.65; acc: 0.47
Batch: 240; loss: 3.54; acc: 0.39
Batch: 260; loss: 4.06; acc: 0.39
Batch: 280; loss: 4.46; acc: 0.39
Batch: 300; loss: 3.69; acc: 0.28
Batch: 320; loss: 3.03; acc: 0.53
Batch: 340; loss: 2.9; acc: 0.47
Batch: 360; loss: 3.2; acc: 0.47
Batch: 380; loss: 3.7; acc: 0.47
Batch: 400; loss: 5.1; acc: 0.31
Batch: 420; loss: 4.34; acc: 0.39
Batch: 440; loss: 4.32; acc: 0.3
Batch: 460; loss: 4.89; acc: 0.33
Batch: 480; loss: 3.09; acc: 0.48
Batch: 500; loss: 4.05; acc: 0.38
Batch: 520; loss: 4.33; acc: 0.36
Batch: 540; loss: 4.55; acc: 0.39
Batch: 560; loss: 4.22; acc: 0.39
Batch: 580; loss: 4.1; acc: 0.33
Batch: 600; loss: 4.19; acc: 0.34
Batch: 620; loss: 3.83; acc: 0.45
Train Epoch over. train_loss: 4.17; train_accuracy: 0.38 

Batch: 0; loss: 4.17; acc: 0.33
Batch: 20; loss: 4.81; acc: 0.33
Batch: 40; loss: 4.38; acc: 0.36
Batch: 60; loss: 4.47; acc: 0.31
Batch: 80; loss: 4.36; acc: 0.34
Batch: 100; loss: 4.5; acc: 0.38
Batch: 120; loss: 3.9; acc: 0.47
Batch: 140; loss: 6.21; acc: 0.27
Val Epoch over. val_loss: 4.611976974329371; val_accuracy: 0.3769904458598726 

Epoch 11 start
Batch: 0; loss: 4.91; acc: 0.3
Batch: 20; loss: 3.94; acc: 0.38
Batch: 40; loss: 3.81; acc: 0.38
Batch: 60; loss: 3.71; acc: 0.39
Batch: 80; loss: 4.35; acc: 0.42
Batch: 100; loss: 3.48; acc: 0.3
Batch: 120; loss: 4.39; acc: 0.36
Batch: 140; loss: 2.93; acc: 0.47
Batch: 160; loss: 4.08; acc: 0.45
Batch: 180; loss: 5.28; acc: 0.36
Batch: 200; loss: 3.52; acc: 0.44
Batch: 220; loss: 3.81; acc: 0.33
Batch: 240; loss: 3.58; acc: 0.45
Batch: 260; loss: 3.46; acc: 0.45
Batch: 280; loss: 4.66; acc: 0.27
Batch: 300; loss: 4.94; acc: 0.33
Batch: 320; loss: 3.41; acc: 0.47
Batch: 340; loss: 3.58; acc: 0.44
Batch: 360; loss: 3.56; acc: 0.53
Batch: 380; loss: 3.69; acc: 0.44
Batch: 400; loss: 3.66; acc: 0.38
Batch: 420; loss: 4.22; acc: 0.41
Batch: 440; loss: 3.61; acc: 0.38
Batch: 460; loss: 3.31; acc: 0.44
Batch: 480; loss: 3.21; acc: 0.39
Batch: 500; loss: 3.39; acc: 0.36
Batch: 520; loss: 3.57; acc: 0.44
Batch: 540; loss: 3.38; acc: 0.47
Batch: 560; loss: 4.07; acc: 0.36
Batch: 580; loss: 2.94; acc: 0.45
Batch: 600; loss: 3.48; acc: 0.42
Batch: 620; loss: 2.95; acc: 0.48
Train Epoch over. train_loss: 3.83; train_accuracy: 0.4 

Batch: 0; loss: 3.43; acc: 0.41
Batch: 20; loss: 4.67; acc: 0.41
Batch: 40; loss: 3.85; acc: 0.34
Batch: 60; loss: 3.04; acc: 0.52
Batch: 80; loss: 3.74; acc: 0.42
Batch: 100; loss: 3.84; acc: 0.33
Batch: 120; loss: 3.33; acc: 0.55
Batch: 140; loss: 5.41; acc: 0.23
Val Epoch over. val_loss: 3.873946041058583; val_accuracy: 0.4096337579617834 

Epoch 12 start
Batch: 0; loss: 3.35; acc: 0.39
Batch: 20; loss: 4.77; acc: 0.27
Batch: 40; loss: 3.16; acc: 0.44
Batch: 60; loss: 3.9; acc: 0.39
Batch: 80; loss: 3.54; acc: 0.48
Batch: 100; loss: 3.48; acc: 0.42
Batch: 120; loss: 4.17; acc: 0.42
Batch: 140; loss: 4.09; acc: 0.42
Batch: 160; loss: 4.05; acc: 0.47
Batch: 180; loss: 3.93; acc: 0.36
Batch: 200; loss: 3.8; acc: 0.47
Batch: 220; loss: 5.01; acc: 0.36
Batch: 240; loss: 3.53; acc: 0.45
Batch: 260; loss: 2.7; acc: 0.41
Batch: 280; loss: 3.62; acc: 0.39
Batch: 300; loss: 3.81; acc: 0.39
Batch: 320; loss: 3.7; acc: 0.44
Batch: 340; loss: 4.23; acc: 0.39
Batch: 360; loss: 3.01; acc: 0.53
Batch: 380; loss: 3.84; acc: 0.3
Batch: 400; loss: 3.86; acc: 0.39
Batch: 420; loss: 4.03; acc: 0.31
Batch: 440; loss: 3.36; acc: 0.48
Batch: 460; loss: 3.38; acc: 0.39
Batch: 480; loss: 3.65; acc: 0.45
Batch: 500; loss: 3.56; acc: 0.52
Batch: 520; loss: 3.56; acc: 0.45
Batch: 540; loss: 3.28; acc: 0.38
Batch: 560; loss: 3.65; acc: 0.47
Batch: 580; loss: 3.88; acc: 0.39
Batch: 600; loss: 3.04; acc: 0.53
Batch: 620; loss: 3.45; acc: 0.38
Train Epoch over. train_loss: 3.81; train_accuracy: 0.41 

Batch: 0; loss: 3.43; acc: 0.38
Batch: 20; loss: 4.59; acc: 0.41
Batch: 40; loss: 3.92; acc: 0.31
Batch: 60; loss: 3.08; acc: 0.52
Batch: 80; loss: 3.7; acc: 0.41
Batch: 100; loss: 3.8; acc: 0.34
Batch: 120; loss: 3.38; acc: 0.59
Batch: 140; loss: 5.44; acc: 0.3
Val Epoch over. val_loss: 3.8754207374183993; val_accuracy: 0.4125199044585987 

Epoch 13 start
Batch: 0; loss: 3.9; acc: 0.41
Batch: 20; loss: 2.81; acc: 0.44
Batch: 40; loss: 3.91; acc: 0.39
Batch: 60; loss: 3.8; acc: 0.47
Batch: 80; loss: 3.7; acc: 0.38
Batch: 100; loss: 2.99; acc: 0.53
Batch: 120; loss: 4.21; acc: 0.31
Batch: 140; loss: 3.9; acc: 0.45
Batch: 160; loss: 3.69; acc: 0.39
Batch: 180; loss: 2.95; acc: 0.45
Batch: 200; loss: 3.68; acc: 0.45
Batch: 220; loss: 4.15; acc: 0.42
Batch: 240; loss: 4.11; acc: 0.3
Batch: 260; loss: 2.46; acc: 0.45
Batch: 280; loss: 2.84; acc: 0.44
Batch: 300; loss: 4.35; acc: 0.41
Batch: 320; loss: 4.11; acc: 0.44
Batch: 340; loss: 3.71; acc: 0.34
Batch: 360; loss: 4.57; acc: 0.28
Batch: 380; loss: 4.33; acc: 0.47
Batch: 400; loss: 3.1; acc: 0.42
Batch: 420; loss: 3.25; acc: 0.42
Batch: 440; loss: 3.59; acc: 0.45
Batch: 460; loss: 3.1; acc: 0.5
Batch: 480; loss: 4.4; acc: 0.34
Batch: 500; loss: 3.56; acc: 0.41
Batch: 520; loss: 3.96; acc: 0.44
Batch: 540; loss: 3.9; acc: 0.38
Batch: 560; loss: 5.08; acc: 0.39
Batch: 580; loss: 3.5; acc: 0.47
Batch: 600; loss: 4.0; acc: 0.36
Batch: 620; loss: 4.27; acc: 0.38
Train Epoch over. train_loss: 3.81; train_accuracy: 0.41 

Batch: 0; loss: 3.34; acc: 0.39
Batch: 20; loss: 4.64; acc: 0.38
Batch: 40; loss: 3.88; acc: 0.31
Batch: 60; loss: 3.13; acc: 0.5
Batch: 80; loss: 3.69; acc: 0.39
Batch: 100; loss: 3.88; acc: 0.33
Batch: 120; loss: 3.35; acc: 0.55
Batch: 140; loss: 5.47; acc: 0.27
Val Epoch over. val_loss: 3.863087675374025; val_accuracy: 0.40644904458598724 

Epoch 14 start
Batch: 0; loss: 3.56; acc: 0.34
Batch: 20; loss: 4.14; acc: 0.45
Batch: 40; loss: 3.51; acc: 0.44
Batch: 60; loss: 4.39; acc: 0.31
Batch: 80; loss: 4.04; acc: 0.52
Batch: 100; loss: 4.59; acc: 0.36
Batch: 120; loss: 3.67; acc: 0.42
Batch: 140; loss: 2.51; acc: 0.53
Batch: 160; loss: 3.56; acc: 0.42
Batch: 180; loss: 3.04; acc: 0.48
Batch: 200; loss: 4.32; acc: 0.44
Batch: 220; loss: 3.99; acc: 0.33
Batch: 240; loss: 4.29; acc: 0.34
Batch: 260; loss: 3.23; acc: 0.44
Batch: 280; loss: 3.57; acc: 0.41
Batch: 300; loss: 3.85; acc: 0.44
Batch: 320; loss: 3.89; acc: 0.42
Batch: 340; loss: 3.05; acc: 0.36
Batch: 360; loss: 3.46; acc: 0.48
Batch: 380; loss: 4.64; acc: 0.39
Batch: 400; loss: 3.15; acc: 0.47
Batch: 420; loss: 3.37; acc: 0.39
Batch: 440; loss: 4.06; acc: 0.39
Batch: 460; loss: 3.93; acc: 0.39
Batch: 480; loss: 4.0; acc: 0.45
Batch: 500; loss: 4.35; acc: 0.3
Batch: 520; loss: 3.71; acc: 0.38
Batch: 540; loss: 4.05; acc: 0.5
Batch: 560; loss: 3.47; acc: 0.45
Batch: 580; loss: 3.58; acc: 0.42
Batch: 600; loss: 3.43; acc: 0.52
Batch: 620; loss: 4.29; acc: 0.36
Train Epoch over. train_loss: 3.81; train_accuracy: 0.41 

Batch: 0; loss: 3.33; acc: 0.41
Batch: 20; loss: 4.77; acc: 0.41
Batch: 40; loss: 3.79; acc: 0.34
Batch: 60; loss: 3.08; acc: 0.5
Batch: 80; loss: 3.78; acc: 0.39
Batch: 100; loss: 3.76; acc: 0.34
Batch: 120; loss: 3.3; acc: 0.56
Batch: 140; loss: 5.44; acc: 0.27
Val Epoch over. val_loss: 3.865740732022911; val_accuracy: 0.41261942675159236 

Epoch 15 start
Batch: 0; loss: 4.08; acc: 0.39
Batch: 20; loss: 4.31; acc: 0.34
Batch: 40; loss: 2.78; acc: 0.56
Batch: 60; loss: 3.87; acc: 0.42
Batch: 80; loss: 4.28; acc: 0.38
Batch: 100; loss: 3.99; acc: 0.39
Batch: 120; loss: 4.2; acc: 0.36
Batch: 140; loss: 3.56; acc: 0.44
Batch: 160; loss: 4.47; acc: 0.38
Batch: 180; loss: 3.42; acc: 0.42
Batch: 200; loss: 3.82; acc: 0.42
Batch: 220; loss: 4.64; acc: 0.33
Batch: 240; loss: 4.05; acc: 0.47
Batch: 260; loss: 4.67; acc: 0.25
Batch: 280; loss: 2.99; acc: 0.39
Batch: 300; loss: 3.97; acc: 0.44
Batch: 320; loss: 3.28; acc: 0.53
Batch: 340; loss: 3.86; acc: 0.36
Batch: 360; loss: 2.89; acc: 0.52
Batch: 380; loss: 3.55; acc: 0.39
Batch: 400; loss: 3.31; acc: 0.44
Batch: 420; loss: 3.99; acc: 0.34
Batch: 440; loss: 4.51; acc: 0.31
Batch: 460; loss: 2.9; acc: 0.47
Batch: 480; loss: 3.82; acc: 0.41
Batch: 500; loss: 5.4; acc: 0.28
Batch: 520; loss: 3.53; acc: 0.45
Batch: 540; loss: 3.51; acc: 0.34
Batch: 560; loss: 3.47; acc: 0.38
Batch: 580; loss: 4.12; acc: 0.34
Batch: 600; loss: 4.67; acc: 0.44
Batch: 620; loss: 3.9; acc: 0.36
Train Epoch over. train_loss: 3.81; train_accuracy: 0.41 

Batch: 0; loss: 3.38; acc: 0.39
Batch: 20; loss: 4.89; acc: 0.34
Batch: 40; loss: 3.86; acc: 0.33
Batch: 60; loss: 3.03; acc: 0.52
Batch: 80; loss: 3.77; acc: 0.41
Batch: 100; loss: 3.94; acc: 0.34
Batch: 120; loss: 3.4; acc: 0.55
Batch: 140; loss: 5.69; acc: 0.27
Val Epoch over. val_loss: 3.8899423893849563; val_accuracy: 0.40863853503184716 

Epoch 16 start
Batch: 0; loss: 5.04; acc: 0.34
Batch: 20; loss: 3.17; acc: 0.53
Batch: 40; loss: 4.05; acc: 0.44
Batch: 60; loss: 3.15; acc: 0.31
Batch: 80; loss: 4.52; acc: 0.33
Batch: 100; loss: 4.35; acc: 0.41
Batch: 120; loss: 3.89; acc: 0.45
Batch: 140; loss: 3.41; acc: 0.42
Batch: 160; loss: 3.58; acc: 0.41
Batch: 180; loss: 4.49; acc: 0.33
Batch: 200; loss: 4.56; acc: 0.3
Batch: 220; loss: 3.19; acc: 0.47
Batch: 240; loss: 3.11; acc: 0.5
Batch: 260; loss: 3.34; acc: 0.45
Batch: 280; loss: 4.07; acc: 0.42
Batch: 300; loss: 3.11; acc: 0.5
Batch: 320; loss: 4.55; acc: 0.38
Batch: 340; loss: 3.54; acc: 0.45
Batch: 360; loss: 3.99; acc: 0.48
Batch: 380; loss: 3.26; acc: 0.41
Batch: 400; loss: 4.06; acc: 0.39
Batch: 420; loss: 3.67; acc: 0.39
Batch: 440; loss: 3.42; acc: 0.48
Batch: 460; loss: 3.49; acc: 0.42
Batch: 480; loss: 3.51; acc: 0.45
Batch: 500; loss: 3.07; acc: 0.39
Batch: 520; loss: 3.69; acc: 0.36
Batch: 540; loss: 3.17; acc: 0.47
Batch: 560; loss: 5.24; acc: 0.3
Batch: 580; loss: 4.09; acc: 0.39
Batch: 600; loss: 3.23; acc: 0.5
Batch: 620; loss: 2.84; acc: 0.52
Train Epoch over. train_loss: 3.81; train_accuracy: 0.41 

Batch: 0; loss: 3.26; acc: 0.39
Batch: 20; loss: 4.67; acc: 0.41
Batch: 40; loss: 3.87; acc: 0.33
Batch: 60; loss: 3.11; acc: 0.52
Batch: 80; loss: 3.67; acc: 0.42
Batch: 100; loss: 3.85; acc: 0.33
Batch: 120; loss: 3.33; acc: 0.55
Batch: 140; loss: 5.44; acc: 0.27
Val Epoch over. val_loss: 3.8618612790563303; val_accuracy: 0.41102707006369427 

Epoch 17 start
Batch: 0; loss: 3.82; acc: 0.48
Batch: 20; loss: 4.31; acc: 0.45
Batch: 40; loss: 3.46; acc: 0.41
Batch: 60; loss: 4.42; acc: 0.3
Batch: 80; loss: 3.91; acc: 0.42
Batch: 100; loss: 3.82; acc: 0.38
Batch: 120; loss: 3.74; acc: 0.41
Batch: 140; loss: 3.53; acc: 0.42
Batch: 160; loss: 3.51; acc: 0.31
Batch: 180; loss: 3.66; acc: 0.41
Batch: 200; loss: 2.92; acc: 0.48
Batch: 220; loss: 3.66; acc: 0.45
Batch: 240; loss: 3.44; acc: 0.52
Batch: 260; loss: 4.36; acc: 0.33
Batch: 280; loss: 4.4; acc: 0.36
Batch: 300; loss: 4.58; acc: 0.44
Batch: 320; loss: 4.12; acc: 0.38
Batch: 340; loss: 3.94; acc: 0.44
Batch: 360; loss: 3.28; acc: 0.44
Batch: 380; loss: 3.38; acc: 0.44
Batch: 400; loss: 3.72; acc: 0.47
Batch: 420; loss: 3.84; acc: 0.36
Batch: 440; loss: 3.16; acc: 0.42
Batch: 460; loss: 4.06; acc: 0.42
Batch: 480; loss: 4.28; acc: 0.28
Batch: 500; loss: 2.89; acc: 0.52
Batch: 520; loss: 4.86; acc: 0.3
Batch: 540; loss: 3.24; acc: 0.53
Batch: 560; loss: 3.04; acc: 0.45
Batch: 580; loss: 4.06; acc: 0.39
Batch: 600; loss: 3.87; acc: 0.3
Batch: 620; loss: 3.81; acc: 0.41
Train Epoch over. train_loss: 3.81; train_accuracy: 0.41 

Batch: 0; loss: 3.22; acc: 0.39
Batch: 20; loss: 4.71; acc: 0.38
Batch: 40; loss: 3.92; acc: 0.33
Batch: 60; loss: 3.05; acc: 0.52
Batch: 80; loss: 3.63; acc: 0.41
Batch: 100; loss: 3.91; acc: 0.34
Batch: 120; loss: 3.36; acc: 0.53
Batch: 140; loss: 5.39; acc: 0.25
Val Epoch over. val_loss: 3.872336955586816; val_accuracy: 0.4101313694267516 

Epoch 18 start
Batch: 0; loss: 4.18; acc: 0.36
Batch: 20; loss: 3.7; acc: 0.41
Batch: 40; loss: 4.02; acc: 0.41
Batch: 60; loss: 4.04; acc: 0.3
Batch: 80; loss: 3.67; acc: 0.47
Batch: 100; loss: 3.05; acc: 0.48
Batch: 120; loss: 3.4; acc: 0.44
Batch: 140; loss: 4.38; acc: 0.38
Batch: 160; loss: 3.96; acc: 0.36
Batch: 180; loss: 4.66; acc: 0.28
Batch: 200; loss: 3.75; acc: 0.45
Batch: 220; loss: 5.11; acc: 0.38
Batch: 240; loss: 4.38; acc: 0.36
Batch: 260; loss: 4.2; acc: 0.38
Batch: 280; loss: 3.84; acc: 0.45
Batch: 300; loss: 3.91; acc: 0.42
Batch: 320; loss: 3.78; acc: 0.42
Batch: 340; loss: 4.21; acc: 0.33
Batch: 360; loss: 4.74; acc: 0.36
Batch: 380; loss: 3.35; acc: 0.5
Batch: 400; loss: 3.81; acc: 0.47
Batch: 420; loss: 4.04; acc: 0.39
Batch: 440; loss: 3.15; acc: 0.48
Batch: 460; loss: 4.19; acc: 0.38
Batch: 480; loss: 3.57; acc: 0.44
Batch: 500; loss: 2.77; acc: 0.41
Batch: 520; loss: 4.1; acc: 0.39
Batch: 540; loss: 2.88; acc: 0.5
Batch: 560; loss: 3.64; acc: 0.36
Batch: 580; loss: 3.95; acc: 0.39
Batch: 600; loss: 2.75; acc: 0.42
Batch: 620; loss: 4.35; acc: 0.38
Train Epoch over. train_loss: 3.81; train_accuracy: 0.41 

Batch: 0; loss: 3.29; acc: 0.39
Batch: 20; loss: 4.72; acc: 0.39
Batch: 40; loss: 3.97; acc: 0.34
Batch: 60; loss: 3.16; acc: 0.52
Batch: 80; loss: 3.63; acc: 0.44
Batch: 100; loss: 3.83; acc: 0.31
Batch: 120; loss: 3.33; acc: 0.59
Batch: 140; loss: 5.48; acc: 0.3
Val Epoch over. val_loss: 3.863569103228818; val_accuracy: 0.41182324840764334 

Epoch 19 start
Batch: 0; loss: 3.74; acc: 0.42
Batch: 20; loss: 2.68; acc: 0.52
Batch: 40; loss: 4.45; acc: 0.41
Batch: 60; loss: 4.43; acc: 0.33
Batch: 80; loss: 4.03; acc: 0.38
Batch: 100; loss: 4.53; acc: 0.38
Batch: 120; loss: 4.23; acc: 0.31
Batch: 140; loss: 4.02; acc: 0.44
Batch: 160; loss: 4.28; acc: 0.41
Batch: 180; loss: 3.97; acc: 0.42
Batch: 200; loss: 3.36; acc: 0.41
Batch: 220; loss: 3.3; acc: 0.45
Batch: 240; loss: 4.34; acc: 0.42
Batch: 260; loss: 5.36; acc: 0.31
Batch: 280; loss: 4.45; acc: 0.28
Batch: 300; loss: 2.81; acc: 0.47
Batch: 320; loss: 3.86; acc: 0.39
Batch: 340; loss: 3.41; acc: 0.48
Batch: 360; loss: 3.75; acc: 0.44
Batch: 380; loss: 3.72; acc: 0.38
Batch: 400; loss: 3.4; acc: 0.42
Batch: 420; loss: 4.15; acc: 0.41
Batch: 440; loss: 3.9; acc: 0.47
Batch: 460; loss: 4.0; acc: 0.42
Batch: 480; loss: 3.55; acc: 0.47
Batch: 500; loss: 3.31; acc: 0.42
Batch: 520; loss: 3.62; acc: 0.33
Batch: 540; loss: 4.27; acc: 0.36
Batch: 560; loss: 5.0; acc: 0.31
Batch: 580; loss: 4.75; acc: 0.42
Batch: 600; loss: 4.43; acc: 0.38
Batch: 620; loss: 4.22; acc: 0.33
Train Epoch over. train_loss: 3.81; train_accuracy: 0.41 

Batch: 0; loss: 3.37; acc: 0.41
Batch: 20; loss: 4.72; acc: 0.39
Batch: 40; loss: 3.83; acc: 0.31
Batch: 60; loss: 3.1; acc: 0.48
Batch: 80; loss: 3.68; acc: 0.41
Batch: 100; loss: 3.86; acc: 0.33
Batch: 120; loss: 3.39; acc: 0.53
Batch: 140; loss: 5.46; acc: 0.25
Val Epoch over. val_loss: 3.8711390343441328; val_accuracy: 0.4094347133757962 

Epoch 20 start
Batch: 0; loss: 4.16; acc: 0.36
Batch: 20; loss: 4.31; acc: 0.45
Batch: 40; loss: 4.46; acc: 0.48
Batch: 60; loss: 4.33; acc: 0.44
Batch: 80; loss: 4.03; acc: 0.38
Batch: 100; loss: 3.03; acc: 0.42
Batch: 120; loss: 3.52; acc: 0.42
Batch: 140; loss: 4.78; acc: 0.38
Batch: 160; loss: 4.61; acc: 0.34
Batch: 180; loss: 4.74; acc: 0.33
Batch: 200; loss: 4.2; acc: 0.47
Batch: 220; loss: 3.48; acc: 0.44
Batch: 240; loss: 4.58; acc: 0.38
Batch: 260; loss: 4.88; acc: 0.31
Batch: 280; loss: 3.74; acc: 0.42
Batch: 300; loss: 4.18; acc: 0.36
Batch: 320; loss: 4.22; acc: 0.36
Batch: 340; loss: 3.57; acc: 0.39
Batch: 360; loss: 3.9; acc: 0.41
Batch: 380; loss: 3.19; acc: 0.41
Batch: 400; loss: 4.01; acc: 0.45
Batch: 420; loss: 3.99; acc: 0.39
Batch: 440; loss: 3.81; acc: 0.38
Batch: 460; loss: 2.26; acc: 0.48
Batch: 480; loss: 3.25; acc: 0.5
Batch: 500; loss: 3.83; acc: 0.36
Batch: 520; loss: 2.93; acc: 0.5
Batch: 540; loss: 2.94; acc: 0.45
Batch: 560; loss: 4.48; acc: 0.38
Batch: 580; loss: 3.6; acc: 0.5
Batch: 600; loss: 3.31; acc: 0.53
Batch: 620; loss: 3.61; acc: 0.34
Train Epoch over. train_loss: 3.81; train_accuracy: 0.41 

Batch: 0; loss: 3.28; acc: 0.39
Batch: 20; loss: 4.71; acc: 0.38
Batch: 40; loss: 3.89; acc: 0.33
Batch: 60; loss: 3.17; acc: 0.5
Batch: 80; loss: 3.63; acc: 0.44
Batch: 100; loss: 3.83; acc: 0.33
Batch: 120; loss: 3.33; acc: 0.53
Batch: 140; loss: 5.44; acc: 0.27
Val Epoch over. val_loss: 3.862675499764218; val_accuracy: 0.40794187898089174 

Epoch 21 start
Batch: 0; loss: 2.94; acc: 0.52
Batch: 20; loss: 3.52; acc: 0.42
Batch: 40; loss: 4.73; acc: 0.34
Batch: 60; loss: 4.77; acc: 0.33
Batch: 80; loss: 3.95; acc: 0.47
Batch: 100; loss: 3.18; acc: 0.44
Batch: 120; loss: 5.28; acc: 0.25
Batch: 140; loss: 3.03; acc: 0.44
Batch: 160; loss: 3.71; acc: 0.47
Batch: 180; loss: 2.45; acc: 0.47
Batch: 200; loss: 4.19; acc: 0.41
Batch: 220; loss: 2.48; acc: 0.55
Batch: 240; loss: 4.38; acc: 0.38
Batch: 260; loss: 3.96; acc: 0.5
Batch: 280; loss: 5.05; acc: 0.39
Batch: 300; loss: 3.99; acc: 0.36
Batch: 320; loss: 3.16; acc: 0.5
Batch: 340; loss: 4.37; acc: 0.42
Batch: 360; loss: 3.51; acc: 0.41
Batch: 380; loss: 3.6; acc: 0.39
Batch: 400; loss: 3.7; acc: 0.41
Batch: 420; loss: 2.8; acc: 0.44
Batch: 440; loss: 3.84; acc: 0.31
Batch: 460; loss: 3.1; acc: 0.41
Batch: 480; loss: 4.24; acc: 0.41
Batch: 500; loss: 4.64; acc: 0.27
Batch: 520; loss: 4.02; acc: 0.36
Batch: 540; loss: 4.33; acc: 0.39
Batch: 560; loss: 5.0; acc: 0.3
Batch: 580; loss: 4.8; acc: 0.44
Batch: 600; loss: 3.6; acc: 0.41
Batch: 620; loss: 3.78; acc: 0.44
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.29; acc: 0.39
Batch: 20; loss: 4.7; acc: 0.38
Batch: 40; loss: 3.87; acc: 0.31
Batch: 60; loss: 3.1; acc: 0.52
Batch: 80; loss: 3.68; acc: 0.41
Batch: 100; loss: 3.83; acc: 0.33
Batch: 120; loss: 3.32; acc: 0.55
Batch: 140; loss: 5.45; acc: 0.27
Val Epoch over. val_loss: 3.856294941750302; val_accuracy: 0.41182324840764334 

Epoch 22 start
Batch: 0; loss: 3.78; acc: 0.45
Batch: 20; loss: 3.0; acc: 0.45
Batch: 40; loss: 3.87; acc: 0.45
Batch: 60; loss: 4.38; acc: 0.34
Batch: 80; loss: 4.54; acc: 0.31
Batch: 100; loss: 3.27; acc: 0.42
Batch: 120; loss: 3.41; acc: 0.39
Batch: 140; loss: 4.17; acc: 0.36
Batch: 160; loss: 4.09; acc: 0.39
Batch: 180; loss: 2.98; acc: 0.42
Batch: 200; loss: 3.82; acc: 0.42
Batch: 220; loss: 3.98; acc: 0.38
Batch: 240; loss: 4.23; acc: 0.28
Batch: 260; loss: 3.26; acc: 0.48
Batch: 280; loss: 2.74; acc: 0.45
Batch: 300; loss: 4.97; acc: 0.33
Batch: 320; loss: 3.62; acc: 0.3
Batch: 340; loss: 4.07; acc: 0.41
Batch: 360; loss: 4.06; acc: 0.36
Batch: 380; loss: 2.87; acc: 0.52
Batch: 400; loss: 3.78; acc: 0.41
Batch: 420; loss: 4.35; acc: 0.41
Batch: 440; loss: 4.04; acc: 0.41
Batch: 460; loss: 2.64; acc: 0.38
Batch: 480; loss: 4.36; acc: 0.31
Batch: 500; loss: 3.01; acc: 0.48
Batch: 520; loss: 2.87; acc: 0.52
Batch: 540; loss: 3.98; acc: 0.41
Batch: 560; loss: 3.34; acc: 0.38
Batch: 580; loss: 4.43; acc: 0.34
Batch: 600; loss: 4.32; acc: 0.38
Batch: 620; loss: 3.55; acc: 0.42
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.29; acc: 0.41
Batch: 20; loss: 4.73; acc: 0.38
Batch: 40; loss: 3.83; acc: 0.33
Batch: 60; loss: 3.09; acc: 0.52
Batch: 80; loss: 3.68; acc: 0.39
Batch: 100; loss: 3.82; acc: 0.33
Batch: 120; loss: 3.33; acc: 0.55
Batch: 140; loss: 5.44; acc: 0.27
Val Epoch over. val_loss: 3.855670584235222; val_accuracy: 0.4101313694267516 

Epoch 23 start
Batch: 0; loss: 2.66; acc: 0.44
Batch: 20; loss: 3.12; acc: 0.42
Batch: 40; loss: 3.11; acc: 0.5
Batch: 60; loss: 3.87; acc: 0.45
Batch: 80; loss: 2.69; acc: 0.45
Batch: 100; loss: 3.16; acc: 0.53
Batch: 120; loss: 3.61; acc: 0.39
Batch: 140; loss: 4.48; acc: 0.28
Batch: 160; loss: 4.73; acc: 0.34
Batch: 180; loss: 3.55; acc: 0.42
Batch: 200; loss: 2.55; acc: 0.52
Batch: 220; loss: 4.91; acc: 0.27
Batch: 240; loss: 3.71; acc: 0.42
Batch: 260; loss: 2.88; acc: 0.38
Batch: 280; loss: 4.26; acc: 0.44
Batch: 300; loss: 5.44; acc: 0.31
Batch: 320; loss: 2.91; acc: 0.42
Batch: 340; loss: 3.66; acc: 0.38
Batch: 360; loss: 4.05; acc: 0.38
Batch: 380; loss: 5.22; acc: 0.38
Batch: 400; loss: 4.48; acc: 0.39
Batch: 420; loss: 3.78; acc: 0.47
Batch: 440; loss: 2.39; acc: 0.56
Batch: 460; loss: 3.91; acc: 0.36
Batch: 480; loss: 4.0; acc: 0.39
Batch: 500; loss: 3.34; acc: 0.41
Batch: 520; loss: 4.88; acc: 0.38
Batch: 540; loss: 3.03; acc: 0.39
Batch: 560; loss: 3.54; acc: 0.41
Batch: 580; loss: 3.8; acc: 0.42
Batch: 600; loss: 4.07; acc: 0.42
Batch: 620; loss: 5.15; acc: 0.33
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.29; acc: 0.39
Batch: 20; loss: 4.72; acc: 0.38
Batch: 40; loss: 3.85; acc: 0.33
Batch: 60; loss: 3.09; acc: 0.52
Batch: 80; loss: 3.69; acc: 0.39
Batch: 100; loss: 3.82; acc: 0.33
Batch: 120; loss: 3.33; acc: 0.55
Batch: 140; loss: 5.44; acc: 0.27
Val Epoch over. val_loss: 3.8558945655822754; val_accuracy: 0.4111265923566879 

Epoch 24 start
Batch: 0; loss: 4.29; acc: 0.34
Batch: 20; loss: 3.27; acc: 0.44
Batch: 40; loss: 4.87; acc: 0.38
Batch: 60; loss: 4.11; acc: 0.41
Batch: 80; loss: 3.94; acc: 0.44
Batch: 100; loss: 4.67; acc: 0.28
Batch: 120; loss: 4.03; acc: 0.48
Batch: 140; loss: 3.38; acc: 0.5
Batch: 160; loss: 3.95; acc: 0.33
Batch: 180; loss: 2.91; acc: 0.48
Batch: 200; loss: 4.25; acc: 0.34
Batch: 220; loss: 4.29; acc: 0.44
Batch: 240; loss: 4.75; acc: 0.33
Batch: 260; loss: 4.03; acc: 0.38
Batch: 280; loss: 3.54; acc: 0.39
Batch: 300; loss: 3.84; acc: 0.48
Batch: 320; loss: 3.73; acc: 0.41
Batch: 340; loss: 4.79; acc: 0.31
Batch: 360; loss: 5.01; acc: 0.44
Batch: 380; loss: 3.21; acc: 0.44
Batch: 400; loss: 4.39; acc: 0.41
Batch: 420; loss: 3.92; acc: 0.39
Batch: 440; loss: 4.4; acc: 0.45
Batch: 460; loss: 3.89; acc: 0.31
Batch: 480; loss: 2.85; acc: 0.5
Batch: 500; loss: 3.61; acc: 0.48
Batch: 520; loss: 3.52; acc: 0.41
Batch: 540; loss: 3.97; acc: 0.5
Batch: 560; loss: 4.6; acc: 0.39
Batch: 580; loss: 2.67; acc: 0.53
Batch: 600; loss: 4.31; acc: 0.42
Batch: 620; loss: 4.65; acc: 0.28
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.3; acc: 0.39
Batch: 20; loss: 4.7; acc: 0.39
Batch: 40; loss: 3.86; acc: 0.33
Batch: 60; loss: 3.06; acc: 0.52
Batch: 80; loss: 3.71; acc: 0.41
Batch: 100; loss: 3.83; acc: 0.33
Batch: 120; loss: 3.35; acc: 0.53
Batch: 140; loss: 5.42; acc: 0.27
Val Epoch over. val_loss: 3.85778162130125; val_accuracy: 0.4112261146496815 

Epoch 25 start
Batch: 0; loss: 3.99; acc: 0.36
Batch: 20; loss: 4.72; acc: 0.27
Batch: 40; loss: 3.66; acc: 0.45
Batch: 60; loss: 4.98; acc: 0.28
Batch: 80; loss: 3.98; acc: 0.36
Batch: 100; loss: 3.57; acc: 0.45
Batch: 120; loss: 3.85; acc: 0.42
Batch: 140; loss: 2.86; acc: 0.48
Batch: 160; loss: 3.77; acc: 0.47
Batch: 180; loss: 3.3; acc: 0.48
Batch: 200; loss: 4.14; acc: 0.42
Batch: 220; loss: 4.17; acc: 0.44
Batch: 240; loss: 3.9; acc: 0.42
Batch: 260; loss: 4.15; acc: 0.33
Batch: 280; loss: 2.7; acc: 0.52
Batch: 300; loss: 3.36; acc: 0.41
Batch: 320; loss: 3.25; acc: 0.44
Batch: 340; loss: 3.27; acc: 0.5
Batch: 360; loss: 3.81; acc: 0.38
Batch: 380; loss: 3.37; acc: 0.34
Batch: 400; loss: 2.93; acc: 0.48
Batch: 420; loss: 4.63; acc: 0.3
Batch: 440; loss: 3.91; acc: 0.39
Batch: 460; loss: 3.45; acc: 0.5
Batch: 480; loss: 3.63; acc: 0.47
Batch: 500; loss: 3.69; acc: 0.39
Batch: 520; loss: 3.25; acc: 0.45
Batch: 540; loss: 3.59; acc: 0.5
Batch: 560; loss: 4.69; acc: 0.36
Batch: 580; loss: 4.13; acc: 0.47
Batch: 600; loss: 4.72; acc: 0.3
Batch: 620; loss: 3.89; acc: 0.36
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.28; acc: 0.39
Batch: 20; loss: 4.73; acc: 0.38
Batch: 40; loss: 3.87; acc: 0.33
Batch: 60; loss: 3.05; acc: 0.52
Batch: 80; loss: 3.68; acc: 0.41
Batch: 100; loss: 3.84; acc: 0.33
Batch: 120; loss: 3.33; acc: 0.55
Batch: 140; loss: 5.44; acc: 0.27
Val Epoch over. val_loss: 3.8572471612577983; val_accuracy: 0.41033041401273884 

Epoch 26 start
Batch: 0; loss: 3.92; acc: 0.38
Batch: 20; loss: 5.0; acc: 0.39
Batch: 40; loss: 5.16; acc: 0.38
Batch: 60; loss: 5.15; acc: 0.36
Batch: 80; loss: 4.69; acc: 0.42
Batch: 100; loss: 3.18; acc: 0.5
Batch: 120; loss: 3.25; acc: 0.42
Batch: 140; loss: 3.81; acc: 0.44
Batch: 160; loss: 3.47; acc: 0.38
Batch: 180; loss: 4.58; acc: 0.31
Batch: 200; loss: 2.62; acc: 0.44
Batch: 220; loss: 4.01; acc: 0.31
Batch: 240; loss: 3.44; acc: 0.33
Batch: 260; loss: 4.77; acc: 0.28
Batch: 280; loss: 3.61; acc: 0.34
Batch: 300; loss: 4.39; acc: 0.34
Batch: 320; loss: 3.52; acc: 0.41
Batch: 340; loss: 3.96; acc: 0.3
Batch: 360; loss: 3.88; acc: 0.36
Batch: 380; loss: 4.15; acc: 0.44
Batch: 400; loss: 3.86; acc: 0.48
Batch: 420; loss: 3.78; acc: 0.39
Batch: 440; loss: 4.42; acc: 0.31
Batch: 460; loss: 4.91; acc: 0.38
Batch: 480; loss: 3.28; acc: 0.44
Batch: 500; loss: 4.28; acc: 0.38
Batch: 520; loss: 3.49; acc: 0.42
Batch: 540; loss: 3.98; acc: 0.39
Batch: 560; loss: 4.27; acc: 0.33
Batch: 580; loss: 3.6; acc: 0.44
Batch: 600; loss: 4.33; acc: 0.31
Batch: 620; loss: 4.05; acc: 0.38
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.29; acc: 0.39
Batch: 20; loss: 4.7; acc: 0.38
Batch: 40; loss: 3.86; acc: 0.33
Batch: 60; loss: 3.06; acc: 0.52
Batch: 80; loss: 3.69; acc: 0.39
Batch: 100; loss: 3.83; acc: 0.33
Batch: 120; loss: 3.34; acc: 0.55
Batch: 140; loss: 5.43; acc: 0.27
Val Epoch over. val_loss: 3.8567253981426264; val_accuracy: 0.4099323248407643 

Epoch 27 start
Batch: 0; loss: 3.37; acc: 0.36
Batch: 20; loss: 4.42; acc: 0.33
Batch: 40; loss: 3.93; acc: 0.39
Batch: 60; loss: 3.58; acc: 0.45
Batch: 80; loss: 3.54; acc: 0.42
Batch: 100; loss: 4.15; acc: 0.42
Batch: 120; loss: 3.8; acc: 0.41
Batch: 140; loss: 3.9; acc: 0.44
Batch: 160; loss: 4.22; acc: 0.36
Batch: 180; loss: 3.65; acc: 0.47
Batch: 200; loss: 3.61; acc: 0.41
Batch: 220; loss: 3.81; acc: 0.36
Batch: 240; loss: 3.15; acc: 0.52
Batch: 260; loss: 3.64; acc: 0.53
Batch: 280; loss: 4.02; acc: 0.42
Batch: 300; loss: 4.16; acc: 0.44
Batch: 320; loss: 3.09; acc: 0.53
Batch: 340; loss: 3.68; acc: 0.36
Batch: 360; loss: 3.69; acc: 0.34
Batch: 380; loss: 2.99; acc: 0.42
Batch: 400; loss: 4.67; acc: 0.36
Batch: 420; loss: 4.47; acc: 0.34
Batch: 440; loss: 3.43; acc: 0.41
Batch: 460; loss: 4.51; acc: 0.36
Batch: 480; loss: 3.81; acc: 0.34
Batch: 500; loss: 3.94; acc: 0.41
Batch: 520; loss: 4.37; acc: 0.38
Batch: 540; loss: 2.63; acc: 0.58
Batch: 560; loss: 3.56; acc: 0.39
Batch: 580; loss: 2.77; acc: 0.55
Batch: 600; loss: 5.16; acc: 0.38
Batch: 620; loss: 2.81; acc: 0.45
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.3; acc: 0.39
Batch: 20; loss: 4.73; acc: 0.39
Batch: 40; loss: 3.86; acc: 0.33
Batch: 60; loss: 3.04; acc: 0.52
Batch: 80; loss: 3.69; acc: 0.39
Batch: 100; loss: 3.85; acc: 0.33
Batch: 120; loss: 3.35; acc: 0.55
Batch: 140; loss: 5.43; acc: 0.27
Val Epoch over. val_loss: 3.8581027726458896; val_accuracy: 0.4085390127388535 

Epoch 28 start
Batch: 0; loss: 3.89; acc: 0.31
Batch: 20; loss: 2.8; acc: 0.45
Batch: 40; loss: 3.67; acc: 0.48
Batch: 60; loss: 3.64; acc: 0.39
Batch: 80; loss: 4.15; acc: 0.44
Batch: 100; loss: 3.96; acc: 0.42
Batch: 120; loss: 2.85; acc: 0.41
Batch: 140; loss: 3.46; acc: 0.36
Batch: 160; loss: 4.37; acc: 0.38
Batch: 180; loss: 4.7; acc: 0.34
Batch: 200; loss: 3.86; acc: 0.42
Batch: 220; loss: 2.21; acc: 0.53
Batch: 240; loss: 3.61; acc: 0.42
Batch: 260; loss: 3.58; acc: 0.33
Batch: 280; loss: 3.39; acc: 0.44
Batch: 300; loss: 3.59; acc: 0.41
Batch: 320; loss: 4.55; acc: 0.38
Batch: 340; loss: 2.68; acc: 0.41
Batch: 360; loss: 3.26; acc: 0.52
Batch: 380; loss: 3.25; acc: 0.52
Batch: 400; loss: 3.88; acc: 0.36
Batch: 420; loss: 4.47; acc: 0.3
Batch: 440; loss: 3.38; acc: 0.38
Batch: 460; loss: 3.92; acc: 0.38
Batch: 480; loss: 3.58; acc: 0.45
Batch: 500; loss: 4.87; acc: 0.34
Batch: 520; loss: 3.48; acc: 0.44
Batch: 540; loss: 3.6; acc: 0.41
Batch: 560; loss: 3.22; acc: 0.44
Batch: 580; loss: 3.24; acc: 0.39
Batch: 600; loss: 3.4; acc: 0.45
Batch: 620; loss: 3.59; acc: 0.39
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.29; acc: 0.39
Batch: 20; loss: 4.75; acc: 0.38
Batch: 40; loss: 3.86; acc: 0.31
Batch: 60; loss: 3.07; acc: 0.52
Batch: 80; loss: 3.69; acc: 0.39
Batch: 100; loss: 3.82; acc: 0.33
Batch: 120; loss: 3.32; acc: 0.55
Batch: 140; loss: 5.45; acc: 0.27
Val Epoch over. val_loss: 3.8563642410715673; val_accuracy: 0.4096337579617834 

Epoch 29 start
Batch: 0; loss: 3.33; acc: 0.42
Batch: 20; loss: 3.27; acc: 0.45
Batch: 40; loss: 4.18; acc: 0.41
Batch: 60; loss: 5.35; acc: 0.31
Batch: 80; loss: 3.55; acc: 0.31
Batch: 100; loss: 3.97; acc: 0.44
Batch: 120; loss: 4.08; acc: 0.36
Batch: 140; loss: 3.53; acc: 0.39
Batch: 160; loss: 4.01; acc: 0.36
Batch: 180; loss: 3.26; acc: 0.45
Batch: 200; loss: 4.58; acc: 0.22
Batch: 220; loss: 3.28; acc: 0.41
Batch: 240; loss: 3.28; acc: 0.47
Batch: 260; loss: 3.86; acc: 0.31
Batch: 280; loss: 3.72; acc: 0.39
Batch: 300; loss: 3.97; acc: 0.23
Batch: 320; loss: 4.03; acc: 0.42
Batch: 340; loss: 3.48; acc: 0.47
Batch: 360; loss: 5.23; acc: 0.41
Batch: 380; loss: 3.57; acc: 0.45
Batch: 400; loss: 2.74; acc: 0.47
Batch: 420; loss: 3.66; acc: 0.53
Batch: 440; loss: 4.63; acc: 0.36
Batch: 460; loss: 3.75; acc: 0.48
Batch: 480; loss: 3.26; acc: 0.47
Batch: 500; loss: 4.08; acc: 0.33
Batch: 520; loss: 4.1; acc: 0.33
Batch: 540; loss: 4.92; acc: 0.39
Batch: 560; loss: 2.98; acc: 0.44
Batch: 580; loss: 3.29; acc: 0.42
Batch: 600; loss: 3.39; acc: 0.39
Batch: 620; loss: 3.81; acc: 0.34
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.27; acc: 0.39
Batch: 20; loss: 4.67; acc: 0.38
Batch: 40; loss: 3.84; acc: 0.33
Batch: 60; loss: 3.09; acc: 0.52
Batch: 80; loss: 3.68; acc: 0.39
Batch: 100; loss: 3.83; acc: 0.33
Batch: 120; loss: 3.34; acc: 0.53
Batch: 140; loss: 5.42; acc: 0.27
Val Epoch over. val_loss: 3.858405111701625; val_accuracy: 0.41062898089171973 

Epoch 30 start
Batch: 0; loss: 3.31; acc: 0.48
Batch: 20; loss: 4.71; acc: 0.39
Batch: 40; loss: 2.73; acc: 0.48
Batch: 60; loss: 3.66; acc: 0.38
Batch: 80; loss: 3.91; acc: 0.41
Batch: 100; loss: 3.7; acc: 0.42
Batch: 120; loss: 3.83; acc: 0.38
Batch: 140; loss: 2.91; acc: 0.44
Batch: 160; loss: 3.46; acc: 0.3
Batch: 180; loss: 3.76; acc: 0.45
Batch: 200; loss: 2.35; acc: 0.59
Batch: 220; loss: 4.21; acc: 0.39
Batch: 240; loss: 3.02; acc: 0.53
Batch: 260; loss: 3.38; acc: 0.48
Batch: 280; loss: 4.04; acc: 0.39
Batch: 300; loss: 3.24; acc: 0.42
Batch: 320; loss: 3.99; acc: 0.44
Batch: 340; loss: 3.57; acc: 0.42
Batch: 360; loss: 4.57; acc: 0.36
Batch: 380; loss: 3.67; acc: 0.41
Batch: 400; loss: 4.14; acc: 0.34
Batch: 420; loss: 4.37; acc: 0.31
Batch: 440; loss: 3.85; acc: 0.41
Batch: 460; loss: 4.83; acc: 0.33
Batch: 480; loss: 3.82; acc: 0.33
Batch: 500; loss: 3.8; acc: 0.38
Batch: 520; loss: 4.76; acc: 0.38
Batch: 540; loss: 3.88; acc: 0.39
Batch: 560; loss: 5.07; acc: 0.38
Batch: 580; loss: 3.83; acc: 0.36
Batch: 600; loss: 4.53; acc: 0.31
Batch: 620; loss: 4.14; acc: 0.38
Train Epoch over. train_loss: 3.79; train_accuracy: 0.41 

Batch: 0; loss: 3.26; acc: 0.38
Batch: 20; loss: 4.7; acc: 0.38
Batch: 40; loss: 3.86; acc: 0.33
Batch: 60; loss: 3.07; acc: 0.52
Batch: 80; loss: 3.66; acc: 0.39
Batch: 100; loss: 3.84; acc: 0.33
Batch: 120; loss: 3.33; acc: 0.53
Batch: 140; loss: 5.42; acc: 0.27
Val Epoch over. val_loss: 3.857987030296569; val_accuracy: 0.41033041401273884 

plots/subspace_training/lenet/2020-01-10 05:00:58/d_dim_50_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 3.82; acc: 0.34
Batch: 40; loss: 3.7; acc: 0.38
Batch: 60; loss: 2.98; acc: 0.42
Batch: 80; loss: 2.59; acc: 0.44
Batch: 100; loss: 1.93; acc: 0.59
Batch: 120; loss: 2.38; acc: 0.61
Batch: 140; loss: 3.84; acc: 0.39
Batch: 160; loss: 3.48; acc: 0.55
Batch: 180; loss: 3.37; acc: 0.52
Batch: 200; loss: 1.65; acc: 0.62
Batch: 220; loss: 2.92; acc: 0.56
Batch: 240; loss: 2.73; acc: 0.61
Batch: 260; loss: 2.53; acc: 0.59
Batch: 280; loss: 2.42; acc: 0.55
Batch: 300; loss: 2.24; acc: 0.58
Batch: 320; loss: 2.24; acc: 0.62
Batch: 340; loss: 1.87; acc: 0.66
Batch: 360; loss: 2.23; acc: 0.59
Batch: 380; loss: 3.46; acc: 0.5
Batch: 400; loss: 2.51; acc: 0.58
Batch: 420; loss: 2.74; acc: 0.5
Batch: 440; loss: 2.86; acc: 0.52
Batch: 460; loss: 2.21; acc: 0.56
Batch: 480; loss: 3.7; acc: 0.34
Batch: 500; loss: 2.65; acc: 0.47
Batch: 520; loss: 2.81; acc: 0.53
Batch: 540; loss: 2.07; acc: 0.58
Batch: 560; loss: 1.63; acc: 0.77
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 2.44; acc: 0.55
Batch: 620; loss: 2.46; acc: 0.69
Train Epoch over. train_loss: 2.84; train_accuracy: 0.52 

Batch: 0; loss: 4.23; acc: 0.27
Batch: 20; loss: 4.18; acc: 0.38
Batch: 40; loss: 2.22; acc: 0.55
Batch: 60; loss: 2.88; acc: 0.59
Batch: 80; loss: 2.71; acc: 0.55
Batch: 100; loss: 3.85; acc: 0.5
Batch: 120; loss: 3.27; acc: 0.58
Batch: 140; loss: 5.25; acc: 0.39
Val Epoch over. val_loss: 3.0689296684447367; val_accuracy: 0.4898487261146497 

Epoch 2 start
Batch: 0; loss: 2.77; acc: 0.48
Batch: 20; loss: 2.37; acc: 0.58
Batch: 40; loss: 2.78; acc: 0.48
Batch: 60; loss: 3.45; acc: 0.44
Batch: 80; loss: 2.64; acc: 0.58
Batch: 100; loss: 2.69; acc: 0.59
Batch: 120; loss: 2.85; acc: 0.55
Batch: 140; loss: 1.44; acc: 0.64
Batch: 160; loss: 2.3; acc: 0.59
Batch: 180; loss: 2.57; acc: 0.58
Batch: 200; loss: 3.06; acc: 0.59
Batch: 220; loss: 1.35; acc: 0.69
Batch: 240; loss: 2.13; acc: 0.64
Batch: 260; loss: 2.7; acc: 0.56
Batch: 280; loss: 3.85; acc: 0.48
Batch: 300; loss: 2.76; acc: 0.56
Batch: 320; loss: 2.08; acc: 0.58
Batch: 340; loss: 2.4; acc: 0.58
Batch: 360; loss: 2.85; acc: 0.48
Batch: 380; loss: 2.63; acc: 0.64
Batch: 400; loss: 2.34; acc: 0.56
Batch: 420; loss: 2.52; acc: 0.56
Batch: 440; loss: 2.28; acc: 0.58
Batch: 460; loss: 2.84; acc: 0.56
Batch: 480; loss: 1.81; acc: 0.66
Batch: 500; loss: 3.06; acc: 0.53
Batch: 520; loss: 2.25; acc: 0.56
Batch: 540; loss: 1.42; acc: 0.7
Batch: 560; loss: 2.93; acc: 0.58
Batch: 580; loss: 2.39; acc: 0.56
Batch: 600; loss: 3.81; acc: 0.48
Batch: 620; loss: 2.5; acc: 0.56
Train Epoch over. train_loss: 2.57; train_accuracy: 0.56 

Batch: 0; loss: 2.54; acc: 0.53
Batch: 20; loss: 3.15; acc: 0.36
Batch: 40; loss: 1.11; acc: 0.72
Batch: 60; loss: 2.21; acc: 0.61
Batch: 80; loss: 2.46; acc: 0.59
Batch: 100; loss: 2.88; acc: 0.48
Batch: 120; loss: 2.94; acc: 0.55
Batch: 140; loss: 4.86; acc: 0.38
Val Epoch over. val_loss: 2.477054009771651; val_accuracy: 0.5550358280254777 

Epoch 3 start
Batch: 0; loss: 2.89; acc: 0.53
Batch: 20; loss: 2.41; acc: 0.5
Batch: 40; loss: 1.98; acc: 0.64
Batch: 60; loss: 2.52; acc: 0.56
Batch: 80; loss: 2.34; acc: 0.64
Batch: 100; loss: 2.78; acc: 0.64
Batch: 120; loss: 2.67; acc: 0.47
Batch: 140; loss: 2.45; acc: 0.55
Batch: 160; loss: 1.51; acc: 0.69
Batch: 180; loss: 1.56; acc: 0.72
Batch: 200; loss: 1.93; acc: 0.61
Batch: 220; loss: 2.79; acc: 0.61
Batch: 240; loss: 1.95; acc: 0.62
Batch: 260; loss: 2.07; acc: 0.59
Batch: 280; loss: 2.64; acc: 0.53
Batch: 300; loss: 1.71; acc: 0.56
Batch: 320; loss: 2.83; acc: 0.59
Batch: 340; loss: 2.54; acc: 0.52
Batch: 360; loss: 2.75; acc: 0.55
Batch: 380; loss: 2.23; acc: 0.59
Batch: 400; loss: 3.01; acc: 0.53
Batch: 420; loss: 3.15; acc: 0.5
Batch: 440; loss: 1.89; acc: 0.62
Batch: 460; loss: 2.18; acc: 0.59
Batch: 480; loss: 3.07; acc: 0.47
Batch: 500; loss: 2.72; acc: 0.52
Batch: 520; loss: 2.5; acc: 0.56
Batch: 540; loss: 1.39; acc: 0.69
Batch: 560; loss: 2.51; acc: 0.59
Batch: 580; loss: 3.45; acc: 0.5
Batch: 600; loss: 2.02; acc: 0.58
Batch: 620; loss: 2.35; acc: 0.55
Train Epoch over. train_loss: 2.6; train_accuracy: 0.55 

Batch: 0; loss: 3.91; acc: 0.45
Batch: 20; loss: 6.38; acc: 0.27
Batch: 40; loss: 2.57; acc: 0.55
Batch: 60; loss: 3.22; acc: 0.53
Batch: 80; loss: 4.11; acc: 0.48
Batch: 100; loss: 3.42; acc: 0.44
Batch: 120; loss: 3.93; acc: 0.53
Batch: 140; loss: 5.41; acc: 0.44
Val Epoch over. val_loss: 3.944246662650139; val_accuracy: 0.46695859872611467 

Epoch 4 start
Batch: 0; loss: 3.76; acc: 0.39
Batch: 20; loss: 3.21; acc: 0.48
Batch: 40; loss: 1.78; acc: 0.67
Batch: 60; loss: 1.76; acc: 0.61
Batch: 80; loss: 1.86; acc: 0.66
Batch: 100; loss: 1.98; acc: 0.7
Batch: 120; loss: 2.68; acc: 0.56
Batch: 140; loss: 3.05; acc: 0.55
Batch: 160; loss: 2.13; acc: 0.61
Batch: 180; loss: 2.18; acc: 0.52
Batch: 200; loss: 2.2; acc: 0.59
Batch: 220; loss: 2.59; acc: 0.59
Batch: 240; loss: 2.14; acc: 0.64
Batch: 260; loss: 1.58; acc: 0.66
Batch: 280; loss: 3.73; acc: 0.42
Batch: 300; loss: 2.43; acc: 0.59
Batch: 320; loss: 2.76; acc: 0.5
Batch: 340; loss: 2.27; acc: 0.66
Batch: 360; loss: 2.26; acc: 0.53
Batch: 380; loss: 3.02; acc: 0.55
Batch: 400; loss: 1.65; acc: 0.64
Batch: 420; loss: 2.42; acc: 0.64
Batch: 440; loss: 4.01; acc: 0.44
Batch: 460; loss: 2.51; acc: 0.55
Batch: 480; loss: 1.89; acc: 0.52
Batch: 500; loss: 3.14; acc: 0.56
Batch: 520; loss: 2.4; acc: 0.61
Batch: 540; loss: 1.97; acc: 0.61
Batch: 560; loss: 3.41; acc: 0.47
Batch: 580; loss: 1.66; acc: 0.66
Batch: 600; loss: 1.9; acc: 0.62
Batch: 620; loss: 2.12; acc: 0.61
Train Epoch over. train_loss: 2.55; train_accuracy: 0.56 

Batch: 0; loss: 2.67; acc: 0.56
Batch: 20; loss: 3.36; acc: 0.44
Batch: 40; loss: 1.0; acc: 0.72
Batch: 60; loss: 2.31; acc: 0.61
Batch: 80; loss: 2.56; acc: 0.5
Batch: 100; loss: 2.42; acc: 0.59
Batch: 120; loss: 2.35; acc: 0.66
Batch: 140; loss: 4.06; acc: 0.39
Val Epoch over. val_loss: 2.40985415581685; val_accuracy: 0.5774283439490446 

Epoch 5 start
Batch: 0; loss: 1.49; acc: 0.67
Batch: 20; loss: 2.37; acc: 0.52
Batch: 40; loss: 2.98; acc: 0.47
Batch: 60; loss: 1.68; acc: 0.56
Batch: 80; loss: 2.17; acc: 0.5
Batch: 100; loss: 3.08; acc: 0.48
Batch: 120; loss: 2.34; acc: 0.58
Batch: 140; loss: 3.35; acc: 0.53
Batch: 160; loss: 4.26; acc: 0.52
Batch: 180; loss: 2.02; acc: 0.66
Batch: 200; loss: 2.32; acc: 0.55
Batch: 220; loss: 3.29; acc: 0.47
Batch: 240; loss: 3.39; acc: 0.44
Batch: 260; loss: 2.02; acc: 0.67
Batch: 280; loss: 3.69; acc: 0.42
Batch: 300; loss: 3.15; acc: 0.39
Batch: 320; loss: 2.71; acc: 0.58
Batch: 340; loss: 3.57; acc: 0.42
Batch: 360; loss: 1.81; acc: 0.59
Batch: 380; loss: 3.03; acc: 0.52
Batch: 400; loss: 2.52; acc: 0.55
Batch: 420; loss: 2.76; acc: 0.59
Batch: 440; loss: 2.67; acc: 0.52
Batch: 460; loss: 2.69; acc: 0.52
Batch: 480; loss: 1.77; acc: 0.69
Batch: 500; loss: 3.23; acc: 0.58
Batch: 520; loss: 1.64; acc: 0.62
Batch: 540; loss: 2.33; acc: 0.56
Batch: 560; loss: 2.4; acc: 0.56
Batch: 580; loss: 2.2; acc: 0.52
Batch: 600; loss: 2.07; acc: 0.53
Batch: 620; loss: 1.96; acc: 0.64
Train Epoch over. train_loss: 2.57; train_accuracy: 0.56 

Batch: 0; loss: 2.28; acc: 0.58
Batch: 20; loss: 2.68; acc: 0.56
Batch: 40; loss: 1.28; acc: 0.66
Batch: 60; loss: 2.47; acc: 0.58
Batch: 80; loss: 2.44; acc: 0.56
Batch: 100; loss: 2.2; acc: 0.58
Batch: 120; loss: 2.9; acc: 0.53
Batch: 140; loss: 4.69; acc: 0.38
Val Epoch over. val_loss: 2.4231726568975267; val_accuracy: 0.5638933121019108 

Epoch 6 start
Batch: 0; loss: 2.61; acc: 0.53
Batch: 20; loss: 3.02; acc: 0.55
Batch: 40; loss: 3.39; acc: 0.52
Batch: 60; loss: 2.41; acc: 0.55
Batch: 80; loss: 4.11; acc: 0.36
Batch: 100; loss: 2.6; acc: 0.59
Batch: 120; loss: 2.82; acc: 0.45
Batch: 140; loss: 1.97; acc: 0.59
Batch: 160; loss: 3.48; acc: 0.38
Batch: 180; loss: 2.08; acc: 0.61
Batch: 200; loss: 3.54; acc: 0.48
Batch: 220; loss: 2.57; acc: 0.48
Batch: 240; loss: 3.63; acc: 0.5
Batch: 260; loss: 3.14; acc: 0.56
Batch: 280; loss: 2.24; acc: 0.55
Batch: 300; loss: 2.34; acc: 0.61
Batch: 320; loss: 2.44; acc: 0.56
Batch: 340; loss: 2.1; acc: 0.59
Batch: 360; loss: 2.02; acc: 0.62
Batch: 380; loss: 2.72; acc: 0.62
Batch: 400; loss: 2.26; acc: 0.62
Batch: 420; loss: 2.37; acc: 0.58
Batch: 440; loss: 2.91; acc: 0.61
Batch: 460; loss: 1.97; acc: 0.64
Batch: 480; loss: 1.85; acc: 0.64
Batch: 500; loss: 1.7; acc: 0.66
Batch: 520; loss: 2.34; acc: 0.5
Batch: 540; loss: 3.61; acc: 0.45
Batch: 560; loss: 2.98; acc: 0.48
Batch: 580; loss: 2.44; acc: 0.53
Batch: 600; loss: 1.62; acc: 0.64
Batch: 620; loss: 2.39; acc: 0.62
Train Epoch over. train_loss: 2.56; train_accuracy: 0.56 

Batch: 0; loss: 2.88; acc: 0.56
Batch: 20; loss: 4.87; acc: 0.42
Batch: 40; loss: 2.2; acc: 0.58
Batch: 60; loss: 2.85; acc: 0.5
Batch: 80; loss: 3.43; acc: 0.45
Batch: 100; loss: 2.8; acc: 0.52
Batch: 120; loss: 3.31; acc: 0.58
Batch: 140; loss: 4.38; acc: 0.42
Val Epoch over. val_loss: 3.212866975243684; val_accuracy: 0.5035828025477707 

Epoch 7 start
Batch: 0; loss: 2.24; acc: 0.64
Batch: 20; loss: 2.01; acc: 0.62
Batch: 40; loss: 2.63; acc: 0.52
Batch: 60; loss: 2.61; acc: 0.52
Batch: 80; loss: 1.31; acc: 0.72
Batch: 100; loss: 2.64; acc: 0.5
Batch: 120; loss: 2.51; acc: 0.58
Batch: 140; loss: 2.66; acc: 0.48
Batch: 160; loss: 2.14; acc: 0.55
Batch: 180; loss: 2.74; acc: 0.52
Batch: 200; loss: 1.92; acc: 0.61
Batch: 220; loss: 2.33; acc: 0.5
Batch: 240; loss: 3.32; acc: 0.52
Batch: 260; loss: 1.51; acc: 0.66
Batch: 280; loss: 2.17; acc: 0.61
Batch: 300; loss: 2.19; acc: 0.56
Batch: 320; loss: 3.08; acc: 0.58
Batch: 340; loss: 2.8; acc: 0.52
Batch: 360; loss: 2.72; acc: 0.53
Batch: 380; loss: 1.64; acc: 0.59
Batch: 400; loss: 2.93; acc: 0.59
Batch: 420; loss: 3.23; acc: 0.5
Batch: 440; loss: 2.66; acc: 0.56
Batch: 460; loss: 3.13; acc: 0.52
Batch: 480; loss: 2.63; acc: 0.64
Batch: 500; loss: 2.24; acc: 0.61
Batch: 520; loss: 2.54; acc: 0.55
Batch: 540; loss: 2.72; acc: 0.52
Batch: 560; loss: 1.61; acc: 0.66
Batch: 580; loss: 1.84; acc: 0.56
Batch: 600; loss: 2.58; acc: 0.61
Batch: 620; loss: 3.49; acc: 0.41
Train Epoch over. train_loss: 2.56; train_accuracy: 0.56 

Batch: 0; loss: 2.21; acc: 0.61
Batch: 20; loss: 3.5; acc: 0.39
Batch: 40; loss: 1.3; acc: 0.7
Batch: 60; loss: 2.51; acc: 0.55
Batch: 80; loss: 2.53; acc: 0.56
Batch: 100; loss: 1.94; acc: 0.59
Batch: 120; loss: 2.7; acc: 0.56
Batch: 140; loss: 4.54; acc: 0.41
Val Epoch over. val_loss: 2.3679685091516776; val_accuracy: 0.5721536624203821 

Epoch 8 start
Batch: 0; loss: 3.11; acc: 0.53
Batch: 20; loss: 2.41; acc: 0.56
Batch: 40; loss: 1.95; acc: 0.55
Batch: 60; loss: 2.74; acc: 0.48
Batch: 80; loss: 1.85; acc: 0.58
Batch: 100; loss: 2.27; acc: 0.53
Batch: 120; loss: 2.69; acc: 0.5
Batch: 140; loss: 2.22; acc: 0.59
Batch: 160; loss: 1.84; acc: 0.69
Batch: 180; loss: 1.89; acc: 0.64
Batch: 200; loss: 2.91; acc: 0.48
Batch: 220; loss: 2.75; acc: 0.53
Batch: 240; loss: 2.05; acc: 0.58
Batch: 260; loss: 2.13; acc: 0.59
Batch: 280; loss: 2.71; acc: 0.52
Batch: 300; loss: 1.28; acc: 0.66
Batch: 320; loss: 1.64; acc: 0.67
Batch: 340; loss: 2.29; acc: 0.62
Batch: 360; loss: 3.15; acc: 0.47
Batch: 380; loss: 2.34; acc: 0.61
Batch: 400; loss: 2.27; acc: 0.59
Batch: 420; loss: 1.81; acc: 0.55
Batch: 440; loss: 2.47; acc: 0.55
Batch: 460; loss: 3.7; acc: 0.44
Batch: 480; loss: 1.91; acc: 0.64
Batch: 500; loss: 1.34; acc: 0.72
Batch: 520; loss: 2.44; acc: 0.59
Batch: 540; loss: 4.17; acc: 0.47
Batch: 560; loss: 2.4; acc: 0.52
Batch: 580; loss: 2.3; acc: 0.58
Batch: 600; loss: 2.42; acc: 0.64
Batch: 620; loss: 2.38; acc: 0.59
Train Epoch over. train_loss: 2.54; train_accuracy: 0.56 

Batch: 0; loss: 2.18; acc: 0.56
Batch: 20; loss: 3.11; acc: 0.5
Batch: 40; loss: 1.15; acc: 0.67
Batch: 60; loss: 2.03; acc: 0.55
Batch: 80; loss: 2.41; acc: 0.56
Batch: 100; loss: 2.09; acc: 0.52
Batch: 120; loss: 2.74; acc: 0.61
Batch: 140; loss: 4.19; acc: 0.38
Val Epoch over. val_loss: 2.33422816407149; val_accuracy: 0.5692675159235668 

Epoch 9 start
Batch: 0; loss: 1.37; acc: 0.66
Batch: 20; loss: 2.2; acc: 0.59
Batch: 40; loss: 2.42; acc: 0.56
Batch: 60; loss: 2.7; acc: 0.56
Batch: 80; loss: 2.41; acc: 0.56
Batch: 100; loss: 2.08; acc: 0.58
Batch: 120; loss: 2.48; acc: 0.56
Batch: 140; loss: 2.83; acc: 0.5
Batch: 160; loss: 2.6; acc: 0.53
Batch: 180; loss: 2.93; acc: 0.56
Batch: 200; loss: 2.86; acc: 0.61
Batch: 220; loss: 2.34; acc: 0.45
Batch: 240; loss: 3.29; acc: 0.55
Batch: 260; loss: 2.56; acc: 0.48
Batch: 280; loss: 2.19; acc: 0.53
Batch: 300; loss: 2.48; acc: 0.52
Batch: 320; loss: 1.58; acc: 0.67
Batch: 340; loss: 1.1; acc: 0.78
Batch: 360; loss: 2.6; acc: 0.55
Batch: 380; loss: 2.29; acc: 0.66
Batch: 400; loss: 2.46; acc: 0.53
Batch: 420; loss: 2.21; acc: 0.62
Batch: 440; loss: 2.87; acc: 0.62
Batch: 460; loss: 3.13; acc: 0.44
Batch: 480; loss: 1.86; acc: 0.73
Batch: 500; loss: 1.09; acc: 0.69
Batch: 520; loss: 3.05; acc: 0.5
Batch: 540; loss: 2.28; acc: 0.59
Batch: 560; loss: 2.94; acc: 0.58
Batch: 580; loss: 2.86; acc: 0.52
Batch: 600; loss: 2.11; acc: 0.56
Batch: 620; loss: 2.12; acc: 0.59
Train Epoch over. train_loss: 2.58; train_accuracy: 0.56 

Batch: 0; loss: 2.83; acc: 0.59
Batch: 20; loss: 2.75; acc: 0.48
Batch: 40; loss: 1.73; acc: 0.64
Batch: 60; loss: 2.42; acc: 0.64
Batch: 80; loss: 2.65; acc: 0.55
Batch: 100; loss: 2.31; acc: 0.55
Batch: 120; loss: 3.06; acc: 0.61
Batch: 140; loss: 4.14; acc: 0.41
Val Epoch over. val_loss: 2.7166243047471257; val_accuracy: 0.5600119426751592 

Epoch 10 start
Batch: 0; loss: 1.93; acc: 0.52
Batch: 20; loss: 2.2; acc: 0.64
Batch: 40; loss: 2.16; acc: 0.55
Batch: 60; loss: 2.21; acc: 0.58
Batch: 80; loss: 1.79; acc: 0.64
Batch: 100; loss: 2.84; acc: 0.56
Batch: 120; loss: 2.35; acc: 0.61
Batch: 140; loss: 3.26; acc: 0.48
Batch: 160; loss: 2.91; acc: 0.52
Batch: 180; loss: 2.49; acc: 0.58
Batch: 200; loss: 2.96; acc: 0.5
Batch: 220; loss: 2.07; acc: 0.56
Batch: 240; loss: 2.26; acc: 0.59
Batch: 260; loss: 2.31; acc: 0.53
Batch: 280; loss: 3.36; acc: 0.48
Batch: 300; loss: 2.29; acc: 0.52
Batch: 320; loss: 2.59; acc: 0.56
Batch: 340; loss: 2.09; acc: 0.59
Batch: 360; loss: 2.14; acc: 0.58
Batch: 380; loss: 1.8; acc: 0.55
Batch: 400; loss: 2.6; acc: 0.48
Batch: 420; loss: 3.04; acc: 0.58
Batch: 440; loss: 2.61; acc: 0.52
Batch: 460; loss: 2.7; acc: 0.59
Batch: 480; loss: 2.37; acc: 0.64
Batch: 500; loss: 2.58; acc: 0.58
Batch: 520; loss: 2.08; acc: 0.56
Batch: 540; loss: 2.05; acc: 0.56
Batch: 560; loss: 3.03; acc: 0.55
Batch: 580; loss: 3.15; acc: 0.47
Batch: 600; loss: 1.87; acc: 0.56
Batch: 620; loss: 2.23; acc: 0.53
Train Epoch over. train_loss: 2.58; train_accuracy: 0.55 

Batch: 0; loss: 2.13; acc: 0.56
Batch: 20; loss: 3.17; acc: 0.45
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 2.02; acc: 0.55
Batch: 80; loss: 2.75; acc: 0.53
Batch: 100; loss: 2.07; acc: 0.53
Batch: 120; loss: 2.82; acc: 0.53
Batch: 140; loss: 4.47; acc: 0.38
Val Epoch over. val_loss: 2.429247121142734; val_accuracy: 0.5483678343949044 

Epoch 11 start
Batch: 0; loss: 2.53; acc: 0.53
Batch: 20; loss: 2.35; acc: 0.55
Batch: 40; loss: 2.26; acc: 0.56
Batch: 60; loss: 1.72; acc: 0.64
Batch: 80; loss: 1.96; acc: 0.62
Batch: 100; loss: 2.07; acc: 0.56
Batch: 120; loss: 2.35; acc: 0.53
Batch: 140; loss: 1.57; acc: 0.67
Batch: 160; loss: 2.56; acc: 0.56
Batch: 180; loss: 2.83; acc: 0.47
Batch: 200; loss: 2.08; acc: 0.64
Batch: 220; loss: 2.41; acc: 0.55
Batch: 240; loss: 1.82; acc: 0.61
Batch: 260; loss: 2.14; acc: 0.59
Batch: 280; loss: 1.62; acc: 0.69
Batch: 300; loss: 1.83; acc: 0.53
Batch: 320; loss: 1.98; acc: 0.58
Batch: 340; loss: 3.47; acc: 0.52
Batch: 360; loss: 1.76; acc: 0.69
Batch: 380; loss: 2.35; acc: 0.53
Batch: 400; loss: 2.18; acc: 0.58
Batch: 420; loss: 1.88; acc: 0.59
Batch: 440; loss: 2.07; acc: 0.62
Batch: 460; loss: 1.89; acc: 0.58
Batch: 480; loss: 1.41; acc: 0.69
Batch: 500; loss: 2.65; acc: 0.55
Batch: 520; loss: 1.84; acc: 0.58
Batch: 540; loss: 1.62; acc: 0.66
Batch: 560; loss: 2.34; acc: 0.61
Batch: 580; loss: 1.03; acc: 0.77
Batch: 600; loss: 2.53; acc: 0.56
Batch: 620; loss: 1.65; acc: 0.61
Train Epoch over. train_loss: 2.05; train_accuracy: 0.6 

Batch: 0; loss: 2.01; acc: 0.62
Batch: 20; loss: 3.07; acc: 0.44
Batch: 40; loss: 0.92; acc: 0.75
Batch: 60; loss: 1.8; acc: 0.61
Batch: 80; loss: 2.09; acc: 0.56
Batch: 100; loss: 1.78; acc: 0.56
Batch: 120; loss: 2.29; acc: 0.56
Batch: 140; loss: 4.02; acc: 0.45
Val Epoch over. val_loss: 2.1041609754987585; val_accuracy: 0.591062898089172 

Epoch 12 start
Batch: 0; loss: 2.34; acc: 0.58
Batch: 20; loss: 2.82; acc: 0.52
Batch: 40; loss: 2.21; acc: 0.52
Batch: 60; loss: 1.84; acc: 0.59
Batch: 80; loss: 1.58; acc: 0.73
Batch: 100; loss: 2.36; acc: 0.61
Batch: 120; loss: 1.64; acc: 0.62
Batch: 140; loss: 1.62; acc: 0.59
Batch: 160; loss: 2.04; acc: 0.58
Batch: 180; loss: 2.1; acc: 0.59
Batch: 200; loss: 2.02; acc: 0.59
Batch: 220; loss: 1.48; acc: 0.66
Batch: 240; loss: 2.67; acc: 0.55
Batch: 260; loss: 1.66; acc: 0.69
Batch: 280; loss: 3.36; acc: 0.48
Batch: 300; loss: 2.38; acc: 0.53
Batch: 320; loss: 1.95; acc: 0.69
Batch: 340; loss: 1.98; acc: 0.58
Batch: 360; loss: 1.62; acc: 0.66
Batch: 380; loss: 2.72; acc: 0.53
Batch: 400; loss: 2.12; acc: 0.55
Batch: 420; loss: 2.22; acc: 0.56
Batch: 440; loss: 1.67; acc: 0.66
Batch: 460; loss: 1.71; acc: 0.61
Batch: 480; loss: 2.0; acc: 0.59
Batch: 500; loss: 1.96; acc: 0.7
Batch: 520; loss: 2.11; acc: 0.62
Batch: 540; loss: 2.29; acc: 0.58
Batch: 560; loss: 1.76; acc: 0.7
Batch: 580; loss: 2.16; acc: 0.59
Batch: 600; loss: 1.57; acc: 0.61
Batch: 620; loss: 2.59; acc: 0.5
Train Epoch over. train_loss: 2.02; train_accuracy: 0.6 

Batch: 0; loss: 1.91; acc: 0.66
Batch: 20; loss: 3.2; acc: 0.42
Batch: 40; loss: 0.94; acc: 0.72
Batch: 60; loss: 1.87; acc: 0.59
Batch: 80; loss: 2.1; acc: 0.56
Batch: 100; loss: 1.79; acc: 0.59
Batch: 120; loss: 2.23; acc: 0.61
Batch: 140; loss: 3.75; acc: 0.41
Val Epoch over. val_loss: 2.112056356706437; val_accuracy: 0.5896695859872612 

Epoch 13 start
Batch: 0; loss: 2.49; acc: 0.59
Batch: 20; loss: 1.83; acc: 0.62
Batch: 40; loss: 2.87; acc: 0.61
Batch: 60; loss: 1.76; acc: 0.59
Batch: 80; loss: 2.35; acc: 0.59
Batch: 100; loss: 2.13; acc: 0.61
Batch: 120; loss: 2.27; acc: 0.62
Batch: 140; loss: 1.73; acc: 0.66
Batch: 160; loss: 1.38; acc: 0.64
Batch: 180; loss: 1.83; acc: 0.56
Batch: 200; loss: 1.24; acc: 0.75
Batch: 220; loss: 1.32; acc: 0.73
Batch: 240; loss: 1.96; acc: 0.58
Batch: 260; loss: 2.74; acc: 0.55
Batch: 280; loss: 2.18; acc: 0.64
Batch: 300; loss: 1.62; acc: 0.67
Batch: 320; loss: 2.11; acc: 0.64
Batch: 340; loss: 1.81; acc: 0.64
Batch: 360; loss: 1.44; acc: 0.66
Batch: 380; loss: 2.05; acc: 0.62
Batch: 400; loss: 1.49; acc: 0.64
Batch: 420; loss: 1.43; acc: 0.75
Batch: 440; loss: 1.63; acc: 0.66
Batch: 460; loss: 1.51; acc: 0.64
Batch: 480; loss: 1.38; acc: 0.77
Batch: 500; loss: 2.05; acc: 0.53
Batch: 520; loss: 1.14; acc: 0.7
Batch: 540; loss: 2.0; acc: 0.66
Batch: 560; loss: 1.71; acc: 0.62
Batch: 580; loss: 1.78; acc: 0.67
Batch: 600; loss: 1.94; acc: 0.62
Batch: 620; loss: 1.85; acc: 0.67
Train Epoch over. train_loss: 2.02; train_accuracy: 0.6 

Batch: 0; loss: 1.86; acc: 0.64
Batch: 20; loss: 2.97; acc: 0.41
Batch: 40; loss: 0.93; acc: 0.72
Batch: 60; loss: 1.88; acc: 0.58
Batch: 80; loss: 2.16; acc: 0.56
Batch: 100; loss: 1.78; acc: 0.59
Batch: 120; loss: 2.33; acc: 0.59
Batch: 140; loss: 3.79; acc: 0.48
Val Epoch over. val_loss: 2.079815058571518; val_accuracy: 0.6013136942675159 

Epoch 14 start
Batch: 0; loss: 2.5; acc: 0.62
Batch: 20; loss: 2.13; acc: 0.59
Batch: 40; loss: 2.78; acc: 0.58
Batch: 60; loss: 1.98; acc: 0.64
Batch: 80; loss: 1.9; acc: 0.62
Batch: 100; loss: 2.18; acc: 0.59
Batch: 120; loss: 2.27; acc: 0.47
Batch: 140; loss: 1.49; acc: 0.73
Batch: 160; loss: 2.22; acc: 0.62
Batch: 180; loss: 1.66; acc: 0.67
Batch: 200; loss: 2.13; acc: 0.59
Batch: 220; loss: 2.15; acc: 0.58
Batch: 240; loss: 1.82; acc: 0.67
Batch: 260; loss: 1.8; acc: 0.56
Batch: 280; loss: 1.63; acc: 0.67
Batch: 300; loss: 1.51; acc: 0.69
Batch: 320; loss: 2.24; acc: 0.55
Batch: 340; loss: 2.01; acc: 0.59
Batch: 360; loss: 2.0; acc: 0.61
Batch: 380; loss: 1.92; acc: 0.62
Batch: 400; loss: 1.67; acc: 0.58
Batch: 420; loss: 2.54; acc: 0.61
Batch: 440; loss: 1.67; acc: 0.66
Batch: 460; loss: 1.43; acc: 0.7
Batch: 480; loss: 2.33; acc: 0.53
Batch: 500; loss: 2.11; acc: 0.64
Batch: 520; loss: 2.23; acc: 0.59
Batch: 540; loss: 1.68; acc: 0.64
Batch: 560; loss: 2.24; acc: 0.62
Batch: 580; loss: 1.09; acc: 0.69
Batch: 600; loss: 2.31; acc: 0.47
Batch: 620; loss: 2.11; acc: 0.58
Train Epoch over. train_loss: 2.02; train_accuracy: 0.61 

Batch: 0; loss: 1.87; acc: 0.67
Batch: 20; loss: 3.07; acc: 0.41
Batch: 40; loss: 0.92; acc: 0.7
Batch: 60; loss: 1.81; acc: 0.59
Batch: 80; loss: 2.12; acc: 0.58
Batch: 100; loss: 1.78; acc: 0.58
Batch: 120; loss: 2.25; acc: 0.58
Batch: 140; loss: 3.72; acc: 0.5
Val Epoch over. val_loss: 2.0886567853818274; val_accuracy: 0.5991242038216561 

Epoch 15 start
Batch: 0; loss: 1.89; acc: 0.52
Batch: 20; loss: 2.11; acc: 0.53
Batch: 40; loss: 1.52; acc: 0.64
Batch: 60; loss: 1.56; acc: 0.61
Batch: 80; loss: 2.03; acc: 0.62
Batch: 100; loss: 2.92; acc: 0.5
Batch: 120; loss: 3.83; acc: 0.48
Batch: 140; loss: 2.27; acc: 0.56
Batch: 160; loss: 2.88; acc: 0.58
Batch: 180; loss: 2.31; acc: 0.56
Batch: 200; loss: 1.41; acc: 0.66
Batch: 220; loss: 2.14; acc: 0.61
Batch: 240; loss: 2.67; acc: 0.53
Batch: 260; loss: 1.86; acc: 0.64
Batch: 280; loss: 2.09; acc: 0.59
Batch: 300; loss: 2.23; acc: 0.59
Batch: 320; loss: 2.42; acc: 0.59
Batch: 340; loss: 1.85; acc: 0.58
Batch: 360; loss: 1.6; acc: 0.66
Batch: 380; loss: 2.08; acc: 0.59
Batch: 400; loss: 2.63; acc: 0.47
Batch: 420; loss: 2.1; acc: 0.55
Batch: 440; loss: 1.52; acc: 0.66
Batch: 460; loss: 1.36; acc: 0.67
Batch: 480; loss: 2.19; acc: 0.55
Batch: 500; loss: 2.89; acc: 0.62
Batch: 520; loss: 2.17; acc: 0.58
Batch: 540; loss: 1.52; acc: 0.61
Batch: 560; loss: 2.86; acc: 0.48
Batch: 580; loss: 2.51; acc: 0.52
Batch: 600; loss: 1.51; acc: 0.72
Batch: 620; loss: 2.99; acc: 0.47
Train Epoch over. train_loss: 2.02; train_accuracy: 0.61 

Batch: 0; loss: 1.85; acc: 0.64
Batch: 20; loss: 3.19; acc: 0.44
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 1.83; acc: 0.59
Batch: 80; loss: 2.13; acc: 0.56
Batch: 100; loss: 1.8; acc: 0.62
Batch: 120; loss: 2.2; acc: 0.56
Batch: 140; loss: 3.8; acc: 0.5
Val Epoch over. val_loss: 2.0908604241480493; val_accuracy: 0.601015127388535 

Epoch 16 start
Batch: 0; loss: 2.62; acc: 0.58
Batch: 20; loss: 2.06; acc: 0.58
Batch: 40; loss: 1.67; acc: 0.61
Batch: 60; loss: 2.39; acc: 0.55
Batch: 80; loss: 2.41; acc: 0.52
Batch: 100; loss: 1.72; acc: 0.61
Batch: 120; loss: 1.63; acc: 0.66
Batch: 140; loss: 1.87; acc: 0.67
Batch: 160; loss: 2.05; acc: 0.61
Batch: 180; loss: 1.94; acc: 0.61
Batch: 200; loss: 1.69; acc: 0.69
Batch: 220; loss: 1.84; acc: 0.62
Batch: 240; loss: 2.2; acc: 0.61
Batch: 260; loss: 2.19; acc: 0.62
Batch: 280; loss: 2.43; acc: 0.61
Batch: 300; loss: 1.53; acc: 0.66
Batch: 320; loss: 2.32; acc: 0.55
Batch: 340; loss: 2.53; acc: 0.59
Batch: 360; loss: 2.37; acc: 0.52
Batch: 380; loss: 1.83; acc: 0.64
Batch: 400; loss: 1.77; acc: 0.62
Batch: 420; loss: 1.55; acc: 0.67
Batch: 440; loss: 1.71; acc: 0.7
Batch: 460; loss: 1.86; acc: 0.72
Batch: 480; loss: 2.1; acc: 0.64
Batch: 500; loss: 1.34; acc: 0.67
Batch: 520; loss: 1.68; acc: 0.7
Batch: 540; loss: 1.42; acc: 0.7
Batch: 560; loss: 2.07; acc: 0.62
Batch: 580; loss: 2.52; acc: 0.61
Batch: 600; loss: 1.84; acc: 0.62
Batch: 620; loss: 2.62; acc: 0.55
Train Epoch over. train_loss: 2.02; train_accuracy: 0.61 

Batch: 0; loss: 1.88; acc: 0.62
Batch: 20; loss: 2.97; acc: 0.42
Batch: 40; loss: 0.91; acc: 0.73
Batch: 60; loss: 1.88; acc: 0.56
Batch: 80; loss: 2.18; acc: 0.55
Batch: 100; loss: 1.76; acc: 0.58
Batch: 120; loss: 2.25; acc: 0.59
Batch: 140; loss: 3.73; acc: 0.47
Val Epoch over. val_loss: 2.0894545217987837; val_accuracy: 0.5979299363057324 

Epoch 17 start
Batch: 0; loss: 1.43; acc: 0.67
Batch: 20; loss: 1.93; acc: 0.66
Batch: 40; loss: 1.54; acc: 0.69
Batch: 60; loss: 2.32; acc: 0.5
Batch: 80; loss: 1.85; acc: 0.59
Batch: 100; loss: 1.47; acc: 0.69
Batch: 120; loss: 2.17; acc: 0.58
Batch: 140; loss: 1.66; acc: 0.62
Batch: 160; loss: 1.98; acc: 0.61
Batch: 180; loss: 2.13; acc: 0.62
Batch: 200; loss: 2.53; acc: 0.59
Batch: 220; loss: 1.87; acc: 0.61
Batch: 240; loss: 1.66; acc: 0.58
Batch: 260; loss: 2.62; acc: 0.5
Batch: 280; loss: 2.09; acc: 0.52
Batch: 300; loss: 1.84; acc: 0.64
Batch: 320; loss: 2.5; acc: 0.53
Batch: 340; loss: 2.09; acc: 0.67
Batch: 360; loss: 2.57; acc: 0.58
Batch: 380; loss: 2.15; acc: 0.56
Batch: 400; loss: 1.83; acc: 0.59
Batch: 420; loss: 2.53; acc: 0.56
Batch: 440; loss: 1.61; acc: 0.62
Batch: 460; loss: 1.6; acc: 0.61
Batch: 480; loss: 2.31; acc: 0.53
Batch: 500; loss: 2.27; acc: 0.56
Batch: 520; loss: 2.64; acc: 0.58
Batch: 540; loss: 2.57; acc: 0.52
Batch: 560; loss: 2.32; acc: 0.56
Batch: 580; loss: 2.2; acc: 0.58
Batch: 600; loss: 1.67; acc: 0.64
Batch: 620; loss: 2.43; acc: 0.61
Train Epoch over. train_loss: 2.02; train_accuracy: 0.61 

Batch: 0; loss: 1.8; acc: 0.62
Batch: 20; loss: 3.08; acc: 0.36
Batch: 40; loss: 0.94; acc: 0.72
Batch: 60; loss: 1.97; acc: 0.58
Batch: 80; loss: 2.15; acc: 0.55
Batch: 100; loss: 1.76; acc: 0.59
Batch: 120; loss: 2.21; acc: 0.58
Batch: 140; loss: 3.83; acc: 0.45
Val Epoch over. val_loss: 2.0883056206308352; val_accuracy: 0.5987261146496815 

Epoch 18 start
Batch: 0; loss: 1.15; acc: 0.67
Batch: 20; loss: 1.85; acc: 0.55
Batch: 40; loss: 2.2; acc: 0.59
Batch: 60; loss: 2.57; acc: 0.62
Batch: 80; loss: 1.76; acc: 0.69
Batch: 100; loss: 1.31; acc: 0.62
Batch: 120; loss: 2.03; acc: 0.59
Batch: 140; loss: 2.21; acc: 0.56
Batch: 160; loss: 1.81; acc: 0.64
Batch: 180; loss: 2.13; acc: 0.52
Batch: 200; loss: 1.52; acc: 0.59
Batch: 220; loss: 2.75; acc: 0.56
Batch: 240; loss: 2.16; acc: 0.61
Batch: 260; loss: 2.69; acc: 0.55
Batch: 280; loss: 1.85; acc: 0.67
Batch: 300; loss: 2.9; acc: 0.47
Batch: 320; loss: 1.3; acc: 0.66
Batch: 340; loss: 2.04; acc: 0.61
Batch: 360; loss: 2.51; acc: 0.56
Batch: 380; loss: 2.21; acc: 0.56
Batch: 400; loss: 1.7; acc: 0.69
Batch: 420; loss: 2.39; acc: 0.62
Batch: 440; loss: 1.79; acc: 0.59
Batch: 460; loss: 1.99; acc: 0.55
Batch: 480; loss: 1.75; acc: 0.73
Batch: 500; loss: 2.38; acc: 0.58
Batch: 520; loss: 2.33; acc: 0.61
Batch: 540; loss: 2.22; acc: 0.62
Batch: 560; loss: 1.81; acc: 0.64
Batch: 580; loss: 2.52; acc: 0.55
Batch: 600; loss: 2.11; acc: 0.64
Batch: 620; loss: 1.86; acc: 0.69
Train Epoch over. train_loss: 2.02; train_accuracy: 0.61 

Batch: 0; loss: 1.87; acc: 0.62
Batch: 20; loss: 3.05; acc: 0.44
Batch: 40; loss: 1.01; acc: 0.72
Batch: 60; loss: 1.95; acc: 0.59
Batch: 80; loss: 2.1; acc: 0.55
Batch: 100; loss: 1.78; acc: 0.61
Batch: 120; loss: 2.19; acc: 0.58
Batch: 140; loss: 3.75; acc: 0.48
Val Epoch over. val_loss: 2.0782214946048274; val_accuracy: 0.598328025477707 

Epoch 19 start
Batch: 0; loss: 1.82; acc: 0.58
Batch: 20; loss: 1.17; acc: 0.69
Batch: 40; loss: 2.12; acc: 0.55
Batch: 60; loss: 2.2; acc: 0.61
Batch: 80; loss: 1.37; acc: 0.59
Batch: 100; loss: 2.24; acc: 0.55
Batch: 120; loss: 2.64; acc: 0.55
Batch: 140; loss: 1.89; acc: 0.59
Batch: 160; loss: 2.61; acc: 0.5
Batch: 180; loss: 2.08; acc: 0.56
Batch: 200; loss: 1.9; acc: 0.66
Batch: 220; loss: 1.84; acc: 0.59
Batch: 240; loss: 1.79; acc: 0.61
Batch: 260; loss: 2.48; acc: 0.58
Batch: 280; loss: 2.08; acc: 0.58
Batch: 300; loss: 1.75; acc: 0.64
Batch: 320; loss: 2.93; acc: 0.5
Batch: 340; loss: 1.15; acc: 0.7
Batch: 360; loss: 2.13; acc: 0.58
Batch: 380; loss: 1.47; acc: 0.7
Batch: 400; loss: 1.95; acc: 0.66
Batch: 420; loss: 2.42; acc: 0.58
Batch: 440; loss: 1.57; acc: 0.67
Batch: 460; loss: 1.51; acc: 0.66
Batch: 480; loss: 2.33; acc: 0.62
Batch: 500; loss: 1.65; acc: 0.69
Batch: 520; loss: 2.25; acc: 0.56
Batch: 540; loss: 2.55; acc: 0.55
Batch: 560; loss: 2.1; acc: 0.59
Batch: 580; loss: 2.07; acc: 0.62
Batch: 600; loss: 2.02; acc: 0.61
Batch: 620; loss: 2.31; acc: 0.55
Train Epoch over. train_loss: 2.02; train_accuracy: 0.61 

Batch: 0; loss: 1.82; acc: 0.62
Batch: 20; loss: 3.31; acc: 0.41
Batch: 40; loss: 0.94; acc: 0.72
Batch: 60; loss: 1.94; acc: 0.61
Batch: 80; loss: 2.05; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.61
Batch: 120; loss: 2.28; acc: 0.59
Batch: 140; loss: 3.8; acc: 0.48
Val Epoch over. val_loss: 2.0890056528862875; val_accuracy: 0.5958399681528662 

Epoch 20 start
Batch: 0; loss: 2.36; acc: 0.56
Batch: 20; loss: 1.99; acc: 0.64
Batch: 40; loss: 2.66; acc: 0.55
Batch: 60; loss: 1.85; acc: 0.61
Batch: 80; loss: 2.49; acc: 0.55
Batch: 100; loss: 1.94; acc: 0.67
Batch: 120; loss: 2.13; acc: 0.61
Batch: 140; loss: 2.27; acc: 0.52
Batch: 160; loss: 2.55; acc: 0.48
Batch: 180; loss: 2.73; acc: 0.53
Batch: 200; loss: 1.72; acc: 0.58
Batch: 220; loss: 2.06; acc: 0.55
Batch: 240; loss: 1.97; acc: 0.56
Batch: 260; loss: 2.52; acc: 0.52
Batch: 280; loss: 2.01; acc: 0.62
Batch: 300; loss: 2.74; acc: 0.48
Batch: 320; loss: 1.56; acc: 0.62
Batch: 340; loss: 1.33; acc: 0.67
Batch: 360; loss: 2.08; acc: 0.62
Batch: 380; loss: 2.25; acc: 0.55
Batch: 400; loss: 2.13; acc: 0.59
Batch: 420; loss: 2.58; acc: 0.55
Batch: 440; loss: 1.62; acc: 0.64
Batch: 460; loss: 2.05; acc: 0.64
Batch: 480; loss: 1.63; acc: 0.61
Batch: 500; loss: 1.93; acc: 0.62
Batch: 520; loss: 1.98; acc: 0.66
Batch: 540; loss: 2.48; acc: 0.58
Batch: 560; loss: 2.03; acc: 0.62
Batch: 580; loss: 1.07; acc: 0.66
Batch: 600; loss: 2.17; acc: 0.59
Batch: 620; loss: 2.27; acc: 0.58
Train Epoch over. train_loss: 2.02; train_accuracy: 0.61 

Batch: 0; loss: 1.9; acc: 0.61
Batch: 20; loss: 3.06; acc: 0.39
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 1.81; acc: 0.58
Batch: 80; loss: 2.15; acc: 0.59
Batch: 100; loss: 1.75; acc: 0.59
Batch: 120; loss: 2.25; acc: 0.55
Batch: 140; loss: 3.76; acc: 0.48
Val Epoch over. val_loss: 2.0843792064174727; val_accuracy: 0.5998208598726115 

Epoch 21 start
Batch: 0; loss: 1.52; acc: 0.64
Batch: 20; loss: 1.95; acc: 0.59
Batch: 40; loss: 1.22; acc: 0.7
Batch: 60; loss: 2.2; acc: 0.55
Batch: 80; loss: 2.18; acc: 0.59
Batch: 100; loss: 1.77; acc: 0.55
Batch: 120; loss: 2.37; acc: 0.62
Batch: 140; loss: 1.74; acc: 0.67
Batch: 160; loss: 1.81; acc: 0.67
Batch: 180; loss: 2.59; acc: 0.52
Batch: 200; loss: 1.95; acc: 0.62
Batch: 220; loss: 1.69; acc: 0.59
Batch: 240; loss: 2.68; acc: 0.48
Batch: 260; loss: 1.47; acc: 0.67
Batch: 280; loss: 2.34; acc: 0.55
Batch: 300; loss: 2.09; acc: 0.62
Batch: 320; loss: 2.3; acc: 0.55
Batch: 340; loss: 2.31; acc: 0.58
Batch: 360; loss: 1.99; acc: 0.62
Batch: 380; loss: 2.18; acc: 0.58
Batch: 400; loss: 2.38; acc: 0.53
Batch: 420; loss: 1.97; acc: 0.56
Batch: 440; loss: 2.31; acc: 0.59
Batch: 460; loss: 1.26; acc: 0.75
Batch: 480; loss: 2.07; acc: 0.61
Batch: 500; loss: 2.65; acc: 0.55
Batch: 520; loss: 2.86; acc: 0.56
Batch: 540; loss: 2.86; acc: 0.48
Batch: 560; loss: 2.16; acc: 0.58
Batch: 580; loss: 2.1; acc: 0.53
Batch: 600; loss: 2.38; acc: 0.58
Batch: 620; loss: 1.93; acc: 0.58
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.85; acc: 0.62
Batch: 20; loss: 3.11; acc: 0.38
Batch: 40; loss: 0.94; acc: 0.73
Batch: 60; loss: 1.87; acc: 0.62
Batch: 80; loss: 2.15; acc: 0.58
Batch: 100; loss: 1.75; acc: 0.61
Batch: 120; loss: 2.21; acc: 0.56
Batch: 140; loss: 3.76; acc: 0.52
Val Epoch over. val_loss: 2.0733660406367793; val_accuracy: 0.6008160828025477 

Epoch 22 start
Batch: 0; loss: 2.14; acc: 0.56
Batch: 20; loss: 1.75; acc: 0.64
Batch: 40; loss: 2.33; acc: 0.56
Batch: 60; loss: 3.0; acc: 0.58
Batch: 80; loss: 2.7; acc: 0.59
Batch: 100; loss: 1.76; acc: 0.62
Batch: 120; loss: 2.37; acc: 0.55
Batch: 140; loss: 2.81; acc: 0.52
Batch: 160; loss: 2.56; acc: 0.5
Batch: 180; loss: 1.63; acc: 0.69
Batch: 200; loss: 1.77; acc: 0.58
Batch: 220; loss: 2.31; acc: 0.66
Batch: 240; loss: 2.46; acc: 0.59
Batch: 260; loss: 1.81; acc: 0.59
Batch: 280; loss: 1.91; acc: 0.58
Batch: 300; loss: 2.35; acc: 0.53
Batch: 320; loss: 1.91; acc: 0.62
Batch: 340; loss: 1.73; acc: 0.64
Batch: 360; loss: 1.96; acc: 0.61
Batch: 380; loss: 1.69; acc: 0.59
Batch: 400; loss: 1.82; acc: 0.62
Batch: 420; loss: 1.71; acc: 0.62
Batch: 440; loss: 2.23; acc: 0.62
Batch: 460; loss: 1.71; acc: 0.69
Batch: 480; loss: 3.24; acc: 0.42
Batch: 500; loss: 1.91; acc: 0.67
Batch: 520; loss: 1.89; acc: 0.64
Batch: 540; loss: 1.66; acc: 0.58
Batch: 560; loss: 2.17; acc: 0.64
Batch: 580; loss: 2.43; acc: 0.56
Batch: 600; loss: 2.48; acc: 0.52
Batch: 620; loss: 1.64; acc: 0.64
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.84; acc: 0.61
Batch: 20; loss: 3.11; acc: 0.38
Batch: 40; loss: 0.95; acc: 0.73
Batch: 60; loss: 1.88; acc: 0.62
Batch: 80; loss: 2.16; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.61
Batch: 120; loss: 2.2; acc: 0.58
Batch: 140; loss: 3.75; acc: 0.52
Val Epoch over. val_loss: 2.0734425517404156; val_accuracy: 0.5998208598726115 

Epoch 23 start
Batch: 0; loss: 2.17; acc: 0.55
Batch: 20; loss: 2.65; acc: 0.52
Batch: 40; loss: 1.61; acc: 0.7
Batch: 60; loss: 2.69; acc: 0.58
Batch: 80; loss: 1.87; acc: 0.64
Batch: 100; loss: 2.31; acc: 0.55
Batch: 120; loss: 2.62; acc: 0.56
Batch: 140; loss: 1.65; acc: 0.69
Batch: 160; loss: 1.96; acc: 0.59
Batch: 180; loss: 2.49; acc: 0.5
Batch: 200; loss: 1.65; acc: 0.7
Batch: 220; loss: 1.95; acc: 0.59
Batch: 240; loss: 1.98; acc: 0.64
Batch: 260; loss: 1.33; acc: 0.75
Batch: 280; loss: 2.38; acc: 0.61
Batch: 300; loss: 2.53; acc: 0.55
Batch: 320; loss: 1.91; acc: 0.61
Batch: 340; loss: 1.67; acc: 0.52
Batch: 360; loss: 1.6; acc: 0.59
Batch: 380; loss: 1.68; acc: 0.62
Batch: 400; loss: 2.1; acc: 0.61
Batch: 420; loss: 1.87; acc: 0.56
Batch: 440; loss: 1.9; acc: 0.61
Batch: 460; loss: 1.77; acc: 0.67
Batch: 480; loss: 2.24; acc: 0.69
Batch: 500; loss: 1.36; acc: 0.66
Batch: 520; loss: 1.99; acc: 0.58
Batch: 540; loss: 2.56; acc: 0.53
Batch: 560; loss: 1.61; acc: 0.66
Batch: 580; loss: 2.3; acc: 0.62
Batch: 600; loss: 2.06; acc: 0.61
Batch: 620; loss: 1.83; acc: 0.58
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.85; acc: 0.62
Batch: 20; loss: 3.09; acc: 0.39
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.89; acc: 0.61
Batch: 80; loss: 2.15; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.61
Batch: 120; loss: 2.22; acc: 0.56
Batch: 140; loss: 3.79; acc: 0.5
Val Epoch over. val_loss: 2.073504201925484; val_accuracy: 0.6003184713375797 

Epoch 24 start
Batch: 0; loss: 2.84; acc: 0.56
Batch: 20; loss: 0.89; acc: 0.78
Batch: 40; loss: 2.43; acc: 0.5
Batch: 60; loss: 2.59; acc: 0.55
Batch: 80; loss: 2.09; acc: 0.62
Batch: 100; loss: 3.02; acc: 0.48
Batch: 120; loss: 1.89; acc: 0.61
Batch: 140; loss: 1.32; acc: 0.72
Batch: 160; loss: 1.25; acc: 0.72
Batch: 180; loss: 1.88; acc: 0.64
Batch: 200; loss: 2.4; acc: 0.47
Batch: 220; loss: 2.65; acc: 0.55
Batch: 240; loss: 2.55; acc: 0.58
Batch: 260; loss: 1.68; acc: 0.55
Batch: 280; loss: 2.34; acc: 0.58
Batch: 300; loss: 1.61; acc: 0.64
Batch: 320; loss: 1.76; acc: 0.61
Batch: 340; loss: 1.89; acc: 0.58
Batch: 360; loss: 2.0; acc: 0.48
Batch: 380; loss: 1.37; acc: 0.61
Batch: 400; loss: 1.51; acc: 0.59
Batch: 420; loss: 2.17; acc: 0.64
Batch: 440; loss: 2.04; acc: 0.56
Batch: 460; loss: 1.88; acc: 0.58
Batch: 480; loss: 1.89; acc: 0.62
Batch: 500; loss: 1.95; acc: 0.59
Batch: 520; loss: 1.56; acc: 0.66
Batch: 540; loss: 1.06; acc: 0.66
Batch: 560; loss: 2.32; acc: 0.62
Batch: 580; loss: 2.2; acc: 0.58
Batch: 600; loss: 2.0; acc: 0.56
Batch: 620; loss: 2.92; acc: 0.55
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.86; acc: 0.64
Batch: 20; loss: 3.08; acc: 0.39
Batch: 40; loss: 0.94; acc: 0.72
Batch: 60; loss: 1.86; acc: 0.62
Batch: 80; loss: 2.16; acc: 0.56
Batch: 100; loss: 1.76; acc: 0.59
Batch: 120; loss: 2.19; acc: 0.55
Batch: 140; loss: 3.71; acc: 0.48
Val Epoch over. val_loss: 2.0750853954606754; val_accuracy: 0.6000199044585988 

Epoch 25 start
Batch: 0; loss: 1.42; acc: 0.72
Batch: 20; loss: 1.52; acc: 0.64
Batch: 40; loss: 2.25; acc: 0.64
Batch: 60; loss: 2.42; acc: 0.48
Batch: 80; loss: 1.88; acc: 0.66
Batch: 100; loss: 1.74; acc: 0.62
Batch: 120; loss: 2.98; acc: 0.47
Batch: 140; loss: 1.64; acc: 0.59
Batch: 160; loss: 2.2; acc: 0.59
Batch: 180; loss: 1.44; acc: 0.72
Batch: 200; loss: 1.56; acc: 0.69
Batch: 220; loss: 1.65; acc: 0.64
Batch: 240; loss: 2.34; acc: 0.59
Batch: 260; loss: 2.31; acc: 0.48
Batch: 280; loss: 1.68; acc: 0.67
Batch: 300; loss: 1.66; acc: 0.69
Batch: 320; loss: 1.44; acc: 0.67
Batch: 340; loss: 1.46; acc: 0.69
Batch: 360; loss: 1.36; acc: 0.7
Batch: 380; loss: 1.6; acc: 0.61
Batch: 400; loss: 1.45; acc: 0.67
Batch: 420; loss: 2.25; acc: 0.58
Batch: 440; loss: 2.33; acc: 0.56
Batch: 460; loss: 2.49; acc: 0.61
Batch: 480; loss: 2.17; acc: 0.59
Batch: 500; loss: 2.21; acc: 0.56
Batch: 520; loss: 2.0; acc: 0.47
Batch: 540; loss: 2.17; acc: 0.64
Batch: 560; loss: 2.27; acc: 0.64
Batch: 580; loss: 2.43; acc: 0.53
Batch: 600; loss: 2.38; acc: 0.61
Batch: 620; loss: 1.29; acc: 0.72
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.85; acc: 0.61
Batch: 20; loss: 3.09; acc: 0.39
Batch: 40; loss: 0.96; acc: 0.72
Batch: 60; loss: 1.88; acc: 0.61
Batch: 80; loss: 2.15; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.61
Batch: 120; loss: 2.2; acc: 0.58
Batch: 140; loss: 3.75; acc: 0.52
Val Epoch over. val_loss: 2.0729199260663074; val_accuracy: 0.6004179936305732 

Epoch 26 start
Batch: 0; loss: 2.32; acc: 0.62
Batch: 20; loss: 1.55; acc: 0.64
Batch: 40; loss: 2.63; acc: 0.56
Batch: 60; loss: 1.68; acc: 0.67
Batch: 80; loss: 2.37; acc: 0.61
Batch: 100; loss: 1.86; acc: 0.59
Batch: 120; loss: 2.75; acc: 0.53
Batch: 140; loss: 2.06; acc: 0.66
Batch: 160; loss: 1.97; acc: 0.55
Batch: 180; loss: 2.7; acc: 0.53
Batch: 200; loss: 1.59; acc: 0.64
Batch: 220; loss: 1.96; acc: 0.58
Batch: 240; loss: 1.59; acc: 0.58
Batch: 260; loss: 2.3; acc: 0.55
Batch: 280; loss: 1.68; acc: 0.64
Batch: 300; loss: 2.72; acc: 0.5
Batch: 320; loss: 2.08; acc: 0.67
Batch: 340; loss: 2.18; acc: 0.64
Batch: 360; loss: 2.01; acc: 0.67
Batch: 380; loss: 2.46; acc: 0.58
Batch: 400; loss: 2.04; acc: 0.69
Batch: 420; loss: 2.21; acc: 0.59
Batch: 440; loss: 1.45; acc: 0.7
Batch: 460; loss: 2.19; acc: 0.59
Batch: 480; loss: 1.41; acc: 0.67
Batch: 500; loss: 2.25; acc: 0.56
Batch: 520; loss: 2.16; acc: 0.52
Batch: 540; loss: 2.53; acc: 0.55
Batch: 560; loss: 2.8; acc: 0.52
Batch: 580; loss: 2.27; acc: 0.64
Batch: 600; loss: 1.83; acc: 0.61
Batch: 620; loss: 1.42; acc: 0.66
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.84; acc: 0.61
Batch: 20; loss: 3.07; acc: 0.41
Batch: 40; loss: 0.97; acc: 0.73
Batch: 60; loss: 1.88; acc: 0.62
Batch: 80; loss: 2.16; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.61
Batch: 120; loss: 2.21; acc: 0.58
Batch: 140; loss: 3.76; acc: 0.52
Val Epoch over. val_loss: 2.0735296307096056; val_accuracy: 0.6006170382165605 

Epoch 27 start
Batch: 0; loss: 2.2; acc: 0.52
Batch: 20; loss: 1.82; acc: 0.61
Batch: 40; loss: 1.87; acc: 0.64
Batch: 60; loss: 1.96; acc: 0.58
Batch: 80; loss: 1.63; acc: 0.72
Batch: 100; loss: 2.86; acc: 0.53
Batch: 120; loss: 1.46; acc: 0.75
Batch: 140; loss: 1.75; acc: 0.66
Batch: 160; loss: 1.01; acc: 0.75
Batch: 180; loss: 2.42; acc: 0.67
Batch: 200; loss: 2.26; acc: 0.55
Batch: 220; loss: 1.71; acc: 0.58
Batch: 240; loss: 2.37; acc: 0.59
Batch: 260; loss: 1.44; acc: 0.7
Batch: 280; loss: 1.62; acc: 0.69
Batch: 300; loss: 2.58; acc: 0.55
Batch: 320; loss: 1.71; acc: 0.64
Batch: 340; loss: 1.86; acc: 0.61
Batch: 360; loss: 2.18; acc: 0.59
Batch: 380; loss: 1.93; acc: 0.58
Batch: 400; loss: 2.68; acc: 0.62
Batch: 420; loss: 2.85; acc: 0.62
Batch: 440; loss: 2.35; acc: 0.56
Batch: 460; loss: 1.35; acc: 0.72
Batch: 480; loss: 1.48; acc: 0.67
Batch: 500; loss: 1.99; acc: 0.58
Batch: 520; loss: 2.58; acc: 0.53
Batch: 540; loss: 1.4; acc: 0.66
Batch: 560; loss: 2.23; acc: 0.53
Batch: 580; loss: 1.69; acc: 0.62
Batch: 600; loss: 2.14; acc: 0.58
Batch: 620; loss: 1.7; acc: 0.64
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.85; acc: 0.62
Batch: 20; loss: 3.06; acc: 0.38
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.88; acc: 0.61
Batch: 80; loss: 2.15; acc: 0.56
Batch: 100; loss: 1.76; acc: 0.61
Batch: 120; loss: 2.21; acc: 0.56
Batch: 140; loss: 3.77; acc: 0.5
Val Epoch over. val_loss: 2.0729304077518975; val_accuracy: 0.601015127388535 

Epoch 28 start
Batch: 0; loss: 1.77; acc: 0.67
Batch: 20; loss: 2.2; acc: 0.58
Batch: 40; loss: 2.34; acc: 0.64
Batch: 60; loss: 1.63; acc: 0.7
Batch: 80; loss: 1.37; acc: 0.7
Batch: 100; loss: 1.71; acc: 0.58
Batch: 120; loss: 2.31; acc: 0.62
Batch: 140; loss: 3.07; acc: 0.5
Batch: 160; loss: 2.25; acc: 0.5
Batch: 180; loss: 1.72; acc: 0.56
Batch: 200; loss: 1.39; acc: 0.7
Batch: 220; loss: 1.54; acc: 0.67
Batch: 240; loss: 1.1; acc: 0.77
Batch: 260; loss: 1.91; acc: 0.53
Batch: 280; loss: 2.03; acc: 0.61
Batch: 300; loss: 2.03; acc: 0.55
Batch: 320; loss: 2.34; acc: 0.58
Batch: 340; loss: 2.33; acc: 0.59
Batch: 360; loss: 2.11; acc: 0.66
Batch: 380; loss: 1.86; acc: 0.62
Batch: 400; loss: 2.49; acc: 0.53
Batch: 420; loss: 1.92; acc: 0.64
Batch: 440; loss: 1.69; acc: 0.62
Batch: 460; loss: 1.66; acc: 0.64
Batch: 480; loss: 2.08; acc: 0.64
Batch: 500; loss: 2.31; acc: 0.59
Batch: 520; loss: 1.99; acc: 0.66
Batch: 540; loss: 1.75; acc: 0.67
Batch: 560; loss: 1.41; acc: 0.61
Batch: 580; loss: 1.84; acc: 0.67
Batch: 600; loss: 2.38; acc: 0.59
Batch: 620; loss: 1.53; acc: 0.66
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.85; acc: 0.62
Batch: 20; loss: 3.1; acc: 0.41
Batch: 40; loss: 0.97; acc: 0.73
Batch: 60; loss: 1.89; acc: 0.62
Batch: 80; loss: 2.16; acc: 0.55
Batch: 100; loss: 1.75; acc: 0.61
Batch: 120; loss: 2.19; acc: 0.56
Batch: 140; loss: 3.75; acc: 0.5
Val Epoch over. val_loss: 2.075671523239962; val_accuracy: 0.6004179936305732 

Epoch 29 start
Batch: 0; loss: 2.02; acc: 0.56
Batch: 20; loss: 1.84; acc: 0.64
Batch: 40; loss: 1.81; acc: 0.5
Batch: 60; loss: 2.41; acc: 0.62
Batch: 80; loss: 1.79; acc: 0.69
Batch: 100; loss: 2.9; acc: 0.55
Batch: 120; loss: 2.38; acc: 0.58
Batch: 140; loss: 1.56; acc: 0.64
Batch: 160; loss: 2.03; acc: 0.66
Batch: 180; loss: 2.56; acc: 0.56
Batch: 200; loss: 2.34; acc: 0.56
Batch: 220; loss: 2.58; acc: 0.62
Batch: 240; loss: 1.95; acc: 0.62
Batch: 260; loss: 2.63; acc: 0.52
Batch: 280; loss: 1.46; acc: 0.69
Batch: 300; loss: 1.83; acc: 0.56
Batch: 320; loss: 2.13; acc: 0.55
Batch: 340; loss: 2.73; acc: 0.59
Batch: 360; loss: 1.72; acc: 0.66
Batch: 380; loss: 1.78; acc: 0.64
Batch: 400; loss: 1.81; acc: 0.69
Batch: 420; loss: 1.79; acc: 0.62
Batch: 440; loss: 2.19; acc: 0.58
Batch: 460; loss: 1.94; acc: 0.67
Batch: 480; loss: 1.97; acc: 0.67
Batch: 500; loss: 1.92; acc: 0.61
Batch: 520; loss: 3.07; acc: 0.58
Batch: 540; loss: 2.41; acc: 0.53
Batch: 560; loss: 1.75; acc: 0.59
Batch: 580; loss: 1.33; acc: 0.7
Batch: 600; loss: 2.2; acc: 0.58
Batch: 620; loss: 1.92; acc: 0.72
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.86; acc: 0.62
Batch: 20; loss: 3.06; acc: 0.41
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.88; acc: 0.61
Batch: 80; loss: 2.13; acc: 0.58
Batch: 100; loss: 1.74; acc: 0.61
Batch: 120; loss: 2.2; acc: 0.56
Batch: 140; loss: 3.75; acc: 0.48
Val Epoch over. val_loss: 2.0725645168571716; val_accuracy: 0.5997213375796179 

Epoch 30 start
Batch: 0; loss: 1.88; acc: 0.53
Batch: 20; loss: 2.97; acc: 0.59
Batch: 40; loss: 2.16; acc: 0.58
Batch: 60; loss: 1.32; acc: 0.69
Batch: 80; loss: 2.07; acc: 0.62
Batch: 100; loss: 1.86; acc: 0.69
Batch: 120; loss: 1.5; acc: 0.66
Batch: 140; loss: 2.85; acc: 0.47
Batch: 160; loss: 1.75; acc: 0.72
Batch: 180; loss: 1.33; acc: 0.69
Batch: 200; loss: 2.16; acc: 0.61
Batch: 220; loss: 2.01; acc: 0.59
Batch: 240; loss: 1.66; acc: 0.67
Batch: 260; loss: 1.84; acc: 0.7
Batch: 280; loss: 1.6; acc: 0.66
Batch: 300; loss: 1.9; acc: 0.62
Batch: 320; loss: 1.81; acc: 0.58
Batch: 340; loss: 1.67; acc: 0.61
Batch: 360; loss: 3.42; acc: 0.48
Batch: 380; loss: 2.02; acc: 0.62
Batch: 400; loss: 2.48; acc: 0.59
Batch: 420; loss: 1.77; acc: 0.64
Batch: 440; loss: 1.71; acc: 0.67
Batch: 460; loss: 2.02; acc: 0.59
Batch: 480; loss: 1.76; acc: 0.66
Batch: 500; loss: 1.72; acc: 0.61
Batch: 520; loss: 2.24; acc: 0.58
Batch: 540; loss: 2.1; acc: 0.66
Batch: 560; loss: 2.48; acc: 0.59
Batch: 580; loss: 1.53; acc: 0.67
Batch: 600; loss: 2.06; acc: 0.61
Batch: 620; loss: 1.87; acc: 0.67
Train Epoch over. train_loss: 1.99; train_accuracy: 0.61 

Batch: 0; loss: 1.86; acc: 0.61
Batch: 20; loss: 3.08; acc: 0.39
Batch: 40; loss: 0.95; acc: 0.72
Batch: 60; loss: 1.88; acc: 0.59
Batch: 80; loss: 2.14; acc: 0.56
Batch: 100; loss: 1.75; acc: 0.61
Batch: 120; loss: 2.19; acc: 0.58
Batch: 140; loss: 3.74; acc: 0.48
Val Epoch over. val_loss: 2.0716290743487655; val_accuracy: 0.5993232484076433 

plots/subspace_training/lenet/2020-01-10 05:00:58/d_dim_100_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 2.24; acc: 0.45
Batch: 40; loss: 1.89; acc: 0.47
Batch: 60; loss: 1.76; acc: 0.53
Batch: 80; loss: 1.37; acc: 0.66
Batch: 100; loss: 1.01; acc: 0.62
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 1.85; acc: 0.59
Batch: 160; loss: 1.26; acc: 0.73
Batch: 180; loss: 1.25; acc: 0.72
Batch: 200; loss: 1.08; acc: 0.75
Batch: 220; loss: 1.96; acc: 0.58
Batch: 240; loss: 1.43; acc: 0.69
Batch: 260; loss: 0.85; acc: 0.73
Batch: 280; loss: 1.28; acc: 0.69
Batch: 300; loss: 1.3; acc: 0.69
Batch: 320; loss: 1.0; acc: 0.73
Batch: 340; loss: 0.9; acc: 0.72
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 1.61; acc: 0.62
Batch: 400; loss: 1.0; acc: 0.77
Batch: 420; loss: 1.01; acc: 0.73
Batch: 440; loss: 1.31; acc: 0.72
Batch: 460; loss: 0.86; acc: 0.75
Batch: 480; loss: 0.96; acc: 0.69
Batch: 500; loss: 1.27; acc: 0.73
Batch: 520; loss: 0.83; acc: 0.72
Batch: 540; loss: 1.21; acc: 0.73
Batch: 560; loss: 0.73; acc: 0.77
Batch: 580; loss: 0.82; acc: 0.73
Batch: 600; loss: 1.27; acc: 0.72
Batch: 620; loss: 1.02; acc: 0.75
Train Epoch over. train_loss: 1.39; train_accuracy: 0.67 

Batch: 0; loss: 0.94; acc: 0.72
Batch: 20; loss: 2.31; acc: 0.55
Batch: 40; loss: 1.01; acc: 0.75
Batch: 60; loss: 0.88; acc: 0.73
Batch: 80; loss: 1.33; acc: 0.7
Batch: 100; loss: 1.3; acc: 0.64
Batch: 120; loss: 0.7; acc: 0.88
Batch: 140; loss: 1.72; acc: 0.66
Val Epoch over. val_loss: 1.236955174974575; val_accuracy: 0.7139729299363057 

Epoch 2 start
Batch: 0; loss: 1.27; acc: 0.72
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 0.93; acc: 0.77
Batch: 60; loss: 1.07; acc: 0.67
Batch: 80; loss: 1.19; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.78
Batch: 120; loss: 1.34; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.72
Batch: 160; loss: 1.08; acc: 0.73
Batch: 180; loss: 1.18; acc: 0.7
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 0.38; acc: 0.83
Batch: 240; loss: 1.05; acc: 0.75
Batch: 260; loss: 0.77; acc: 0.77
Batch: 280; loss: 1.29; acc: 0.7
Batch: 300; loss: 1.35; acc: 0.7
Batch: 320; loss: 0.79; acc: 0.83
Batch: 340; loss: 1.41; acc: 0.69
Batch: 360; loss: 2.35; acc: 0.52
Batch: 380; loss: 0.9; acc: 0.83
Batch: 400; loss: 1.91; acc: 0.66
Batch: 420; loss: 1.03; acc: 0.72
Batch: 440; loss: 0.89; acc: 0.7
Batch: 460; loss: 1.67; acc: 0.7
Batch: 480; loss: 1.01; acc: 0.72
Batch: 500; loss: 1.69; acc: 0.66
Batch: 520; loss: 1.17; acc: 0.75
Batch: 540; loss: 0.55; acc: 0.75
Batch: 560; loss: 1.44; acc: 0.72
Batch: 580; loss: 0.94; acc: 0.77
Batch: 600; loss: 0.79; acc: 0.78
Batch: 620; loss: 1.25; acc: 0.7
Train Epoch over. train_loss: 1.11; train_accuracy: 0.74 

Batch: 0; loss: 0.79; acc: 0.75
Batch: 20; loss: 2.23; acc: 0.61
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.83; acc: 0.72
Batch: 80; loss: 0.88; acc: 0.75
Batch: 100; loss: 1.37; acc: 0.77
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 1.53; acc: 0.7
Val Epoch over. val_loss: 1.0927314010395366; val_accuracy: 0.7434315286624203 

Epoch 3 start
Batch: 0; loss: 0.77; acc: 0.8
Batch: 20; loss: 1.15; acc: 0.69
Batch: 40; loss: 0.75; acc: 0.8
Batch: 60; loss: 1.01; acc: 0.75
Batch: 80; loss: 0.66; acc: 0.78
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 1.39; acc: 0.64
Batch: 160; loss: 0.67; acc: 0.77
Batch: 180; loss: 0.94; acc: 0.78
Batch: 200; loss: 1.3; acc: 0.77
Batch: 220; loss: 1.19; acc: 0.75
Batch: 240; loss: 1.59; acc: 0.67
Batch: 260; loss: 0.79; acc: 0.77
Batch: 280; loss: 0.91; acc: 0.78
Batch: 300; loss: 0.79; acc: 0.77
Batch: 320; loss: 1.17; acc: 0.8
Batch: 340; loss: 1.17; acc: 0.78
Batch: 360; loss: 0.99; acc: 0.78
Batch: 380; loss: 0.58; acc: 0.88
Batch: 400; loss: 2.0; acc: 0.58
Batch: 420; loss: 1.89; acc: 0.61
Batch: 440; loss: 0.7; acc: 0.83
Batch: 460; loss: 1.23; acc: 0.81
Batch: 480; loss: 1.07; acc: 0.72
Batch: 500; loss: 1.13; acc: 0.73
Batch: 520; loss: 0.64; acc: 0.77
Batch: 540; loss: 0.86; acc: 0.75
Batch: 560; loss: 1.39; acc: 0.66
Batch: 580; loss: 1.02; acc: 0.73
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.79; acc: 0.78
Train Epoch over. train_loss: 1.07; train_accuracy: 0.75 

Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 1.91; acc: 0.61
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.73
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 1.19; acc: 0.77
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 1.45; acc: 0.7
Val Epoch over. val_loss: 0.9994047397071388; val_accuracy: 0.7669187898089171 

Epoch 4 start
Batch: 0; loss: 0.76; acc: 0.78
Batch: 20; loss: 1.94; acc: 0.62
Batch: 40; loss: 0.81; acc: 0.81
Batch: 60; loss: 1.42; acc: 0.75
Batch: 80; loss: 1.25; acc: 0.75
Batch: 100; loss: 0.5; acc: 0.78
Batch: 120; loss: 1.02; acc: 0.75
Batch: 140; loss: 0.97; acc: 0.72
Batch: 160; loss: 0.93; acc: 0.81
Batch: 180; loss: 1.09; acc: 0.77
Batch: 200; loss: 1.46; acc: 0.77
Batch: 220; loss: 0.98; acc: 0.66
Batch: 240; loss: 0.85; acc: 0.83
Batch: 260; loss: 0.63; acc: 0.77
Batch: 280; loss: 1.32; acc: 0.78
Batch: 300; loss: 1.21; acc: 0.77
Batch: 320; loss: 1.55; acc: 0.67
Batch: 340; loss: 1.29; acc: 0.72
Batch: 360; loss: 0.77; acc: 0.78
Batch: 380; loss: 1.72; acc: 0.7
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 1.18; acc: 0.75
Batch: 440; loss: 1.28; acc: 0.7
Batch: 460; loss: 1.04; acc: 0.77
Batch: 480; loss: 1.23; acc: 0.69
Batch: 500; loss: 1.35; acc: 0.73
Batch: 520; loss: 0.97; acc: 0.72
Batch: 540; loss: 0.55; acc: 0.8
Batch: 560; loss: 1.31; acc: 0.75
Batch: 580; loss: 0.9; acc: 0.81
Batch: 600; loss: 0.94; acc: 0.78
Batch: 620; loss: 1.31; acc: 0.72
Train Epoch over. train_loss: 1.05; train_accuracy: 0.75 

Batch: 0; loss: 0.73; acc: 0.75
Batch: 20; loss: 2.47; acc: 0.56
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 1.46; acc: 0.75
Batch: 120; loss: 0.6; acc: 0.86
Batch: 140; loss: 1.65; acc: 0.73
Val Epoch over. val_loss: 1.1337115356496945; val_accuracy: 0.7350716560509554 

Epoch 5 start
Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 1.39; acc: 0.7
Batch: 40; loss: 1.53; acc: 0.72
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 1.17; acc: 0.8
Batch: 100; loss: 1.62; acc: 0.67
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 1.21; acc: 0.73
Batch: 160; loss: 1.3; acc: 0.77
Batch: 180; loss: 1.35; acc: 0.77
Batch: 200; loss: 0.92; acc: 0.78
Batch: 220; loss: 1.21; acc: 0.72
Batch: 240; loss: 1.59; acc: 0.73
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 1.5; acc: 0.7
Batch: 300; loss: 0.95; acc: 0.77
Batch: 320; loss: 1.22; acc: 0.77
Batch: 340; loss: 1.32; acc: 0.78
Batch: 360; loss: 0.72; acc: 0.73
Batch: 380; loss: 0.73; acc: 0.83
Batch: 400; loss: 1.02; acc: 0.75
Batch: 420; loss: 1.24; acc: 0.7
Batch: 440; loss: 1.06; acc: 0.73
Batch: 460; loss: 1.32; acc: 0.67
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.56; acc: 0.88
Batch: 540; loss: 1.31; acc: 0.69
Batch: 560; loss: 1.47; acc: 0.7
Batch: 580; loss: 0.74; acc: 0.81
Batch: 600; loss: 0.88; acc: 0.78
Batch: 620; loss: 1.22; acc: 0.83
Train Epoch over. train_loss: 1.04; train_accuracy: 0.76 

Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 1.76; acc: 0.64
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 0.93; acc: 0.8
Batch: 100; loss: 1.69; acc: 0.67
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 1.66; acc: 0.7
Val Epoch over. val_loss: 1.0520558567943088; val_accuracy: 0.7560708598726115 

Epoch 6 start
Batch: 0; loss: 0.7; acc: 0.75
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 1.16; acc: 0.75
Batch: 60; loss: 1.56; acc: 0.66
Batch: 80; loss: 1.2; acc: 0.78
Batch: 100; loss: 0.76; acc: 0.81
Batch: 120; loss: 1.15; acc: 0.78
Batch: 140; loss: 1.14; acc: 0.78
Batch: 160; loss: 1.16; acc: 0.67
Batch: 180; loss: 1.12; acc: 0.8
Batch: 200; loss: 0.91; acc: 0.8
Batch: 220; loss: 0.8; acc: 0.81
Batch: 240; loss: 1.3; acc: 0.67
Batch: 260; loss: 1.17; acc: 0.75
Batch: 280; loss: 0.93; acc: 0.84
Batch: 300; loss: 1.39; acc: 0.78
Batch: 320; loss: 0.91; acc: 0.83
Batch: 340; loss: 1.45; acc: 0.7
Batch: 360; loss: 1.3; acc: 0.67
Batch: 380; loss: 1.13; acc: 0.69
Batch: 400; loss: 0.78; acc: 0.78
Batch: 420; loss: 0.85; acc: 0.84
Batch: 440; loss: 1.46; acc: 0.77
Batch: 460; loss: 0.95; acc: 0.77
Batch: 480; loss: 1.36; acc: 0.73
Batch: 500; loss: 0.98; acc: 0.77
Batch: 520; loss: 1.17; acc: 0.73
Batch: 540; loss: 0.79; acc: 0.84
Batch: 560; loss: 1.59; acc: 0.67
Batch: 580; loss: 1.51; acc: 0.67
Batch: 600; loss: 0.91; acc: 0.77
Batch: 620; loss: 1.4; acc: 0.73
Train Epoch over. train_loss: 1.04; train_accuracy: 0.76 

Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 2.0; acc: 0.59
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.95; acc: 0.78
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.74; acc: 0.7
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 1.67; acc: 0.66
Val Epoch over. val_loss: 1.059387378252236; val_accuracy: 0.7539808917197452 

Epoch 7 start
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 1.33; acc: 0.72
Batch: 40; loss: 1.02; acc: 0.75
Batch: 60; loss: 1.29; acc: 0.73
Batch: 80; loss: 1.0; acc: 0.83
Batch: 100; loss: 1.21; acc: 0.75
Batch: 120; loss: 0.98; acc: 0.77
Batch: 140; loss: 1.15; acc: 0.77
Batch: 160; loss: 1.33; acc: 0.62
Batch: 180; loss: 1.7; acc: 0.73
Batch: 200; loss: 0.9; acc: 0.81
Batch: 220; loss: 0.94; acc: 0.8
Batch: 240; loss: 1.21; acc: 0.69
Batch: 260; loss: 0.8; acc: 0.77
Batch: 280; loss: 1.1; acc: 0.78
Batch: 300; loss: 1.19; acc: 0.78
Batch: 320; loss: 0.83; acc: 0.77
Batch: 340; loss: 1.39; acc: 0.67
Batch: 360; loss: 1.41; acc: 0.77
Batch: 380; loss: 1.09; acc: 0.72
Batch: 400; loss: 1.23; acc: 0.75
Batch: 420; loss: 1.09; acc: 0.69
Batch: 440; loss: 0.87; acc: 0.67
Batch: 460; loss: 1.52; acc: 0.7
Batch: 480; loss: 1.32; acc: 0.69
Batch: 500; loss: 0.91; acc: 0.73
Batch: 520; loss: 1.25; acc: 0.66
Batch: 540; loss: 0.86; acc: 0.84
Batch: 560; loss: 0.9; acc: 0.78
Batch: 580; loss: 1.22; acc: 0.78
Batch: 600; loss: 0.97; acc: 0.75
Batch: 620; loss: 1.54; acc: 0.73
Train Epoch over. train_loss: 1.04; train_accuracy: 0.76 

Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 1.81; acc: 0.61
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 1.24; acc: 0.72
Batch: 80; loss: 1.41; acc: 0.75
Batch: 100; loss: 1.43; acc: 0.72
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 1.85; acc: 0.64
Val Epoch over. val_loss: 1.1301047800073198; val_accuracy: 0.7569665605095541 

Epoch 8 start
Batch: 0; loss: 1.02; acc: 0.78
Batch: 20; loss: 1.39; acc: 0.75
Batch: 40; loss: 1.0; acc: 0.78
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.89; acc: 0.72
Batch: 100; loss: 1.24; acc: 0.78
Batch: 120; loss: 1.18; acc: 0.69
Batch: 140; loss: 0.55; acc: 0.81
Batch: 160; loss: 0.83; acc: 0.81
Batch: 180; loss: 1.21; acc: 0.78
Batch: 200; loss: 0.98; acc: 0.77
Batch: 220; loss: 0.8; acc: 0.75
Batch: 240; loss: 1.37; acc: 0.7
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.93; acc: 0.8
Batch: 300; loss: 0.69; acc: 0.83
Batch: 320; loss: 0.86; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.73; acc: 0.83
Batch: 400; loss: 1.16; acc: 0.73
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 1.12; acc: 0.8
Batch: 460; loss: 0.67; acc: 0.78
Batch: 480; loss: 0.99; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.78
Batch: 520; loss: 0.71; acc: 0.83
Batch: 540; loss: 1.55; acc: 0.66
Batch: 560; loss: 0.88; acc: 0.8
Batch: 580; loss: 0.84; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.92; acc: 0.73
Train Epoch over. train_loss: 1.02; train_accuracy: 0.77 

Batch: 0; loss: 1.02; acc: 0.75
Batch: 20; loss: 2.33; acc: 0.56
Batch: 40; loss: 0.66; acc: 0.81
Batch: 60; loss: 1.16; acc: 0.72
Batch: 80; loss: 1.23; acc: 0.72
Batch: 100; loss: 1.55; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 1.69; acc: 0.64
Val Epoch over. val_loss: 1.1621155059261687; val_accuracy: 0.7431329617834395 

Epoch 9 start
Batch: 0; loss: 1.06; acc: 0.73
Batch: 20; loss: 1.03; acc: 0.75
Batch: 40; loss: 0.68; acc: 0.86
Batch: 60; loss: 1.26; acc: 0.72
Batch: 80; loss: 1.01; acc: 0.69
Batch: 100; loss: 1.02; acc: 0.77
Batch: 120; loss: 1.34; acc: 0.7
Batch: 140; loss: 1.04; acc: 0.86
Batch: 160; loss: 1.04; acc: 0.75
Batch: 180; loss: 1.5; acc: 0.72
Batch: 200; loss: 0.7; acc: 0.86
Batch: 220; loss: 1.69; acc: 0.7
Batch: 240; loss: 1.31; acc: 0.7
Batch: 260; loss: 1.53; acc: 0.69
Batch: 280; loss: 1.62; acc: 0.69
Batch: 300; loss: 1.51; acc: 0.77
Batch: 320; loss: 0.91; acc: 0.77
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 1.14; acc: 0.72
Batch: 380; loss: 1.0; acc: 0.73
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 1.2; acc: 0.73
Batch: 440; loss: 0.94; acc: 0.73
Batch: 460; loss: 1.35; acc: 0.69
Batch: 480; loss: 0.48; acc: 0.89
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 1.68; acc: 0.66
Batch: 540; loss: 0.86; acc: 0.77
Batch: 560; loss: 1.36; acc: 0.73
Batch: 580; loss: 0.69; acc: 0.83
Batch: 600; loss: 1.06; acc: 0.73
Batch: 620; loss: 1.27; acc: 0.7
Train Epoch over. train_loss: 1.03; train_accuracy: 0.76 

Batch: 0; loss: 1.15; acc: 0.8
Batch: 20; loss: 1.73; acc: 0.59
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 1.28; acc: 0.7
Batch: 80; loss: 1.15; acc: 0.73
Batch: 100; loss: 1.57; acc: 0.75
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 2.02; acc: 0.61
Val Epoch over. val_loss: 1.208231172364229; val_accuracy: 0.7405453821656051 

Epoch 10 start
Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.23; acc: 0.8
Batch: 40; loss: 1.1; acc: 0.72
Batch: 60; loss: 0.95; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 1.74; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.78
Batch: 140; loss: 1.58; acc: 0.7
Batch: 160; loss: 1.17; acc: 0.83
Batch: 180; loss: 1.41; acc: 0.73
Batch: 200; loss: 1.05; acc: 0.8
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 1.47; acc: 0.69
Batch: 260; loss: 0.88; acc: 0.84
Batch: 280; loss: 1.15; acc: 0.8
Batch: 300; loss: 0.93; acc: 0.72
Batch: 320; loss: 1.04; acc: 0.75
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.72; acc: 0.81
Batch: 380; loss: 1.19; acc: 0.78
Batch: 400; loss: 0.77; acc: 0.81
Batch: 420; loss: 1.35; acc: 0.78
Batch: 440; loss: 0.95; acc: 0.81
Batch: 460; loss: 1.3; acc: 0.73
Batch: 480; loss: 0.91; acc: 0.81
Batch: 500; loss: 0.82; acc: 0.73
Batch: 520; loss: 0.84; acc: 0.75
Batch: 540; loss: 0.99; acc: 0.8
Batch: 560; loss: 1.34; acc: 0.73
Batch: 580; loss: 0.87; acc: 0.83
Batch: 600; loss: 0.8; acc: 0.77
Batch: 620; loss: 1.01; acc: 0.84
Train Epoch over. train_loss: 1.03; train_accuracy: 0.77 

Batch: 0; loss: 0.78; acc: 0.8
Batch: 20; loss: 1.84; acc: 0.66
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 1.13; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.75
Batch: 100; loss: 1.58; acc: 0.78
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 1.56; acc: 0.67
Val Epoch over. val_loss: 1.0386636707053822; val_accuracy: 0.7561703821656051 

Epoch 11 start
Batch: 0; loss: 1.51; acc: 0.72
Batch: 20; loss: 0.75; acc: 0.78
Batch: 40; loss: 1.63; acc: 0.75
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 1.02; acc: 0.77
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 1.51; acc: 0.64
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 1.61; acc: 0.77
Batch: 180; loss: 0.83; acc: 0.83
Batch: 200; loss: 1.02; acc: 0.8
Batch: 220; loss: 0.89; acc: 0.8
Batch: 240; loss: 0.77; acc: 0.78
Batch: 260; loss: 0.55; acc: 0.78
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.8; acc: 0.78
Batch: 320; loss: 0.73; acc: 0.8
Batch: 340; loss: 1.47; acc: 0.73
Batch: 360; loss: 0.45; acc: 0.8
Batch: 380; loss: 1.35; acc: 0.67
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.73; acc: 0.86
Batch: 460; loss: 0.83; acc: 0.81
Batch: 480; loss: 1.17; acc: 0.75
Batch: 500; loss: 0.9; acc: 0.73
Batch: 520; loss: 0.82; acc: 0.8
Batch: 540; loss: 1.51; acc: 0.81
Batch: 560; loss: 1.23; acc: 0.75
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 1.45; acc: 0.7
Batch: 620; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.87; train_accuracy: 0.79 

Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 1.66; acc: 0.66
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 1.36; acc: 0.8
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 1.48; acc: 0.69
Val Epoch over. val_loss: 0.90714402971374; val_accuracy: 0.792296974522293 

Epoch 12 start
Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.81; acc: 0.83
Batch: 60; loss: 1.24; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.98; acc: 0.73
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.55; acc: 0.89
Batch: 160; loss: 0.89; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.73
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.92; acc: 0.73
Batch: 240; loss: 1.06; acc: 0.75
Batch: 260; loss: 0.74; acc: 0.81
Batch: 280; loss: 0.98; acc: 0.8
Batch: 300; loss: 1.05; acc: 0.78
Batch: 320; loss: 0.77; acc: 0.83
Batch: 340; loss: 1.16; acc: 0.72
Batch: 360; loss: 0.8; acc: 0.83
Batch: 380; loss: 1.27; acc: 0.77
Batch: 400; loss: 0.82; acc: 0.72
Batch: 420; loss: 0.66; acc: 0.72
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 1.3; acc: 0.73
Batch: 480; loss: 1.02; acc: 0.77
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.91; acc: 0.81
Batch: 540; loss: 0.98; acc: 0.75
Batch: 560; loss: 0.64; acc: 0.84
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.98; acc: 0.81
Batch: 620; loss: 0.97; acc: 0.77
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

Batch: 0; loss: 0.66; acc: 0.78
Batch: 20; loss: 1.52; acc: 0.67
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.8
Batch: 100; loss: 1.33; acc: 0.78
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 1.47; acc: 0.69
Val Epoch over. val_loss: 0.9037651119718126; val_accuracy: 0.7900079617834395 

Epoch 13 start
Batch: 0; loss: 0.71; acc: 0.81
Batch: 20; loss: 0.75; acc: 0.84
Batch: 40; loss: 1.01; acc: 0.77
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.87; acc: 0.78
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 1.41; acc: 0.75
Batch: 140; loss: 0.93; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.78; acc: 0.73
Batch: 220; loss: 0.92; acc: 0.8
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 1.37; acc: 0.77
Batch: 280; loss: 1.14; acc: 0.73
Batch: 300; loss: 0.97; acc: 0.8
Batch: 320; loss: 0.89; acc: 0.81
Batch: 340; loss: 0.86; acc: 0.81
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.84; acc: 0.81
Batch: 400; loss: 0.57; acc: 0.83
Batch: 420; loss: 0.89; acc: 0.83
Batch: 440; loss: 1.18; acc: 0.75
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.92; acc: 0.88
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.7; acc: 0.83
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 1.16; acc: 0.78
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 1.62; acc: 0.67
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.87; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 1.35; acc: 0.78
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 1.51; acc: 0.67
Val Epoch over. val_loss: 0.905534425643599; val_accuracy: 0.790406050955414 

Epoch 14 start
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 1.0; acc: 0.77
Batch: 40; loss: 1.12; acc: 0.73
Batch: 60; loss: 1.04; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.8
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 1.02; acc: 0.78
Batch: 160; loss: 1.52; acc: 0.75
Batch: 180; loss: 0.93; acc: 0.77
Batch: 200; loss: 0.72; acc: 0.8
Batch: 220; loss: 0.95; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.83; acc: 0.75
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.97; acc: 0.83
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.42; acc: 0.84
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.92; acc: 0.78
Batch: 420; loss: 0.67; acc: 0.78
Batch: 440; loss: 0.97; acc: 0.81
Batch: 460; loss: 0.94; acc: 0.75
Batch: 480; loss: 0.58; acc: 0.81
Batch: 500; loss: 0.91; acc: 0.75
Batch: 520; loss: 0.98; acc: 0.8
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 1.05; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 1.21; acc: 0.78
Batch: 620; loss: 1.15; acc: 0.72
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 1.56; acc: 0.64
Batch: 40; loss: 0.45; acc: 0.81
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 1.29; acc: 0.78
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 0.8966192497759108; val_accuracy: 0.7917993630573248 

Epoch 15 start
Batch: 0; loss: 1.15; acc: 0.77
Batch: 20; loss: 0.76; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 1.2; acc: 0.73
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.66; acc: 0.86
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 1.24; acc: 0.78
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.84; acc: 0.8
Batch: 240; loss: 1.23; acc: 0.69
Batch: 260; loss: 1.74; acc: 0.72
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.97; acc: 0.8
Batch: 320; loss: 0.4; acc: 0.83
Batch: 340; loss: 0.89; acc: 0.8
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 1.08; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.72; acc: 0.84
Batch: 500; loss: 1.15; acc: 0.81
Batch: 520; loss: 1.34; acc: 0.73
Batch: 540; loss: 0.9; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.8; acc: 0.8
Batch: 600; loss: 0.82; acc: 0.86
Batch: 620; loss: 1.23; acc: 0.69
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 1.67; acc: 0.69
Batch: 40; loss: 0.49; acc: 0.81
Batch: 60; loss: 0.84; acc: 0.8
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 1.31; acc: 0.78
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 1.48; acc: 0.67
Val Epoch over. val_loss: 0.8967750941871837; val_accuracy: 0.7916003184713376 

Epoch 16 start
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.89; acc: 0.84
Batch: 60; loss: 0.76; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.83
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 1.15; acc: 0.81
Batch: 160; loss: 0.89; acc: 0.81
Batch: 180; loss: 1.08; acc: 0.72
Batch: 200; loss: 1.52; acc: 0.78
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.86; acc: 0.8
Batch: 280; loss: 0.74; acc: 0.81
Batch: 300; loss: 0.57; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.78
Batch: 340; loss: 1.1; acc: 0.75
Batch: 360; loss: 0.95; acc: 0.77
Batch: 380; loss: 1.41; acc: 0.72
Batch: 400; loss: 0.75; acc: 0.8
Batch: 420; loss: 0.99; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.81
Batch: 460; loss: 0.8; acc: 0.81
Batch: 480; loss: 0.79; acc: 0.86
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 0.77; acc: 0.81
Batch: 540; loss: 0.85; acc: 0.88
Batch: 560; loss: 1.07; acc: 0.77
Batch: 580; loss: 0.89; acc: 0.81
Batch: 600; loss: 0.93; acc: 0.78
Batch: 620; loss: 0.83; acc: 0.78
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 1.73; acc: 0.62
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 1.27; acc: 0.8
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 1.51; acc: 0.69
Val Epoch over. val_loss: 0.8950617503208719; val_accuracy: 0.7906050955414012 

Epoch 17 start
Batch: 0; loss: 1.23; acc: 0.73
Batch: 20; loss: 0.9; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.98; acc: 0.81
Batch: 80; loss: 0.82; acc: 0.8
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.87; acc: 0.8
Batch: 140; loss: 1.23; acc: 0.78
Batch: 160; loss: 0.79; acc: 0.84
Batch: 180; loss: 1.2; acc: 0.77
Batch: 200; loss: 0.62; acc: 0.81
Batch: 220; loss: 1.06; acc: 0.8
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 1.22; acc: 0.81
Batch: 280; loss: 1.04; acc: 0.75
Batch: 300; loss: 0.88; acc: 0.72
Batch: 320; loss: 1.05; acc: 0.78
Batch: 340; loss: 0.59; acc: 0.8
Batch: 360; loss: 0.75; acc: 0.81
Batch: 380; loss: 0.63; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.91
Batch: 420; loss: 0.92; acc: 0.8
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.98; acc: 0.8
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.97; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.81
Batch: 560; loss: 0.81; acc: 0.84
Batch: 580; loss: 0.91; acc: 0.81
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 1.04; acc: 0.72
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.77
Batch: 20; loss: 1.6; acc: 0.66
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 1.26; acc: 0.78
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 1.43; acc: 0.69
Val Epoch over. val_loss: 0.8956070401865965; val_accuracy: 0.7926950636942676 

Epoch 18 start
Batch: 0; loss: 0.35; acc: 0.84
Batch: 20; loss: 1.08; acc: 0.75
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.88; acc: 0.77
Batch: 160; loss: 0.83; acc: 0.81
Batch: 180; loss: 0.98; acc: 0.8
Batch: 200; loss: 1.31; acc: 0.75
Batch: 220; loss: 1.06; acc: 0.77
Batch: 240; loss: 0.62; acc: 0.91
Batch: 260; loss: 1.51; acc: 0.61
Batch: 280; loss: 0.84; acc: 0.77
Batch: 300; loss: 1.01; acc: 0.75
Batch: 320; loss: 0.76; acc: 0.8
Batch: 340; loss: 1.36; acc: 0.75
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.9; acc: 0.8
Batch: 400; loss: 0.71; acc: 0.83
Batch: 420; loss: 0.74; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 1.46; acc: 0.73
Batch: 480; loss: 0.59; acc: 0.89
Batch: 500; loss: 0.71; acc: 0.77
Batch: 520; loss: 1.09; acc: 0.72
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.72; acc: 0.83
Batch: 580; loss: 1.12; acc: 0.73
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 1.66; acc: 0.66
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 1.28; acc: 0.81
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 1.46; acc: 0.67
Val Epoch over. val_loss: 0.8920376146105444; val_accuracy: 0.793093152866242 

Epoch 19 start
Batch: 0; loss: 0.84; acc: 0.84
Batch: 20; loss: 0.86; acc: 0.84
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.85; acc: 0.83
Batch: 80; loss: 0.73; acc: 0.8
Batch: 100; loss: 1.0; acc: 0.81
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.81
Batch: 160; loss: 0.94; acc: 0.77
Batch: 180; loss: 0.79; acc: 0.8
Batch: 200; loss: 0.83; acc: 0.83
Batch: 220; loss: 0.87; acc: 0.77
Batch: 240; loss: 1.17; acc: 0.81
Batch: 260; loss: 1.09; acc: 0.73
Batch: 280; loss: 0.89; acc: 0.86
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.91; acc: 0.78
Batch: 340; loss: 0.86; acc: 0.81
Batch: 360; loss: 0.78; acc: 0.91
Batch: 380; loss: 0.57; acc: 0.86
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 1.36; acc: 0.73
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.74; acc: 0.83
Batch: 480; loss: 0.83; acc: 0.8
Batch: 500; loss: 0.63; acc: 0.78
Batch: 520; loss: 1.06; acc: 0.78
Batch: 540; loss: 0.96; acc: 0.73
Batch: 560; loss: 0.87; acc: 0.81
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.8; acc: 0.73
Batch: 620; loss: 0.68; acc: 0.84
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 1.71; acc: 0.66
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.75
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 1.3; acc: 0.8
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 1.4; acc: 0.7
Val Epoch over. val_loss: 0.9042461838122386; val_accuracy: 0.7895103503184714 

Epoch 20 start
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 0.87; acc: 0.83
Batch: 40; loss: 1.15; acc: 0.75
Batch: 60; loss: 1.21; acc: 0.8
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.99; acc: 0.81
Batch: 140; loss: 0.78; acc: 0.84
Batch: 160; loss: 0.94; acc: 0.69
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 1.02; acc: 0.75
Batch: 220; loss: 0.73; acc: 0.84
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 1.37; acc: 0.69
Batch: 280; loss: 0.87; acc: 0.72
Batch: 300; loss: 1.19; acc: 0.73
Batch: 320; loss: 0.48; acc: 0.81
Batch: 340; loss: 1.13; acc: 0.73
Batch: 360; loss: 0.85; acc: 0.8
Batch: 380; loss: 1.04; acc: 0.77
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 1.28; acc: 0.72
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.79; acc: 0.78
Batch: 500; loss: 0.74; acc: 0.81
Batch: 520; loss: 0.73; acc: 0.78
Batch: 540; loss: 1.12; acc: 0.8
Batch: 560; loss: 1.11; acc: 0.8
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.74; acc: 0.78
Batch: 620; loss: 1.27; acc: 0.72
Train Epoch over. train_loss: 0.85; train_accuracy: 0.8 

Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 1.68; acc: 0.64
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 1.27; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 1.47; acc: 0.67
Val Epoch over. val_loss: 0.897183390085105; val_accuracy: 0.7923964968152867 

Epoch 21 start
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.8; acc: 0.81
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 1.18; acc: 0.69
Batch: 80; loss: 0.83; acc: 0.81
Batch: 100; loss: 1.03; acc: 0.8
Batch: 120; loss: 1.07; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.83
Batch: 160; loss: 1.02; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.98; acc: 0.78
Batch: 220; loss: 0.83; acc: 0.81
Batch: 240; loss: 1.22; acc: 0.7
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 1.22; acc: 0.75
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 1.16; acc: 0.72
Batch: 340; loss: 1.35; acc: 0.67
Batch: 360; loss: 0.9; acc: 0.8
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.96; acc: 0.75
Batch: 420; loss: 0.99; acc: 0.73
Batch: 440; loss: 1.04; acc: 0.81
Batch: 460; loss: 0.7; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.78; acc: 0.77
Batch: 520; loss: 0.83; acc: 0.77
Batch: 540; loss: 1.05; acc: 0.77
Batch: 560; loss: 0.69; acc: 0.89
Batch: 580; loss: 1.21; acc: 0.72
Batch: 600; loss: 0.82; acc: 0.83
Batch: 620; loss: 0.85; acc: 0.78
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 1.68; acc: 0.64
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 1.27; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 1.43; acc: 0.66
Val Epoch over. val_loss: 0.8902788134707007; val_accuracy: 0.7925955414012739 

Epoch 22 start
Batch: 0; loss: 0.87; acc: 0.83
Batch: 20; loss: 0.83; acc: 0.77
Batch: 40; loss: 1.28; acc: 0.75
Batch: 60; loss: 0.86; acc: 0.83
Batch: 80; loss: 1.29; acc: 0.73
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 1.24; acc: 0.77
Batch: 140; loss: 1.01; acc: 0.81
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 1.1; acc: 0.78
Batch: 200; loss: 0.94; acc: 0.83
Batch: 220; loss: 0.9; acc: 0.83
Batch: 240; loss: 1.05; acc: 0.75
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.85; acc: 0.73
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 1.06; acc: 0.78
Batch: 360; loss: 0.96; acc: 0.78
Batch: 380; loss: 0.79; acc: 0.83
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 0.95; acc: 0.72
Batch: 440; loss: 0.68; acc: 0.8
Batch: 460; loss: 0.68; acc: 0.88
Batch: 480; loss: 0.76; acc: 0.84
Batch: 500; loss: 0.71; acc: 0.83
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.48; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.88
Batch: 580; loss: 1.38; acc: 0.8
Batch: 600; loss: 1.32; acc: 0.72
Batch: 620; loss: 0.85; acc: 0.78
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 1.69; acc: 0.64
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 1.27; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 1.43; acc: 0.66
Val Epoch over. val_loss: 0.8892397580632738; val_accuracy: 0.7921974522292994 

Epoch 23 start
Batch: 0; loss: 0.94; acc: 0.83
Batch: 20; loss: 0.8; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 1.48; acc: 0.73
Batch: 140; loss: 0.89; acc: 0.81
Batch: 160; loss: 1.58; acc: 0.67
Batch: 180; loss: 0.94; acc: 0.75
Batch: 200; loss: 0.87; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.92; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.75
Batch: 280; loss: 1.11; acc: 0.81
Batch: 300; loss: 0.93; acc: 0.77
Batch: 320; loss: 0.73; acc: 0.83
Batch: 340; loss: 0.63; acc: 0.77
Batch: 360; loss: 0.4; acc: 0.83
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 1.08; acc: 0.81
Batch: 420; loss: 1.05; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.82; acc: 0.81
Batch: 480; loss: 0.84; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.81
Batch: 520; loss: 1.13; acc: 0.73
Batch: 540; loss: 1.6; acc: 0.73
Batch: 560; loss: 0.74; acc: 0.84
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 1.02; acc: 0.78
Batch: 620; loss: 1.16; acc: 0.73
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 1.68; acc: 0.66
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.77
Batch: 100; loss: 1.27; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 1.41; acc: 0.66
Val Epoch over. val_loss: 0.8889969538921004; val_accuracy: 0.7929936305732485 

Epoch 24 start
Batch: 0; loss: 1.34; acc: 0.72
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 1.05; acc: 0.8
Batch: 80; loss: 0.96; acc: 0.81
Batch: 100; loss: 1.77; acc: 0.69
Batch: 120; loss: 0.7; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.81
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.98; acc: 0.8
Batch: 220; loss: 0.88; acc: 0.8
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.67; acc: 0.8
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.77; acc: 0.81
Batch: 360; loss: 0.82; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 1.08; acc: 0.8
Batch: 420; loss: 0.84; acc: 0.83
Batch: 440; loss: 1.33; acc: 0.78
Batch: 460; loss: 1.01; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 1.13; acc: 0.77
Batch: 520; loss: 0.4; acc: 0.83
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 1.11; acc: 0.75
Batch: 580; loss: 0.6; acc: 0.8
Batch: 600; loss: 0.86; acc: 0.78
Batch: 620; loss: 1.53; acc: 0.75
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 1.7; acc: 0.64
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 1.27; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 1.42; acc: 0.66
Val Epoch over. val_loss: 0.8890595689510844; val_accuracy: 0.7924960191082803 

Epoch 25 start
Batch: 0; loss: 0.9; acc: 0.8
Batch: 20; loss: 0.97; acc: 0.81
Batch: 40; loss: 1.34; acc: 0.77
Batch: 60; loss: 0.89; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 1.13; acc: 0.81
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.83
Batch: 160; loss: 0.7; acc: 0.86
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.95; acc: 0.78
Batch: 220; loss: 1.36; acc: 0.77
Batch: 240; loss: 0.91; acc: 0.8
Batch: 260; loss: 1.23; acc: 0.73
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.8
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.86; acc: 0.8
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.88; acc: 0.77
Batch: 400; loss: 0.93; acc: 0.8
Batch: 420; loss: 1.28; acc: 0.77
Batch: 440; loss: 1.4; acc: 0.77
Batch: 460; loss: 1.16; acc: 0.77
Batch: 480; loss: 0.85; acc: 0.78
Batch: 500; loss: 0.83; acc: 0.77
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.89; acc: 0.81
Batch: 560; loss: 1.36; acc: 0.72
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.81; acc: 0.75
Batch: 620; loss: 0.75; acc: 0.8
Train Epoch over. train_loss: 0.83; train_accuracy: 0.81 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 1.69; acc: 0.62
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 1.28; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 1.41; acc: 0.67
Val Epoch over. val_loss: 0.8891235140098888; val_accuracy: 0.793093152866242 

Epoch 26 start
Batch: 0; loss: 1.25; acc: 0.72
Batch: 20; loss: 0.7; acc: 0.8
Batch: 40; loss: 1.32; acc: 0.72
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.69; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.83
Batch: 180; loss: 1.17; acc: 0.77
Batch: 200; loss: 0.46; acc: 0.91
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.7; acc: 0.81
Batch: 260; loss: 0.75; acc: 0.8
Batch: 280; loss: 0.5; acc: 0.81
Batch: 300; loss: 0.87; acc: 0.81
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 1.4; acc: 0.84
Batch: 400; loss: 0.74; acc: 0.83
Batch: 420; loss: 1.19; acc: 0.7
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.82; acc: 0.75
Batch: 480; loss: 0.63; acc: 0.8
Batch: 500; loss: 0.97; acc: 0.8
Batch: 520; loss: 1.06; acc: 0.78
Batch: 540; loss: 1.14; acc: 0.69
Batch: 560; loss: 0.85; acc: 0.8
Batch: 580; loss: 0.65; acc: 0.84
Batch: 600; loss: 0.85; acc: 0.73
Batch: 620; loss: 1.05; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 1.7; acc: 0.62
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.81; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 1.27; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 1.42; acc: 0.66
Val Epoch over. val_loss: 0.8873390322847731; val_accuracy: 0.7923964968152867 

Epoch 27 start
Batch: 0; loss: 1.33; acc: 0.77
Batch: 20; loss: 0.46; acc: 0.91
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 1.57; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.86
Batch: 100; loss: 0.85; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.72; acc: 0.78
Batch: 160; loss: 0.89; acc: 0.8
Batch: 180; loss: 1.12; acc: 0.83
Batch: 200; loss: 0.86; acc: 0.78
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.81
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 1.07; acc: 0.78
Batch: 320; loss: 0.77; acc: 0.84
Batch: 340; loss: 0.97; acc: 0.83
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.62; acc: 0.81
Batch: 420; loss: 1.1; acc: 0.73
Batch: 440; loss: 1.04; acc: 0.8
Batch: 460; loss: 0.85; acc: 0.8
Batch: 480; loss: 0.69; acc: 0.88
Batch: 500; loss: 0.96; acc: 0.81
Batch: 520; loss: 0.7; acc: 0.8
Batch: 540; loss: 0.58; acc: 0.8
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 1.37; acc: 0.67
Batch: 620; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 1.69; acc: 0.64
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 1.27; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 1.41; acc: 0.67
Val Epoch over. val_loss: 0.8881155632104084; val_accuracy: 0.7926950636942676 

Epoch 28 start
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.86
Batch: 60; loss: 1.38; acc: 0.73
Batch: 80; loss: 0.9; acc: 0.77
Batch: 100; loss: 0.78; acc: 0.78
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.83; acc: 0.73
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.94; acc: 0.81
Batch: 220; loss: 1.03; acc: 0.75
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.61; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.83
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 1.02; acc: 0.78
Batch: 340; loss: 0.85; acc: 0.77
Batch: 360; loss: 0.82; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.83
Batch: 400; loss: 1.26; acc: 0.75
Batch: 420; loss: 0.99; acc: 0.81
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.65; acc: 0.75
Batch: 480; loss: 0.83; acc: 0.81
Batch: 500; loss: 1.0; acc: 0.81
Batch: 520; loss: 0.89; acc: 0.81
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.65; acc: 0.78
Batch: 580; loss: 0.83; acc: 0.81
Batch: 600; loss: 0.6; acc: 0.88
Batch: 620; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 1.7; acc: 0.64
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 1.28; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 1.41; acc: 0.67
Val Epoch over. val_loss: 0.8890186076521114; val_accuracy: 0.7925955414012739 

Epoch 29 start
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 1.19; acc: 0.73
Batch: 40; loss: 1.07; acc: 0.77
Batch: 60; loss: 1.34; acc: 0.73
Batch: 80; loss: 0.7; acc: 0.84
Batch: 100; loss: 1.29; acc: 0.75
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.71; acc: 0.84
Batch: 160; loss: 0.81; acc: 0.81
Batch: 180; loss: 0.77; acc: 0.84
Batch: 200; loss: 1.13; acc: 0.77
Batch: 220; loss: 1.04; acc: 0.72
Batch: 240; loss: 0.93; acc: 0.78
Batch: 260; loss: 0.73; acc: 0.78
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 1.18; acc: 0.7
Batch: 320; loss: 0.63; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.81
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.7; acc: 0.81
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 1.08; acc: 0.84
Batch: 460; loss: 0.67; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.83
Batch: 500; loss: 1.18; acc: 0.72
Batch: 520; loss: 1.03; acc: 0.75
Batch: 540; loss: 1.33; acc: 0.73
Batch: 560; loss: 0.79; acc: 0.86
Batch: 580; loss: 0.9; acc: 0.83
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 1.7; acc: 0.66
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.77
Batch: 100; loss: 1.28; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 1.4; acc: 0.66
Val Epoch over. val_loss: 0.8888413036704823; val_accuracy: 0.7926950636942676 

Epoch 30 start
Batch: 0; loss: 0.77; acc: 0.72
Batch: 20; loss: 1.23; acc: 0.7
Batch: 40; loss: 1.27; acc: 0.77
Batch: 60; loss: 1.06; acc: 0.75
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.79; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.91; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.76; acc: 0.78
Batch: 240; loss: 0.93; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 1.0; acc: 0.78
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.71; acc: 0.84
Batch: 360; loss: 1.58; acc: 0.66
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.75; acc: 0.84
Batch: 420; loss: 0.87; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 1.15; acc: 0.75
Batch: 480; loss: 0.98; acc: 0.78
Batch: 500; loss: 1.11; acc: 0.73
Batch: 520; loss: 1.03; acc: 0.78
Batch: 540; loss: 1.0; acc: 0.72
Batch: 560; loss: 1.42; acc: 0.72
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.82; acc: 0.81
Batch: 620; loss: 1.15; acc: 0.81
Train Epoch over. train_loss: 0.83; train_accuracy: 0.8 

Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 1.71; acc: 0.64
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.82; acc: 0.77
Batch: 80; loss: 0.7; acc: 0.77
Batch: 100; loss: 1.28; acc: 0.8
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 1.41; acc: 0.66
Val Epoch over. val_loss: 0.8884505385616023; val_accuracy: 0.7929936305732485 

plots/subspace_training/lenet/2020-01-10 05:00:58/d_dim_200_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 1.71; acc: 0.44
Batch: 40; loss: 1.23; acc: 0.62
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.05; acc: 0.7
Batch: 100; loss: 0.71; acc: 0.77
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.98; acc: 0.72
Batch: 160; loss: 1.12; acc: 0.72
Batch: 180; loss: 0.78; acc: 0.72
Batch: 200; loss: 0.64; acc: 0.73
Batch: 220; loss: 0.82; acc: 0.78
Batch: 240; loss: 0.94; acc: 0.75
Batch: 260; loss: 1.15; acc: 0.72
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.84; acc: 0.8
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.47; acc: 0.81
Batch: 380; loss: 0.66; acc: 0.75
Batch: 400; loss: 0.57; acc: 0.77
Batch: 420; loss: 0.76; acc: 0.83
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.83; acc: 0.8
Batch: 480; loss: 0.7; acc: 0.88
Batch: 500; loss: 1.0; acc: 0.77
Batch: 520; loss: 0.94; acc: 0.73
Batch: 540; loss: 0.57; acc: 0.78
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 0.62; acc: 0.8
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.78; acc: 0.8
Train Epoch over. train_loss: 0.96; train_accuracy: 0.74 

Batch: 0; loss: 0.45; acc: 0.81
Batch: 20; loss: 0.84; acc: 0.8
Batch: 40; loss: 0.46; acc: 0.83
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 1.87; acc: 0.66
Val Epoch over. val_loss: 0.7097680014409836; val_accuracy: 0.794984076433121 

Epoch 2 start
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.55; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.77
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.8
Batch: 160; loss: 0.84; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.86; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 1.11; acc: 0.77
Batch: 320; loss: 0.63; acc: 0.83
Batch: 340; loss: 0.66; acc: 0.78
Batch: 360; loss: 0.9; acc: 0.73
Batch: 380; loss: 0.72; acc: 0.83
Batch: 400; loss: 0.91; acc: 0.81
Batch: 420; loss: 0.84; acc: 0.78
Batch: 440; loss: 0.73; acc: 0.77
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.8; acc: 0.77
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.9; acc: 0.8
Batch: 620; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.82 

Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.95; acc: 0.8
Batch: 40; loss: 0.28; acc: 0.86
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.99; acc: 0.8
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 1.54; acc: 0.66
Val Epoch over. val_loss: 0.640180520286226; val_accuracy: 0.8152866242038217 

Epoch 3 start
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.72
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.86
Batch: 80; loss: 0.83; acc: 0.84
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.48; acc: 0.81
Batch: 200; loss: 0.56; acc: 0.89
Batch: 220; loss: 0.65; acc: 0.78
Batch: 240; loss: 0.81; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.92; acc: 0.72
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.75; acc: 0.77
Batch: 340; loss: 0.69; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 1.01; acc: 0.75
Batch: 420; loss: 0.82; acc: 0.75
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.85; acc: 0.78
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.65; acc: 0.81
Batch: 580; loss: 0.62; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.6; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.88; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 1.42; acc: 0.64
Val Epoch over. val_loss: 0.6112030763060424; val_accuracy: 0.8318073248407644 

Epoch 4 start
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 1.23; acc: 0.72
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.8; acc: 0.83
Batch: 80; loss: 0.6; acc: 0.89
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.49; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.75; acc: 0.8
Batch: 220; loss: 0.33; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.83
Batch: 260; loss: 0.37; acc: 0.84
Batch: 280; loss: 0.63; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.81
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.86
Batch: 380; loss: 0.73; acc: 0.78
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.8
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.65; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.53; acc: 0.8
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.88; acc: 0.78
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.57; train_accuracy: 0.84 

Batch: 0; loss: 0.62; acc: 0.77
Batch: 20; loss: 1.12; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.91
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 0.55; acc: 0.91
Batch: 140; loss: 1.7; acc: 0.66
Val Epoch over. val_loss: 0.6748452095469092; val_accuracy: 0.8153861464968153 

Epoch 5 start
Batch: 0; loss: 0.56; acc: 0.89
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.97; acc: 0.84
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.82; acc: 0.78
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 0.79; acc: 0.77
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.81
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.87; acc: 0.81
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.86
Batch: 600; loss: 0.66; acc: 0.84
Batch: 620; loss: 0.82; acc: 0.84
Train Epoch over. train_loss: 0.57; train_accuracy: 0.84 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.81
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.64; acc: 0.77
Batch: 100; loss: 0.7; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 1.52; acc: 0.61
Val Epoch over. val_loss: 0.5563916482363537; val_accuracy: 0.8355891719745223 

Epoch 6 start
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 1.08; acc: 0.73
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.7; acc: 0.8
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.79; acc: 0.8
Batch: 260; loss: 0.8; acc: 0.81
Batch: 280; loss: 0.53; acc: 0.81
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.68; acc: 0.78
Batch: 340; loss: 0.78; acc: 0.75
Batch: 360; loss: 0.38; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.8
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.67; acc: 0.8
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.89
Batch: 520; loss: 0.7; acc: 0.86
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.72; acc: 0.84
Batch: 580; loss: 0.67; acc: 0.83
Batch: 600; loss: 0.53; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.56; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 1.15; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.78
Batch: 100; loss: 0.63; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 1.43; acc: 0.69
Val Epoch over. val_loss: 0.5479902234047082; val_accuracy: 0.8457404458598726 

Epoch 7 start
Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.88; acc: 0.78
Batch: 160; loss: 0.79; acc: 0.78
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.77; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.81
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.72; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.56; acc: 0.78
Batch: 360; loss: 0.85; acc: 0.78
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.49; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.81
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.72; acc: 0.83
Batch: 520; loss: 0.51; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.56; train_accuracy: 0.85 

Batch: 0; loss: 0.36; acc: 0.84
Batch: 20; loss: 1.15; acc: 0.72
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 2.02; acc: 0.61
Val Epoch over. val_loss: 0.6090603406262246; val_accuracy: 0.8256369426751592 

Epoch 8 start
Batch: 0; loss: 0.92; acc: 0.77
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.82; acc: 0.78
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.86
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.8; acc: 0.8
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.88; acc: 0.83
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.84
Batch: 300; loss: 0.22; acc: 0.91
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.66; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.66; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.31; acc: 0.84
Batch: 600; loss: 0.75; acc: 0.77
Batch: 620; loss: 0.91; acc: 0.75
Train Epoch over. train_loss: 0.55; train_accuracy: 0.85 

Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 1.34; acc: 0.67
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.88
Batch: 100; loss: 0.9; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 1.31; acc: 0.67
Val Epoch over. val_loss: 0.6491962399832003; val_accuracy: 0.8246417197452229 

Epoch 9 start
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 1.1; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.92
Batch: 140; loss: 0.85; acc: 0.8
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.75; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 1.1; acc: 0.77
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.87; acc: 0.78
Batch: 300; loss: 0.67; acc: 0.75
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.78; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.66; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.91; acc: 0.77
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.67; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.85 

Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.97; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.9; acc: 0.8
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 1.4; acc: 0.66
Val Epoch over. val_loss: 0.5966611924065146; val_accuracy: 0.8408638535031847 

Epoch 10 start
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.47; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.84
Batch: 200; loss: 0.68; acc: 0.78
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.6; acc: 0.8
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.81; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.83
Batch: 460; loss: 0.68; acc: 0.78
Batch: 480; loss: 0.6; acc: 0.81
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.94; acc: 0.77
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.96; acc: 0.77
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.94; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.84
Train Epoch over. train_loss: 0.55; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.83
Batch: 20; loss: 1.12; acc: 0.72
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.92; acc: 0.83
Batch: 100; loss: 1.0; acc: 0.81
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 1.37; acc: 0.61
Val Epoch over. val_loss: 0.6086598251275955; val_accuracy: 0.8321058917197452 

Epoch 11 start
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.71; acc: 0.84
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 0.49; acc: 0.84
Batch: 220; loss: 0.94; acc: 0.81
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.9; acc: 0.83
Batch: 360; loss: 0.29; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.83
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.74; acc: 0.83
Batch: 540; loss: 1.04; acc: 0.81
Batch: 560; loss: 0.72; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.84
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.95; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.74; acc: 0.83
Batch: 100; loss: 0.82; acc: 0.84
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 1.44; acc: 0.64
Val Epoch over. val_loss: 0.48721689019043735; val_accuracy: 0.8641520700636943 

Epoch 12 start
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.65; acc: 0.8
Batch: 240; loss: 0.86; acc: 0.77
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.88
Batch: 320; loss: 0.58; acc: 0.84
Batch: 340; loss: 0.74; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.77; acc: 0.78
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.78; acc: 0.8
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.64; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.88 

Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.71; acc: 0.83
Batch: 100; loss: 0.79; acc: 0.84
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 1.38; acc: 0.69
Val Epoch over. val_loss: 0.4802700665061641; val_accuracy: 0.8640525477707006 

Epoch 13 start
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.91
Batch: 140; loss: 0.82; acc: 0.84
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.66; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.61; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.63; acc: 0.86
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.67; acc: 0.77
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.88 

Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.93; acc: 0.77
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.95
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.84
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 1.34; acc: 0.7
Val Epoch over. val_loss: 0.48298635338522067; val_accuracy: 0.863156847133758 

Epoch 14 start
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.83
Batch: 200; loss: 0.21; acc: 0.88
Batch: 220; loss: 0.82; acc: 0.78
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.5; acc: 0.81
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.51; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.59; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.77
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.91
Batch: 580; loss: 0.76; acc: 0.84
Batch: 600; loss: 0.55; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.84
Train Epoch over. train_loss: 0.44; train_accuracy: 0.88 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.95
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.83
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 1.34; acc: 0.67
Val Epoch over. val_loss: 0.47986176809307873; val_accuracy: 0.8655453821656051 

Epoch 15 start
Batch: 0; loss: 0.73; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.67; acc: 0.77
Batch: 120; loss: 0.73; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.23; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.77
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.64; acc: 0.78
Train Epoch over. train_loss: 0.44; train_accuracy: 0.88 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.84
Batch: 100; loss: 0.84; acc: 0.81
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 1.33; acc: 0.69
Val Epoch over. val_loss: 0.4764501174354249; val_accuracy: 0.8654458598726115 

Epoch 16 start
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.71; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.62; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.86
Batch: 200; loss: 0.67; acc: 0.88
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.73; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.73; acc: 0.78
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.6; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.94; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.63; acc: 0.86
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 1.3; acc: 0.67
Val Epoch over. val_loss: 0.47810656401761775; val_accuracy: 0.8651472929936306 

Epoch 17 start
Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.74; acc: 0.78
Batch: 280; loss: 0.69; acc: 0.81
Batch: 300; loss: 0.48; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.7; acc: 0.81
Batch: 360; loss: 0.71; acc: 0.81
Batch: 380; loss: 0.33; acc: 0.86
Batch: 400; loss: 0.5; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.8; acc: 0.8
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.8; acc: 0.8
Batch: 540; loss: 0.36; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.71; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.89; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 1.32; acc: 0.66
Val Epoch over. val_loss: 0.4739544406817977; val_accuracy: 0.8688296178343949 

Epoch 18 start
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.52; acc: 0.81
Batch: 200; loss: 0.6; acc: 0.8
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.81
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.63; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.49; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.6; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.81
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.83; acc: 0.78
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 1.27; acc: 0.75
Val Epoch over. val_loss: 0.4781536875162155; val_accuracy: 0.865047770700637 

Epoch 19 start
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.78; acc: 0.77
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.58; acc: 0.81
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.68; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.87; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.8
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.84; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.81; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 1.27; acc: 0.72
Val Epoch over. val_loss: 0.47725832519257905; val_accuracy: 0.8680334394904459 

Epoch 20 start
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.5; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.58; acc: 0.81
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.56; acc: 0.88
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.53; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.34; acc: 0.81
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.89
Batch: 440; loss: 0.84; acc: 0.8
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.75; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.79; acc: 0.8
Batch: 620; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.89; acc: 0.8
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.64; acc: 0.86
Batch: 100; loss: 0.8; acc: 0.84
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 1.3; acc: 0.7
Val Epoch over. val_loss: 0.4756312210848377; val_accuracy: 0.865843949044586 

Epoch 21 start
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.78; acc: 0.77
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.81; acc: 0.8
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.3; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.8
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.79; acc: 0.84
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 1.25; acc: 0.7
Val Epoch over. val_loss: 0.46939238222541324; val_accuracy: 0.867734872611465 

Epoch 22 start
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.73; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.65; acc: 0.86
Batch: 220; loss: 0.78; acc: 0.8
Batch: 240; loss: 0.51; acc: 0.75
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.86
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.62; acc: 0.83
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.43; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.83
Batch: 580; loss: 0.68; acc: 0.88
Batch: 600; loss: 0.58; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.83
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 1.25; acc: 0.69
Val Epoch over. val_loss: 0.46951498603744873; val_accuracy: 0.8679339171974523 

Epoch 23 start
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.86
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.83
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.69; acc: 0.77
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.6; acc: 0.83
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 1.26; acc: 0.69
Val Epoch over. val_loss: 0.469302039047715; val_accuracy: 0.8670382165605095 

Epoch 24 start
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.79; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.62; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.78
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.92
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.86; acc: 0.77
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.61; acc: 0.88
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.77; acc: 0.81
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.84; acc: 0.78
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.86
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 1.24; acc: 0.69
Val Epoch over. val_loss: 0.46947048106201134; val_accuracy: 0.8675358280254777 

Epoch 25 start
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.47; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.84
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.69; acc: 0.78
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.96; acc: 0.77
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.84; acc: 0.81
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 1.24; acc: 0.69
Val Epoch over. val_loss: 0.46918225933791724; val_accuracy: 0.8679339171974523 

Epoch 26 start
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.69; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.81
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.73; acc: 0.84
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.69; acc: 0.84
Batch: 320; loss: 0.37; acc: 0.84
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.63; acc: 0.84
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.49; acc: 0.88
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.84; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 1.23; acc: 0.69
Val Epoch over. val_loss: 0.46874336153268814; val_accuracy: 0.8675358280254777 

Epoch 27 start
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.63; acc: 0.89
Batch: 200; loss: 0.65; acc: 0.81
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.72; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.94
Batch: 500; loss: 0.54; acc: 0.81
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 1.25; acc: 0.69
Val Epoch over. val_loss: 0.4703525274422518; val_accuracy: 0.8682324840764332 

Epoch 28 start
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.83
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.86
Batch: 400; loss: 0.82; acc: 0.84
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.74; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.81
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.85; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 1.24; acc: 0.7
Val Epoch over. val_loss: 0.4692113118091966; val_accuracy: 0.8671377388535032 

Epoch 29 start
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 1.01; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.89
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.84
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.84; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 1.23; acc: 0.7
Val Epoch over. val_loss: 0.46942085383614157; val_accuracy: 0.8682324840764332 

Epoch 30 start
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.87; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.6; acc: 0.81
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.89
Batch: 500; loss: 0.66; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.59; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.19; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.52; acc: 0.88
Batch: 20; loss: 0.83; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 1.22; acc: 0.69
Val Epoch over. val_loss: 0.47067195637400744; val_accuracy: 0.8687300955414012 

plots/subspace_training/lenet/2020-01-10 05:00:58/d_dim_300_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 1.53; acc: 0.59
Batch: 40; loss: 1.65; acc: 0.5
Batch: 60; loss: 1.15; acc: 0.72
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.61; acc: 0.75
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.87; acc: 0.78
Batch: 160; loss: 1.13; acc: 0.77
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.93; acc: 0.7
Batch: 240; loss: 0.88; acc: 0.72
Batch: 260; loss: 0.75; acc: 0.75
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.81
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.84; acc: 0.78
Batch: 360; loss: 0.32; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.83
Batch: 520; loss: 0.5; acc: 0.83
Batch: 540; loss: 0.62; acc: 0.8
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.61; acc: 0.78
Train Epoch over. train_loss: 0.86; train_accuracy: 0.77 

Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.6; acc: 0.83
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.66; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.84
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 1.06; acc: 0.77
Val Epoch over. val_loss: 0.5805530480708286; val_accuracy: 0.8286226114649682 

Epoch 2 start
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.83
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.96; acc: 0.77
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.83
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.65; acc: 0.81
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.86
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.84; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.75; acc: 0.83
Batch: 520; loss: 0.73; acc: 0.8
Batch: 540; loss: 0.26; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.8
Batch: 580; loss: 0.23; acc: 0.89
Batch: 600; loss: 0.65; acc: 0.78
Batch: 620; loss: 0.76; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.63; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.93; acc: 0.78
Val Epoch over. val_loss: 0.5133628049853501; val_accuracy: 0.8468351910828026 

Epoch 3 start
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.74; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.61; acc: 0.88
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.55; acc: 0.84
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.66; acc: 0.8
Batch: 420; loss: 0.76; acc: 0.8
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.87; acc: 0.83
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.74; acc: 0.86
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.75
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.98; acc: 0.81
Val Epoch over. val_loss: 0.49513227155633793; val_accuracy: 0.8553941082802548 

Epoch 4 start
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.89; acc: 0.75
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.89
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.8
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.77; acc: 0.78
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.83
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.82; acc: 0.83
Batch: 460; loss: 0.81; acc: 0.78
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.75
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.48; train_accuracy: 0.86 

Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.7
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 1.03; acc: 0.81
Val Epoch over. val_loss: 0.5354337478709069; val_accuracy: 0.8454418789808917 

Epoch 5 start
Batch: 0; loss: 0.48; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.94; acc: 0.81
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.82; acc: 0.81
Batch: 360; loss: 0.35; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.74; acc: 0.77
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.59; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.87 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.72; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 1.08; acc: 0.8
Val Epoch over. val_loss: 0.4556960910321421; val_accuracy: 0.8688296178343949 

Epoch 6 start
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.84
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.84
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.8; acc: 0.81
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.88
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.68; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.73; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 1.09; acc: 0.77
Val Epoch over. val_loss: 0.45278178378465067; val_accuracy: 0.8684315286624203 

Epoch 7 start
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.84
Batch: 160; loss: 0.73; acc: 0.88
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.66; acc: 0.81
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.53; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.42; acc: 0.81
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.73; acc: 0.78
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.88; acc: 0.81
Val Epoch over. val_loss: 0.4635144100067722; val_accuracy: 0.8675358280254777 

Epoch 8 start
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.91; acc: 0.77
Batch: 100; loss: 0.6; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.84
Batch: 200; loss: 0.41; acc: 0.83
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.86
Batch: 540; loss: 1.07; acc: 0.8
Batch: 560; loss: 0.35; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.91
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.81
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 1.04; acc: 0.83
Val Epoch over. val_loss: 0.42607619770013605; val_accuracy: 0.8812699044585988 

Epoch 9 start
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.61; acc: 0.8
Batch: 200; loss: 0.39; acc: 0.83
Batch: 220; loss: 0.96; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.81
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 1.06; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.89; acc: 0.77
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.54; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.27; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.72; acc: 0.8
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.65; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.79; acc: 0.81
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 1.0; acc: 0.75
Val Epoch over. val_loss: 0.5763456721784203; val_accuracy: 0.8416600318471338 

Epoch 10 start
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.66; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.49; acc: 0.77
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.81
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.89; acc: 0.81
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.97; acc: 0.83
Val Epoch over. val_loss: 0.4625009411269692; val_accuracy: 0.8740047770700637 

Epoch 11 start
Batch: 0; loss: 0.25; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.74; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.83
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.28; acc: 0.88
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.3708297797259252; val_accuracy: 0.8915207006369427 

Epoch 12 start
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.57; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.84; acc: 0.81
Val Epoch over. val_loss: 0.3652368686428875; val_accuracy: 0.8945063694267515 

Epoch 13 start
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.61; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.81; acc: 0.81
Val Epoch over. val_loss: 0.37204797217135976; val_accuracy: 0.8916202229299363 

Epoch 14 start
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.91
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.11; acc: 0.94
Batch: 220; loss: 0.64; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.98
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.74; acc: 0.83
Val Epoch over. val_loss: 0.36199855154297156; val_accuracy: 0.8950039808917197 

Epoch 15 start
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.61; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.23; acc: 0.86
Batch: 140; loss: 0.76; acc: 0.81
Val Epoch over. val_loss: 0.36088937383358644; val_accuracy: 0.894406847133758 

Epoch 16 start
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.95
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.86
Batch: 260; loss: 0.61; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.92
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.76; acc: 0.81
Val Epoch over. val_loss: 0.36588953724902146; val_accuracy: 0.8946058917197452 

Epoch 17 start
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.47; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.36; acc: 0.83
Batch: 280; loss: 0.32; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.28; acc: 0.88
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.64; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.55; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.61; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.88
Batch: 140; loss: 0.74; acc: 0.84
Val Epoch over. val_loss: 0.3681915171539328; val_accuracy: 0.8941082802547771 

Epoch 18 start
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.53; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.81
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.66; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.34; acc: 0.84
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.88
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.35969682747296466; val_accuracy: 0.8946058917197452 

Epoch 19 start
Batch: 0; loss: 0.47; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.86
Val Epoch over. val_loss: 0.3605777806821902; val_accuracy: 0.8956011146496815 

Epoch 20 start
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.74; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.71; acc: 0.86
Val Epoch over. val_loss: 0.35945270938953017; val_accuracy: 0.8955015923566879 

Epoch 21 start
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.65; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.22; acc: 0.88
Batch: 140; loss: 0.71; acc: 0.84
Val Epoch over. val_loss: 0.35787045440761145; val_accuracy: 0.8953025477707006 

Epoch 22 start
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.92
Batch: 140; loss: 0.64; acc: 0.88
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.68; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.42; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.54; acc: 0.88
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.72; acc: 0.84
Val Epoch over. val_loss: 0.35709282250446117; val_accuracy: 0.8950039808917197 

Epoch 23 start
Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.5; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.63; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.5; acc: 0.88
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.24; acc: 0.86
Batch: 140; loss: 0.73; acc: 0.84
Val Epoch over. val_loss: 0.3574939834037025; val_accuracy: 0.8958001592356688 

Epoch 24 start
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.92
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.92
Batch: 200; loss: 0.61; acc: 0.81
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.54; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.72; acc: 0.84
Val Epoch over. val_loss: 0.3572883278985692; val_accuracy: 0.895203025477707 

Epoch 25 start
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.58; acc: 0.89
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.57; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.84
Val Epoch over. val_loss: 0.35681417366121987; val_accuracy: 0.8958001592356688 

Epoch 26 start
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.84
Val Epoch over. val_loss: 0.35610322839325403; val_accuracy: 0.8961982484076433 

Epoch 27 start
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.54; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.83
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.72; acc: 0.86
Val Epoch over. val_loss: 0.3570435860544253; val_accuracy: 0.8961982484076433 

Epoch 28 start
Batch: 0; loss: 0.49; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.84
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.89
Batch: 140; loss: 0.72; acc: 0.84
Val Epoch over. val_loss: 0.3565819276745912; val_accuracy: 0.8959992038216561 

Epoch 29 start
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.83
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.84
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.86
Batch: 140; loss: 0.72; acc: 0.84
Val Epoch over. val_loss: 0.3572174179705845; val_accuracy: 0.8958996815286624 

Epoch 30 start
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.86
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.14; acc: 0.92
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.88
Batch: 140; loss: 0.72; acc: 0.84
Val Epoch over. val_loss: 0.35684725464244554; val_accuracy: 0.8957006369426752 

plots/subspace_training/lenet/2020-01-10 05:00:58/d_dim_400_lr_0.1_seed_1_epochs_30_batchsize_64
Epoch 1 start
Batch: 0; loss: 18.69; acc: 0.05
Batch: 20; loss: 1.69; acc: 0.45
Batch: 40; loss: 1.4; acc: 0.52
Batch: 60; loss: 1.46; acc: 0.47
Batch: 80; loss: 1.28; acc: 0.56
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 0.66; acc: 0.73
Batch: 140; loss: 1.13; acc: 0.67
Batch: 160; loss: 1.03; acc: 0.69
Batch: 180; loss: 0.83; acc: 0.77
Batch: 200; loss: 0.53; acc: 0.84
Batch: 220; loss: 0.86; acc: 0.72
Batch: 240; loss: 0.56; acc: 0.77
Batch: 260; loss: 0.87; acc: 0.75
Batch: 280; loss: 0.93; acc: 0.72
Batch: 300; loss: 0.93; acc: 0.72
Batch: 320; loss: 0.59; acc: 0.77
Batch: 340; loss: 0.57; acc: 0.78
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.69; acc: 0.75
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.77; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.81; acc: 0.81
Batch: 520; loss: 0.6; acc: 0.78
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.81
Train Epoch over. train_loss: 0.87; train_accuracy: 0.75 

Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.7; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.84
Batch: 100; loss: 0.78; acc: 0.81
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.85; acc: 0.75
Val Epoch over. val_loss: 0.5375765626597556; val_accuracy: 0.84484474522293 

Epoch 2 start
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.78
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.49; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.58; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.51; acc: 0.8
Batch: 620; loss: 0.52; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.83; acc: 0.7
Val Epoch over. val_loss: 0.42926664498581246; val_accuracy: 0.8706210191082803 

Epoch 3 start
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.92; acc: 0.75
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.97
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.85; acc: 0.78
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.46; acc: 0.92
Batch: 480; loss: 0.79; acc: 0.77
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.83; acc: 0.78
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.77; acc: 0.78
Val Epoch over. val_loss: 0.3856488021600778; val_accuracy: 0.8863455414012739 

Epoch 4 start
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.62; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.55; acc: 0.89
Batch: 220; loss: 0.15; acc: 0.92
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.35553178053562806; val_accuracy: 0.8964968152866242 

Epoch 5 start
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.83
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.49; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.57; acc: 0.8
Val Epoch over. val_loss: 0.3492429879060976; val_accuracy: 0.8971934713375797 

Epoch 6 start
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.44; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.62; acc: 0.81
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.8
Val Epoch over. val_loss: 0.33107171672734487; val_accuracy: 0.8995820063694268 

Epoch 7 start
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.84
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.92
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.19; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.79; acc: 0.83
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.55; acc: 0.81
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.78
Val Epoch over. val_loss: 0.36313100443903806; val_accuracy: 0.8947054140127388 

Epoch 8 start
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.91
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.8
Train Epoch over. train_loss: 0.33; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.94
Batch: 140; loss: 0.56; acc: 0.83
Val Epoch over. val_loss: 0.3344863803855553; val_accuracy: 0.8983877388535032 

Epoch 9 start
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.63; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.84
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.88
Batch: 260; loss: 0.16; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.86
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.66; acc: 0.83
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.61; acc: 0.81
Val Epoch over. val_loss: 0.3390363253131034; val_accuracy: 0.8986863057324841 

Epoch 10 start
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.69; acc: 0.86
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.24; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.58; acc: 0.84
Val Epoch over. val_loss: 0.4011459533765817; val_accuracy: 0.8808718152866242 

Epoch 11 start
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.98; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.36; acc: 0.95
Batch: 620; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.26; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.84
Val Epoch over. val_loss: 0.2898555546761698; val_accuracy: 0.9143113057324841 

Epoch 12 start
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.47; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.95
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.88
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.84
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.88
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.86
Val Epoch over. val_loss: 0.28246726506170194; val_accuracy: 0.9179936305732485 

Epoch 13 start
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.31; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.88
Val Epoch over. val_loss: 0.2836839433545899; val_accuracy: 0.9145103503184714 

Epoch 14 start
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.14; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.23; acc: 0.89
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.66; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.88
Val Epoch over. val_loss: 0.2830119727381096; val_accuracy: 0.9163017515923567 

Epoch 15 start
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.91
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.59; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.56; acc: 0.86
Val Epoch over. val_loss: 0.2818680094780436; val_accuracy: 0.9173964968152867 

Epoch 16 start
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.52; acc: 0.86
Val Epoch over. val_loss: 0.2814856314213033; val_accuracy: 0.9161027070063694 

Epoch 17 start
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.84
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.89
Batch: 360; loss: 0.61; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.59; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.62; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.86
Val Epoch over. val_loss: 0.27898316742603185; val_accuracy: 0.9161027070063694 

Epoch 18 start
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.88
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.47; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.2; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.88
Val Epoch over. val_loss: 0.28058150270657173; val_accuracy: 0.9169984076433121 

Epoch 19 start
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.96; acc: 0.88
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.86
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.88
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.88
Val Epoch over. val_loss: 0.2816984685742931; val_accuracy: 0.9185907643312102 

Epoch 20 start
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.46; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.49; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.56; acc: 0.88
Val Epoch over. val_loss: 0.28067335253878006; val_accuracy: 0.9173964968152867 

Epoch 21 start
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.2; acc: 0.88
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.86
Val Epoch over. val_loss: 0.27826442583731026; val_accuracy: 0.9169984076433121 

Epoch 22 start
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.83
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.92
Batch: 220; loss: 0.46; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.84
Val Epoch over. val_loss: 0.2776223237442363; val_accuracy: 0.9177945859872612 

Epoch 23 start
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.11; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.88
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.1; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.86
Val Epoch over. val_loss: 0.2780213024301134; val_accuracy: 0.9171974522292994 

Epoch 24 start
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.49; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.97
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.86
Val Epoch over. val_loss: 0.27796693708581527; val_accuracy: 0.9166998407643312 

Epoch 25 start
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.77; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.88
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.86
Val Epoch over. val_loss: 0.27783786573085434; val_accuracy: 0.9164012738853503 

Epoch 26 start
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.53; acc: 0.86
Val Epoch over. val_loss: 0.2777399488125637; val_accuracy: 0.9171974522292994 

Epoch 27 start
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.88
Val Epoch over. val_loss: 0.2784440780331375; val_accuracy: 0.9171974522292994 

Epoch 28 start
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.54; acc: 0.86
Val Epoch over. val_loss: 0.27768182557574506; val_accuracy: 0.9167993630573248 

Epoch 29 start
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.35; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.55; acc: 0.89
Batch: 400; loss: 0.6; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.91
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.39; acc: 0.92
Batch: 620; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.86
Val Epoch over. val_loss: 0.27767909735820856; val_accuracy: 0.9176950636942676 

Epoch 30 start
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.08; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.86
Val Epoch over. val_loss: 0.27788215374965575; val_accuracy: 0.9168988853503185 

plots/subspace_training/lenet/2020-01-10 05:00:58/d_dim_500_lr_0.1_seed_1_epochs_30_batchsize_64
plots/subspace_training/lenet/2020-01-10 05:00:58/d_dim_XXXXX_lr_0.1_seed_1_epochs_30_batchsize_64
