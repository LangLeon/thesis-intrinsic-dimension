Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.84; acc: 0.39
Batch: 60; loss: 1.75; acc: 0.31
Batch: 80; loss: 1.72; acc: 0.38
Batch: 100; loss: 1.08; acc: 0.66
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 1.3; acc: 0.53
Batch: 160; loss: 0.46; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.81
Batch: 220; loss: 0.74; acc: 0.67
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09902543028828445; val_accuracy: 0.9690485668789809 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.4; acc: 0.83
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07522822532114709; val_accuracy: 0.9767117834394905 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.94
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.16623268119848458; val_accuracy: 0.9484474522292994 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06314098761434768; val_accuracy: 0.980593152866242 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059831318226020044; val_accuracy: 0.9830812101910829 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05024870318971622; val_accuracy: 0.9853702229299363 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.062274814506245266; val_accuracy: 0.9808917197452229 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.95
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.13799826568289167; val_accuracy: 0.9620820063694268 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05442297885752028; val_accuracy: 0.9840764331210191 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06372684401691339; val_accuracy: 0.9837778662420382 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05883722685894389; val_accuracy: 0.9847730891719745 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05306962751753771; val_accuracy: 0.9855692675159236 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053902884980865345; val_accuracy: 0.9859673566878981 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052574600621013885; val_accuracy: 0.9867635350318471 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05944906666305414; val_accuracy: 0.9856687898089171 

Epoch 16 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06893110600341658; val_accuracy: 0.984375 

Epoch 17 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05926260515857654; val_accuracy: 0.9861664012738853 

Epoch 18 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0613562076761844; val_accuracy: 0.9865644904458599 

Epoch 19 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06796885673312625; val_accuracy: 0.986265923566879 

Epoch 20 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062239357881295455; val_accuracy: 0.9871616242038217 

Epoch 21 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06359733660129985; val_accuracy: 0.9871616242038217 

Epoch 22 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0702266863244734; val_accuracy: 0.9865644904458599 

Epoch 23 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06364279752893812; val_accuracy: 0.9869625796178344 

Epoch 24 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07004247553599109; val_accuracy: 0.9866640127388535 

Epoch 25 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06975105649251846; val_accuracy: 0.9860668789808917 

Epoch 26 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0704312747830798; val_accuracy: 0.9859673566878981 

Epoch 27 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07255442521184872; val_accuracy: 0.9871616242038217 

Epoch 28 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07479943966220139; val_accuracy: 0.9877587579617835 

Epoch 29 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07117672271694347; val_accuracy: 0.9867635350318471 

Epoch 30 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07219387956295803; val_accuracy: 0.9868630573248408 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07189014710628303; val_accuracy: 0.9872611464968153 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0731455711707188; val_accuracy: 0.9868630573248408 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07366437635793807; val_accuracy: 0.9872611464968153 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07480468339980788; val_accuracy: 0.9871616242038217 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07889490546124756; val_accuracy: 0.9864649681528662 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07566459177975442; val_accuracy: 0.9878582802547771 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07736616894887512; val_accuracy: 0.9872611464968153 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07693497997939966; val_accuracy: 0.9875597133757962 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07742168312999094; val_accuracy: 0.9874601910828026 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07904235069539137; val_accuracy: 0.9873606687898089 

Epoch 41 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07970991094780576; val_accuracy: 0.9871616242038217 

Epoch 42 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07955309544589109; val_accuracy: 0.9871616242038217 

Epoch 43 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08003335132910187; val_accuracy: 0.987062101910828 

Epoch 44 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08059692615346545; val_accuracy: 0.9875597133757962 

Epoch 45 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08073882241347793; val_accuracy: 0.9879578025477707 

Epoch 46 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08119331580248608; val_accuracy: 0.9872611464968153 

Epoch 47 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08132906974690735; val_accuracy: 0.9875597133757962 

Epoch 48 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08169810943732596; val_accuracy: 0.9879578025477707 

Epoch 49 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08199405978629544; val_accuracy: 0.9874601910828026 

Epoch 50 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08242962670743845; val_accuracy: 0.9875597133757962 

plots/no_subspace_training/reg_lenet/2020-01-18 22:04:41/d_dim_1000_lr_0.1_gamma_0.8_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.84; acc: 0.39
Batch: 60; loss: 1.8; acc: 0.28
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.08; acc: 0.64
Batch: 120; loss: 1.03; acc: 0.61
Batch: 140; loss: 0.79; acc: 0.67
Batch: 160; loss: 0.43; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.7
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09870741827188025; val_accuracy: 0.970640923566879 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.44; acc: 0.81
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07772430924663118; val_accuracy: 0.9763136942675159 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.92
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09226842487978328; val_accuracy: 0.9716361464968153 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06208688689834753; val_accuracy: 0.9807921974522293 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05948511195505501; val_accuracy: 0.9821855095541401 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05302100683188742; val_accuracy: 0.9846735668789809 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.06360738615321505; val_accuracy: 0.9810907643312102 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.11478909923677232; val_accuracy: 0.9680533439490446 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.06797376507596606; val_accuracy: 0.9808917197452229 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06390368414058048; val_accuracy: 0.9830812101910829 

Epoch 11 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052640243747811406; val_accuracy: 0.9857683121019108 

Epoch 12 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055827149492540175; val_accuracy: 0.9851711783439491 

Epoch 13 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05276716127991676; val_accuracy: 0.9867635350318471 

Epoch 14 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.21; acc: 0.98
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057440276572089284; val_accuracy: 0.9864649681528662 

Epoch 15 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05335282078783983; val_accuracy: 0.9868630573248408 

Epoch 16 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06159618454184502; val_accuracy: 0.9851711783439491 

Epoch 17 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052200160801980144; val_accuracy: 0.9875597133757962 

Epoch 18 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059547497493446254; val_accuracy: 0.9873606687898089 

Epoch 19 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06403067266675318; val_accuracy: 0.9857683121019108 

Epoch 20 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06598391884546372; val_accuracy: 0.9860668789808917 

Epoch 21 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05779449126807747; val_accuracy: 0.9867635350318471 

Epoch 22 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0602143265686597; val_accuracy: 0.9872611464968153 

Epoch 23 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058698691214155996; val_accuracy: 0.9874601910828026 

Epoch 24 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06072324319819736; val_accuracy: 0.9872611464968153 

Epoch 25 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061795360296015525; val_accuracy: 0.9875597133757962 

Epoch 26 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06262706917752127; val_accuracy: 0.9876592356687898 

Epoch 27 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06423499420949608; val_accuracy: 0.9877587579617835 

Epoch 28 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06587171772862696; val_accuracy: 0.9865644904458599 

Epoch 29 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06661950341265672; val_accuracy: 0.9868630573248408 

Epoch 30 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06591700667598445; val_accuracy: 0.988156847133758 

Epoch 31 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06540295382024376; val_accuracy: 0.9880573248407644 

Epoch 32 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06678577531485042; val_accuracy: 0.9880573248407644 

Epoch 33 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06730707829735082; val_accuracy: 0.988156847133758 

Epoch 34 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06765957219395669; val_accuracy: 0.9874601910828026 

Epoch 35 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06826444871866019; val_accuracy: 0.988156847133758 

Epoch 36 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06819767305615601; val_accuracy: 0.9875597133757962 

Epoch 37 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06956565788217411; val_accuracy: 0.9880573248407644 

Epoch 38 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.01; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06969702091945965; val_accuracy: 0.9875597133757962 

Epoch 39 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06917884471310172; val_accuracy: 0.988156847133758 

Epoch 40 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07055114693702406; val_accuracy: 0.9875597133757962 

Epoch 41 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07095000859658429; val_accuracy: 0.9879578025477707 

Epoch 42 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07069706869353155; val_accuracy: 0.9878582802547771 

Epoch 43 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0709781394262982; val_accuracy: 0.9876592356687898 

Epoch 44 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07136177077035236; val_accuracy: 0.9879578025477707 

Epoch 45 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07146549784833459; val_accuracy: 0.9880573248407644 

Epoch 46 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07172171633904147; val_accuracy: 0.9877587579617835 

Epoch 47 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07215620942745998; val_accuracy: 0.9877587579617835 

Epoch 48 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07246411781591974; val_accuracy: 0.9879578025477707 

Epoch 49 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0724269964607658; val_accuracy: 0.9883558917197452 

Epoch 50 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07279054924940608; val_accuracy: 0.9882563694267515 

plots/no_subspace_training/reg_lenet/2020-01-18 22:14:07/d_dim_1000_lr_0.1_gamma_0.8_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.82; acc: 0.25
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.08; acc: 0.66
Batch: 120; loss: 2.72; acc: 0.41
Batch: 140; loss: 1.57; acc: 0.58
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.72; acc: 0.67
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.48; train_accuracy: 0.84 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10707343103969173; val_accuracy: 0.9675557324840764 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09289759767662947; val_accuracy: 0.9707404458598726 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06930061372791886; val_accuracy: 0.9793988853503185 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057153662975141954; val_accuracy: 0.9831807324840764 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.95
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05726758998100925; val_accuracy: 0.9820859872611465 

Epoch 6 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05003062697352877; val_accuracy: 0.9852707006369427 

Epoch 7 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.0593474967417064; val_accuracy: 0.9826831210191083 

Epoch 8 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0675292608037496; val_accuracy: 0.9822850318471338 

Epoch 9 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 0.98
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05646475005871172; val_accuracy: 0.9841759554140127 

Epoch 10 start
The current lr is: 0.08000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05786111409876757; val_accuracy: 0.9836783439490446 

Epoch 11 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05270187483186935; val_accuracy: 0.9859673566878981 

Epoch 12 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052404313662629216; val_accuracy: 0.9853702229299363 

Epoch 13 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052641369544776384; val_accuracy: 0.9858678343949044 

Epoch 14 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061450449476955805; val_accuracy: 0.9838773885350318 

Epoch 15 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06075288991259921; val_accuracy: 0.984375 

Epoch 16 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05855301295401184; val_accuracy: 0.9849721337579618 

Epoch 17 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05283210317420352; val_accuracy: 0.9864649681528662 

Epoch 18 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05769542379269175; val_accuracy: 0.987062101910828 

Epoch 19 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060530101702471445; val_accuracy: 0.9858678343949044 

Epoch 20 start
The current lr is: 0.051200000000000016
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.21; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05740497384671193; val_accuracy: 0.986265923566879 

Epoch 21 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05773950123767944; val_accuracy: 0.9868630573248408 

Epoch 22 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060082562552515865; val_accuracy: 0.9865644904458599 

Epoch 23 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05957063095300061; val_accuracy: 0.9867635350318471 

Epoch 24 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060984127484499266; val_accuracy: 0.9864649681528662 

Epoch 25 start
The current lr is: 0.04096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05990189755228675; val_accuracy: 0.9875597133757962 

Epoch 26 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06197868638737186; val_accuracy: 0.9867635350318471 

Epoch 27 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06233618920965559; val_accuracy: 0.9867635350318471 

Epoch 28 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06288275191453611; val_accuracy: 0.9868630573248408 

Epoch 29 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06389298332724602; val_accuracy: 0.9865644904458599 

Epoch 30 start
The current lr is: 0.03276800000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06383469800470741; val_accuracy: 0.9872611464968153 

Epoch 31 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06322182245125436; val_accuracy: 0.9871616242038217 

Epoch 32 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06425252129697497; val_accuracy: 0.987062101910828 

Epoch 33 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06540623487560612; val_accuracy: 0.987062101910828 

Epoch 34 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0657205534683671; val_accuracy: 0.9867635350318471 

Epoch 35 start
The current lr is: 0.026214400000000013
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06593552165350337; val_accuracy: 0.9871616242038217 

Epoch 36 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0660505851932392; val_accuracy: 0.9866640127388535 

Epoch 37 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06699546051632826; val_accuracy: 0.9866640127388535 

Epoch 38 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06695519022311375; val_accuracy: 0.9867635350318471 

Epoch 39 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06673217213647381; val_accuracy: 0.9871616242038217 

Epoch 40 start
The current lr is: 0.020971520000000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06762668205674287; val_accuracy: 0.9874601910828026 

Epoch 41 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06722997883512716; val_accuracy: 0.9874601910828026 

Epoch 42 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06744670953340591; val_accuracy: 0.987062101910828 

Epoch 43 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06806397950573331; val_accuracy: 0.9871616242038217 

Epoch 44 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06828232493939673; val_accuracy: 0.9872611464968153 

Epoch 45 start
The current lr is: 0.016777216000000008
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06849332215489855; val_accuracy: 0.987062101910828 

Epoch 46 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0686459033542378; val_accuracy: 0.9872611464968153 

Epoch 47 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06882541009764763; val_accuracy: 0.987062101910828 

Epoch 48 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06917265721946765; val_accuracy: 0.9869625796178344 

Epoch 49 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0690884182977069; val_accuracy: 0.987062101910828 

Epoch 50 start
The current lr is: 0.013421772800000007
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06897510379363017; val_accuracy: 0.987062101910828 

plots/no_subspace_training/reg_lenet/2020-01-18 22:23:22/d_dim_1000_lr_0.1_gamma_0.8_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.71; acc: 0.34
Batch: 80; loss: 1.71; acc: 0.36
Batch: 100; loss: 1.1; acc: 0.64
Batch: 120; loss: 1.18; acc: 0.55
Batch: 140; loss: 0.59; acc: 0.73
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.76; acc: 0.66
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09557163164873791; val_accuracy: 0.9705414012738853 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.5; acc: 0.8
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.09271920375098848; val_accuracy: 0.970640923566879 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07985993023890599; val_accuracy: 0.9762141719745223 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06458542263431913; val_accuracy: 0.9802945859872612 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05587261858259796; val_accuracy: 0.9834792993630573 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05232464211287013; val_accuracy: 0.9847730891719745 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.06440311278791944; val_accuracy: 0.9815883757961783 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.4180338297774837; val_accuracy: 0.912718949044586 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.6; acc: 0.91
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.05841229768220786; val_accuracy: 0.9831807324840764 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057932467597305394; val_accuracy: 0.9845740445859873 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.95
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0516358352960295; val_accuracy: 0.9861664012738853 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054690610214023834; val_accuracy: 0.9851711783439491 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.18379377113406065; val_accuracy: 0.961484872611465 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057969899076944706; val_accuracy: 0.9850716560509554 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 1.59; acc: 0.88
Batch: 20; loss: 1.33; acc: 0.88
Batch: 40; loss: 1.88; acc: 0.86
Batch: 60; loss: 1.17; acc: 0.86
Batch: 80; loss: 2.22; acc: 0.84
Batch: 100; loss: 1.84; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.94
Val Epoch over. val_loss: 1.3622254474907165; val_accuracy: 0.8700238853503185 

Epoch 16 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 1.93; acc: 0.86
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05016220230490539; val_accuracy: 0.9873606687898089 

Epoch 17 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05225788740215787; val_accuracy: 0.9873606687898089 

Epoch 18 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051706929637747964; val_accuracy: 0.9877587579617835 

Epoch 19 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055362859206974126; val_accuracy: 0.9869625796178344 

Epoch 20 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.19; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05560456297010373; val_accuracy: 0.9878582802547771 

Epoch 21 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056895406287946516; val_accuracy: 0.9867635350318471 

Epoch 22 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05837692268145312; val_accuracy: 0.9872611464968153 

Epoch 23 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05781417311566651; val_accuracy: 0.9876592356687898 

Epoch 24 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05990526803834423; val_accuracy: 0.987062101910828 

Epoch 25 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05991136016929226; val_accuracy: 0.9873606687898089 

Epoch 26 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06152444385039579; val_accuracy: 0.9867635350318471 

Epoch 27 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06192264594374948; val_accuracy: 0.9869625796178344 

Epoch 28 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06118422006345858; val_accuracy: 0.9878582802547771 

Epoch 29 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0634110864892507; val_accuracy: 0.9866640127388535 

Epoch 30 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06317656069614325; val_accuracy: 0.9871616242038217 

Epoch 31 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06252221971940083; val_accuracy: 0.9871616242038217 

Epoch 32 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06339428521645298; val_accuracy: 0.9872611464968153 

Epoch 33 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06390811659538062; val_accuracy: 0.9871616242038217 

Epoch 34 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0642629997081058; val_accuracy: 0.9871616242038217 

Epoch 35 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06466188832263278; val_accuracy: 0.9872611464968153 

Epoch 36 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06464793670708967; val_accuracy: 0.9874601910828026 

Epoch 37 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06522628142955197; val_accuracy: 0.9872611464968153 

Epoch 38 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06534345182263927; val_accuracy: 0.9871616242038217 

Epoch 39 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06568719982911067; val_accuracy: 0.9873606687898089 

Epoch 40 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06626830563233917; val_accuracy: 0.9871616242038217 

Epoch 41 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06628644684697413; val_accuracy: 0.987062101910828 

Epoch 42 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06628477535430033; val_accuracy: 0.9872611464968153 

Epoch 43 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06673773862184233; val_accuracy: 0.9869625796178344 

Epoch 44 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06685325982654171; val_accuracy: 0.9874601910828026 

Epoch 45 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06689315967878719; val_accuracy: 0.9872611464968153 

Epoch 46 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06722601038065686; val_accuracy: 0.9872611464968153 

Epoch 47 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06736532921434209; val_accuracy: 0.987062101910828 

Epoch 48 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06730692999757779; val_accuracy: 0.9872611464968153 

Epoch 49 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06742413797575957; val_accuracy: 0.9871616242038217 

Epoch 50 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06744767231926037; val_accuracy: 0.9872611464968153 

plots/no_subspace_training/reg_lenet/2020-01-18 22:32:39/d_dim_1000_lr_0.1_gamma_0.4_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.84; acc: 0.39
Batch: 60; loss: 1.79; acc: 0.3
Batch: 80; loss: 1.69; acc: 0.36
Batch: 100; loss: 1.07; acc: 0.66
Batch: 120; loss: 0.89; acc: 0.67
Batch: 140; loss: 1.17; acc: 0.59
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.69
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10140019880643317; val_accuracy: 0.9690485668789809 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.073883866831945; val_accuracy: 0.976015127388535 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.10934044484784648; val_accuracy: 0.9677547770700637 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06435279280042193; val_accuracy: 0.979796974522293 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05937804087142276; val_accuracy: 0.982484076433121 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05126462964590188; val_accuracy: 0.9847730891719745 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058493629762321524; val_accuracy: 0.9822850318471338 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0826331991820958; val_accuracy: 0.9781050955414012 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.06371346996373432; val_accuracy: 0.9818869426751592 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07016300262918898; val_accuracy: 0.9814888535031847 

Epoch 11 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.22; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049660511267413; val_accuracy: 0.9872611464968153 

Epoch 12 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04968843832137478; val_accuracy: 0.9875597133757962 

Epoch 13 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04842646937271592; val_accuracy: 0.9876592356687898 

Epoch 14 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.21; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051015598260483165; val_accuracy: 0.9872611464968153 

Epoch 15 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04999008632389603; val_accuracy: 0.9880573248407644 

Epoch 16 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054930297598527494; val_accuracy: 0.9867635350318471 

Epoch 17 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052169155780297176; val_accuracy: 0.9877587579617835 

Epoch 18 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05570100926479716; val_accuracy: 0.9867635350318471 

Epoch 19 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05850871374747556; val_accuracy: 0.9867635350318471 

Epoch 20 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05689584705859992; val_accuracy: 0.9871616242038217 

Epoch 21 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057584288630895555; val_accuracy: 0.9874601910828026 

Epoch 22 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05855746588604466; val_accuracy: 0.9872611464968153 

Epoch 23 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05828715967619495; val_accuracy: 0.987062101910828 

Epoch 24 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05922099933692604; val_accuracy: 0.987062101910828 

Epoch 25 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058466748470903204; val_accuracy: 0.9874601910828026 

Epoch 26 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06000162464133493; val_accuracy: 0.9873606687898089 

Epoch 27 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0604004335536319; val_accuracy: 0.987062101910828 

Epoch 28 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06113634154105642; val_accuracy: 0.9873606687898089 

Epoch 29 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06180790095192611; val_accuracy: 0.9869625796178344 

Epoch 30 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06070712787710178; val_accuracy: 0.9874601910828026 

Epoch 31 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06130619586747923; val_accuracy: 0.9873606687898089 

Epoch 32 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061566383927870706; val_accuracy: 0.9872611464968153 

Epoch 33 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06204879191365971; val_accuracy: 0.9872611464968153 

Epoch 34 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06208467739782515; val_accuracy: 0.9873606687898089 

Epoch 35 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06217288116740573; val_accuracy: 0.9872611464968153 

Epoch 36 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06228494608573094; val_accuracy: 0.9873606687898089 

Epoch 37 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06278104956742304; val_accuracy: 0.9874601910828026 

Epoch 38 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06271539543085038; val_accuracy: 0.9874601910828026 

Epoch 39 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06261698204051157; val_accuracy: 0.9874601910828026 

Epoch 40 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0628897384236193; val_accuracy: 0.9876592356687898 

Epoch 41 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06287836938337156; val_accuracy: 0.9874601910828026 

Epoch 42 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06289308475460975; val_accuracy: 0.9874601910828026 

Epoch 43 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06307699104213411; val_accuracy: 0.9872611464968153 

Epoch 44 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06305689117900885; val_accuracy: 0.9873606687898089 

Epoch 45 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06308645375405147; val_accuracy: 0.9874601910828026 

Epoch 46 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06321184811698403; val_accuracy: 0.9874601910828026 

Epoch 47 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06329440415664843; val_accuracy: 0.9875597133757962 

Epoch 48 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06332909534121775; val_accuracy: 0.9873606687898089 

Epoch 49 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06336057252565007; val_accuracy: 0.9874601910828026 

Epoch 50 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06338528757262382; val_accuracy: 0.9873606687898089 

plots/no_subspace_training/reg_lenet/2020-01-18 22:42:05/d_dim_1000_lr_0.1_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.79; acc: 0.28
Batch: 80; loss: 1.7; acc: 0.38
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 1.23; acc: 0.53
Batch: 140; loss: 0.87; acc: 0.69
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.71; acc: 0.7
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10084542829045065; val_accuracy: 0.9680533439490446 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.11152954912109739; val_accuracy: 0.9633757961783439 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06558966658012882; val_accuracy: 0.9804936305732485 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05625273574404656; val_accuracy: 0.9821855095541401 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06340356090456058; val_accuracy: 0.9806926751592356 

Epoch 6 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04693829156695657; val_accuracy: 0.9873606687898089 

Epoch 7 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04752990171598021; val_accuracy: 0.98546974522293 

Epoch 8 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.01; acc: 0.98
Val Epoch over. val_loss: 0.10164598000657027; val_accuracy: 0.9721337579617835 

Epoch 9 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04821500542816842; val_accuracy: 0.9860668789808917 

Epoch 10 start
The current lr is: 0.04000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05190887380462543; val_accuracy: 0.9853702229299363 

Epoch 11 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047295686355821646; val_accuracy: 0.986265923566879 

Epoch 12 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048049258056339944; val_accuracy: 0.9864649681528662 

Epoch 13 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0471974544369491; val_accuracy: 0.9864649681528662 

Epoch 14 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04883196615394513; val_accuracy: 0.9859673566878981 

Epoch 15 start
The current lr is: 0.016000000000000004
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05040562437598113; val_accuracy: 0.9859673566878981 

Epoch 16 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04833778178995582; val_accuracy: 0.9871616242038217 

Epoch 17 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04863383165400499; val_accuracy: 0.987062101910828 

Epoch 18 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.048109468689579875; val_accuracy: 0.9872611464968153 

Epoch 19 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049401697791685725; val_accuracy: 0.9867635350318471 

Epoch 20 start
The current lr is: 0.006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.19; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05000437646629704; val_accuracy: 0.9869625796178344 

Epoch 21 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049967389553785324; val_accuracy: 0.9864649681528662 

Epoch 22 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05013637299275702; val_accuracy: 0.9865644904458599 

Epoch 23 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.97
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0502226377131453; val_accuracy: 0.9865644904458599 

Epoch 24 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05057448893785477; val_accuracy: 0.9865644904458599 

Epoch 25 start
The current lr is: 0.0025600000000000006
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050113422594442486; val_accuracy: 0.987062101910828 

Epoch 26 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050193841408961895; val_accuracy: 0.9867635350318471 

Epoch 27 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.19; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050280252507158146; val_accuracy: 0.9867635350318471 

Epoch 28 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05032832196848408; val_accuracy: 0.9866640127388535 

Epoch 29 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05037649125335323; val_accuracy: 0.9866640127388535 

Epoch 30 start
The current lr is: 0.0010240000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05016904578182348; val_accuracy: 0.9868630573248408 

Epoch 31 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05025881284456344; val_accuracy: 0.987062101910828 

Epoch 32 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05031339176426268; val_accuracy: 0.987062101910828 

Epoch 33 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05034458383348337; val_accuracy: 0.987062101910828 

Epoch 34 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05040785620451733; val_accuracy: 0.9868630573248408 

Epoch 35 start
The current lr is: 0.0004096000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05044427402554803; val_accuracy: 0.9867635350318471 

Epoch 36 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05044885976299359; val_accuracy: 0.9868630573248408 

Epoch 37 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05046047818413965; val_accuracy: 0.9867635350318471 

Epoch 38 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.19; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05046220375284268; val_accuracy: 0.9867635350318471 

Epoch 39 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05046779505766121; val_accuracy: 0.9868630573248408 

Epoch 40 start
The current lr is: 0.00016384000000000006
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05047118903440275; val_accuracy: 0.9868630573248408 

Epoch 41 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05047443395207642; val_accuracy: 0.9868630573248408 

Epoch 42 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05047782344423282; val_accuracy: 0.9868630573248408 

Epoch 43 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05048213218143032; val_accuracy: 0.9868630573248408 

Epoch 44 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050487417893804565; val_accuracy: 0.9868630573248408 

Epoch 45 start
The current lr is: 6.553600000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05048864493799058; val_accuracy: 0.9868630573248408 

Epoch 46 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05049009175057624; val_accuracy: 0.9868630573248408 

Epoch 47 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05049095245873093; val_accuracy: 0.9868630573248408 

Epoch 48 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05049211204431619; val_accuracy: 0.9868630573248408 

Epoch 49 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05049678730736872; val_accuracy: 0.9868630573248408 

Epoch 50 start
The current lr is: 2.6214400000000015e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05049776864849079; val_accuracy: 0.9868630573248408 

plots/no_subspace_training/reg_lenet/2020-01-18 22:51:26/d_dim_1000_lr_0.1_gamma_0.4_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.84; acc: 0.39
Batch: 60; loss: 1.79; acc: 0.28
Batch: 80; loss: 1.67; acc: 0.38
Batch: 100; loss: 1.04; acc: 0.72
Batch: 120; loss: 1.12; acc: 0.58
Batch: 140; loss: 0.78; acc: 0.7
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.49; acc: 0.83
Batch: 220; loss: 0.68; acc: 0.72
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10059202654631275; val_accuracy: 0.96875 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.37; acc: 0.84
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.06; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08874299844643872; val_accuracy: 0.9703423566878981 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08437273513739275; val_accuracy: 0.9750199044585988 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06158976458535073; val_accuracy: 0.9810907643312102 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0610939197859187; val_accuracy: 0.9810907643312102 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.95
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05201548992828199; val_accuracy: 0.9844745222929936 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.05668385782439238; val_accuracy: 0.9827826433121019 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06720975124437338; val_accuracy: 0.9819864649681529 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.92
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06315776867092035; val_accuracy: 0.9826831210191083 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06934745631115452; val_accuracy: 0.9833797770700637 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05568682615924033; val_accuracy: 0.9855692675159236 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06058676994055699; val_accuracy: 0.9846735668789809 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05420615678285338; val_accuracy: 0.9861664012738853 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.19; acc: 0.98
Batch: 680; loss: 0.39; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055707428936555885; val_accuracy: 0.9861664012738853 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08396835012990199; val_accuracy: 0.9789012738853503 

Epoch 16 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05211374272776258; val_accuracy: 0.9868630573248408 

Epoch 17 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05131137214458672; val_accuracy: 0.9872611464968153 

Epoch 18 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05147823662894547; val_accuracy: 0.9872611464968153 

Epoch 19 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05284797065671842; val_accuracy: 0.9876592356687898 

Epoch 20 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05328640638358274; val_accuracy: 0.9869625796178344 

Epoch 21 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05426424350233594; val_accuracy: 0.987062101910828 

Epoch 22 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05471843108534813; val_accuracy: 0.9872611464968153 

Epoch 23 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05522877943648654; val_accuracy: 0.9866640127388535 

Epoch 24 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05627845348730968; val_accuracy: 0.987062101910828 

Epoch 25 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056080768798377104; val_accuracy: 0.987062101910828 

Epoch 26 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057201891164680954; val_accuracy: 0.987062101910828 

Epoch 27 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057841079155350944; val_accuracy: 0.9869625796178344 

Epoch 28 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057597818292060475; val_accuracy: 0.9869625796178344 

Epoch 29 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05842956445019716; val_accuracy: 0.987062101910828 

Epoch 30 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05810133436587966; val_accuracy: 0.9874601910828026 

Epoch 31 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05836049550373083; val_accuracy: 0.9869625796178344 

Epoch 32 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05845303672134496; val_accuracy: 0.9868630573248408 

Epoch 33 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05865035981983895; val_accuracy: 0.987062101910828 

Epoch 34 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05866844767028359; val_accuracy: 0.9868630573248408 

Epoch 35 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058722142035224634; val_accuracy: 0.9872611464968153 

Epoch 36 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058702174241945244; val_accuracy: 0.9869625796178344 

Epoch 37 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05895526996653551; val_accuracy: 0.987062101910828 

Epoch 38 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05893988904964392; val_accuracy: 0.9869625796178344 

Epoch 39 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059067020750349496; val_accuracy: 0.987062101910828 

Epoch 40 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0591341829413821; val_accuracy: 0.987062101910828 

Epoch 41 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0591593295762873; val_accuracy: 0.9869625796178344 

Epoch 42 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05921127379035494; val_accuracy: 0.9871616242038217 

Epoch 43 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05945380577805695; val_accuracy: 0.9873606687898089 

Epoch 44 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059531604883017814; val_accuracy: 0.9871616242038217 

Epoch 45 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05945977020510443; val_accuracy: 0.987062101910828 

Epoch 46 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05953114491644179; val_accuracy: 0.9869625796178344 

Epoch 47 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05957155473008277; val_accuracy: 0.987062101910828 

Epoch 48 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05958966796944855; val_accuracy: 0.987062101910828 

Epoch 49 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05960953088512846; val_accuracy: 0.987062101910828 

Epoch 50 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059622830337589715; val_accuracy: 0.987062101910828 

plots/no_subspace_training/reg_lenet/2020-01-18 23:00:46/d_dim_1000_lr_0.1_gamma_0.2_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.8; acc: 0.28
Batch: 80; loss: 1.71; acc: 0.34
Batch: 100; loss: 1.02; acc: 0.67
Batch: 120; loss: 1.21; acc: 0.62
Batch: 140; loss: 0.57; acc: 0.77
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.51; acc: 0.8
Batch: 220; loss: 0.75; acc: 0.69
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10400182931761073; val_accuracy: 0.9674562101910829 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.08905843564659167; val_accuracy: 0.971437101910828 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08190088510323482; val_accuracy: 0.9758160828025477 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05907575927295123; val_accuracy: 0.9823845541401274 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06031598732065244; val_accuracy: 0.9820859872611465 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05265093606653487; val_accuracy: 0.9839769108280255 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05921767243913784; val_accuracy: 0.9823845541401274 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.95
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07093826249526565; val_accuracy: 0.9792993630573248 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Val Epoch over. val_loss: 0.061602728571861415; val_accuracy: 0.9817874203821656 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06520968203426926; val_accuracy: 0.9835788216560509 

Epoch 11 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04722389218154227; val_accuracy: 0.9872611464968153 

Epoch 12 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04835312327097176; val_accuracy: 0.9871616242038217 

Epoch 13 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047580586449735486; val_accuracy: 0.9865644904458599 

Epoch 14 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049995853429197504; val_accuracy: 0.9859673566878981 

Epoch 15 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050609009850556684; val_accuracy: 0.9867635350318471 

Epoch 16 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05081660279138073; val_accuracy: 0.9861664012738853 

Epoch 17 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153292314309603; val_accuracy: 0.9864649681528662 

Epoch 18 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051725900809096684; val_accuracy: 0.986265923566879 

Epoch 19 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05336479885373146; val_accuracy: 0.9864649681528662 

Epoch 20 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.2; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.95
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053123048085505796; val_accuracy: 0.9858678343949044 

Epoch 21 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05304672450396666; val_accuracy: 0.9860668789808917 

Epoch 22 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05340144897152664; val_accuracy: 0.986265923566879 

Epoch 23 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053330160511337266; val_accuracy: 0.9859673566878981 

Epoch 24 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05360357945037496; val_accuracy: 0.986265923566879 

Epoch 25 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05351671756832463; val_accuracy: 0.9861664012738853 

Epoch 26 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053801401975048575; val_accuracy: 0.9861664012738853 

Epoch 27 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.19; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05404279945762294; val_accuracy: 0.986265923566879 

Epoch 28 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054170047188070926; val_accuracy: 0.986265923566879 

Epoch 29 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054200076515887194; val_accuracy: 0.9860668789808917 

Epoch 30 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05396558116575715; val_accuracy: 0.9860668789808917 

Epoch 31 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054130908956003795; val_accuracy: 0.9861664012738853 

Epoch 32 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054214407968672974; val_accuracy: 0.9861664012738853 

Epoch 33 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05428331454468381; val_accuracy: 0.9861664012738853 

Epoch 34 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0543193283259489; val_accuracy: 0.9861664012738853 

Epoch 35 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054338302914124385; val_accuracy: 0.9861664012738853 

Epoch 36 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054382140327031446; val_accuracy: 0.9861664012738853 

Epoch 37 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054417617144478354; val_accuracy: 0.9861664012738853 

Epoch 38 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.19; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054457399495847665; val_accuracy: 0.9861664012738853 

Epoch 39 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05446037263342529; val_accuracy: 0.9861664012738853 

Epoch 40 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05448039644842694; val_accuracy: 0.9860668789808917 

Epoch 41 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054486521347692815; val_accuracy: 0.9860668789808917 

Epoch 42 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05449078652034899; val_accuracy: 0.9860668789808917 

Epoch 43 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054498469682445955; val_accuracy: 0.9861664012738853 

Epoch 44 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0545074241413812; val_accuracy: 0.9861664012738853 

Epoch 45 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05451231437978471; val_accuracy: 0.9861664012738853 

Epoch 46 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05452045485092576; val_accuracy: 0.9861664012738853 

Epoch 47 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05452732018129841; val_accuracy: 0.9861664012738853 

Epoch 48 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05453337448987232; val_accuracy: 0.9861664012738853 

Epoch 49 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05454264343923824; val_accuracy: 0.9861664012738853 

Epoch 50 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05454812590388736; val_accuracy: 0.9861664012738853 

plots/no_subspace_training/reg_lenet/2020-01-18 23:09:55/d_dim_1000_lr_0.1_gamma_0.2_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.84; acc: 0.39
Batch: 60; loss: 1.76; acc: 0.3
Batch: 80; loss: 1.73; acc: 0.38
Batch: 100; loss: 1.01; acc: 0.72
Batch: 120; loss: 2.05; acc: 0.44
Batch: 140; loss: 1.0; acc: 0.69
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.74; acc: 0.67
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.94
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10505979605446196; val_accuracy: 0.9682523885350318 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.41; acc: 0.83
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.94
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.09995520525392454; val_accuracy: 0.96875 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09397303455384673; val_accuracy: 0.970640923566879 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06380413287574319; val_accuracy: 0.9807921974522293 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.06718847388105029; val_accuracy: 0.9801950636942676 

Epoch 6 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050737105570970824; val_accuracy: 0.9842754777070064 

Epoch 7 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04981765248308516; val_accuracy: 0.9848726114649682 

Epoch 8 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057586786497360584; val_accuracy: 0.9831807324840764 

Epoch 9 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05108547201202174; val_accuracy: 0.9852707006369427 

Epoch 10 start
The current lr is: 0.020000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05409688270016081; val_accuracy: 0.9844745222929936 

Epoch 11 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05122830972644934; val_accuracy: 0.9852707006369427 

Epoch 12 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051154721148644285; val_accuracy: 0.9852707006369427 

Epoch 13 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05091668489822157; val_accuracy: 0.9847730891719745 

Epoch 14 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051074334746523266; val_accuracy: 0.9856687898089171 

Epoch 15 start
The current lr is: 0.004000000000000001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05142420743870887; val_accuracy: 0.9850716560509554 

Epoch 16 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051357220288864365; val_accuracy: 0.9853702229299363 

Epoch 17 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05126180442844986; val_accuracy: 0.9855692675159236 

Epoch 18 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05128825966055226; val_accuracy: 0.98546974522293 

Epoch 19 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05137408439330994; val_accuracy: 0.9856687898089171 

Epoch 20 start
The current lr is: 0.0008000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.23; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05148148441770274; val_accuracy: 0.9856687898089171 

Epoch 21 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.97
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051494560257834235; val_accuracy: 0.9856687898089171 

Epoch 22 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05151023879457431; val_accuracy: 0.9856687898089171 

Epoch 23 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05151219938875763; val_accuracy: 0.9856687898089171 

Epoch 24 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05152807344391847; val_accuracy: 0.9857683121019108 

Epoch 25 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153010693040623; val_accuracy: 0.9857683121019108 

Epoch 26 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153116823476591; val_accuracy: 0.9857683121019108 

Epoch 27 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.21; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153109961349493; val_accuracy: 0.9857683121019108 

Epoch 28 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153163463162009; val_accuracy: 0.9857683121019108 

Epoch 29 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153275637110327; val_accuracy: 0.9857683121019108 

Epoch 30 start
The current lr is: 3.200000000000001e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153279103765822; val_accuracy: 0.9857683121019108 

Epoch 31 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051532635429672374; val_accuracy: 0.9857683121019108 

Epoch 32 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051532776777152046; val_accuracy: 0.9857683121019108 

Epoch 33 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051532874536362426; val_accuracy: 0.9857683121019108 

Epoch 34 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051533034581477476; val_accuracy: 0.9857683121019108 

Epoch 35 start
The current lr is: 6.400000000000003e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153322786946965; val_accuracy: 0.9857683121019108 

Epoch 36 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153316183454672; val_accuracy: 0.9857683121019108 

Epoch 37 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153310559927278; val_accuracy: 0.9857683121019108 

Epoch 38 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.21; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153307055307042; val_accuracy: 0.9857683121019108 

Epoch 39 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.21; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153302212429654; val_accuracy: 0.9857683121019108 

Epoch 40 start
The current lr is: 1.2800000000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153290588100245; val_accuracy: 0.9857683121019108 

Epoch 41 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153289100356922; val_accuracy: 0.9857683121019108 

Epoch 42 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153285716749301; val_accuracy: 0.9857683121019108 

Epoch 43 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153284682210084; val_accuracy: 0.9857683121019108 

Epoch 44 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0515328320158515; val_accuracy: 0.9857683121019108 

Epoch 45 start
The current lr is: 2.560000000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153280197624947; val_accuracy: 0.9857683121019108 

Epoch 46 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.21; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153279640017801; val_accuracy: 0.9857683121019108 

Epoch 47 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.03; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051532798250959175; val_accuracy: 0.9857683121019108 

Epoch 48 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051532793790102005; val_accuracy: 0.9857683121019108 

Epoch 49 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.17; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05153280525070846; val_accuracy: 0.9857683121019108 

Epoch 50 start
The current lr is: 5.120000000000003e-08
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051532806982849816; val_accuracy: 0.9857683121019108 

plots/no_subspace_training/reg_lenet/2020-01-18 23:19:16/d_dim_1000_lr_0.1_gamma_0.2_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.8; acc: 0.28
Batch: 80; loss: 1.8; acc: 0.34
Batch: 100; loss: 0.98; acc: 0.73
Batch: 120; loss: 1.11; acc: 0.59
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.74; acc: 0.7
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10005839349357945; val_accuracy: 0.9689490445859873 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08239803828631237; val_accuracy: 0.973328025477707 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06680269340041337; val_accuracy: 0.9791003184713376 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05856949060111289; val_accuracy: 0.9823845541401274 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06166968845827564; val_accuracy: 0.9823845541401274 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049360572840947255; val_accuracy: 0.9857683121019108 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06489975280632639; val_accuracy: 0.9798964968152867 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.95
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.94
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.2755726227050374; val_accuracy: 0.9298367834394905 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.06279865552665322; val_accuracy: 0.9822850318471338 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05952392226666402; val_accuracy: 0.9850716560509554 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06384555679882407; val_accuracy: 0.9833797770700637 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.95
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059193390296997536; val_accuracy: 0.9840764331210191 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06541734585052084; val_accuracy: 0.9841759554140127 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.52; acc: 0.92
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058817749498945894; val_accuracy: 0.9849721337579618 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062531511376428; val_accuracy: 0.9838773885350318 

Epoch 16 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05024879110182167; val_accuracy: 0.9874601910828026 

Epoch 17 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050271024049562255; val_accuracy: 0.9874601910828026 

Epoch 18 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04975963265273222; val_accuracy: 0.9876592356687898 

Epoch 19 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05156261561687585; val_accuracy: 0.987062101910828 

Epoch 20 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051521675934077825; val_accuracy: 0.9877587579617835 

Epoch 21 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05298376273197733; val_accuracy: 0.9874601910828026 

Epoch 22 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053673298946421616; val_accuracy: 0.9874601910828026 

Epoch 23 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05344588316644833; val_accuracy: 0.9873606687898089 

Epoch 24 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054699325518813104; val_accuracy: 0.9875597133757962 

Epoch 25 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05397732456208794; val_accuracy: 0.987062101910828 

Epoch 26 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05421804938061982; val_accuracy: 0.9873606687898089 

Epoch 27 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05515792999108126; val_accuracy: 0.987062101910828 

Epoch 28 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05579219943588706; val_accuracy: 0.9869625796178344 

Epoch 29 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05720698271112837; val_accuracy: 0.9874601910828026 

Epoch 30 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05578339771385406; val_accuracy: 0.9871616242038217 

Epoch 31 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056119687807787755; val_accuracy: 0.9873606687898089 

Epoch 32 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056280206295715014; val_accuracy: 0.9873606687898089 

Epoch 33 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056390469787037295; val_accuracy: 0.9872611464968153 

Epoch 34 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056482654539452996; val_accuracy: 0.9872611464968153 

Epoch 35 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056599643888177384; val_accuracy: 0.9872611464968153 

Epoch 36 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0565913589137375; val_accuracy: 0.9872611464968153 

Epoch 37 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05666506805332603; val_accuracy: 0.9872611464968153 

Epoch 38 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.056677668503705106; val_accuracy: 0.9872611464968153 

Epoch 39 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05677399048759679; val_accuracy: 0.9872611464968153 

Epoch 40 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05679581594315304; val_accuracy: 0.9871616242038217 

Epoch 41 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05680701051168381; val_accuracy: 0.9872611464968153 

Epoch 42 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05685114352756245; val_accuracy: 0.9872611464968153 

Epoch 43 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05699996806823524; val_accuracy: 0.9872611464968153 

Epoch 44 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057063680687907396; val_accuracy: 0.9872611464968153 

Epoch 45 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05702842005593761; val_accuracy: 0.9872611464968153 

Epoch 46 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05703550458523878; val_accuracy: 0.9872611464968153 

Epoch 47 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05704324018613548; val_accuracy: 0.9872611464968153 

Epoch 48 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057051469447316636; val_accuracy: 0.9872611464968153 

Epoch 49 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.057061404464351144; val_accuracy: 0.9872611464968153 

Epoch 50 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05706893453362641; val_accuracy: 0.9872611464968153 

plots/no_subspace_training/reg_lenet/2020-01-18 23:28:32/d_dim_1000_lr_0.1_gamma_0.13_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.14
Batch: 40; loss: 1.84; acc: 0.39
Batch: 60; loss: 1.77; acc: 0.3
Batch: 80; loss: 1.65; acc: 0.36
Batch: 100; loss: 1.03; acc: 0.72
Batch: 120; loss: 1.42; acc: 0.52
Batch: 140; loss: 1.01; acc: 0.64
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.77; acc: 0.69
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10404294166880049; val_accuracy: 0.9674562101910829 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.44; acc: 0.83
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.08067262834709162; val_accuracy: 0.9745222929936306 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11300832507716622; val_accuracy: 0.9655652866242038 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06314015488146217; val_accuracy: 0.9809912420382165 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06331320052408869; val_accuracy: 0.9810907643312102 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.95
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05260161149084188; val_accuracy: 0.9844745222929936 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06008572285627104; val_accuracy: 0.9819864649681529 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1342488265103975; val_accuracy: 0.9636743630573248 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05368512470251436; val_accuracy: 0.9852707006369427 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05859949216721164; val_accuracy: 0.9849721337579618 

Epoch 11 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04686126964771824; val_accuracy: 0.9874601910828026 

Epoch 12 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04684663302959151; val_accuracy: 0.9874601910828026 

Epoch 13 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04588728101485094; val_accuracy: 0.9873606687898089 

Epoch 14 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0470920897023693; val_accuracy: 0.9871616242038217 

Epoch 15 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04704468476639432; val_accuracy: 0.9869625796178344 

Epoch 16 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04775016377591024; val_accuracy: 0.9869625796178344 

Epoch 17 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04807598067886511; val_accuracy: 0.9872611464968153 

Epoch 18 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04790953181351826; val_accuracy: 0.9875597133757962 

Epoch 19 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049614185858873804; val_accuracy: 0.9867635350318471 

Epoch 20 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.17; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049655537055746005; val_accuracy: 0.9874601910828026 

Epoch 21 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049504888238041264; val_accuracy: 0.9872611464968153 

Epoch 22 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04961596187322762; val_accuracy: 0.9871616242038217 

Epoch 23 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04957598450172479; val_accuracy: 0.9872611464968153 

Epoch 24 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04985779716045993; val_accuracy: 0.987062101910828 

Epoch 25 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04970115209650842; val_accuracy: 0.9873606687898089 

Epoch 26 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049827895085712906; val_accuracy: 0.9873606687898089 

Epoch 27 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04990486100695695; val_accuracy: 0.9871616242038217 

Epoch 28 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049982372315446284; val_accuracy: 0.9871616242038217 

Epoch 29 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05005945839509843; val_accuracy: 0.9873606687898089 

Epoch 30 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049887384815960174; val_accuracy: 0.9874601910828026 

Epoch 31 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04991433584386376; val_accuracy: 0.9874601910828026 

Epoch 32 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049944457473458756; val_accuracy: 0.9874601910828026 

Epoch 33 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0499699557330578; val_accuracy: 0.9874601910828026 

Epoch 34 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04999732897634719; val_accuracy: 0.9874601910828026 

Epoch 35 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05001837487338455; val_accuracy: 0.9874601910828026 

Epoch 36 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050033399419989556; val_accuracy: 0.9874601910828026 

Epoch 37 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05004987614170001; val_accuracy: 0.9874601910828026 

Epoch 38 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05006221900130533; val_accuracy: 0.9874601910828026 

Epoch 39 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05007193242288699; val_accuracy: 0.9873606687898089 

Epoch 40 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0500763145269482; val_accuracy: 0.9873606687898089 

Epoch 41 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05007773471675861; val_accuracy: 0.9873606687898089 

Epoch 42 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050079134880167665; val_accuracy: 0.9873606687898089 

Epoch 43 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05008081973168501; val_accuracy: 0.9873606687898089 

Epoch 44 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05008223057267772; val_accuracy: 0.9873606687898089 

Epoch 45 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05008361208590732; val_accuracy: 0.9873606687898089 

Epoch 46 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05008516263715021; val_accuracy: 0.9873606687898089 

Epoch 47 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05008647344104803; val_accuracy: 0.9873606687898089 

Epoch 48 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05008799798644272; val_accuracy: 0.9873606687898089 

Epoch 49 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05009126331016516; val_accuracy: 0.9873606687898089 

Epoch 50 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050092563317839504; val_accuracy: 0.9873606687898089 

plots/no_subspace_training/reg_lenet/2020-01-18 23:37:54/d_dim_1000_lr_0.1_gamma_0.13_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.78; acc: 0.28
Batch: 80; loss: 1.64; acc: 0.38
Batch: 100; loss: 1.04; acc: 0.7
Batch: 120; loss: 1.27; acc: 0.55
Batch: 140; loss: 0.55; acc: 0.78
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.69
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10490144421435466; val_accuracy: 0.9674562101910829 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07844537627070573; val_accuracy: 0.9761146496815286 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11244825546623795; val_accuracy: 0.9667595541401274 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06305423013533756; val_accuracy: 0.9808917197452229 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.27; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06262253799066422; val_accuracy: 0.9815883757961783 

Epoch 6 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047659812981535675; val_accuracy: 0.9860668789808917 

Epoch 7 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.95
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04693678287184162; val_accuracy: 0.9855692675159236 

Epoch 8 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05115954416572668; val_accuracy: 0.9842754777070064 

Epoch 9 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04734199204642302; val_accuracy: 0.9860668789808917 

Epoch 10 start
The current lr is: 0.013000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04788677100163356; val_accuracy: 0.9860668789808917 

Epoch 11 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04678165990930454; val_accuracy: 0.9865644904458599 

Epoch 12 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04660123297173506; val_accuracy: 0.9865644904458599 

Epoch 13 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046728416612953134; val_accuracy: 0.9865644904458599 

Epoch 14 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046794789137354326; val_accuracy: 0.9865644904458599 

Epoch 15 start
The current lr is: 0.0016900000000000003
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.047023751624640384; val_accuracy: 0.9864649681528662 

Epoch 16 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046969787638847994; val_accuracy: 0.9864649681528662 

Epoch 17 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046933557007722795; val_accuracy: 0.9864649681528662 

Epoch 18 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046907200721228955; val_accuracy: 0.9865644904458599 

Epoch 19 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046902423380476656; val_accuracy: 0.9865644904458599 

Epoch 20 start
The current lr is: 0.00021970000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.95
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046909661334791; val_accuracy: 0.9866640127388535 

Epoch 21 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691054489297472; val_accuracy: 0.9866640127388535 

Epoch 22 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691128878836419; val_accuracy: 0.9866640127388535 

Epoch 23 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691181336618533; val_accuracy: 0.9866640127388535 

Epoch 24 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046912799452900125; val_accuracy: 0.9866640127388535 

Epoch 25 start
The current lr is: 2.8561000000000007e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691350713941702; val_accuracy: 0.9866640127388535 

Epoch 26 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691345916147445; val_accuracy: 0.9866640127388535 

Epoch 27 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.22; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046913322274851954; val_accuracy: 0.9866640127388535 

Epoch 28 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691317841220813; val_accuracy: 0.9866640127388535 

Epoch 29 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691312352942813; val_accuracy: 0.9866640127388535 

Epoch 30 start
The current lr is: 3.7129300000000005e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691306193163441; val_accuracy: 0.9866640127388535 

Epoch 31 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046912996205175; val_accuracy: 0.9866640127388535 

Epoch 32 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691295992511853; val_accuracy: 0.9866640127388535 

Epoch 33 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046912906038912995; val_accuracy: 0.9866640127388535 

Epoch 34 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691286681658903; val_accuracy: 0.9866640127388535 

Epoch 35 start
The current lr is: 4.826809000000001e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691283236358576; val_accuracy: 0.9866640127388535 

Epoch 36 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282863829546; val_accuracy: 0.9866640127388535 

Epoch 37 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282870947935; val_accuracy: 0.9866640127388535 

Epoch 38 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.22; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282239784101; val_accuracy: 0.9866640127388535 

Epoch 39 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282346559938; val_accuracy: 0.9866640127388535 

Epoch 40 start
The current lr is: 6.274851700000001e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282182836988; val_accuracy: 0.9866640127388535 

Epoch 41 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046912822374113046; val_accuracy: 0.9866640127388535 

Epoch 42 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046912820736883555; val_accuracy: 0.9866640127388535 

Epoch 43 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046912821282626714; val_accuracy: 0.9866640127388535 

Epoch 44 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282009622853; val_accuracy: 0.9866640127388535 

Epoch 45 start
The current lr is: 8.157307210000002e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282154363432; val_accuracy: 0.9866640127388535 

Epoch 46 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.22; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282085552337; val_accuracy: 0.9866640127388535 

Epoch 47 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046912820618243735; val_accuracy: 0.9866640127388535 

Epoch 48 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282125889875; val_accuracy: 0.9866640127388535 

Epoch 49 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.17; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046912822374113046; val_accuracy: 0.9866640127388535 

Epoch 50 start
The current lr is: 1.0604499373000003e-09
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04691282199446563; val_accuracy: 0.9866640127388535 

plots/no_subspace_training/reg_lenet/2020-01-18 23:47:18/d_dim_1000_lr_0.1_gamma_0.13_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.76; acc: 0.3
Batch: 80; loss: 1.68; acc: 0.36
Batch: 100; loss: 1.02; acc: 0.72
Batch: 120; loss: 1.13; acc: 0.61
Batch: 140; loss: 0.47; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.75; acc: 0.7
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10051646562898235; val_accuracy: 0.9693471337579618 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.46; acc: 0.81
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.12236232077999479; val_accuracy: 0.9617834394904459 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.10480427030165484; val_accuracy: 0.9669585987261147 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060712034416616344; val_accuracy: 0.9814888535031847 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06415793745760705; val_accuracy: 0.9807921974522293 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05092320337322107; val_accuracy: 0.9844745222929936 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05528642230068043; val_accuracy: 0.9836783439490446 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.279036969563384; val_accuracy: 0.9313296178343949 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.39; acc: 0.92
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.060476685761456275; val_accuracy: 0.9819864649681529 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05937118384583741; val_accuracy: 0.9849721337579618 

Epoch 11 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.18; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05363762034637153; val_accuracy: 0.9850716560509554 

Epoch 12 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.055119897944817114; val_accuracy: 0.9850716560509554 

Epoch 13 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054724914181953785; val_accuracy: 0.9856687898089171 

Epoch 14 start
The current lr is: 0.1
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.97
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.19; acc: 0.98
Batch: 680; loss: 0.43; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05424924664626456; val_accuracy: 0.9868630573248408 

Epoch 15 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061338313564563256; val_accuracy: 0.9844745222929936 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050067714231598906; val_accuracy: 0.9878582802547771 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.050538451879457304; val_accuracy: 0.9876592356687898 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04997645375455261; val_accuracy: 0.9876592356687898 

Epoch 19 start
The current lr is: 0.010000000000000002
slurmstepd: error: _is_a_lwp: open() /proc/202787/status failed: No such file or directory
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0506381006899533; val_accuracy: 0.9873606687898089 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051315944452004825; val_accuracy: 0.9876592356687898 

Epoch 21 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05207828989928695; val_accuracy: 0.9872611464968153 

Epoch 22 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.052563529627717986; val_accuracy: 0.9874601910828026 

Epoch 23 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05243799339529056; val_accuracy: 0.9875597133757962 

Epoch 24 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.053442082729688874; val_accuracy: 0.9864649681528662 

Epoch 25 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05306800991106944; val_accuracy: 0.9871616242038217 

Epoch 26 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05349866614030425; val_accuracy: 0.9867635350318471 

Epoch 27 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05394662024488874; val_accuracy: 0.9872611464968153 

Epoch 28 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05384541302919388; val_accuracy: 0.9875597133757962 

Epoch 29 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05462228179357614; val_accuracy: 0.9865644904458599 

Epoch 30 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05413398644916571; val_accuracy: 0.9871616242038217 

Epoch 31 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054289289625586976; val_accuracy: 0.9871616242038217 

Epoch 32 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05441223383898948; val_accuracy: 0.987062101910828 

Epoch 33 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05450254291960388; val_accuracy: 0.9869625796178344 

Epoch 34 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05456596620048687; val_accuracy: 0.9868630573248408 

Epoch 35 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054633589497037754; val_accuracy: 0.9868630573248408 

Epoch 36 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05465147755802817; val_accuracy: 0.9868630573248408 

Epoch 37 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05468219214943564; val_accuracy: 0.9868630573248408 

Epoch 38 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05469662060213697; val_accuracy: 0.9868630573248408 

Epoch 39 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054728228931024574; val_accuracy: 0.9868630573248408 

Epoch 40 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05475384182041618; val_accuracy: 0.9869625796178344 

Epoch 41 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054772354994609855; val_accuracy: 0.9868630573248408 

Epoch 42 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05482282451573451; val_accuracy: 0.9868630573248408 

Epoch 43 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05486613049343893; val_accuracy: 0.9868630573248408 

Epoch 44 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05491691685406266; val_accuracy: 0.9867635350318471 

Epoch 45 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054897498268230704; val_accuracy: 0.987062101910828 

Epoch 46 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054902314499115486; val_accuracy: 0.987062101910828 

Epoch 47 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054906878620386124; val_accuracy: 0.987062101910828 

Epoch 48 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05491128713367092; val_accuracy: 0.987062101910828 

Epoch 49 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05491645067076015; val_accuracy: 0.987062101910828 

Epoch 50 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.0; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.054920445131078646; val_accuracy: 0.987062101910828 

plots/no_subspace_training/reg_lenet/2020-01-18 23:56:31/d_dim_1000_lr_0.1_gamma_0.1_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.86; acc: 0.39
Batch: 60; loss: 1.76; acc: 0.3
Batch: 80; loss: 1.76; acc: 0.38
Batch: 100; loss: 1.02; acc: 0.72
Batch: 120; loss: 1.04; acc: 0.64
Batch: 140; loss: 0.72; acc: 0.72
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.83
Batch: 220; loss: 0.71; acc: 0.7
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1030045792840089; val_accuracy: 0.9680533439490446 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.43; acc: 0.83
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.0741057877850001; val_accuracy: 0.9767117834394905 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.11; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06743603726481176; val_accuracy: 0.9807921974522293 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06424329555623091; val_accuracy: 0.9807921974522293 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.31; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05958546128622286; val_accuracy: 0.9821855095541401 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05128289434085986; val_accuracy: 0.9853702229299363 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06486662301667936; val_accuracy: 0.9806926751592356 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.94
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07779859113180713; val_accuracy: 0.9799960191082803 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05764333545497269; val_accuracy: 0.984375 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058352394444737464; val_accuracy: 0.9839769108280255 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046280972683315824; val_accuracy: 0.9868630573248408 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.045810067231298254; val_accuracy: 0.9872611464968153 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04571736134161615; val_accuracy: 0.9878582802547771 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04639786804557606; val_accuracy: 0.9878582802547771 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04672660915904744; val_accuracy: 0.9878582802547771 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04722533270621755; val_accuracy: 0.9872611464968153 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.97
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0478779227965197; val_accuracy: 0.9874601910828026 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04772221804804103; val_accuracy: 0.9877587579617835 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04896988949863015; val_accuracy: 0.9877587579617835 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.2; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04909364283559429; val_accuracy: 0.987062101910828 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049222101475212984; val_accuracy: 0.9872611464968153 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049325988550854336; val_accuracy: 0.9872611464968153 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04935688772209131; val_accuracy: 0.9873606687898089 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04943079275974802; val_accuracy: 0.9874601910828026 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04941886677673668; val_accuracy: 0.9873606687898089 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049449145461723305; val_accuracy: 0.9873606687898089 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.2; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04950231629287361; val_accuracy: 0.9873606687898089 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04955752917630657; val_accuracy: 0.9872611464968153 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049590721346770124; val_accuracy: 0.9873606687898089 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04960345192130204; val_accuracy: 0.9874601910828026 

Epoch 31 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04961265870340311; val_accuracy: 0.9874601910828026 

Epoch 32 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0496228837710657; val_accuracy: 0.9874601910828026 

Epoch 33 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049633152569365346; val_accuracy: 0.9873606687898089 

Epoch 34 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0496449172259516; val_accuracy: 0.9873606687898089 

Epoch 35 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049655324737926956; val_accuracy: 0.9873606687898089 

Epoch 36 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049664761372812234; val_accuracy: 0.9873606687898089 

Epoch 37 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04967304478120652; val_accuracy: 0.9873606687898089 

Epoch 38 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049681634659979754; val_accuracy: 0.9873606687898089 

Epoch 39 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04968966325377203; val_accuracy: 0.9873606687898089 

Epoch 40 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049696273840726564; val_accuracy: 0.9873606687898089 

Epoch 41 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0496968601587092; val_accuracy: 0.9873606687898089 

Epoch 42 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04969736983536915; val_accuracy: 0.9873606687898089 

Epoch 43 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04969801796469719; val_accuracy: 0.9873606687898089 

Epoch 44 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04969858689008245; val_accuracy: 0.9873606687898089 

Epoch 45 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04969915453415767; val_accuracy: 0.9873606687898089 

Epoch 46 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.2; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04969972977118128; val_accuracy: 0.9873606687898089 

Epoch 47 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.049700306147147134; val_accuracy: 0.9873606687898089 

Epoch 48 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04970090243087453; val_accuracy: 0.9873606687898089 

Epoch 49 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04970183679062849; val_accuracy: 0.9873606687898089 

Epoch 50 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.01; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04970239636719607; val_accuracy: 0.9873606687898089 

plots/no_subspace_training/reg_lenet/2020-01-19 00:05:51/d_dim_1000_lr_0.1_gamma_0.1_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.16; acc: 0.16
Batch: 40; loss: 1.85; acc: 0.39
Batch: 60; loss: 1.81; acc: 0.27
Batch: 80; loss: 1.72; acc: 0.38
Batch: 100; loss: 1.02; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.61
Batch: 140; loss: 0.7; acc: 0.73
Batch: 160; loss: 0.56; acc: 0.77
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.69
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.47; train_accuracy: 0.84 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10256153779329767; val_accuracy: 0.9673566878980892 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.07904531562309357; val_accuracy: 0.9749203821656051 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.11553035655124172; val_accuracy: 0.9648686305732485 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.95
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058149981745489084; val_accuracy: 0.9834792993630573 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.058028148166882766; val_accuracy: 0.9825835987261147 

Epoch 6 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04664209069814652; val_accuracy: 0.98546974522293 

Epoch 7 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04578848847538043; val_accuracy: 0.9857683121019108 

Epoch 8 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.95
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.051314778270045665; val_accuracy: 0.9847730891719745 

Epoch 9 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046527413164923904; val_accuracy: 0.9861664012738853 

Epoch 10 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04783599519995368; val_accuracy: 0.9866640127388535 

Epoch 11 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04633613061278489; val_accuracy: 0.9864649681528662 

Epoch 12 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04615435113382947; val_accuracy: 0.986265923566879 

Epoch 13 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046187422718781575; val_accuracy: 0.9864649681528662 

Epoch 14 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04619029273462903; val_accuracy: 0.9863654458598726 

Epoch 15 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04622437992388276; val_accuracy: 0.9864649681528662 

Epoch 16 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04622793304407673; val_accuracy: 0.9864649681528662 

Epoch 17 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04623029599334024; val_accuracy: 0.9863654458598726 

Epoch 18 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04623159977376081; val_accuracy: 0.9863654458598726 

Epoch 19 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04623503240335519; val_accuracy: 0.9863654458598726 

Epoch 20 start
The current lr is: 0.00010000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.24; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624215639225996; val_accuracy: 0.9863654458598726 

Epoch 21 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.97
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624247128606602; val_accuracy: 0.9863654458598726 

Epoch 22 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046242768573723024; val_accuracy: 0.9863654458598726 

Epoch 23 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624306251573714; val_accuracy: 0.9863654458598726 

Epoch 24 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624336170163124; val_accuracy: 0.9863654458598726 

Epoch 25 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624366114853294; val_accuracy: 0.9863654458598726 

Epoch 26 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624360671658425; val_accuracy: 0.9863654458598726 

Epoch 27 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.2; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624348010417003; val_accuracy: 0.9863654458598726 

Epoch 28 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624342555358152; val_accuracy: 0.9863654458598726 

Epoch 29 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046243356623847015; val_accuracy: 0.9863654458598726 

Epoch 30 start
The current lr is: 1.0000000000000004e-06
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624329773104115; val_accuracy: 0.9863654458598726 

Epoch 31 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624329063638001; val_accuracy: 0.9863654458598726 

Epoch 32 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624329035164444; val_accuracy: 0.9863654458598726 

Epoch 33 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328536877207; val_accuracy: 0.9863654458598726 

Epoch 34 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624329080247575; val_accuracy: 0.9863654458598726 

Epoch 35 start
The current lr is: 1.0000000000000005e-07
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046243279223229475; val_accuracy: 0.9863654458598726 

Epoch 36 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328067063526; val_accuracy: 0.9863654458598726 

Epoch 37 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624327972151671; val_accuracy: 0.9863654458598726 

Epoch 38 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.2; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624327976897264; val_accuracy: 0.9863654458598726 

Epoch 39 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624327974524468; val_accuracy: 0.9863654458598726 

Epoch 40 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.97
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328207058512; val_accuracy: 0.9863654458598726 

Epoch 41 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046243282165496974; val_accuracy: 0.9863654458598726 

Epoch 42 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046243282118041046; val_accuracy: 0.9863654458598726 

Epoch 43 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.046243282331592715; val_accuracy: 0.9863654458598726 

Epoch 44 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328218922494; val_accuracy: 0.9863654458598726 

Epoch 45 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328157229788; val_accuracy: 0.9863654458598726 

Epoch 46 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.2; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328157229788; val_accuracy: 0.9863654458598726 

Epoch 47 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328157229788; val_accuracy: 0.9863654458598726 

Epoch 48 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328157229788; val_accuracy: 0.9863654458598726 

Epoch 49 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328157229788; val_accuracy: 0.9863654458598726 

Epoch 50 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 1.0 

Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.04624328157229788; val_accuracy: 0.9863654458598726 

plots/no_subspace_training/reg_lenet/2020-01-19 00:15:11/d_dim_1000_lr_0.1_gamma_0.1_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6793885667612598; val_accuracy: 0.7602507961783439 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2485229139495048; val_accuracy: 0.9201831210191083 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23167125062103483; val_accuracy: 0.9279458598726115 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12629204577985842; val_accuracy: 0.9617834394904459 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11235162876783662; val_accuracy: 0.9659633757961783 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09975027205173377; val_accuracy: 0.9702428343949044 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08910946265622309; val_accuracy: 0.9728304140127388 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.94
Val Epoch over. val_loss: 0.371542626363077; val_accuracy: 0.8847531847133758 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07975280178105755; val_accuracy: 0.9773089171974523 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09406590438002993; val_accuracy: 0.9726313694267515 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07663299055520896; val_accuracy: 0.9775079617834395 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1250393803996645; val_accuracy: 0.9627786624203821 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.94
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10089064674202804; val_accuracy: 0.9701433121019108 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06925415564447072; val_accuracy: 0.9791998407643312 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.94
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10913359556513227; val_accuracy: 0.964968152866242 

Epoch 16 start
The current lr is: 0.008
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08837627486628331; val_accuracy: 0.973328025477707 

Epoch 17 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06366152062442652; val_accuracy: 0.981687898089172 

Epoch 18 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.12; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15723463117032294; val_accuracy: 0.9459593949044586 

Epoch 19 start
The current lr is: 0.008
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062031638944984244; val_accuracy: 0.9820859872611465 

Epoch 20 start
The current lr is: 0.008
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06339559635254229; val_accuracy: 0.9803941082802548 

Epoch 21 start
The current lr is: 0.008
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07230511088232705; val_accuracy: 0.9789012738853503 

Epoch 22 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.94
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.18; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06638022042383813; val_accuracy: 0.9800955414012739 

Epoch 23 start
The current lr is: 0.008
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.94
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07059320961688734; val_accuracy: 0.9786027070063694 

Epoch 24 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06075779270309552; val_accuracy: 0.9826831210191083 

Epoch 25 start
The current lr is: 0.008
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06859446044085891; val_accuracy: 0.9794984076433121 

Epoch 26 start
The current lr is: 0.008
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06440174242683278; val_accuracy: 0.981687898089172 

Epoch 27 start
The current lr is: 0.008
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.19; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.13011025397735795; val_accuracy: 0.9616839171974523 

Epoch 28 start
The current lr is: 0.008
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06137892805561898; val_accuracy: 0.9823845541401274 

Epoch 29 start
The current lr is: 0.008
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06593989293761314; val_accuracy: 0.9796974522292994 

Epoch 30 start
The current lr is: 0.008
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06483350840106511; val_accuracy: 0.9818869426751592 

Epoch 31 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07140462389986986; val_accuracy: 0.9793988853503185 

Epoch 32 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06630794887852137; val_accuracy: 0.9803941082802548 

Epoch 33 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060659941902775676; val_accuracy: 0.9829816878980892 

Epoch 34 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07604197718819995; val_accuracy: 0.978702229299363 

Epoch 35 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06356803913879547; val_accuracy: 0.9817874203821656 

Epoch 36 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061414988936891984; val_accuracy: 0.9828821656050956 

Epoch 37 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06289433944187346; val_accuracy: 0.9819864649681529 

Epoch 38 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06267811597627439; val_accuracy: 0.9821855095541401 

Epoch 39 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06650049737684287; val_accuracy: 0.9814888535031847 

Epoch 40 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10384101864353866; val_accuracy: 0.9702428343949044 

Epoch 41 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0664182253490398; val_accuracy: 0.9821855095541401 

Epoch 42 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05916274168119309; val_accuracy: 0.9828821656050956 

Epoch 43 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06357459452264248; val_accuracy: 0.9820859872611465 

Epoch 44 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.95
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06363771772783273; val_accuracy: 0.9826831210191083 

Epoch 45 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06436538689170673; val_accuracy: 0.9829816878980892 

Epoch 46 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06044868453388001; val_accuracy: 0.9836783439490446 

Epoch 47 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06029424571971984; val_accuracy: 0.9835788216560509 

Epoch 48 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0633727937201216; val_accuracy: 0.9826831210191083 

Epoch 49 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12504783925262225; val_accuracy: 0.9644705414012739 

Epoch 50 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06265999006617601; val_accuracy: 0.98328025477707 

plots/no_subspace_training/reg_lenet/2020-01-19 00:24:32/d_dim_1000_lr_0.01_gamma_0.8_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6799450672356186; val_accuracy: 0.7602507961783439 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24724265709993945; val_accuracy: 0.9206807324840764 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2342583914613648; val_accuracy: 0.9268511146496815 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12592832283799055; val_accuracy: 0.9623805732484076 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11311782151460648; val_accuracy: 0.966062898089172 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09959337364431399; val_accuracy: 0.9700437898089171 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0893841910941206; val_accuracy: 0.9723328025477707 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.94
Val Epoch over. val_loss: 0.3552564228795896; val_accuracy: 0.8875398089171974 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07961123828200778; val_accuracy: 0.9770103503184714 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09403030574321747; val_accuracy: 0.9729299363057324 

Epoch 11 start
The current lr is: 0.008
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07587492914431414; val_accuracy: 0.9778065286624203 

Epoch 12 start
The current lr is: 0.008
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10433158342530773; val_accuracy: 0.9686504777070064 

Epoch 13 start
The current lr is: 0.008
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.94
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09496924219427595; val_accuracy: 0.9726313694267515 

Epoch 14 start
The current lr is: 0.008
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06981476982402954; val_accuracy: 0.9796974522292994 

Epoch 15 start
The current lr is: 0.008
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0928628888859111; val_accuracy: 0.9723328025477707 

Epoch 16 start
The current lr is: 0.008
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10554221101627229; val_accuracy: 0.9664609872611465 

Epoch 17 start
The current lr is: 0.008
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.4; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06420729500330558; val_accuracy: 0.9814888535031847 

Epoch 18 start
The current lr is: 0.008
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1612754359034596; val_accuracy: 0.9443670382165605 

Epoch 19 start
The current lr is: 0.008
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06315946764057609; val_accuracy: 0.9818869426751592 

Epoch 20 start
The current lr is: 0.008
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06431295262400512; val_accuracy: 0.9808917197452229 

Epoch 21 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06981539358473887; val_accuracy: 0.9794984076433121 

Epoch 22 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.18; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06521193051983597; val_accuracy: 0.9807921974522293 

Epoch 23 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06759254437438242; val_accuracy: 0.9800955414012739 

Epoch 24 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06082510748866257; val_accuracy: 0.982484076433121 

Epoch 25 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06480953147172168; val_accuracy: 0.9808917197452229 

Epoch 26 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06365305992068758; val_accuracy: 0.982484076433121 

Epoch 27 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0941818529016273; val_accuracy: 0.9716361464968153 

Epoch 28 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060612733481795925; val_accuracy: 0.9827826433121019 

Epoch 29 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06264176388170309; val_accuracy: 0.9814888535031847 

Epoch 30 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.95
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06350825176497174; val_accuracy: 0.9818869426751592 

Epoch 31 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06969190213330992; val_accuracy: 0.9800955414012739 

Epoch 32 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06633051363810612; val_accuracy: 0.9800955414012739 

Epoch 33 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06074480209380958; val_accuracy: 0.9828821656050956 

Epoch 34 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07290173378909469; val_accuracy: 0.9796974522292994 

Epoch 35 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06150572774279269; val_accuracy: 0.9826831210191083 

Epoch 36 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.94
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.21; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061672202080108554; val_accuracy: 0.9821855095541401 

Epoch 37 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06072217358905039; val_accuracy: 0.9822850318471338 

Epoch 38 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.19; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0632381528900687; val_accuracy: 0.9818869426751592 

Epoch 39 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06435750906539571; val_accuracy: 0.9815883757961783 

Epoch 40 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08895943408748906; val_accuracy: 0.9743232484076433 

Epoch 41 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06093466440868226; val_accuracy: 0.9831807324840764 

Epoch 42 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.0; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05777063442595825; val_accuracy: 0.98328025477707 

Epoch 43 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05880619632969996; val_accuracy: 0.9838773885350318 

Epoch 44 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06139691333007661; val_accuracy: 0.9823845541401274 

Epoch 45 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06384080214200506; val_accuracy: 0.9830812101910829 

Epoch 46 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0594309222332801; val_accuracy: 0.9831807324840764 

Epoch 47 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.22; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06050678076827602; val_accuracy: 0.9831807324840764 

Epoch 48 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06276281354770918; val_accuracy: 0.9823845541401274 

Epoch 49 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.19; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.22; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09281911729437531; val_accuracy: 0.9729299363057324 

Epoch 50 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.02; train_accuracy: 0.99 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061917831729741614; val_accuracy: 0.9829816878980892 

plots/no_subspace_training/reg_lenet/2020-01-19 00:33:53/d_dim_1000_lr_0.01_gamma_0.8_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.97; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.75
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.77
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.8
Val Epoch over. val_loss: 0.6787518247677262; val_accuracy: 0.7605493630573248 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.78
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.78
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2501555095147935; val_accuracy: 0.9190883757961783 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.229605453480391; val_accuracy: 0.9286425159235668 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1264157809886583; val_accuracy: 0.9625796178343949 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11341070464462231; val_accuracy: 0.966062898089172 

Epoch 6 start
The current lr is: 0.008
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09910483356968612; val_accuracy: 0.9701433121019108 

Epoch 7 start
The current lr is: 0.008
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09062780973732851; val_accuracy: 0.9724323248407644 

Epoch 8 start
The current lr is: 0.008
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2898812685517748; val_accuracy: 0.9074442675159236 

Epoch 9 start
The current lr is: 0.008
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.94
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08221977510175128; val_accuracy: 0.9759156050955414 

Epoch 10 start
The current lr is: 0.008
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09341957477057815; val_accuracy: 0.9728304140127388 

Epoch 11 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07836019788767881; val_accuracy: 0.9768113057324841 

Epoch 12 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1003260256331058; val_accuracy: 0.9705414012738853 

Epoch 13 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.91
Batch: 620; loss: 0.11; acc: 0.94
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09816604437437026; val_accuracy: 0.9711385350318471 

Epoch 14 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.25; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07416314615090941; val_accuracy: 0.9782046178343949 

Epoch 15 start
The current lr is: 0.006400000000000001
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08905092199706728; val_accuracy: 0.974422770700637 

Epoch 16 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09396489588603092; val_accuracy: 0.9716361464968153 

Epoch 17 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.41; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06689257872332434; val_accuracy: 0.9814888535031847 

Epoch 18 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10874951123052341; val_accuracy: 0.9632762738853503 

Epoch 19 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06587642884463261; val_accuracy: 0.9814888535031847 

Epoch 20 start
The current lr is: 0.005120000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06619867057225128; val_accuracy: 0.9801950636942676 

Epoch 21 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.071073449246443; val_accuracy: 0.9796974522292994 

Epoch 22 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.94
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.2; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06863243766366296; val_accuracy: 0.979796974522293 

Epoch 23 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06812052013483016; val_accuracy: 0.9796974522292994 

Epoch 24 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06423531427599821; val_accuracy: 0.9814888535031847 

Epoch 25 start
The current lr is: 0.004096000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06445927844400619; val_accuracy: 0.9811902866242038 

Epoch 26 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062489334683699216; val_accuracy: 0.9821855095541401 

Epoch 27 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06330580072133404; val_accuracy: 0.9809912420382165 

Epoch 28 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06340029043187002; val_accuracy: 0.9819864649681529 

Epoch 29 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06401513896550343; val_accuracy: 0.9808917197452229 

Epoch 30 start
The current lr is: 0.0032768000000000007
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.94
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06388227393625269; val_accuracy: 0.9814888535031847 

Epoch 31 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.066006932122882; val_accuracy: 0.9811902866242038 

Epoch 32 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06541654003701013; val_accuracy: 0.9807921974522293 

Epoch 33 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06210480741919226; val_accuracy: 0.9823845541401274 

Epoch 34 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06807696831169402; val_accuracy: 0.9804936305732485 

Epoch 35 start
The current lr is: 0.002621440000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.95
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06161240426598081; val_accuracy: 0.9828821656050956 

Epoch 36 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061832861239268525; val_accuracy: 0.9820859872611465 

Epoch 37 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061633255997091345; val_accuracy: 0.9825835987261147 

Epoch 38 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06378095288565204; val_accuracy: 0.9813893312101911 

Epoch 39 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06369569000734646; val_accuracy: 0.9815883757961783 

Epoch 40 start
The current lr is: 0.002097152000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06699740445348108; val_accuracy: 0.9809912420382165 

Epoch 41 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06048577030895242; val_accuracy: 0.9827826433121019 

Epoch 42 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05969577734684868; val_accuracy: 0.9830812101910829 

Epoch 43 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06094831660105165; val_accuracy: 0.9828821656050956 

Epoch 44 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061080749686451476; val_accuracy: 0.982484076433121 

Epoch 45 start
The current lr is: 0.001677721600000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06280783449958084; val_accuracy: 0.982484076433121 

Epoch 46 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.18; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059356331184601326; val_accuracy: 0.9829816878980892 

Epoch 47 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.25; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06038980874096512; val_accuracy: 0.9833797770700637 

Epoch 48 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061602248412788295; val_accuracy: 0.9822850318471338 

Epoch 49 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06229979860459923; val_accuracy: 0.9821855095541401 

Epoch 50 start
The current lr is: 0.0013421772800000006
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06084572566542656; val_accuracy: 0.9827826433121019 

plots/no_subspace_training/reg_lenet/2020-01-19 00:43:12/d_dim_1000_lr_0.01_gamma_0.8_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.97; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6796897608003799; val_accuracy: 0.7600517515923567 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.78
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2493863851781104; val_accuracy: 0.9195859872611465 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23464449616089747; val_accuracy: 0.926453025477707 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12696913729427725; val_accuracy: 0.962281050955414 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11240930257329515; val_accuracy: 0.9657643312101911 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09974616803940695; val_accuracy: 0.9699442675159236 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08951805154703985; val_accuracy: 0.9732285031847133 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.94
Val Epoch over. val_loss: 0.36914897643646616; val_accuracy: 0.8855493630573248 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07977766942256576; val_accuracy: 0.9769108280254777 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09434442760743153; val_accuracy: 0.972531847133758 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07631318807412105; val_accuracy: 0.977109872611465 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.97
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.13148433245291377; val_accuracy: 0.9598925159235668 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.94
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1002398208495538; val_accuracy: 0.9709394904458599 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06853947093247607; val_accuracy: 0.9791998407643312 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10487307254580935; val_accuracy: 0.9671576433121019 

Epoch 16 start
The current lr is: 0.004
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0697263932674174; val_accuracy: 0.9799960191082803 

Epoch 17 start
The current lr is: 0.004
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06294884138805851; val_accuracy: 0.9817874203821656 

Epoch 18 start
The current lr is: 0.004
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07930938268353226; val_accuracy: 0.9738256369426752 

Epoch 19 start
The current lr is: 0.004
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0625986448210326; val_accuracy: 0.9813893312101911 

Epoch 20 start
The current lr is: 0.004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06308339325011156; val_accuracy: 0.9812898089171974 

Epoch 21 start
The current lr is: 0.004
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.14; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06778260255173134; val_accuracy: 0.9796974522292994 

Epoch 22 start
The current lr is: 0.004
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.19; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06485638491050073; val_accuracy: 0.9810907643312102 

Epoch 23 start
The current lr is: 0.004
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06468570332048805; val_accuracy: 0.9802945859872612 

Epoch 24 start
The current lr is: 0.004
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06180087787805089; val_accuracy: 0.9826831210191083 

Epoch 25 start
The current lr is: 0.004
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.16; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0616370542746061; val_accuracy: 0.9819864649681529 

Epoch 26 start
The current lr is: 0.004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06071787341764778; val_accuracy: 0.9826831210191083 

Epoch 27 start
The current lr is: 0.004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.18; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06376558734448093; val_accuracy: 0.9804936305732485 

Epoch 28 start
The current lr is: 0.004
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061484041772070966; val_accuracy: 0.9827826433121019 

Epoch 29 start
The current lr is: 0.004
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062431478161056334; val_accuracy: 0.9813893312101911 

Epoch 30 start
The current lr is: 0.004
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06273887062053772; val_accuracy: 0.9814888535031847 

Epoch 31 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06124240957247983; val_accuracy: 0.9820859872611465 

Epoch 32 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060969685767866245; val_accuracy: 0.9819864649681529 

Epoch 33 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06025302047087888; val_accuracy: 0.9825835987261147 

Epoch 34 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06315421394908884; val_accuracy: 0.9815883757961783 

Epoch 35 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05931465548409778; val_accuracy: 0.9831807324840764 

Epoch 36 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.18; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06008579094366283; val_accuracy: 0.9827826433121019 

Epoch 37 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.0; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059715052924243506; val_accuracy: 0.9825835987261147 

Epoch 38 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.95
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061049765296232926; val_accuracy: 0.9821855095541401 

Epoch 39 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06052759012124341; val_accuracy: 0.982484076433121 

Epoch 40 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0624078147255691; val_accuracy: 0.9819864649681529 

Epoch 41 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.0; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05944525124208563; val_accuracy: 0.9826831210191083 

Epoch 42 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05867935739647431; val_accuracy: 0.9828821656050956 

Epoch 43 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.059745351777999264; val_accuracy: 0.9825835987261147 

Epoch 44 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.01; acc: 0.98
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05979017520643723; val_accuracy: 0.9827826433121019 

Epoch 45 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06083853369590583; val_accuracy: 0.9827826433121019 

Epoch 46 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.0; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05867809255364215; val_accuracy: 0.9830812101910829 

Epoch 47 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0588459706254256; val_accuracy: 0.9830812101910829 

Epoch 48 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05989203774103313; val_accuracy: 0.9827826433121019 

Epoch 49 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.2; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05978464596210771; val_accuracy: 0.9830812101910829 

Epoch 50 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.03; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.05916293623863132; val_accuracy: 0.9833797770700637 

plots/no_subspace_training/reg_lenet/2020-01-19 00:52:35/d_dim_1000_lr_0.01_gamma_0.4_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.13; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.8
Val Epoch over. val_loss: 0.6788117523026315; val_accuracy: 0.7604498407643312 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.78
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24642772251253675; val_accuracy: 0.9206807324840764 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23608227482267247; val_accuracy: 0.926453025477707 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12633134991784764; val_accuracy: 0.9619824840764332 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11326625679803502; val_accuracy: 0.9657643312101911 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.91
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09907974720380869; val_accuracy: 0.96984474522293 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08893033022144038; val_accuracy: 0.9731289808917197 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.3601500288031663; val_accuracy: 0.8867436305732485 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07992426206351845; val_accuracy: 0.9767117834394905 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09493841280698016; val_accuracy: 0.972531847133758 

Epoch 11 start
The current lr is: 0.004
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07441022525879608; val_accuracy: 0.9776074840764332 

Epoch 12 start
The current lr is: 0.004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07975097242624136; val_accuracy: 0.9761146496815286 

Epoch 13 start
The current lr is: 0.004
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.92
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.94
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0798466501484631; val_accuracy: 0.9769108280254777 

Epoch 14 start
The current lr is: 0.004
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.05; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07268211683317734; val_accuracy: 0.9783041401273885 

Epoch 15 start
The current lr is: 0.004
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07637310289083772; val_accuracy: 0.9783041401273885 

Epoch 16 start
The current lr is: 0.004
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08026969475541146; val_accuracy: 0.9757165605095541 

Epoch 17 start
The current lr is: 0.004
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.06; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.4; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06725639171281438; val_accuracy: 0.9814888535031847 

Epoch 18 start
The current lr is: 0.004
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08434196592421289; val_accuracy: 0.9734275477707006 

Epoch 19 start
The current lr is: 0.004
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0665168369034673; val_accuracy: 0.9812898089171974 

Epoch 20 start
The current lr is: 0.004
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06659815135370394; val_accuracy: 0.9801950636942676 

Epoch 21 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06706115151429252; val_accuracy: 0.9804936305732485 

Epoch 22 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06560682169618501; val_accuracy: 0.980593152866242 

Epoch 23 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0655582561068664; val_accuracy: 0.9813893312101911 

Epoch 24 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06568767353417768; val_accuracy: 0.9811902866242038 

Epoch 25 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06340005435049534; val_accuracy: 0.9819864649681529 

Epoch 26 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0639693030292631; val_accuracy: 0.9823845541401274 

Epoch 27 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0627752478786145; val_accuracy: 0.9823845541401274 

Epoch 28 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06498392129162694; val_accuracy: 0.9818869426751592 

Epoch 29 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06488355154254634; val_accuracy: 0.9803941082802548 

Epoch 30 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.064686481526513; val_accuracy: 0.981687898089172 

Epoch 31 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06378642653536265; val_accuracy: 0.9819864649681529 

Epoch 32 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06368933034360788; val_accuracy: 0.9812898089171974 

Epoch 33 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06346429008871886; val_accuracy: 0.9817874203821656 

Epoch 34 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06498435597605766; val_accuracy: 0.981687898089172 

Epoch 35 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0628573404755562; val_accuracy: 0.9817874203821656 

Epoch 36 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06311980051219843; val_accuracy: 0.9822850318471338 

Epoch 37 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0630318478793855; val_accuracy: 0.9810907643312102 

Epoch 38 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06347197453212586; val_accuracy: 0.9819864649681529 

Epoch 39 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.94
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06286119478665719; val_accuracy: 0.9815883757961783 

Epoch 40 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06344278739278864; val_accuracy: 0.9814888535031847 

Epoch 41 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.94
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06279048609553249; val_accuracy: 0.981687898089172 

Epoch 42 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06251373080311308; val_accuracy: 0.9819864649681529 

Epoch 43 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06286540309524839; val_accuracy: 0.9820859872611465 

Epoch 44 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062414163581220206; val_accuracy: 0.9821855095541401 

Epoch 45 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06274847290033747; val_accuracy: 0.9817874203821656 

Epoch 46 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.18; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06223808838778241; val_accuracy: 0.981687898089172 

Epoch 47 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.25; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062320649315407324; val_accuracy: 0.981687898089172 

Epoch 48 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06288797339531267; val_accuracy: 0.9819864649681529 

Epoch 49 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06278848801116654; val_accuracy: 0.9814888535031847 

Epoch 50 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06240803227540414; val_accuracy: 0.9818869426751592 

plots/no_subspace_training/reg_lenet/2020-01-19 01:01:52/d_dim_1000_lr_0.01_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6800425386732551; val_accuracy: 0.7598527070063694 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2477850777328394; val_accuracy: 0.9208797770700637 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23053908673156598; val_accuracy: 0.9287420382165605 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12595006593378485; val_accuracy: 0.9624800955414012 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11238149627094056; val_accuracy: 0.9656648089171974 

Epoch 6 start
The current lr is: 0.004
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09956070149590256; val_accuracy: 0.9705414012738853 

Epoch 7 start
The current lr is: 0.004
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.94
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09518546428365313; val_accuracy: 0.9717356687898089 

Epoch 8 start
The current lr is: 0.004
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15148997121745614; val_accuracy: 0.9510350318471338 

Epoch 9 start
The current lr is: 0.004
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0900114584168431; val_accuracy: 0.9732285031847133 

Epoch 10 start
The current lr is: 0.004
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09144291419322324; val_accuracy: 0.973328025477707 

Epoch 11 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08638723821017393; val_accuracy: 0.9736265923566879 

Epoch 12 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.92
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08767180379693675; val_accuracy: 0.9732285031847133 

Epoch 13 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08682708330689722; val_accuracy: 0.9755175159235668 

Epoch 14 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08491997743488118; val_accuracy: 0.9745222929936306 

Epoch 15 start
The current lr is: 0.0016000000000000003
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08548820237065577; val_accuracy: 0.9745222929936306 

Epoch 16 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.23; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08266905607406501; val_accuracy: 0.9756170382165605 

Epoch 17 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.95
Batch: 520; loss: 0.45; acc: 0.95
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08173922165801191; val_accuracy: 0.9762141719745223 

Epoch 18 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.95
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.05; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08175035174580136; val_accuracy: 0.9757165605095541 

Epoch 19 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0814247119720954; val_accuracy: 0.9755175159235668 

Epoch 20 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08085847021952557; val_accuracy: 0.9767117834394905 

Epoch 21 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0811470026613041; val_accuracy: 0.9764132165605095 

Epoch 22 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.92
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08089196098268411; val_accuracy: 0.9764132165605095 

Epoch 23 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08076654220367693; val_accuracy: 0.9762141719745223 

Epoch 24 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08151102500261774; val_accuracy: 0.9767117834394905 

Epoch 25 start
The current lr is: 0.00025600000000000004
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08035952623957282; val_accuracy: 0.9765127388535032 

Epoch 26 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08066058130400955; val_accuracy: 0.9766122611464968 

Epoch 27 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.05; acc: 1.0
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.21; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08040144069084695; val_accuracy: 0.9764132165605095 

Epoch 28 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.94
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08049730472503953; val_accuracy: 0.9766122611464968 

Epoch 29 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.95
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.92
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08048089342132496; val_accuracy: 0.9765127388535032 

Epoch 30 start
The current lr is: 0.00010240000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08038177720870182; val_accuracy: 0.9766122611464968 

Epoch 31 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08037001784345146; val_accuracy: 0.9766122611464968 

Epoch 32 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.94
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08039361252716393; val_accuracy: 0.9765127388535032 

Epoch 33 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08031550355872531; val_accuracy: 0.9765127388535032 

Epoch 34 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08032167707658877; val_accuracy: 0.9765127388535032 

Epoch 35 start
The current lr is: 4.0960000000000014e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.07; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0802036766081479; val_accuracy: 0.9766122611464968 

Epoch 36 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08020310910644045; val_accuracy: 0.9766122611464968 

Epoch 37 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08022698534616998; val_accuracy: 0.9766122611464968 

Epoch 38 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.22; acc: 0.95
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08021001413369634; val_accuracy: 0.9766122611464968 

Epoch 39 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08020331812607255; val_accuracy: 0.9766122611464968 

Epoch 40 start
The current lr is: 1.6384000000000008e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.07; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08020023178238019; val_accuracy: 0.9766122611464968 

Epoch 41 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.18; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0802020722893393; val_accuracy: 0.9766122611464968 

Epoch 42 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.95
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08020184727705967; val_accuracy: 0.9766122611464968 

Epoch 43 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08020282081168169; val_accuracy: 0.9766122611464968 

Epoch 44 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.2; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08020086662404856; val_accuracy: 0.9766122611464968 

Epoch 45 start
The current lr is: 6.553600000000004e-06
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.13; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08019851579977448; val_accuracy: 0.9766122611464968 

Epoch 46 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08019665476812679; val_accuracy: 0.9766122611464968 

Epoch 47 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.31; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08019627668675344; val_accuracy: 0.9766122611464968 

Epoch 48 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.06; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0801958630608905; val_accuracy: 0.9766122611464968 

Epoch 49 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.26; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08019805431460879; val_accuracy: 0.9766122611464968 

Epoch 50 start
The current lr is: 2.621440000000001e-06
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.95
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08019678629222948; val_accuracy: 0.9766122611464968 

plots/no_subspace_training/reg_lenet/2020-01-19 01:11:21/d_dim_1000_lr_0.01_gamma_0.4_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 0.99; acc: 0.69
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.97; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.79; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.77
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6800115341973153; val_accuracy: 0.7596536624203821 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2485878987202219; val_accuracy: 0.9201831210191083 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.22942676537545623; val_accuracy: 0.9286425159235668 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.88
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1263972236567242; val_accuracy: 0.9621815286624203 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11305517867017703; val_accuracy: 0.9654657643312102 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.91
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09917536999579447; val_accuracy: 0.9699442675159236 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08923898758307384; val_accuracy: 0.9730294585987261 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.94
Val Epoch over. val_loss: 0.3709524692433655; val_accuracy: 0.8851512738853503 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08004722181873716; val_accuracy: 0.9768113057324841 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09538606117675259; val_accuracy: 0.972531847133758 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07626505822512754; val_accuracy: 0.9766122611464968 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.12461882727635894; val_accuracy: 0.9627786624203821 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.94
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.10704726184819155; val_accuracy: 0.96875 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.11; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06968164656335002; val_accuracy: 0.978702229299363 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11010374726763197; val_accuracy: 0.9652667197452229 

Epoch 16 start
The current lr is: 0.002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06502468141305978; val_accuracy: 0.9809912420382165 

Epoch 17 start
The current lr is: 0.002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.95
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06276830782888422; val_accuracy: 0.9821855095541401 

Epoch 18 start
The current lr is: 0.002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.03; acc: 0.97
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0651568442013613; val_accuracy: 0.9801950636942676 

Epoch 19 start
The current lr is: 0.002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0630739445851487; val_accuracy: 0.9813893312101911 

Epoch 20 start
The current lr is: 0.002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06308399306930554; val_accuracy: 0.9814888535031847 

Epoch 21 start
The current lr is: 0.002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06512811834550207; val_accuracy: 0.9813893312101911 

Epoch 22 start
The current lr is: 0.002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.06; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.2; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06337410135633627; val_accuracy: 0.9813893312101911 

Epoch 23 start
The current lr is: 0.002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06313252290058288; val_accuracy: 0.9814888535031847 

Epoch 24 start
The current lr is: 0.002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.0; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06294116254445094; val_accuracy: 0.9813893312101911 

Epoch 25 start
The current lr is: 0.002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06098940082274045; val_accuracy: 0.9820859872611465 

Epoch 26 start
The current lr is: 0.002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06146022936056374; val_accuracy: 0.9822850318471338 

Epoch 27 start
The current lr is: 0.002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06042268656336578; val_accuracy: 0.9826831210191083 

Epoch 28 start
The current lr is: 0.002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.0; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06274820728951198; val_accuracy: 0.9818869426751592 

Epoch 29 start
The current lr is: 0.002
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0625270698694097; val_accuracy: 0.9815883757961783 

Epoch 30 start
The current lr is: 0.002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06284847944547796; val_accuracy: 0.9807921974522293 

Epoch 31 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06093605165221509; val_accuracy: 0.9822850318471338 

Epoch 32 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06117670494280043; val_accuracy: 0.9821855095541401 

Epoch 33 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06099459326048945; val_accuracy: 0.9821855095541401 

Epoch 34 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0618481052102177; val_accuracy: 0.9819864649681529 

Epoch 35 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06053290421226222; val_accuracy: 0.9821855095541401 

Epoch 36 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060775442808202115; val_accuracy: 0.982484076433121 

Epoch 37 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06094977782837525; val_accuracy: 0.9818869426751592 

Epoch 38 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060905238411798596; val_accuracy: 0.9819864649681529 

Epoch 39 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06075831026930338; val_accuracy: 0.9823845541401274 

Epoch 40 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06084451412746481; val_accuracy: 0.9825835987261147 

Epoch 41 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.95
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.94
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06077702337531907; val_accuracy: 0.9822850318471338 

Epoch 42 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060352821840317386; val_accuracy: 0.9819864649681529 

Epoch 43 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06077671233730711; val_accuracy: 0.9822850318471338 

Epoch 44 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06031984329271089; val_accuracy: 0.9823845541401274 

Epoch 45 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060832426414652994; val_accuracy: 0.9819864649681529 

Epoch 46 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060573391355336854; val_accuracy: 0.9819864649681529 

Epoch 47 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.25; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.060547166616673684; val_accuracy: 0.9819864649681529 

Epoch 48 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06060544091757316; val_accuracy: 0.9820859872611465 

Epoch 49 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.02; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06052040443963306; val_accuracy: 0.9819864649681529 

Epoch 50 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06054132334460878; val_accuracy: 0.9819864649681529 

plots/no_subspace_training/reg_lenet/2020-01-19 01:20:36/d_dim_1000_lr_0.01_gamma_0.2_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6800424724247804; val_accuracy: 0.7596536624203821 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.5; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24847404241182242; val_accuracy: 0.9200835987261147 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23291560243459264; val_accuracy: 0.9272492038216561 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1263670503476243; val_accuracy: 0.962281050955414 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11304857828624688; val_accuracy: 0.9663614649681529 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09931281863883802; val_accuracy: 0.9700437898089171 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08937144414729374; val_accuracy: 0.9726313694267515 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.94
Val Epoch over. val_loss: 0.37870326960922046; val_accuracy: 0.8829617834394905 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0799046890086429; val_accuracy: 0.9772093949044586 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09401378379600822; val_accuracy: 0.9728304140127388 

Epoch 11 start
The current lr is: 0.002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0743409492977106; val_accuracy: 0.9766122611464968 

Epoch 12 start
The current lr is: 0.002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07510595697506219; val_accuracy: 0.9766122611464968 

Epoch 13 start
The current lr is: 0.002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.92
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.94
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07415645657356378; val_accuracy: 0.9791003184713376 

Epoch 14 start
The current lr is: 0.002
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07338473506888767; val_accuracy: 0.9788017515923567 

Epoch 15 start
The current lr is: 0.002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07432568172930153; val_accuracy: 0.9784036624203821 

Epoch 16 start
The current lr is: 0.002
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0736057300856159; val_accuracy: 0.9791998407643312 

Epoch 17 start
The current lr is: 0.002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.41; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07008774799241382; val_accuracy: 0.9799960191082803 

Epoch 18 start
The current lr is: 0.002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07230783690502689; val_accuracy: 0.977906050955414 

Epoch 19 start
The current lr is: 0.002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07007825151560412; val_accuracy: 0.9794984076433121 

Epoch 20 start
The current lr is: 0.002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06924508547612057; val_accuracy: 0.9802945859872612 

Epoch 21 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06938383512341292; val_accuracy: 0.9796974522292994 

Epoch 22 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06910885944013383; val_accuracy: 0.9802945859872612 

Epoch 23 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.94
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06879341338945043; val_accuracy: 0.9809912420382165 

Epoch 24 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0699050239150881; val_accuracy: 0.979796974522293 

Epoch 25 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0685005989162975; val_accuracy: 0.9808917197452229 

Epoch 26 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06908819194479733; val_accuracy: 0.9799960191082803 

Epoch 27 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06818403075834747; val_accuracy: 0.9808917197452229 

Epoch 28 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06889377624555758; val_accuracy: 0.9799960191082803 

Epoch 29 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06850514833809464; val_accuracy: 0.9810907643312102 

Epoch 30 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06868153575594259; val_accuracy: 0.9806926751592356 

Epoch 31 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06844644007646734; val_accuracy: 0.9807921974522293 

Epoch 32 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06850440947303346; val_accuracy: 0.9806926751592356 

Epoch 33 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0683715172159444; val_accuracy: 0.9812898089171974 

Epoch 34 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0684741649207226; val_accuracy: 0.9809912420382165 

Epoch 35 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06822567577859398; val_accuracy: 0.9804936305732485 

Epoch 36 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.94
Batch: 260; loss: 0.2; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06826685962213833; val_accuracy: 0.9806926751592356 

Epoch 37 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06832762177630215; val_accuracy: 0.9806926751592356 

Epoch 38 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06819195581517022; val_accuracy: 0.9806926751592356 

Epoch 39 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06822745602838932; val_accuracy: 0.9807921974522293 

Epoch 40 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06824232647373418; val_accuracy: 0.9810907643312102 

Epoch 41 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06823667151865306; val_accuracy: 0.9809912420382165 

Epoch 42 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06822874376870644; val_accuracy: 0.9808917197452229 

Epoch 43 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06823679940051335; val_accuracy: 0.9808917197452229 

Epoch 44 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06822896981315249; val_accuracy: 0.9808917197452229 

Epoch 45 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.94
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06821856858908751; val_accuracy: 0.9808917197452229 

Epoch 46 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06821781688719798; val_accuracy: 0.9808917197452229 

Epoch 47 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06821662352485641; val_accuracy: 0.9808917197452229 

Epoch 48 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0682212072216971; val_accuracy: 0.9808917197452229 

Epoch 49 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.97
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06821972592051621; val_accuracy: 0.9807921974522293 

Epoch 50 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.94
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06821299975465057; val_accuracy: 0.9807921974522293 

plots/no_subspace_training/reg_lenet/2020-01-19 01:29:59/d_dim_1000_lr_0.01_gamma_0.2_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.38; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.13; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6799310271147709; val_accuracy: 0.7600517515923567 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24693093036010766; val_accuracy: 0.9208797770700637 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2325794461901021; val_accuracy: 0.9278463375796179 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12617081766770144; val_accuracy: 0.9624800955414012 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1131273000056197; val_accuracy: 0.9661624203821656 

Epoch 6 start
The current lr is: 0.002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10100891782789473; val_accuracy: 0.9705414012738853 

Epoch 7 start
The current lr is: 0.002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09919562833798919; val_accuracy: 0.9702428343949044 

Epoch 8 start
The current lr is: 0.002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.08; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11222941117586603; val_accuracy: 0.9665605095541401 

Epoch 9 start
The current lr is: 0.002
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09530756513404239; val_accuracy: 0.9726313694267515 

Epoch 10 start
The current lr is: 0.002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09408951289714522; val_accuracy: 0.9722332802547771 

Epoch 11 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09283309756143457; val_accuracy: 0.9722332802547771 

Epoch 12 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.91
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.37; acc: 0.94
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.92
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09237523267793048; val_accuracy: 0.9731289808917197 

Epoch 13 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.89
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09183820232654073; val_accuracy: 0.9730294585987261 

Epoch 14 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09223256846237335; val_accuracy: 0.9732285031847133 

Epoch 15 start
The current lr is: 0.0004000000000000001
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09212547296266647; val_accuracy: 0.9724323248407644 

Epoch 16 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.94
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.27; acc: 0.97
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.95
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09139050460260385; val_accuracy: 0.9729299363057324 

Epoch 17 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09133535127635974; val_accuracy: 0.9730294585987261 

Epoch 18 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09134480649024058; val_accuracy: 0.9729299363057324 

Epoch 19 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.12; acc: 0.92
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116624843827478; val_accuracy: 0.9730294585987261 

Epoch 20 start
The current lr is: 8.000000000000002e-05
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09112674771410645; val_accuracy: 0.9734275477707006 

Epoch 21 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09113179219613789; val_accuracy: 0.9730294585987261 

Epoch 22 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.05; acc: 1.0
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09114431407136522; val_accuracy: 0.9729299363057324 

Epoch 23 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09114916361631102; val_accuracy: 0.9729299363057324 

Epoch 24 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09118621473669246; val_accuracy: 0.9729299363057324 

Epoch 25 start
The current lr is: 1.6000000000000003e-05
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.94
Batch: 620; loss: 0.06; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116442062577625; val_accuracy: 0.9729299363057324 

Epoch 26 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116575012730944; val_accuracy: 0.9729299363057324 

Epoch 27 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116282565578533; val_accuracy: 0.9729299363057324 

Epoch 28 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.92
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116186557491873; val_accuracy: 0.9729299363057324 

Epoch 29 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116258355937186; val_accuracy: 0.9729299363057324 

Epoch 30 start
The current lr is: 3.2000000000000007e-06
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116165275881245; val_accuracy: 0.9729299363057324 

Epoch 31 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116152303803499; val_accuracy: 0.9729299363057324 

Epoch 32 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.04; acc: 0.97
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.92
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116151195707595; val_accuracy: 0.9729299363057324 

Epoch 33 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.19; acc: 0.98
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116112682849738; val_accuracy: 0.9729299363057324 

Epoch 34 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116113999751722; val_accuracy: 0.9729299363057324 

Epoch 35 start
The current lr is: 6.400000000000002e-07
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.94
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116090907697465; val_accuracy: 0.9729299363057324 

Epoch 36 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116071996510408; val_accuracy: 0.9729299363057324 

Epoch 37 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116070442328787; val_accuracy: 0.9729299363057324 

Epoch 38 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.26; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116066342136663; val_accuracy: 0.9729299363057324 

Epoch 39 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116060834876291; val_accuracy: 0.9729299363057324 

Epoch 40 start
The current lr is: 1.2800000000000006e-07
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116050783710875; val_accuracy: 0.9729299363057324 

Epoch 41 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116050366098714; val_accuracy: 0.9729299363057324 

Epoch 42 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116049352914664; val_accuracy: 0.9729299363057324 

Epoch 43 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116049564093541; val_accuracy: 0.9729299363057324 

Epoch 44 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.92
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116047224516322; val_accuracy: 0.9729299363057324 

Epoch 45 start
The current lr is: 2.5600000000000014e-08
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116046479458262; val_accuracy: 0.9729299363057324 

Epoch 46 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116046854360088; val_accuracy: 0.9729299363057324 

Epoch 47 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0911604708214854; val_accuracy: 0.9729299363057324 

Epoch 48 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.94
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0911604716045082; val_accuracy: 0.9729299363057324 

Epoch 49 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.13; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116047960083196; val_accuracy: 0.9729299363057324 

Epoch 50 start
The current lr is: 5.120000000000002e-09
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09116047513997479; val_accuracy: 0.9729299363057324 

plots/no_subspace_training/reg_lenet/2020-01-19 01:39:19/d_dim_1000_lr_0.01_gamma_0.2_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.45
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.97; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.77
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6787982848799152; val_accuracy: 0.7603503184713376 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2474937822883296; val_accuracy: 0.9206807324840764 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23473055565812787; val_accuracy: 0.9269506369426752 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.89
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12639451425546294; val_accuracy: 0.962281050955414 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11303294609021988; val_accuracy: 0.9666600318471338 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09968794085037937; val_accuracy: 0.9700437898089171 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08938919271158564; val_accuracy: 0.9728304140127388 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.94
Val Epoch over. val_loss: 0.3727654485850577; val_accuracy: 0.8837579617834395 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07965967218112793; val_accuracy: 0.9772093949044586 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09362546404360965; val_accuracy: 0.9731289808917197 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07684922951516832; val_accuracy: 0.9768113057324841 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.12556796098590656; val_accuracy: 0.9628781847133758 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.92
Batch: 620; loss: 0.1; acc: 0.94
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10048542069686446; val_accuracy: 0.9705414012738853 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06846730285294496; val_accuracy: 0.9793988853503185 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10711870786205978; val_accuracy: 0.9662619426751592 

Epoch 16 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06434908498576872; val_accuracy: 0.9815883757961783 

Epoch 17 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.38; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06292422256984148; val_accuracy: 0.9823845541401274 

Epoch 18 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.0; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06346741800380361; val_accuracy: 0.9804936305732485 

Epoch 19 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0631333257006422; val_accuracy: 0.9812898089171974 

Epoch 20 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06281478252190693; val_accuracy: 0.9815883757961783 

Epoch 21 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06435295666573913; val_accuracy: 0.9811902866242038 

Epoch 22 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.21; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06323624383302251; val_accuracy: 0.9810907643312102 

Epoch 23 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06291416371656451; val_accuracy: 0.9814888535031847 

Epoch 24 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06343350996067569; val_accuracy: 0.9815883757961783 

Epoch 25 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06155701128730349; val_accuracy: 0.9823845541401274 

Epoch 26 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062144055476139305; val_accuracy: 0.9822850318471338 

Epoch 27 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06088075967161519; val_accuracy: 0.9829816878980892 

Epoch 28 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06320732514000242; val_accuracy: 0.9818869426751592 

Epoch 29 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06253701232516082; val_accuracy: 0.9819864649681529 

Epoch 30 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.94
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06285370356005848; val_accuracy: 0.9818869426751592 

Epoch 31 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06161027935564898; val_accuracy: 0.9822850318471338 

Epoch 32 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06177689827931155; val_accuracy: 0.9818869426751592 

Epoch 33 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06157190143872218; val_accuracy: 0.9820859872611465 

Epoch 34 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0619317310963087; val_accuracy: 0.9814888535031847 

Epoch 35 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06141724229855522; val_accuracy: 0.9820859872611465 

Epoch 36 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061507248871360616; val_accuracy: 0.9822850318471338 

Epoch 37 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061653233910347244; val_accuracy: 0.9818869426751592 

Epoch 38 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06147244962964468; val_accuracy: 0.9821855095541401 

Epoch 39 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.94
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.02; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06154184216621575; val_accuracy: 0.981687898089172 

Epoch 40 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06155573927862629; val_accuracy: 0.9817874203821656 

Epoch 41 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.94
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061605087890746486; val_accuracy: 0.9817874203821656 

Epoch 42 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06142218093014067; val_accuracy: 0.9819864649681529 

Epoch 43 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.0; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06168207094358031; val_accuracy: 0.9819864649681529 

Epoch 44 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06140955504338453; val_accuracy: 0.9821855095541401 

Epoch 45 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0614164649823289; val_accuracy: 0.9818869426751592 

Epoch 46 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.19; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06142770457467076; val_accuracy: 0.9820859872611465 

Epoch 47 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.26; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06144118138179658; val_accuracy: 0.9819864649681529 

Epoch 48 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061454240280162; val_accuracy: 0.9819864649681529 

Epoch 49 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06145525523194462; val_accuracy: 0.9819864649681529 

Epoch 50 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06145312342863933; val_accuracy: 0.9819864649681529 

plots/no_subspace_training/reg_lenet/2020-01-19 01:48:33/d_dim_1000_lr_0.01_gamma_0.13_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 0.99; acc: 0.69
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.97; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.94; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.8
Val Epoch over. val_loss: 0.680024244033607; val_accuracy: 0.7601512738853503 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.78
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24755113206471607; val_accuracy: 0.9205812101910829 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.228933502225929; val_accuracy: 0.9285429936305732 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12610053479861302; val_accuracy: 0.9624800955414012 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11290992053735788; val_accuracy: 0.9661624203821656 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09933803809486377; val_accuracy: 0.9701433121019108 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08901626194358632; val_accuracy: 0.9732285031847133 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.94
Val Epoch over. val_loss: 0.36902015641996055; val_accuracy: 0.8853503184713376 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07959818721387037; val_accuracy: 0.9766122611464968 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09485403025985524; val_accuracy: 0.9728304140127388 

Epoch 11 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07401690404316422; val_accuracy: 0.9768113057324841 

Epoch 12 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.33; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07356176399596177; val_accuracy: 0.9775079617834395 

Epoch 13 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.92
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.94
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07295239458133461; val_accuracy: 0.9790007961783439 

Epoch 14 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.95
Batch: 760; loss: 0.04; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07322656783233782; val_accuracy: 0.978702229299363 

Epoch 15 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07353917709818691; val_accuracy: 0.9785031847133758 

Epoch 16 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07256523711950916; val_accuracy: 0.9791003184713376 

Epoch 17 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.41; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07080373894067327; val_accuracy: 0.9801950636942676 

Epoch 18 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07120720504717842; val_accuracy: 0.9789012738853503 

Epoch 19 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07077984785910245; val_accuracy: 0.9795979299363057 

Epoch 20 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06992179612113032; val_accuracy: 0.9792993630573248 

Epoch 21 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07004920205777618; val_accuracy: 0.979796974522293 

Epoch 22 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06996561798035719; val_accuracy: 0.9803941082802548 

Epoch 23 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06971292067200514; val_accuracy: 0.9800955414012739 

Epoch 24 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07052485618716592; val_accuracy: 0.9795979299363057 

Epoch 25 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06973951982607128; val_accuracy: 0.979796974522293 

Epoch 26 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07015156674726754; val_accuracy: 0.9796974522292994 

Epoch 27 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06959659179114992; val_accuracy: 0.9801950636942676 

Epoch 28 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06989006434751165; val_accuracy: 0.9794984076433121 

Epoch 29 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.07; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.95
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06974981773858237; val_accuracy: 0.9799960191082803 

Epoch 30 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.94
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06978325437825576; val_accuracy: 0.9799960191082803 

Epoch 31 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06973711776125963; val_accuracy: 0.9800955414012739 

Epoch 32 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06972662008302227; val_accuracy: 0.9800955414012739 

Epoch 33 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06969975203180769; val_accuracy: 0.9800955414012739 

Epoch 34 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06970415022342828; val_accuracy: 0.9799960191082803 

Epoch 35 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06967164717235003; val_accuracy: 0.9800955414012739 

Epoch 36 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.25; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06965911141626395; val_accuracy: 0.9801950636942676 

Epoch 37 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.23; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06966714864703501; val_accuracy: 0.9801950636942676 

Epoch 38 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06964742213155434; val_accuracy: 0.9800955414012739 

Epoch 39 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963078200722196; val_accuracy: 0.9801950636942676 

Epoch 40 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0696298204670856; val_accuracy: 0.9801950636942676 

Epoch 41 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0696305414649901; val_accuracy: 0.9801950636942676 

Epoch 42 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963107933047091; val_accuracy: 0.9801950636942676 

Epoch 43 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963198225067299; val_accuracy: 0.9801950636942676 

Epoch 44 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.94
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963237078421435; val_accuracy: 0.9801950636942676 

Epoch 45 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.94
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963281539879787; val_accuracy: 0.9800955414012739 

Epoch 46 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963289511289186; val_accuracy: 0.9800955414012739 

Epoch 47 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963391513059473; val_accuracy: 0.9800955414012739 

Epoch 48 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963446666338262; val_accuracy: 0.9801950636942676 

Epoch 49 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963729788400945; val_accuracy: 0.9801950636942676 

Epoch 50 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.92
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06963709315913878; val_accuracy: 0.9801950636942676 

plots/no_subspace_training/reg_lenet/2020-01-19 01:57:55/d_dim_1000_lr_0.01_gamma_0.13_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.44
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.45
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.75
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6798119936019752; val_accuracy: 0.7602507961783439 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24754819591903382; val_accuracy: 0.9204816878980892 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23272961641477932; val_accuracy: 0.927547770700637 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1263775426396139; val_accuracy: 0.9620820063694268 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11272811320177309; val_accuracy: 0.9661624203821656 

Epoch 6 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1016963021414485; val_accuracy: 0.9700437898089171 

Epoch 7 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10064383897527009; val_accuracy: 0.9697452229299363 

Epoch 8 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10583238716527915; val_accuracy: 0.9684514331210191 

Epoch 9 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09733669300842437; val_accuracy: 0.9718351910828026 

Epoch 10 start
The current lr is: 0.0013000000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09625838715939006; val_accuracy: 0.9721337579617835 

Epoch 11 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09564104405747857; val_accuracy: 0.9720342356687898 

Epoch 12 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.91
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09524674855979384; val_accuracy: 0.9726313694267515 

Epoch 13 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.17; acc: 0.89
Batch: 640; loss: 0.04; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09513226946353152; val_accuracy: 0.9722332802547771 

Epoch 14 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.095380436581601; val_accuracy: 0.9729299363057324 

Epoch 15 start
The current lr is: 0.00016900000000000002
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09528175845837138; val_accuracy: 0.9723328025477707 

Epoch 16 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.94
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09509894099964458; val_accuracy: 0.9723328025477707 

Epoch 17 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.91
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.47; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09503085643147967; val_accuracy: 0.9723328025477707 

Epoch 18 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.19; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09499495806302993; val_accuracy: 0.9721337579617835 

Epoch 19 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.92
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09495677281716827; val_accuracy: 0.9723328025477707 

Epoch 20 start
The current lr is: 2.197e-05
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491265546174565; val_accuracy: 0.9721337579617835 

Epoch 21 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491479026663835; val_accuracy: 0.9721337579617835 

Epoch 22 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491542119319273; val_accuracy: 0.9721337579617835 

Epoch 23 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491689685898222; val_accuracy: 0.9721337579617835 

Epoch 24 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.97
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491888768259127; val_accuracy: 0.9721337579617835 

Epoch 25 start
The current lr is: 2.8561000000000005e-06
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491875703642323; val_accuracy: 0.9721337579617835 

Epoch 26 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491911233895144; val_accuracy: 0.9721337579617835 

Epoch 27 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.94
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491897127620734; val_accuracy: 0.9721337579617835 

Epoch 28 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491894173489254; val_accuracy: 0.9721337579617835 

Epoch 29 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491924716124109; val_accuracy: 0.9721337579617835 

Epoch 30 start
The current lr is: 3.7129300000000007e-07
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491933132432828; val_accuracy: 0.9721337579617835 

Epoch 31 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491935341506247; val_accuracy: 0.9721337579617835 

Epoch 32 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.92
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491933920201223; val_accuracy: 0.9721337579617835 

Epoch 33 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491933312765352; val_accuracy: 0.9721337579617835 

Epoch 34 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.06; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0949194007048941; val_accuracy: 0.9721337579617835 

Epoch 35 start
The current lr is: 4.8268090000000015e-08
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.94
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.94
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491941956862522; val_accuracy: 0.9721337579617835 

Epoch 36 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.094919417100917; val_accuracy: 0.9721337579617835 

Epoch 37 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491941714837293; val_accuracy: 0.9721337579617835 

Epoch 38 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.04; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.27; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0949194192601617; val_accuracy: 0.9721337579617835 

Epoch 39 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491942001945654; val_accuracy: 0.9721337579617835 

Epoch 40 start
The current lr is: 6.2748517000000015e-09
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491942196514956; val_accuracy: 0.9721337579617835 

Epoch 41 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491941902288206; val_accuracy: 0.9721337579617835 

Epoch 42 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491942196514956; val_accuracy: 0.9721337579617835 

Epoch 43 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491942170414196; val_accuracy: 0.9721337579617835 

Epoch 44 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.94
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491942279562829; val_accuracy: 0.9721337579617835 

Epoch 45 start
The current lr is: 8.157307210000002e-10
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0949194227244444; val_accuracy: 0.9721337579617835 

Epoch 46 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0949194227244444; val_accuracy: 0.9721337579617835 

Epoch 47 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491942243970883; val_accuracy: 0.9721337579617835 

Epoch 48 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.94
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09491942243970883; val_accuracy: 0.9721337579617835 

Epoch 49 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.14; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0949194223922529; val_accuracy: 0.9721337579617835 

Epoch 50 start
The current lr is: 1.0604499373000003e-10
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0949194223922529; val_accuracy: 0.9721337579617835 

plots/no_subspace_training/reg_lenet/2020-01-19 02:07:08/d_dim_1000_lr_0.01_gamma_0.13_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.64
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.8
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.6801494895272954; val_accuracy: 0.7598527070063694 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24751497923758378; val_accuracy: 0.9205812101910829 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.233232525812023; val_accuracy: 0.927547770700637 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12642789636827578; val_accuracy: 0.9627786624203821 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11321040387650963; val_accuracy: 0.9664609872611465 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.91
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09924613981015364; val_accuracy: 0.9701433121019108 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08908084146441168; val_accuracy: 0.972531847133758 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.05; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.94
Val Epoch over. val_loss: 0.37116540313526325; val_accuracy: 0.8844546178343949 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07967724800584422; val_accuracy: 0.9772093949044586 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09350760779373206; val_accuracy: 0.972531847133758 

Epoch 11 start
The current lr is: 0.01
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07601971333478666; val_accuracy: 0.9773089171974523 

Epoch 12 start
The current lr is: 0.01
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.36; acc: 0.94
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.1271356964709273; val_accuracy: 0.9619824840764332 

Epoch 13 start
The current lr is: 0.01
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.1; acc: 0.94
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10252593123134535; val_accuracy: 0.970640923566879 

Epoch 14 start
The current lr is: 0.01
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0685989920548193; val_accuracy: 0.9798964968152867 

Epoch 15 start
The current lr is: 0.01
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.11063416202546685; val_accuracy: 0.9653662420382165 

Epoch 16 start
The current lr is: 0.001
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06412971992591385; val_accuracy: 0.9815883757961783 

Epoch 17 start
The current lr is: 0.001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.39; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06298903849236905; val_accuracy: 0.982484076433121 

Epoch 18 start
The current lr is: 0.001
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.0; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.95
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06302595216852085; val_accuracy: 0.9814888535031847 

Epoch 19 start
The current lr is: 0.001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.09; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06310734445834236; val_accuracy: 0.9811902866242038 

Epoch 20 start
The current lr is: 0.001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.14; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06290108379522327; val_accuracy: 0.981687898089172 

Epoch 21 start
The current lr is: 0.001
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06409785486045916; val_accuracy: 0.9820859872611465 

Epoch 22 start
The current lr is: 0.001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.21; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.05; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06324420482584625; val_accuracy: 0.9811902866242038 

Epoch 23 start
The current lr is: 0.001
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06284252777221097; val_accuracy: 0.9822850318471338 

Epoch 24 start
The current lr is: 0.001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.0; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.02; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06379216113335388; val_accuracy: 0.981687898089172 

Epoch 25 start
The current lr is: 0.001
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.02; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.0; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.0; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061887077548226734; val_accuracy: 0.9821855095541401 

Epoch 26 start
The current lr is: 0.001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06256155956797539; val_accuracy: 0.9818869426751592 

Epoch 27 start
The current lr is: 0.001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.0; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061306287670970724; val_accuracy: 0.982484076433121 

Epoch 28 start
The current lr is: 0.001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.95
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06318889306798862; val_accuracy: 0.9814888535031847 

Epoch 29 start
The current lr is: 0.001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.95
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 0.98
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062487938043892764; val_accuracy: 0.9817874203821656 

Epoch 30 start
The current lr is: 0.001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.94
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06295587748858579; val_accuracy: 0.9818869426751592 

Epoch 31 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.02; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.0; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.0; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062062400185568314; val_accuracy: 0.9820859872611465 

Epoch 32 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.97
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.95
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.062087145260280106; val_accuracy: 0.9817874203821656 

Epoch 33 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.0; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06193341346825384; val_accuracy: 0.9821855095541401 

Epoch 34 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.0; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.0; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06210502543172259; val_accuracy: 0.9820859872611465 

Epoch 35 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.0; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06183831079323201; val_accuracy: 0.9820859872611465 

Epoch 36 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.05; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 0.98
Batch: 600; loss: 0.0; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06190451805236613; val_accuracy: 0.9821855095541401 

Epoch 37 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.02; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.0; acc: 1.0
Batch: 140; loss: 0.04; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.0; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.0; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061985930356725005; val_accuracy: 0.9820859872611465 

Epoch 38 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.97
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.07; acc: 0.95
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.0; acc: 1.0
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06186571172705502; val_accuracy: 0.9819864649681529 

Epoch 39 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.1; acc: 0.94
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.97
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.0; acc: 1.0
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06187276427106113; val_accuracy: 0.9822850318471338 

Epoch 40 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.0; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.0; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061896832600520674; val_accuracy: 0.9821855095541401 

Epoch 41 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.95
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.94
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.02; acc: 0.98
Batch: 540; loss: 0.04; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.0; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06196243700328147; val_accuracy: 0.9820859872611465 

Epoch 42 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.97
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06186553968745432; val_accuracy: 0.9821855095541401 

Epoch 43 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.97
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06203076188826257; val_accuracy: 0.9820859872611465 

Epoch 44 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.02; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.11; acc: 0.95
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.02; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06187928374263511; val_accuracy: 0.9821855095541401 

Epoch 45 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.97
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.97
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.95
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.97
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.061831292619181286; val_accuracy: 0.9821855095541401 

Epoch 46 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.0; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.02; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.18; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06183848685472255; val_accuracy: 0.9821855095541401 

Epoch 47 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.97
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.27; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06184586601414878; val_accuracy: 0.9821855095541401 

Epoch 48 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.0; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.04; acc: 1.0
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.0; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06185262751095234; val_accuracy: 0.9821855095541401 

Epoch 49 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.02; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.25; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06185987964272499; val_accuracy: 0.9821855095541401 

Epoch 50 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.06; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.0; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.04; train_accuracy: 0.99 

Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.06186123172141564; val_accuracy: 0.9821855095541401 

plots/no_subspace_training/reg_lenet/2020-01-19 02:16:27/d_dim_1000_lr_0.01_gamma_0.1_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.13; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.98; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.75; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.46; acc: 0.8
Val Epoch over. val_loss: 0.6785650940457727; val_accuracy: 0.7607484076433121 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2463526961625002; val_accuracy: 0.9210788216560509 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.7; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.23216031553449146; val_accuracy: 0.9276472929936306 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12529378374860545; val_accuracy: 0.962281050955414 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.19; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1136575705448913; val_accuracy: 0.9658638535031847 

Epoch 6 start
The current lr is: 0.01
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09859584181741544; val_accuracy: 0.9702428343949044 

Epoch 7 start
The current lr is: 0.01
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08870165123586442; val_accuracy: 0.9731289808917197 

Epoch 8 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.92
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.09; acc: 0.95
Val Epoch over. val_loss: 0.36864178143678955; val_accuracy: 0.8850517515923567 

Epoch 9 start
The current lr is: 0.01
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.09; train_accuracy: 0.97 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.08022856387268205; val_accuracy: 0.9765127388535032 

Epoch 10 start
The current lr is: 0.01
Batch: 0; loss: 0.06; acc: 0.95
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.08; train_accuracy: 0.98 

Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09463409585937572; val_accuracy: 0.972531847133758 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.95
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.07; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0741247858875876; val_accuracy: 0.9774084394904459 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.04; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07339254816531375; val_accuracy: 0.9775079617834395 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.06; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.97
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.92
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.94
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07261084193351922; val_accuracy: 0.9791003184713376 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.97
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.05; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07342219504581135; val_accuracy: 0.9788017515923567 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.04; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07340971456401667; val_accuracy: 0.9789012738853503 

Epoch 16 start
The current lr is: 0.001
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.12; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07247459589485909; val_accuracy: 0.9785031847133758 

Epoch 17 start
The current lr is: 0.001
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.03; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.42; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07144577993424076; val_accuracy: 0.9794984076433121 

Epoch 18 start
The current lr is: 0.001
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07133030371776053; val_accuracy: 0.978702229299363 

Epoch 19 start
The current lr is: 0.001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07134115819338781; val_accuracy: 0.9791003184713376 

Epoch 20 start
The current lr is: 0.001
Batch: 0; loss: 0.02; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.98
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.06; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07066175052124983; val_accuracy: 0.9789012738853503 

Epoch 21 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.03; acc: 0.98
Batch: 780; loss: 0.06; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07078684524745699; val_accuracy: 0.9794984076433121 

Epoch 22 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.13; acc: 0.94
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07078322682790696; val_accuracy: 0.9795979299363057 

Epoch 23 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07056591626565167; val_accuracy: 0.9796974522292994 

Epoch 24 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07108886407059468; val_accuracy: 0.9791998407643312 

Epoch 25 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.02; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07062051445245743; val_accuracy: 0.9795979299363057 

Epoch 26 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.0; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07087970373167354; val_accuracy: 0.9793988853503185 

Epoch 27 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.15; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.95
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.01; acc: 1.0
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.94
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0705364944686176; val_accuracy: 0.9794984076433121 

Epoch 28 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07066061286030302; val_accuracy: 0.9794984076433121 

Epoch 29 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.08; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.06; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.94
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 0.98
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.03; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.97
Batch: 780; loss: 0.02; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07066805530220839; val_accuracy: 0.9796974522292994 

Epoch 30 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.94
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07068300660058952; val_accuracy: 0.9795979299363057 

Epoch 31 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.02; acc: 0.98
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.06; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.0; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07066196801176497; val_accuracy: 0.9796974522292994 

Epoch 32 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.08; acc: 0.95
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.05; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07065016574635627; val_accuracy: 0.9795979299363057 

Epoch 33 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.08; acc: 0.95
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.03; acc: 0.98
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.01; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07063501197726103; val_accuracy: 0.9795979299363057 

Epoch 34 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.0; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.06; acc: 0.97
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.04; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.04; acc: 0.97
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.03; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07062972348871505; val_accuracy: 0.9795979299363057 

Epoch 35 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.01; acc: 1.0
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.01; acc: 1.0
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07061637449226561; val_accuracy: 0.9795979299363057 

Epoch 36 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.0; acc: 1.0
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.1; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.0; acc: 1.0
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.01; acc: 1.0
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07060673923059634; val_accuracy: 0.9795979299363057 

Epoch 37 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.02; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07060474074285501; val_accuracy: 0.9795979299363057 

Epoch 38 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.01; acc: 1.0
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.0; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.01; acc: 1.0
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07059708709834488; val_accuracy: 0.9796974522292994 

Epoch 39 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.03; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.02; acc: 0.98
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 1.0
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.04; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.01; acc: 1.0
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.08; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.01; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058657767476549; val_accuracy: 0.9796974522292994 

Epoch 40 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.01; acc: 1.0
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.08; acc: 0.95
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058212403089377; val_accuracy: 0.9795979299363057 

Epoch 41 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.02; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.01; acc: 1.0
Batch: 300; loss: 0.05; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.95
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.05; acc: 1.0
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.02; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058201333994318; val_accuracy: 0.9795979299363057 

Epoch 42 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.1; acc: 0.97
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.02; acc: 0.98
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.01; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058190623191511; val_accuracy: 0.9795979299363057 

Epoch 43 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.0; acc: 1.0
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.01; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.02; acc: 0.98
Batch: 220; loss: 0.02; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.02; acc: 1.0
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.02; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.03; acc: 0.98
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 1.0
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.04; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058193000733473; val_accuracy: 0.9796974522292994 

Epoch 44 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.0; acc: 1.0
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.02; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.05; acc: 0.98
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.02; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.03; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058184050545571; val_accuracy: 0.9796974522292994 

Epoch 45 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.02; acc: 1.0
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.03; acc: 0.98
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 0.98
Batch: 280; loss: 0.02; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.01; acc: 1.0
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.06; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.94
Batch: 540; loss: 0.02; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.07; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058182622122157; val_accuracy: 0.9796974522292994 

Epoch 46 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.01; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.05; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.03; acc: 0.98
Batch: 260; loss: 0.09; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.01; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.02; acc: 0.98
Batch: 440; loss: 0.18; acc: 0.98
Batch: 460; loss: 0.01; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.03; acc: 0.98
Batch: 720; loss: 0.02; acc: 0.98
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058163158073547; val_accuracy: 0.9796974522292994 

Epoch 47 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.02; acc: 1.0
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058185179996643; val_accuracy: 0.9796974522292994 

Epoch 48 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.01; acc: 1.0
Batch: 220; loss: 0.03; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.03; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.01; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.01; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.01; acc: 1.0
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.01; acc: 1.0
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058191923483921; val_accuracy: 0.9796974522292994 

Epoch 49 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.03; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.01; acc: 1.0
Batch: 280; loss: 0.02; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.02; acc: 1.0
Batch: 340; loss: 0.21; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.01; acc: 1.0
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.02; acc: 1.0
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058294134060289; val_accuracy: 0.9796974522292994 

Epoch 50 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.03; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.01; acc: 1.0
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.02; acc: 1.0
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.1; acc: 0.92
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.02; acc: 0.98
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.03; acc: 0.98
Batch: 560; loss: 0.02; acc: 0.98
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.01; acc: 1.0
Batch: 640; loss: 0.02; acc: 1.0
Batch: 660; loss: 0.01; acc: 1.0
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.02; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.06; train_accuracy: 0.98 

Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.04; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.01; acc: 1.0
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.07058279436959583; val_accuracy: 0.9796974522292994 

plots/no_subspace_training/reg_lenet/2020-01-19 02:25:50/d_dim_1000_lr_0.01_gamma_0.1_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.01
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.25; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.16; acc: 0.27
Batch: 120; loss: 2.12; acc: 0.39
Batch: 140; loss: 2.09; acc: 0.31
Batch: 160; loss: 2.09; acc: 0.27
Batch: 180; loss: 2.08; acc: 0.25
Batch: 200; loss: 1.96; acc: 0.28
Batch: 220; loss: 1.93; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.76; acc: 0.42
Batch: 280; loss: 1.74; acc: 0.47
Batch: 300; loss: 1.69; acc: 0.45
Batch: 320; loss: 1.47; acc: 0.52
Batch: 340; loss: 1.39; acc: 0.59
Batch: 360; loss: 1.48; acc: 0.53
Batch: 380; loss: 1.55; acc: 0.44
Batch: 400; loss: 1.32; acc: 0.62
Batch: 420; loss: 1.44; acc: 0.47
Batch: 440; loss: 1.37; acc: 0.44
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.66
Batch: 500; loss: 1.07; acc: 0.7
Batch: 520; loss: 0.98; acc: 0.67
Batch: 540; loss: 1.13; acc: 0.67
Batch: 560; loss: 1.1; acc: 0.66
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 0.97; acc: 0.66
Batch: 640; loss: 0.71; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.69
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.8; acc: 0.77
Batch: 760; loss: 0.92; acc: 0.67
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 1.43; train_accuracy: 0.53 

Batch: 0; loss: 0.65; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.69
Batch: 40; loss: 0.63; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.57; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.47; acc: 0.8
Val Epoch over. val_loss: 0.679528423365514; val_accuracy: 0.7598527070063694 

Epoch 2 start
The current lr is: 0.01
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.62; acc: 0.77
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.75
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24906918050567056; val_accuracy: 0.9195859872611465 

Epoch 3 start
The current lr is: 0.01
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.98
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.2350238360417117; val_accuracy: 0.9267515923566879 

Epoch 4 start
The current lr is: 0.01
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.98
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12553274932845382; val_accuracy: 0.9624800955414012 

Epoch 5 start
The current lr is: 0.01
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.03; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1133693801179813; val_accuracy: 0.9662619426751592 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.04; acc: 1.0
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.04; acc: 1.0
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.11; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10177006451472355; val_accuracy: 0.9700437898089171 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10097082440925252; val_accuracy: 0.9705414012738853 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.95
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.03; acc: 1.0
Batch: 600; loss: 0.09; acc: 1.0
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.10402767663928354; val_accuracy: 0.9696457006369427 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.92
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.04; acc: 0.98
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09810300672035309; val_accuracy: 0.9720342356687898 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.94
Batch: 480; loss: 0.02; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.06; acc: 0.97
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09718729206805776; val_accuracy: 0.9715366242038217 

Epoch 11 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.94
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.03; acc: 0.98
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09676866100472249; val_accuracy: 0.9719347133757962 

Epoch 12 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.06; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.04; acc: 1.0
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.91
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.95
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09648506063374744; val_accuracy: 0.9724323248407644 

Epoch 13 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.95
Batch: 220; loss: 0.06; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.19; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.89
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09645184874534607; val_accuracy: 0.9720342356687898 

Epoch 14 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09663945353429788; val_accuracy: 0.9723328025477707 

Epoch 15 start
The current lr is: 0.00010000000000000002
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0965400415051515; val_accuracy: 0.9721337579617835 

Epoch 16 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.94
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.01; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.04; acc: 0.98
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09647885294761627; val_accuracy: 0.9721337579617835 

Epoch 17 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.06; acc: 1.0
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.91
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.47; acc: 0.95
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.97
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.04; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09644602462174787; val_accuracy: 0.9721337579617835 

Epoch 18 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.04; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.12; acc: 0.95
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09642124909219468; val_accuracy: 0.9721337579617835 

Epoch 19 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.92
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09640209402912742; val_accuracy: 0.9721337579617835 

Epoch 20 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.03; acc: 0.98
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.02; acc: 0.98
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.98
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.03; acc: 1.0
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09638043891662246; val_accuracy: 0.9721337579617835 

Epoch 21 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.07; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.98
Batch: 400; loss: 0.05; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.21; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.03; acc: 1.0
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637991597603081; val_accuracy: 0.9721337579617835 

Epoch 22 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.03; acc: 1.0
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637906881654339; val_accuracy: 0.9721337579617835 

Epoch 23 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0963786043891102; val_accuracy: 0.9721337579617835 

Epoch 24 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.04; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.0963780418228192; val_accuracy: 0.9721337579617835 

Epoch 25 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.05; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.15; acc: 0.98
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.14; acc: 0.91
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.01; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637724347175307; val_accuracy: 0.9721337579617835 

Epoch 26 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.08; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.01; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.94
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.02; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637726558621522; val_accuracy: 0.9721337579617835 

Epoch 27 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.02; acc: 0.98
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.06; acc: 1.0
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.94
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.94
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.05; acc: 1.0
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637709920573387; val_accuracy: 0.9721337579617835 

Epoch 28 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.29; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.02; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637700688022717; val_accuracy: 0.9721337579617835 

Epoch 29 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.04; acc: 0.98
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.97
Batch: 740; loss: 0.04; acc: 0.98
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637699428067845; val_accuracy: 0.9721337579617835 

Epoch 30 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.03; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.07; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696136999283; val_accuracy: 0.9721337579617835 

Epoch 31 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.21; acc: 0.97
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.02; acc: 1.0
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.01; acc: 1.0
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696409870865; val_accuracy: 0.9721337579617835 

Epoch 32 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.97
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.92
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.05; acc: 0.97
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.08; acc: 0.95
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.07; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696120389708; val_accuracy: 0.9721337579617835 

Epoch 33 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.98
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.03; acc: 1.0
Batch: 680; loss: 0.05; acc: 0.98
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696568848221; val_accuracy: 0.9721337579617835 

Epoch 34 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.01; acc: 1.0
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.05; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.09; acc: 0.97
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.03; acc: 1.0
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.03; acc: 1.0
Batch: 540; loss: 0.06; acc: 0.98
Batch: 560; loss: 0.01; acc: 1.0
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.98
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696984087586; val_accuracy: 0.9721337579617835 

Epoch 35 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.04; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.04; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.05; acc: 0.97
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.04; acc: 0.98
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.05; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.11; acc: 0.94
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.04; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696995951567; val_accuracy: 0.9721337579617835 

Epoch 36 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.01; acc: 1.0
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.03; acc: 1.0
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.98
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.01; acc: 1.0
Batch: 420; loss: 0.07; acc: 0.95
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.02; acc: 1.0
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.05; acc: 0.98
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.14; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696943750047; val_accuracy: 0.9721337579617835 

Epoch 37 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.02; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696789518284; val_accuracy: 0.9721337579617835 

Epoch 38 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.05; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.03; acc: 1.0
Batch: 60; loss: 0.05; acc: 1.0
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.03; acc: 0.98
Batch: 440; loss: 0.03; acc: 0.98
Batch: 460; loss: 0.05; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.97
Batch: 620; loss: 0.04; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.03; acc: 0.98
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696715961595; val_accuracy: 0.9721337579617835 

Epoch 39 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.02; acc: 1.0
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.06; acc: 0.97
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.04; acc: 0.98
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.06; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.07; acc: 0.97
Batch: 380; loss: 0.03; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.05; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.98
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.02; acc: 1.0
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.13; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696827483025; val_accuracy: 0.9721337579617835 

Epoch 40 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.03; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.04; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.02; acc: 1.0
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.06; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696912903694; val_accuracy: 0.9721337579617835 

Epoch 41 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.05; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.02; acc: 1.0
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.07; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.06; acc: 0.95
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.07; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.04; acc: 1.0
Batch: 720; loss: 0.02; acc: 1.0
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

Epoch 42 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.05; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.24; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.03; acc: 1.0
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

Epoch 43 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.01; acc: 1.0
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.02; acc: 1.0
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.03; acc: 0.98
Batch: 220; loss: 0.03; acc: 1.0
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.02; acc: 1.0
Batch: 280; loss: 0.04; acc: 1.0
Batch: 300; loss: 0.03; acc: 1.0
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.04; acc: 0.97
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.06; acc: 0.98
Batch: 460; loss: 0.04; acc: 1.0
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.05; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.04; acc: 1.0
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

Epoch 44 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.06; acc: 0.98
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.01; acc: 1.0
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.07; acc: 0.98
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.04; acc: 0.97
Batch: 520; loss: 0.03; acc: 0.98
Batch: 540; loss: 0.05; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.94
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.09; acc: 0.95
Batch: 760; loss: 0.04; acc: 1.0
Batch: 780; loss: 0.04; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

Epoch 45 start
The current lr is: 1.0000000000000006e-10
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.03; acc: 0.98
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.04; acc: 1.0
Batch: 120; loss: 0.01; acc: 1.0
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.04; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.05; acc: 0.98
Batch: 260; loss: 0.04; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.98
Batch: 320; loss: 0.03; acc: 0.98
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.06; acc: 1.0
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.94
Batch: 540; loss: 0.03; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.05; acc: 0.97
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

Epoch 46 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.02; acc: 1.0
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.05; acc: 0.97
Batch: 160; loss: 0.03; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.04; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.92
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.1; acc: 0.95
Batch: 340; loss: 0.03; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.04; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.98
Batch: 460; loss: 0.03; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.07; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

Epoch 47 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.08; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.03; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.03; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.32; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.07; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.05; acc: 0.98
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

Epoch 48 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.05; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.09; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.92
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.02; acc: 1.0
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.04; acc: 0.97
Batch: 400; loss: 0.04; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.03; acc: 1.0
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.04; acc: 1.0
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.02; acc: 1.0
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.02; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.03; acc: 1.0
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

Epoch 49 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.15; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.03; acc: 1.0
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.08; acc: 0.97
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.29; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.14; acc: 0.98
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.05; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.06; acc: 1.0
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

Epoch 50 start
The current lr is: 1.0000000000000004e-11
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.05; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.02; acc: 1.0
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.04; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.04; acc: 0.98
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.95
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.05; acc: 0.98
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.04; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.05; acc: 1.0
Batch: 660; loss: 0.02; acc: 1.0
Batch: 680; loss: 0.06; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.03; acc: 1.0
Train Epoch over. train_loss: 0.1; train_accuracy: 0.97 

Batch: 0; loss: 0.04; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.02; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.0; acc: 1.0
Val Epoch over. val_loss: 0.09637696855956582; val_accuracy: 0.9721337579617835 

plots/no_subspace_training/reg_lenet/2020-01-19 02:35:10/d_dim_1000_lr_0.01_gamma_0.1_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161154382547754; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.028870618267424; val_accuracy: 0.3690286624203822 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.38
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.42
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508773811303886; val_accuracy: 0.4570063694267516 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.509504836076384; val_accuracy: 0.5687699044585988 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878772475916869; val_accuracy: 0.6332603503184714 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.21; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.62
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.052412164818709; val_accuracy: 0.6973527070063694 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.8261917233467102; val_accuracy: 0.7790605095541401 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6589765577179612; val_accuracy: 0.8039410828025477 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.536063662949641; val_accuracy: 0.8444466560509554 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.4435366112145649; val_accuracy: 0.8716162420382165 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.72; acc: 0.72
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3871867818057917; val_accuracy: 0.8839570063694268 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3392776544591424; val_accuracy: 0.9021695859872612 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.30627779586679615; val_accuracy: 0.9115246815286624 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.98
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.27940132018107516; val_accuracy: 0.919984076433121 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.2711711688216325; val_accuracy: 0.9175955414012739 

Epoch 16 start
The current lr is: 0.0008
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.23987702830771732; val_accuracy: 0.9298367834394905 

Epoch 17 start
The current lr is: 0.0008
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.17; acc: 0.98
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.41; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.84
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2226254287988517; val_accuracy: 0.9362062101910829 

Epoch 18 start
The current lr is: 0.0008
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2144912776958411; val_accuracy: 0.9370023885350318 

Epoch 19 start
The current lr is: 0.0008
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.81
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.12; acc: 0.98
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.20185120382411464; val_accuracy: 0.9406847133757962 

Epoch 20 start
The current lr is: 0.0008
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.13; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19970705998456403; val_accuracy: 0.9398885350318471 

Epoch 21 start
The current lr is: 0.0008
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.08; acc: 1.0
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.09; acc: 1.0
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.27; acc: 0.88
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.20382096938741434; val_accuracy: 0.940187101910828 

Epoch 22 start
The current lr is: 0.0008
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.98
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.88
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.12; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.18054549690264804; val_accuracy: 0.9455613057324841 

Epoch 23 start
The current lr is: 0.0008
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.86
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.08; acc: 0.97
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1848579316761843; val_accuracy: 0.9449641719745223 

Epoch 24 start
The current lr is: 0.0008
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.09; acc: 1.0
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.08; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16864145141403386; val_accuracy: 0.9500398089171974 

Epoch 25 start
The current lr is: 0.0008
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.11; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16890491867900653; val_accuracy: 0.9497412420382165 

Epoch 26 start
The current lr is: 0.0008
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15821514283395877; val_accuracy: 0.9526273885350318 

Epoch 27 start
The current lr is: 0.0008
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.92
Batch: 460; loss: 0.08; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.09; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15600675593610783; val_accuracy: 0.9543192675159236 

Epoch 28 start
The current lr is: 0.0008
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.1; acc: 0.98
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.03; acc: 1.0
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15248278839288243; val_accuracy: 0.9539211783439491 

Epoch 29 start
The current lr is: 0.0008
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.86
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.05; acc: 1.0
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15630974736847694; val_accuracy: 0.9537221337579618 

Epoch 30 start
The current lr is: 0.0008
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.11; acc: 1.0
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14562991374190065; val_accuracy: 0.9581011146496815 

Epoch 31 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.06; acc: 1.0
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.07; acc: 1.0
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.04; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15050889081825877; val_accuracy: 0.9541202229299363 

Epoch 32 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.08; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14219816484648712; val_accuracy: 0.9583001592356688 

Epoch 33 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.08; acc: 1.0
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.08; acc: 1.0
Batch: 480; loss: 0.16; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1388258328957922; val_accuracy: 0.9593949044585988 

Epoch 34 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.03; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14103301279958647; val_accuracy: 0.9586982484076433 

Epoch 35 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.05; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13395705842857908; val_accuracy: 0.9610867834394905 

Epoch 36 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.05; acc: 1.0
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13635092681855154; val_accuracy: 0.9585987261146497 

Epoch 37 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.98
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.08; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1347004313876105; val_accuracy: 0.9609872611464968 

Epoch 38 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.08; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.39; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.07; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13058765163751923; val_accuracy: 0.960390127388535 

Epoch 39 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.05; acc: 1.0
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.15; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13590353991907494; val_accuracy: 0.9582006369426752 

Epoch 40 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.05; acc: 0.97
Batch: 140; loss: 0.05; acc: 1.0
Batch: 160; loss: 0.11; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.98
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.04; acc: 1.0
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.1; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.13; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1254432362142452; val_accuracy: 0.9617834394904459 

Epoch 41 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.06; acc: 1.0
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.94
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.07; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.13; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.04; acc: 1.0
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1311029005249974; val_accuracy: 0.960390127388535 

Epoch 42 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.03; acc: 1.0
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.07; acc: 1.0
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.14; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12363282611273277; val_accuracy: 0.9620820063694268 

Epoch 43 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.03; acc: 0.98
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.03; acc: 0.98
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.05; acc: 0.97
Batch: 220; loss: 0.05; acc: 1.0
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.04; acc: 0.98
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12192528938079715; val_accuracy: 0.9648686305732485 

Epoch 44 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.05; acc: 1.0
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.08; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.22; acc: 0.86
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.05; acc: 0.98
Batch: 740; loss: 0.15; acc: 0.92
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12183603392854618; val_accuracy: 0.9629777070063694 

Epoch 45 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.13; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.04; acc: 0.98
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.07; acc: 1.0
Batch: 120; loss: 0.03; acc: 1.0
Batch: 140; loss: 0.21; acc: 0.88
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.95
Batch: 200; loss: 0.05; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.07; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.03; acc: 1.0
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.89
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13590469161511226; val_accuracy: 0.956906847133758 

Epoch 46 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.04; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.06; acc: 0.97
Batch: 160; loss: 0.06; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.03; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.03; acc: 1.0
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.05; acc: 0.98
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11856606768764508; val_accuracy: 0.9635748407643312 

Epoch 47 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.06; acc: 1.0
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.02; acc: 1.0
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.06; acc: 0.98
Batch: 660; loss: 0.06; acc: 1.0
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.92
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11955644607923593; val_accuracy: 0.964171974522293 

Epoch 48 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.88
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.05; acc: 1.0
Batch: 220; loss: 0.17; acc: 0.98
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.91
Batch: 440; loss: 0.07; acc: 0.98
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.05; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11647622696580781; val_accuracy: 0.9652667197452229 

Epoch 49 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.04; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.98
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.13; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.92
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12036749310079653; val_accuracy: 0.963077229299363 

Epoch 50 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.07; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.06; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.98
Batch: 240; loss: 0.06; acc: 1.0
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.07; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.05; acc: 0.97
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.14; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.06; acc: 1.0
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.03; acc: 0.98
Batch: 640; loss: 0.07; acc: 1.0
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.12; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.11; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.03; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.11818702898587391; val_accuracy: 0.9632762738853503 

plots/no_subspace_training/reg_lenet/2020-01-19 02:44:30/d_dim_1000_lr_0.001_gamma_0.8_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161153820669575; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.028858578888474; val_accuracy: 0.3688296178343949 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.39
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508560221665983; val_accuracy: 0.45710589171974525 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.38
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5094966448036728; val_accuracy: 0.5688694267515924 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.287857357863408; val_accuracy: 0.6331608280254777 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.0523613657161688; val_accuracy: 0.6972531847133758 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.83
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.826159011786151; val_accuracy: 0.779359076433121 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6589349204567587; val_accuracy: 0.8040406050955414 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.536017749719559; val_accuracy: 0.8441480891719745 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.4434904575727548; val_accuracy: 0.8714171974522293 

Epoch 11 start
The current lr is: 0.0008
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.62; acc: 0.8
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.54; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.73; acc: 0.72
Batch: 480; loss: 0.74; acc: 0.75
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.5; acc: 0.78
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.78
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.78
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3961568387450686; val_accuracy: 0.8824641719745223 

Epoch 12 start
The current lr is: 0.0008
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.83
Batch: 60; loss: 0.32; acc: 0.94
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.81
Batch: 520; loss: 0.28; acc: 0.97
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.78
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.35395927200461647; val_accuracy: 0.897890127388535 

Epoch 13 start
The current lr is: 0.0008
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.58; acc: 0.78
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.48; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.45; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.95
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.32328592743843226; val_accuracy: 0.9058519108280255 

Epoch 14 start
The current lr is: 0.0008
Batch: 0; loss: 0.3; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.81
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.98
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.83
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.2981725983369123; val_accuracy: 0.9155055732484076 

Epoch 15 start
The current lr is: 0.0008
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.94
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.44; acc: 0.83
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.84
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2882307750783908; val_accuracy: 0.9125199044585988 

Epoch 16 start
The current lr is: 0.0008
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.83
Batch: 420; loss: 0.35; acc: 0.84
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.25820534249210053; val_accuracy: 0.9248606687898089 

Epoch 17 start
The current lr is: 0.0008
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.19; acc: 0.98
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.95
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.23892682124000447; val_accuracy: 0.9300358280254777 

Epoch 18 start
The current lr is: 0.0008
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.98
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2282470869742761; val_accuracy: 0.932921974522293 

Epoch 19 start
The current lr is: 0.0008
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.41; acc: 0.81
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.98
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.89
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.88
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.21364989217109742; val_accuracy: 0.9386942675159236 

Epoch 20 start
The current lr is: 0.0008
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.91
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.20986147530993837; val_accuracy: 0.9371019108280255 

Epoch 21 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.08; acc: 1.0
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.12; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2088496256501052; val_accuracy: 0.9377985668789809 

Epoch 22 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.86
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19077263784351622; val_accuracy: 0.9437699044585988 

Epoch 23 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.89
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19104829909885004; val_accuracy: 0.9441679936305732 

Epoch 24 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.37; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.98
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17928504633011333; val_accuracy: 0.9464570063694268 

Epoch 25 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.06; acc: 0.97
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.07; acc: 1.0
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17829331466156967; val_accuracy: 0.946656050955414 

Epoch 26 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.12; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.04; acc: 1.0
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16864766287291125; val_accuracy: 0.9498407643312102 

Epoch 27 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.98
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.91
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.88
Batch: 540; loss: 0.2; acc: 0.91
Batch: 560; loss: 0.1; acc: 0.98
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16609478858151253; val_accuracy: 0.9503383757961783 

Epoch 28 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.08; acc: 1.0
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.04; acc: 1.0
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16284115992154286; val_accuracy: 0.9509355095541401 

Epoch 29 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.86
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.06; acc: 1.0
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16448316715989902; val_accuracy: 0.951234076433121 

Epoch 30 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.12; acc: 1.0
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.14; acc: 0.98
Batch: 600; loss: 0.06; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.15663379070105826; val_accuracy: 0.955015923566879 

Epoch 31 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15874705420937507; val_accuracy: 0.9521297770700637 

Epoch 32 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.07; acc: 0.98
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1522225464700134; val_accuracy: 0.9557125796178344 

Epoch 33 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.09; acc: 1.0
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.26; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1488180843887815; val_accuracy: 0.9568073248407644 

Epoch 34 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.04; acc: 1.0
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.95
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.05; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.98
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.14947987810536556; val_accuracy: 0.955812101910828 

Epoch 35 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.05; acc: 1.0
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.07; acc: 1.0
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.32; acc: 0.86
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14398917963929997; val_accuracy: 0.9586982484076433 

Epoch 36 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.04; acc: 1.0
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.03; acc: 1.0
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.06; acc: 1.0
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.1; acc: 0.98
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14426863309778984; val_accuracy: 0.9575039808917197 

Epoch 37 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.19; acc: 0.94
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.98
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.1; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14286853726597348; val_accuracy: 0.9591958598726115 

Epoch 38 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.98
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13988577107050618; val_accuracy: 0.9586982484076433 

Epoch 39 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.15; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.92
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14184066529866238; val_accuracy: 0.9578025477707006 

Epoch 40 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.15; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.98
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.98
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.04; acc: 0.98
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.05; acc: 1.0
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.92
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.09; acc: 1.0
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13501788344779972; val_accuracy: 0.9597929936305732 

Epoch 41 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.04; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.11; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.08; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.44; acc: 0.88
Batch: 480; loss: 0.08; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.09; acc: 0.98
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.05; acc: 1.0
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13767718179685295; val_accuracy: 0.9584992038216561 

Epoch 42 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.04; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.11; acc: 0.95
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.09; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.92
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.98
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13268702371627283; val_accuracy: 0.9608877388535032 

Epoch 43 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.03; acc: 1.0
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.16; acc: 0.92
Batch: 260; loss: 0.04; acc: 1.0
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.05; acc: 0.98
Batch: 320; loss: 0.07; acc: 1.0
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.1; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.15; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1318121485317209; val_accuracy: 0.9619824840764332 

Epoch 44 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.04; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.91
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.09; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.86
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.18; acc: 0.92
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.13147348727864824; val_accuracy: 0.9611863057324841 

Epoch 45 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.22; acc: 0.88
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.94
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.94
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.08; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.89
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.14004971091724505; val_accuracy: 0.9566082802547771 

Epoch 46 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.08; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.04; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.16; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.04; acc: 1.0
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.23; acc: 0.95
Batch: 420; loss: 0.06; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.09; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.06; acc: 1.0
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.08; acc: 1.0
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.1286427818215007; val_accuracy: 0.9608877388535032 

Epoch 47 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.09; acc: 0.98
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.08; acc: 1.0
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.07; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.03; acc: 1.0
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.88
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12919783882891675; val_accuracy: 0.962281050955414 

Epoch 48 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.09; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.88
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.98
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.92
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.97
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.08; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.06; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.05; acc: 1.0
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12663269556679163; val_accuracy: 0.963077229299363 

Epoch 49 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.1; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.06; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12953775762847275; val_accuracy: 0.9607882165605095 

Epoch 50 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.06; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.07; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.07; acc: 1.0
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.04; acc: 0.98
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.04; acc: 1.0
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.07; acc: 0.97
Batch: 780; loss: 0.04; acc: 1.0
Train Epoch over. train_loss: 0.14; train_accuracy: 0.96 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.01; acc: 1.0
Val Epoch over. val_loss: 0.12727133913119887; val_accuracy: 0.9612858280254777 

plots/no_subspace_training/reg_lenet/2020-01-19 02:53:49/d_dim_1000_lr_0.001_gamma_0.8_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161154154759304; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.0288679136592114; val_accuracy: 0.3690286624203822 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.38
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.42
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508738230747782; val_accuracy: 0.4570063694267516 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5095040433725733; val_accuracy: 0.5687699044585988 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878823189219093; val_accuracy: 0.633359872611465 

Epoch 6 start
The current lr is: 0.0008
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.69
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.2; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 1.39; acc: 0.59
Batch: 160; loss: 1.33; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.27; acc: 0.64
Batch: 220; loss: 1.24; acc: 0.62
Batch: 240; loss: 1.2; acc: 0.64
Batch: 260; loss: 1.24; acc: 0.59
Batch: 280; loss: 1.2; acc: 0.61
Batch: 300; loss: 1.21; acc: 0.72
Batch: 320; loss: 1.15; acc: 0.73
Batch: 340; loss: 1.32; acc: 0.59
Batch: 360; loss: 1.28; acc: 0.53
Batch: 380; loss: 1.26; acc: 0.58
Batch: 400; loss: 1.26; acc: 0.7
Batch: 420; loss: 1.08; acc: 0.77
Batch: 440; loss: 1.17; acc: 0.64
Batch: 460; loss: 1.07; acc: 0.78
Batch: 480; loss: 1.17; acc: 0.73
Batch: 500; loss: 1.39; acc: 0.67
Batch: 520; loss: 1.21; acc: 0.7
Batch: 540; loss: 1.11; acc: 0.75
Batch: 560; loss: 1.3; acc: 0.62
Batch: 580; loss: 1.24; acc: 0.62
Batch: 600; loss: 1.08; acc: 0.72
Batch: 620; loss: 1.05; acc: 0.75
Batch: 640; loss: 1.11; acc: 0.67
Batch: 660; loss: 1.22; acc: 0.61
Batch: 680; loss: 1.13; acc: 0.66
Batch: 700; loss: 1.12; acc: 0.66
Batch: 720; loss: 1.07; acc: 0.72
Batch: 740; loss: 1.16; acc: 0.72
Batch: 760; loss: 1.03; acc: 0.77
Batch: 780; loss: 1.22; acc: 0.56
Train Epoch over. train_loss: 1.22; train_accuracy: 0.65 

Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.24; acc: 0.55
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.77
Batch: 100; loss: 1.07; acc: 0.7
Batch: 120; loss: 1.19; acc: 0.66
Batch: 140; loss: 0.89; acc: 0.81
Val Epoch over. val_loss: 1.0982482478876783; val_accuracy: 0.6846138535031847 

Epoch 7 start
The current lr is: 0.0008
Batch: 0; loss: 1.01; acc: 0.67
Batch: 20; loss: 1.06; acc: 0.72
Batch: 40; loss: 1.31; acc: 0.67
Batch: 60; loss: 1.02; acc: 0.75
Batch: 80; loss: 1.15; acc: 0.72
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 1.16; acc: 0.7
Batch: 160; loss: 1.03; acc: 0.69
Batch: 180; loss: 1.02; acc: 0.7
Batch: 200; loss: 1.07; acc: 0.69
Batch: 220; loss: 1.35; acc: 0.53
Batch: 240; loss: 1.04; acc: 0.77
Batch: 260; loss: 1.11; acc: 0.66
Batch: 280; loss: 0.94; acc: 0.81
Batch: 300; loss: 1.07; acc: 0.73
Batch: 320; loss: 1.11; acc: 0.66
Batch: 340; loss: 1.36; acc: 0.61
Batch: 360; loss: 1.12; acc: 0.72
Batch: 380; loss: 1.06; acc: 0.77
Batch: 400; loss: 0.97; acc: 0.72
Batch: 420; loss: 1.04; acc: 0.69
Batch: 440; loss: 1.05; acc: 0.75
Batch: 460; loss: 0.96; acc: 0.73
Batch: 480; loss: 1.12; acc: 0.66
Batch: 500; loss: 1.01; acc: 0.7
Batch: 520; loss: 1.18; acc: 0.64
Batch: 540; loss: 1.13; acc: 0.7
Batch: 560; loss: 1.07; acc: 0.67
Batch: 580; loss: 1.15; acc: 0.69
Batch: 600; loss: 1.01; acc: 0.75
Batch: 620; loss: 1.06; acc: 0.66
Batch: 640; loss: 0.95; acc: 0.7
Batch: 660; loss: 0.96; acc: 0.72
Batch: 680; loss: 0.89; acc: 0.72
Batch: 700; loss: 0.91; acc: 0.77
Batch: 720; loss: 0.96; acc: 0.72
Batch: 740; loss: 0.99; acc: 0.7
Batch: 760; loss: 1.05; acc: 0.66
Batch: 780; loss: 0.91; acc: 0.69
Train Epoch over. train_loss: 1.04; train_accuracy: 0.71 

Batch: 0; loss: 0.94; acc: 0.78
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.78; acc: 0.8
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.84; acc: 0.8
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 1.03; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.84
Val Epoch over. val_loss: 0.909459285295693; val_accuracy: 0.7580613057324841 

Epoch 8 start
The current lr is: 0.0008
Batch: 0; loss: 0.79; acc: 0.78
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.89; acc: 0.75
Batch: 60; loss: 0.92; acc: 0.72
Batch: 80; loss: 1.14; acc: 0.62
Batch: 100; loss: 1.04; acc: 0.66
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.82; acc: 0.81
Batch: 160; loss: 0.94; acc: 0.7
Batch: 180; loss: 0.98; acc: 0.69
Batch: 200; loss: 0.87; acc: 0.73
Batch: 220; loss: 1.01; acc: 0.78
Batch: 240; loss: 0.88; acc: 0.77
Batch: 260; loss: 0.91; acc: 0.75
Batch: 280; loss: 0.72; acc: 0.81
Batch: 300; loss: 0.96; acc: 0.73
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 0.95; acc: 0.67
Batch: 360; loss: 0.77; acc: 0.83
Batch: 380; loss: 0.94; acc: 0.72
Batch: 400; loss: 0.96; acc: 0.72
Batch: 420; loss: 0.92; acc: 0.75
Batch: 440; loss: 0.82; acc: 0.75
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.74; acc: 0.8
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.78; acc: 0.75
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 0.78; acc: 0.77
Batch: 580; loss: 0.48; acc: 0.92
Batch: 600; loss: 1.0; acc: 0.72
Batch: 620; loss: 0.86; acc: 0.75
Batch: 640; loss: 0.93; acc: 0.72
Batch: 660; loss: 0.72; acc: 0.72
Batch: 680; loss: 0.91; acc: 0.66
Batch: 700; loss: 0.64; acc: 0.83
Batch: 720; loss: 0.62; acc: 0.86
Batch: 740; loss: 0.86; acc: 0.8
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.67; acc: 0.81
Train Epoch over. train_loss: 0.86; train_accuracy: 0.76 

Batch: 0; loss: 0.79; acc: 0.83
Batch: 20; loss: 0.86; acc: 0.69
Batch: 40; loss: 0.69; acc: 0.77
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.86
Batch: 100; loss: 0.74; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.86
Val Epoch over. val_loss: 0.7524011573594087; val_accuracy: 0.7836385350318471 

Epoch 9 start
The current lr is: 0.0008
Batch: 0; loss: 0.8; acc: 0.81
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.86; acc: 0.78
Batch: 60; loss: 0.88; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.84; acc: 0.73
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 0.78; acc: 0.8
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.73; acc: 0.77
Batch: 220; loss: 0.77; acc: 0.75
Batch: 240; loss: 0.86; acc: 0.73
Batch: 260; loss: 0.71; acc: 0.78
Batch: 280; loss: 0.68; acc: 0.78
Batch: 300; loss: 0.79; acc: 0.75
Batch: 320; loss: 0.68; acc: 0.84
Batch: 340; loss: 0.65; acc: 0.77
Batch: 360; loss: 0.71; acc: 0.75
Batch: 380; loss: 0.91; acc: 0.73
Batch: 400; loss: 0.6; acc: 0.89
Batch: 420; loss: 0.84; acc: 0.75
Batch: 440; loss: 0.81; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.73
Batch: 480; loss: 0.7; acc: 0.84
Batch: 500; loss: 0.78; acc: 0.73
Batch: 520; loss: 0.68; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.67; acc: 0.88
Batch: 580; loss: 0.82; acc: 0.72
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.83; acc: 0.72
Batch: 640; loss: 0.73; acc: 0.81
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.72; acc: 0.77
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.67; acc: 0.83
Batch: 760; loss: 0.54; acc: 0.89
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.72; train_accuracy: 0.8 

Batch: 0; loss: 0.65; acc: 0.92
Batch: 20; loss: 0.76; acc: 0.77
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.89
Val Epoch over. val_loss: 0.6266995026807117; val_accuracy: 0.8228503184713376 

Epoch 10 start
The current lr is: 0.0008
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.79; acc: 0.73
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.58; acc: 0.8
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.79; acc: 0.8
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.74; acc: 0.78
Batch: 460; loss: 0.71; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.89
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.7; acc: 0.8
Batch: 600; loss: 0.69; acc: 0.81
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.76; acc: 0.78
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.64; acc: 0.8
Batch: 720; loss: 0.79; acc: 0.75
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.61; train_accuracy: 0.83 

Batch: 0; loss: 0.58; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.53; acc: 0.88
Batch: 60; loss: 0.54; acc: 0.78
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.97
Val Epoch over. val_loss: 0.5286103215566866; val_accuracy: 0.8490246815286624 

Epoch 11 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.7; acc: 0.78
Batch: 60; loss: 0.51; acc: 0.81
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.95
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.65; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.91
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.59; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.89
Batch: 460; loss: 0.84; acc: 0.67
Batch: 480; loss: 0.86; acc: 0.64
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.6; acc: 0.75
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.78; acc: 0.77
Batch: 600; loss: 0.56; acc: 0.78
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.81
Batch: 680; loss: 0.5; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.85 

Batch: 0; loss: 0.52; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.23; acc: 0.97
Val Epoch over. val_loss: 0.4741862460875967; val_accuracy: 0.8618630573248408 

Epoch 12 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.64; acc: 0.77
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.91
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.78
Batch: 380; loss: 0.58; acc: 0.81
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.47; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.91
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.67; acc: 0.77
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.48; train_accuracy: 0.86 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.42503114414822524; val_accuracy: 0.8765923566878981 

Epoch 13 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.65; acc: 0.78
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.81
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.6; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3872913295865818; val_accuracy: 0.8873407643312102 

Epoch 14 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.8
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.8
Batch: 340; loss: 0.56; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.33; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.6; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3563744933077484; val_accuracy: 0.8974920382165605 

Epoch 15 start
The current lr is: 0.0006400000000000002
Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.81
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.97
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.32; acc: 0.84
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3400703295590771; val_accuracy: 0.8975915605095541 

Epoch 16 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.8
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.88
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.307944489465018; val_accuracy: 0.9116242038216561 

Epoch 17 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.94
Batch: 160; loss: 0.48; acc: 0.89
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.3; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.83
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.6; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.21; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2917170252674704; val_accuracy: 0.9140127388535032 

Epoch 18 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.83
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.86
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.98
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.27718669071698643; val_accuracy: 0.9178941082802548 

Epoch 19 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.8
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.97
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.81
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2638487241165653; val_accuracy: 0.9246616242038217 

Epoch 20 start
The current lr is: 0.0005120000000000001
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.15; acc: 0.98
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.54; acc: 0.83
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.78
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.25302155330112786; val_accuracy: 0.9278463375796179 

Epoch 21 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.97
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.31; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2501096249480916; val_accuracy: 0.9266520700636943 

Epoch 22 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.39; acc: 0.84
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.84
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.84
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23626717770839953; val_accuracy: 0.9311305732484076 

Epoch 23 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.91
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23082426408673548; val_accuracy: 0.932921974522293 

Epoch 24 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.2; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2231990772590136; val_accuracy: 0.9352109872611465 

Epoch 25 start
The current lr is: 0.0004096000000000001
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.86
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2188575114271823; val_accuracy: 0.935609076433121 

Epoch 26 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2114117099031521; val_accuracy: 0.9384952229299363 

Epoch 27 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2071427441184308; val_accuracy: 0.939390923566879 

Epoch 28 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.98
Batch: 240; loss: 0.24; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.86
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.20391302645965748; val_accuracy: 0.9399880573248408 

Epoch 29 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.81
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.1; acc: 1.0
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.8
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.20241868571870647; val_accuracy: 0.9416799363057324 

Epoch 30 start
The current lr is: 0.0003276800000000001
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.1; acc: 0.98
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19837366977030305; val_accuracy: 0.9414808917197452 

Epoch 31 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.78
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19603387700619213; val_accuracy: 0.942078025477707 

Epoch 32 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.09; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.8
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.19207034414266325; val_accuracy: 0.9433718152866242 

Epoch 33 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.13; acc: 0.97
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18933009019323216; val_accuracy: 0.9453622611464968 

Epoch 34 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.08; acc: 1.0
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.08; acc: 1.0
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18719908875075114; val_accuracy: 0.9446656050955414 

Epoch 35 start
The current lr is: 0.0002621440000000001
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.08; acc: 1.0
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.09; acc: 1.0
Batch: 260; loss: 0.09; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1843101671736711; val_accuracy: 0.9456608280254777 

Epoch 36 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.89
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1818850015521429; val_accuracy: 0.9451632165605095 

Epoch 37 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.54; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.18066069255968567; val_accuracy: 0.946656050955414 

Epoch 38 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.23; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.81
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17912464154658803; val_accuracy: 0.9479498407643312 

Epoch 39 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.08; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.11; acc: 1.0
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.91
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17737715522859507; val_accuracy: 0.9481488853503185 

Epoch 40 start
The current lr is: 0.0002097152000000001
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.09; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.17; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.19; acc: 0.98
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.09; acc: 1.0
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.91
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.174969392383744; val_accuracy: 0.9486464968152867 

Epoch 41 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.22; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.08; acc: 1.0
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.81
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17478241396557753; val_accuracy: 0.9488455414012739 

Epoch 42 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.35; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1723188571869188; val_accuracy: 0.9489450636942676 

Epoch 43 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.06; acc: 0.98
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.08; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.1; acc: 1.0
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.08; acc: 1.0
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.14; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17189680799177498; val_accuracy: 0.9494426751592356 

Epoch 44 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.15; acc: 0.92
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17076824553263414; val_accuracy: 0.9494426751592356 

Epoch 45 start
The current lr is: 0.0001677721600000001
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.08; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1717019374156074; val_accuracy: 0.948546974522293 

Epoch 46 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16807932603605993; val_accuracy: 0.9498407643312102 

Epoch 47 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.08; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.13; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.98
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.92
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16754556309645344; val_accuracy: 0.9511345541401274 

Epoch 48 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.32; acc: 0.86
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.11; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16688709836571838; val_accuracy: 0.9507364649681529 

Epoch 49 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.33; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.09; acc: 0.98
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.13; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.98
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.16; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.88
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16655443060644873; val_accuracy: 0.9502388535031847 

Epoch 50 start
The current lr is: 0.00013421772800000008
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.95
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.12; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.1; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.14; acc: 0.92
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1647163895047774; val_accuracy: 0.9510350318471338 

plots/no_subspace_training/reg_lenet/2020-01-19 03:03:08/d_dim_1000_lr_0.001_gamma_0.8_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161154154759304; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.0288657481503334; val_accuracy: 0.3690286624203822 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.38
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.42
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.750862047930432; val_accuracy: 0.4570063694267516 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5094966205062381; val_accuracy: 0.5688694267515924 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878659545995628; val_accuracy: 0.6332603503184714 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.052367924884626; val_accuracy: 0.6973527070063694 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.8261827678437446; val_accuracy: 0.7791600318471338 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6589727952222156; val_accuracy: 0.8038415605095541 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5360683024309243; val_accuracy: 0.8444466560509554 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.443552932636753; val_accuracy: 0.8715167197452229 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.72; acc: 0.72
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3872021956808248; val_accuracy: 0.8838574840764332 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3392798119954243; val_accuracy: 0.9021695859872612 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.3062840714860874; val_accuracy: 0.9116242038216561 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.98
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2793921543059835; val_accuracy: 0.9200835987261147 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.2711428675776834; val_accuracy: 0.9177945859872612 

Epoch 16 start
The current lr is: 0.0004
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24085057975285373; val_accuracy: 0.9315286624203821 

Epoch 17 start
The current lr is: 0.0004
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.98
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23326507872741692; val_accuracy: 0.9326234076433121 

Epoch 18 start
The current lr is: 0.0004
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.19; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.95
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22633821564685008; val_accuracy: 0.9338176751592356 

Epoch 19 start
The current lr is: 0.0004
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.81
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.98
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.89
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.22095943311122573; val_accuracy: 0.9359076433121019 

Epoch 20 start
The current lr is: 0.0004
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.95
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.13; acc: 1.0
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.8
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21499851747018517; val_accuracy: 0.9370023885350318 

Epoch 21 start
The current lr is: 0.0004
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.98
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.1; acc: 1.0
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.98
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21459224549638237; val_accuracy: 0.9375995222929936 

Epoch 22 start
The current lr is: 0.0004
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.86
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.98
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.78
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.20490570786367557; val_accuracy: 0.940187101910828 

Epoch 23 start
The current lr is: 0.0004
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.97
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.97
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.89
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.78
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2017565167206488; val_accuracy: 0.9405851910828026 

Epoch 24 start
The current lr is: 0.0004
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.07; acc: 1.0
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.09; acc: 0.98
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.78
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19535091021068537; val_accuracy: 0.9426751592356688 

Epoch 25 start
The current lr is: 0.0004
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.07; acc: 0.97
Batch: 660; loss: 0.13; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.95
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19300809808692354; val_accuracy: 0.9430732484076433 

Epoch 26 start
The current lr is: 0.0004
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.05; acc: 1.0
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.2; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.94
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18690035482690592; val_accuracy: 0.9448646496815286 

Epoch 27 start
The current lr is: 0.0004
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.98
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.08; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.91
Batch: 460; loss: 0.1; acc: 0.98
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18373708443561937; val_accuracy: 0.9462579617834395 

Epoch 28 start
The current lr is: 0.0004
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.13; acc: 0.98
Batch: 240; loss: 0.21; acc: 0.89
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.98
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.81
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.18127425805113878; val_accuracy: 0.9448646496815286 

Epoch 29 start
The current lr is: 0.0004
Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.86
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.05; acc: 1.0
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.81
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.13; acc: 0.98
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.07; acc: 0.98
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.11; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.07; acc: 1.0
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.18074243608268964; val_accuracy: 0.9471536624203821 

Epoch 30 start
The current lr is: 0.0004
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.98
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17638331886594463; val_accuracy: 0.9486464968152867 

Epoch 31 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.19; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17376662733828185; val_accuracy: 0.9488455414012739 

Epoch 32 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.1; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.17; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.14; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17224442674096224; val_accuracy: 0.9490445859872612 

Epoch 33 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.84
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.12; acc: 0.97
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.28; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.15; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.98
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17084970408279426; val_accuracy: 0.9495421974522293 

Epoch 34 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.06; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.97
Batch: 240; loss: 0.14; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.17; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.94
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.06; acc: 1.0
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.15; acc: 0.95
Batch: 680; loss: 0.07; acc: 1.0
Batch: 700; loss: 0.17; acc: 0.98
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.17015446236084222; val_accuracy: 0.9491441082802548 

Epoch 35 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.1; acc: 0.98
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.11; acc: 0.97
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.94
Batch: 240; loss: 0.07; acc: 1.0
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1686726999083522; val_accuracy: 0.9502388535031847 

Epoch 36 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.08; acc: 1.0
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.04; acc: 1.0
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.98
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.13; acc: 0.97
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16698293485174512; val_accuracy: 0.9492436305732485 

Epoch 37 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.09; acc: 0.98
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.51; acc: 0.88
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16672988015280407; val_accuracy: 0.9508359872611465 

Epoch 38 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.97
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.46; acc: 0.92
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1659271589889648; val_accuracy: 0.9514331210191083 

Epoch 39 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.11; acc: 0.98
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.09; acc: 1.0
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1647107612555194; val_accuracy: 0.9514331210191083 

Epoch 40 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.08; acc: 1.0
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.98
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.07; acc: 1.0
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.94 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16325305701251241; val_accuracy: 0.951234076433121 

Epoch 41 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.09; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1633465405150204; val_accuracy: 0.9517316878980892 

Epoch 42 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.11; acc: 0.98
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.161152363701421; val_accuracy: 0.9514331210191083 

Epoch 43 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.05; acc: 0.98
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.07; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.07; acc: 0.97
Batch: 220; loss: 0.09; acc: 1.0
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.08; acc: 1.0
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.09; acc: 0.98
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16086557638018753; val_accuracy: 0.9525278662420382 

Epoch 44 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.89
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.13; acc: 0.92
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.13; acc: 0.94
Batch: 720; loss: 0.08; acc: 0.98
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.1; acc: 0.98
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16009530088134633; val_accuracy: 0.9527269108280255 

Epoch 45 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.07; acc: 0.97
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.98
Batch: 120; loss: 0.06; acc: 1.0
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.09; acc: 0.97
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.14; acc: 0.92
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16127042918448237; val_accuracy: 0.9514331210191083 

Epoch 46 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.95
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.29; acc: 0.94
Batch: 420; loss: 0.07; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.12; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.158255532882206; val_accuracy: 0.9527269108280255 

Epoch 47 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.94
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.06; acc: 1.0
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.13; acc: 0.92
Batch: 660; loss: 0.1; acc: 1.0
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15798788441784062; val_accuracy: 0.9533240445859873 

Epoch 48 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.86
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.08; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.1; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.08; acc: 1.0
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15802903513714767; val_accuracy: 0.9534235668789809 

Epoch 49 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.2; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.07; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.17; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.1; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.91
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15756145079329514; val_accuracy: 0.9529259554140127 

Epoch 50 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.97
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.1; acc: 0.98
Batch: 340; loss: 0.24; acc: 0.97
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.12; acc: 1.0
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.13; acc: 0.92
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.07; acc: 0.97
Train Epoch over. train_loss: 0.18; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15686321282272886; val_accuracy: 0.9534235668789809 

plots/no_subspace_training/reg_lenet/2020-01-19 03:12:29/d_dim_1000_lr_0.001_gamma_0.4_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.21611541699452; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.02886175957455; val_accuracy: 0.3688296178343949 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.39
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.31
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508665907914471; val_accuracy: 0.4570063694267516 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.51; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5095118314597258; val_accuracy: 0.568968949044586 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.28787219752172; val_accuracy: 0.6331608280254777 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.0523636443599773; val_accuracy: 0.6972531847133758 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.83
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.8261565668567731; val_accuracy: 0.779359076433121 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6589314628178906; val_accuracy: 0.8039410828025477 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5360149913912367; val_accuracy: 0.8442476114649682 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.44348041770184876; val_accuracy: 0.8714171974522293 

Epoch 11 start
The current lr is: 0.0004
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.62; acc: 0.78
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.43; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.75; acc: 0.72
Batch: 480; loss: 0.77; acc: 0.69
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.52; acc: 0.78
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.71; acc: 0.78
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.41595988326771244; val_accuracy: 0.8778861464968153 

Epoch 12 start
The current lr is: 0.0004
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.38; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.58; acc: 0.78
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.89
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.63; acc: 0.77
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.39112552440470194; val_accuracy: 0.8871417197452229 

Epoch 13 start
The current lr is: 0.0004
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.62; acc: 0.78
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.95
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.5; acc: 0.8
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.52; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.29; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36929841406026465; val_accuracy: 0.8926154458598726 

Epoch 14 start
The current lr is: 0.0004
Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.41; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.81
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.89
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.35133497056308066; val_accuracy: 0.8994824840764332 

Epoch 15 start
The current lr is: 0.0004
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.81
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.49; acc: 0.81
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.94
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.95
Batch: 460; loss: 0.33; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.3391642220270861; val_accuracy: 0.9001791401273885 

Epoch 16 start
The current lr is: 0.0004
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3177843898250039; val_accuracy: 0.9080414012738853 

Epoch 17 start
The current lr is: 0.0004
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.81
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.83
Batch: 340; loss: 0.27; acc: 0.95
Batch: 360; loss: 0.44; acc: 0.83
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.86
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.3042929075231218; val_accuracy: 0.9096337579617835 

Epoch 18 start
The current lr is: 0.0004
Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.81
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.97
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.98
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.2912250359062177; val_accuracy: 0.9144108280254777 

Epoch 19 start
The current lr is: 0.0004
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.8
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.95
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.49; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.81
Batch: 740; loss: 0.16; acc: 0.98
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.59; acc: 0.77
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.2798484447085933; val_accuracy: 0.9187898089171974 

Epoch 20 start
The current lr is: 0.0004
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.81
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.98
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.15; acc: 0.95
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.83
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.98
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.94
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.2689063689511293; val_accuracy: 0.9223726114649682 

Epoch 21 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.98
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.97
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.26480603975000655; val_accuracy: 0.9243630573248408 

Epoch 22 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.83
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.2603674830429873; val_accuracy: 0.9251592356687898 

Epoch 23 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.83
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.25672919125219057; val_accuracy: 0.926453025477707 

Epoch 24 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.97
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.97
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.25278828634767775; val_accuracy: 0.9282444267515924 

Epoch 25 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.91
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.1; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.28; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.78
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2495178515269498; val_accuracy: 0.928343949044586 

Epoch 26 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.95
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.83
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24639446416478247; val_accuracy: 0.9300358280254777 

Epoch 27 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.15; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24268496667693376; val_accuracy: 0.9304339171974523 

Epoch 28 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.86
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.83
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.09; acc: 1.0
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.15; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24009292492061662; val_accuracy: 0.9303343949044586 

Epoch 29 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.08; acc: 1.0
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.77
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.14; acc: 1.0
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2379501311072878; val_accuracy: 0.9315286624203821 

Epoch 30 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.83
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.12; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23550365801165057; val_accuracy: 0.9325238853503185 

Epoch 31 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.84
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23331902579517122; val_accuracy: 0.9326234076433121 

Epoch 32 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2321286618851932; val_accuracy: 0.9326234076433121 

Epoch 33 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.94
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.84
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2311163182091561; val_accuracy: 0.932921974522293 

Epoch 34 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.37; acc: 0.84
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.97
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22999412018307455; val_accuracy: 0.9330214968152867 

Epoch 35 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22914482840591935; val_accuracy: 0.9333200636942676 

Epoch 36 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.83
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22752627331739778; val_accuracy: 0.9340167197452229 

Epoch 37 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.95
Batch: 280; loss: 0.15; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.53; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.59; acc: 0.86
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22700641494078241; val_accuracy: 0.9345143312101911 

Epoch 38 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.17; acc: 0.97
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.55; acc: 0.88
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.18; acc: 0.97
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22596022745321512; val_accuracy: 0.9345143312101911 

Epoch 39 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.22; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.84
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22472817562283223; val_accuracy: 0.9347133757961783 

Epoch 40 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.98
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.86
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2237504379005189; val_accuracy: 0.9351114649681529 

Epoch 41 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.1; acc: 1.0
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2233350076683008; val_accuracy: 0.9349124203821656 

Epoch 42 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.98
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2228266925663705; val_accuracy: 0.9350119426751592 

Epoch 43 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.81
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.09; acc: 1.0
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.18; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.89
Batch: 560; loss: 0.22; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22243386884308924; val_accuracy: 0.9353105095541401 

Epoch 44 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.84
Batch: 700; loss: 0.19; acc: 0.91
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.86
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22199294016144838; val_accuracy: 0.9353105095541401 

Epoch 45 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.3; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2216997288499668; val_accuracy: 0.9355095541401274 

Epoch 46 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.13; acc: 0.98
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2213969328411066; val_accuracy: 0.9357085987261147 

Epoch 47 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.43; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.98
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.22087112413185417; val_accuracy: 0.9361066878980892 

Epoch 48 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.16; acc: 0.98
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2206801645173009; val_accuracy: 0.9354100318471338 

Epoch 49 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.28; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.81
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.39; acc: 0.84
Batch: 620; loss: 0.18; acc: 0.98
Batch: 640; loss: 0.28; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.22023366303865316; val_accuracy: 0.9359076433121019 

Epoch 50 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21977144297046267; val_accuracy: 0.9360071656050956 

plots/no_subspace_training/reg_lenet/2020-01-19 03:21:44/d_dim_1000_lr_0.001_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.216115740454121; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.02887124544496; val_accuracy: 0.3690286624203822 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.38
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.42
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508698010900219; val_accuracy: 0.45710589171974525 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.38
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5095111032959763; val_accuracy: 0.5687699044585988 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.66
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878978305561528; val_accuracy: 0.633359872611465 

Epoch 6 start
The current lr is: 0.0004
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.64
Batch: 40; loss: 1.22; acc: 0.69
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.21; acc: 0.66
Batch: 100; loss: 1.46; acc: 0.52
Batch: 120; loss: 1.26; acc: 0.67
Batch: 140; loss: 1.41; acc: 0.58
Batch: 160; loss: 1.35; acc: 0.61
Batch: 180; loss: 1.3; acc: 0.67
Batch: 200; loss: 1.29; acc: 0.64
Batch: 220; loss: 1.26; acc: 0.61
Batch: 240; loss: 1.24; acc: 0.64
Batch: 260; loss: 1.26; acc: 0.59
Batch: 280; loss: 1.23; acc: 0.58
Batch: 300; loss: 1.25; acc: 0.7
Batch: 320; loss: 1.19; acc: 0.7
Batch: 340; loss: 1.35; acc: 0.58
Batch: 360; loss: 1.31; acc: 0.53
Batch: 380; loss: 1.28; acc: 0.61
Batch: 400; loss: 1.31; acc: 0.66
Batch: 420; loss: 1.14; acc: 0.75
Batch: 440; loss: 1.22; acc: 0.64
Batch: 460; loss: 1.13; acc: 0.78
Batch: 480; loss: 1.23; acc: 0.73
Batch: 500; loss: 1.44; acc: 0.58
Batch: 520; loss: 1.28; acc: 0.67
Batch: 540; loss: 1.18; acc: 0.7
Batch: 560; loss: 1.36; acc: 0.61
Batch: 580; loss: 1.32; acc: 0.61
Batch: 600; loss: 1.15; acc: 0.7
Batch: 620; loss: 1.14; acc: 0.7
Batch: 640; loss: 1.18; acc: 0.64
Batch: 660; loss: 1.3; acc: 0.59
Batch: 680; loss: 1.19; acc: 0.64
Batch: 700; loss: 1.2; acc: 0.59
Batch: 720; loss: 1.15; acc: 0.7
Batch: 740; loss: 1.27; acc: 0.7
Batch: 760; loss: 1.11; acc: 0.69
Batch: 780; loss: 1.29; acc: 0.55
Train Epoch over. train_loss: 1.26; train_accuracy: 0.64 

Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 0.99; acc: 0.7
Batch: 60; loss: 1.09; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 1.15; acc: 0.67
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.190181009708696; val_accuracy: 0.6671974522292994 

Epoch 7 start
The current lr is: 0.0004
Batch: 0; loss: 1.11; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.73
Batch: 40; loss: 1.39; acc: 0.61
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.26; acc: 0.75
Batch: 100; loss: 1.28; acc: 0.64
Batch: 120; loss: 1.16; acc: 0.7
Batch: 140; loss: 1.27; acc: 0.7
Batch: 160; loss: 1.16; acc: 0.7
Batch: 180; loss: 1.11; acc: 0.72
Batch: 200; loss: 1.17; acc: 0.64
Batch: 220; loss: 1.44; acc: 0.53
Batch: 240; loss: 1.16; acc: 0.72
Batch: 260; loss: 1.2; acc: 0.61
Batch: 280; loss: 1.09; acc: 0.77
Batch: 300; loss: 1.2; acc: 0.69
Batch: 320; loss: 1.26; acc: 0.58
Batch: 340; loss: 1.47; acc: 0.59
Batch: 360; loss: 1.27; acc: 0.64
Batch: 380; loss: 1.19; acc: 0.72
Batch: 400; loss: 1.1; acc: 0.67
Batch: 420; loss: 1.18; acc: 0.62
Batch: 440; loss: 1.16; acc: 0.7
Batch: 460; loss: 1.09; acc: 0.72
Batch: 480; loss: 1.26; acc: 0.58
Batch: 500; loss: 1.18; acc: 0.69
Batch: 520; loss: 1.32; acc: 0.62
Batch: 540; loss: 1.3; acc: 0.67
Batch: 560; loss: 1.2; acc: 0.55
Batch: 580; loss: 1.31; acc: 0.66
Batch: 600; loss: 1.19; acc: 0.73
Batch: 620; loss: 1.22; acc: 0.61
Batch: 640; loss: 1.13; acc: 0.67
Batch: 660; loss: 1.14; acc: 0.64
Batch: 680; loss: 1.05; acc: 0.73
Batch: 700; loss: 1.06; acc: 0.75
Batch: 720; loss: 1.14; acc: 0.66
Batch: 740; loss: 1.17; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.66
Batch: 780; loss: 1.06; acc: 0.66
Train Epoch over. train_loss: 1.17; train_accuracy: 0.67 

Batch: 0; loss: 1.1; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.58
Batch: 40; loss: 0.91; acc: 0.75
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.07; acc: 0.77
Batch: 120; loss: 1.18; acc: 0.72
Batch: 140; loss: 0.89; acc: 0.83
Val Epoch over. val_loss: 1.0942064747688875; val_accuracy: 0.7083001592356688 

Epoch 8 start
The current lr is: 0.0004
Batch: 0; loss: 0.94; acc: 0.75
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 1.05; acc: 0.73
Batch: 60; loss: 1.08; acc: 0.66
Batch: 80; loss: 1.29; acc: 0.58
Batch: 100; loss: 1.2; acc: 0.59
Batch: 120; loss: 0.96; acc: 0.8
Batch: 140; loss: 1.02; acc: 0.75
Batch: 160; loss: 1.14; acc: 0.66
Batch: 180; loss: 1.19; acc: 0.62
Batch: 200; loss: 1.08; acc: 0.69
Batch: 220; loss: 1.19; acc: 0.66
Batch: 240; loss: 1.08; acc: 0.64
Batch: 260; loss: 1.13; acc: 0.7
Batch: 280; loss: 0.9; acc: 0.72
Batch: 300; loss: 1.17; acc: 0.66
Batch: 320; loss: 1.07; acc: 0.72
Batch: 340; loss: 1.15; acc: 0.61
Batch: 360; loss: 0.99; acc: 0.78
Batch: 380; loss: 1.11; acc: 0.67
Batch: 400; loss: 1.2; acc: 0.64
Batch: 420; loss: 1.16; acc: 0.7
Batch: 440; loss: 1.03; acc: 0.69
Batch: 460; loss: 0.98; acc: 0.78
Batch: 480; loss: 0.93; acc: 0.83
Batch: 500; loss: 0.93; acc: 0.83
Batch: 520; loss: 1.0; acc: 0.73
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 0.99; acc: 0.7
Batch: 580; loss: 0.69; acc: 0.91
Batch: 600; loss: 1.21; acc: 0.61
Batch: 620; loss: 1.1; acc: 0.67
Batch: 640; loss: 1.12; acc: 0.69
Batch: 660; loss: 0.94; acc: 0.73
Batch: 680; loss: 1.15; acc: 0.61
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 0.84; acc: 0.8
Batch: 740; loss: 1.11; acc: 0.69
Batch: 760; loss: 1.07; acc: 0.73
Batch: 780; loss: 0.9; acc: 0.75
Train Epoch over. train_loss: 1.08; train_accuracy: 0.7 

Batch: 0; loss: 1.01; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.73
Batch: 60; loss: 0.92; acc: 0.73
Batch: 80; loss: 0.94; acc: 0.78
Batch: 100; loss: 0.97; acc: 0.73
Batch: 120; loss: 1.09; acc: 0.77
Batch: 140; loss: 0.79; acc: 0.83
Val Epoch over. val_loss: 1.0002724008195718; val_accuracy: 0.7273089171974523 

Epoch 9 start
The current lr is: 0.0004
Batch: 0; loss: 1.02; acc: 0.8
Batch: 20; loss: 0.9; acc: 0.75
Batch: 40; loss: 1.12; acc: 0.73
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 0.95; acc: 0.7
Batch: 100; loss: 1.07; acc: 0.66
Batch: 120; loss: 0.96; acc: 0.66
Batch: 140; loss: 1.03; acc: 0.72
Batch: 160; loss: 0.89; acc: 0.81
Batch: 180; loss: 0.78; acc: 0.83
Batch: 200; loss: 0.99; acc: 0.72
Batch: 220; loss: 0.99; acc: 0.7
Batch: 240; loss: 1.13; acc: 0.61
Batch: 260; loss: 0.99; acc: 0.69
Batch: 280; loss: 0.95; acc: 0.72
Batch: 300; loss: 1.01; acc: 0.7
Batch: 320; loss: 0.94; acc: 0.73
Batch: 340; loss: 0.88; acc: 0.77
Batch: 360; loss: 0.89; acc: 0.73
Batch: 380; loss: 1.1; acc: 0.72
Batch: 400; loss: 0.92; acc: 0.83
Batch: 420; loss: 1.1; acc: 0.61
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 0.89; acc: 0.7
Batch: 480; loss: 0.97; acc: 0.73
Batch: 500; loss: 1.07; acc: 0.69
Batch: 520; loss: 0.98; acc: 0.59
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.92; acc: 0.83
Batch: 580; loss: 1.08; acc: 0.67
Batch: 600; loss: 0.89; acc: 0.8
Batch: 620; loss: 1.12; acc: 0.64
Batch: 640; loss: 1.03; acc: 0.7
Batch: 660; loss: 0.92; acc: 0.72
Batch: 680; loss: 0.97; acc: 0.75
Batch: 700; loss: 0.95; acc: 0.72
Batch: 720; loss: 0.95; acc: 0.7
Batch: 740; loss: 0.89; acc: 0.75
Batch: 760; loss: 0.8; acc: 0.81
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.99; train_accuracy: 0.73 

Batch: 0; loss: 0.92; acc: 0.81
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 0.77; acc: 0.8
Batch: 60; loss: 0.83; acc: 0.77
Batch: 80; loss: 0.83; acc: 0.84
Batch: 100; loss: 0.89; acc: 0.77
Batch: 120; loss: 1.01; acc: 0.75
Batch: 140; loss: 0.69; acc: 0.86
Val Epoch over. val_loss: 0.9078673268579374; val_accuracy: 0.7612460191082803 

Epoch 10 start
The current lr is: 0.0004
Batch: 0; loss: 0.85; acc: 0.78
Batch: 20; loss: 0.9; acc: 0.8
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 0.86; acc: 0.78
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.82; acc: 0.73
Batch: 160; loss: 1.16; acc: 0.58
Batch: 180; loss: 0.98; acc: 0.64
Batch: 200; loss: 0.9; acc: 0.8
Batch: 220; loss: 0.89; acc: 0.72
Batch: 240; loss: 0.96; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.8
Batch: 280; loss: 1.06; acc: 0.64
Batch: 300; loss: 0.88; acc: 0.8
Batch: 320; loss: 0.88; acc: 0.73
Batch: 340; loss: 0.83; acc: 0.8
Batch: 360; loss: 0.83; acc: 0.75
Batch: 380; loss: 0.9; acc: 0.78
Batch: 400; loss: 1.18; acc: 0.69
Batch: 420; loss: 0.93; acc: 0.69
Batch: 440; loss: 1.04; acc: 0.72
Batch: 460; loss: 1.01; acc: 0.81
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.99; acc: 0.73
Batch: 520; loss: 0.84; acc: 0.77
Batch: 540; loss: 0.82; acc: 0.78
Batch: 560; loss: 0.95; acc: 0.73
Batch: 580; loss: 1.05; acc: 0.7
Batch: 600; loss: 1.0; acc: 0.73
Batch: 620; loss: 0.9; acc: 0.7
Batch: 640; loss: 1.0; acc: 0.7
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.81; acc: 0.83
Batch: 700; loss: 0.93; acc: 0.72
Batch: 720; loss: 1.03; acc: 0.69
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 1.03; acc: 0.69
Batch: 780; loss: 0.75; acc: 0.81
Train Epoch over. train_loss: 0.9; train_accuracy: 0.75 

Batch: 0; loss: 0.87; acc: 0.81
Batch: 20; loss: 0.97; acc: 0.64
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.77; acc: 0.83
Batch: 100; loss: 0.82; acc: 0.81
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 0.59; acc: 0.89
Val Epoch over. val_loss: 0.8236947408906973; val_accuracy: 0.7777667197452229 

Epoch 11 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.75; acc: 0.78
Batch: 20; loss: 0.84; acc: 0.75
Batch: 40; loss: 0.95; acc: 0.67
Batch: 60; loss: 0.81; acc: 0.7
Batch: 80; loss: 1.14; acc: 0.61
Batch: 100; loss: 0.81; acc: 0.75
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.79; acc: 0.86
Batch: 160; loss: 0.94; acc: 0.73
Batch: 180; loss: 0.9; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.84
Batch: 220; loss: 0.85; acc: 0.73
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.77
Batch: 280; loss: 0.83; acc: 0.8
Batch: 300; loss: 1.02; acc: 0.7
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 1.04; acc: 0.69
Batch: 360; loss: 0.9; acc: 0.72
Batch: 380; loss: 0.82; acc: 0.75
Batch: 400; loss: 0.85; acc: 0.69
Batch: 420; loss: 0.87; acc: 0.75
Batch: 440; loss: 0.89; acc: 0.75
Batch: 460; loss: 1.2; acc: 0.58
Batch: 480; loss: 1.24; acc: 0.53
Batch: 500; loss: 0.71; acc: 0.86
Batch: 520; loss: 0.94; acc: 0.7
Batch: 540; loss: 0.88; acc: 0.77
Batch: 560; loss: 0.77; acc: 0.81
Batch: 580; loss: 1.05; acc: 0.67
Batch: 600; loss: 0.9; acc: 0.7
Batch: 620; loss: 0.74; acc: 0.84
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.8; acc: 0.83
Batch: 680; loss: 0.88; acc: 0.73
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.78; acc: 0.8
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.86; acc: 0.78
Batch: 780; loss: 0.67; acc: 0.91
Train Epoch over. train_loss: 0.84; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.81
Batch: 20; loss: 0.93; acc: 0.66
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.84
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.55; acc: 0.88
Val Epoch over. val_loss: 0.7901667572890118; val_accuracy: 0.7879179936305732 

Epoch 12 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.73; acc: 0.81
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.78
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.95; acc: 0.69
Batch: 100; loss: 0.68; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.87; acc: 0.72
Batch: 160; loss: 0.86; acc: 0.78
Batch: 180; loss: 0.82; acc: 0.75
Batch: 200; loss: 0.79; acc: 0.77
Batch: 220; loss: 0.86; acc: 0.73
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.79; acc: 0.81
Batch: 280; loss: 0.86; acc: 0.77
Batch: 300; loss: 0.91; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.93; acc: 0.75
Batch: 360; loss: 0.93; acc: 0.69
Batch: 380; loss: 0.87; acc: 0.75
Batch: 400; loss: 0.8; acc: 0.77
Batch: 420; loss: 0.9; acc: 0.72
Batch: 440; loss: 0.85; acc: 0.73
Batch: 460; loss: 0.76; acc: 0.83
Batch: 480; loss: 0.82; acc: 0.75
Batch: 500; loss: 0.96; acc: 0.73
Batch: 520; loss: 0.63; acc: 0.84
Batch: 540; loss: 0.74; acc: 0.75
Batch: 560; loss: 0.71; acc: 0.8
Batch: 580; loss: 0.88; acc: 0.81
Batch: 600; loss: 0.9; acc: 0.77
Batch: 620; loss: 0.78; acc: 0.81
Batch: 640; loss: 0.83; acc: 0.75
Batch: 660; loss: 0.82; acc: 0.75
Batch: 680; loss: 0.83; acc: 0.78
Batch: 700; loss: 0.87; acc: 0.8
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 1.0; acc: 0.7
Batch: 780; loss: 0.82; acc: 0.77
Train Epoch over. train_loss: 0.81; train_accuracy: 0.78 

Batch: 0; loss: 0.8; acc: 0.84
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.81
Batch: 80; loss: 0.7; acc: 0.88
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.92; acc: 0.73
Batch: 140; loss: 0.52; acc: 0.88
Val Epoch over. val_loss: 0.7595222072236857; val_accuracy: 0.7928941082802548 

Epoch 13 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.78; acc: 0.78
Batch: 20; loss: 0.9; acc: 0.77
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 0.76; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.8
Batch: 100; loss: 0.79; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 0.92; acc: 0.73
Batch: 180; loss: 0.79; acc: 0.8
Batch: 200; loss: 0.93; acc: 0.78
Batch: 220; loss: 0.76; acc: 0.83
Batch: 240; loss: 0.84; acc: 0.78
Batch: 260; loss: 0.67; acc: 0.75
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.73; acc: 0.83
Batch: 360; loss: 0.89; acc: 0.78
Batch: 380; loss: 0.65; acc: 0.88
Batch: 400; loss: 0.78; acc: 0.78
Batch: 420; loss: 0.9; acc: 0.78
Batch: 440; loss: 0.75; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.68; acc: 0.81
Batch: 520; loss: 0.74; acc: 0.78
Batch: 540; loss: 0.84; acc: 0.7
Batch: 560; loss: 0.87; acc: 0.78
Batch: 580; loss: 0.8; acc: 0.72
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 0.86; acc: 0.77
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.77
Batch: 680; loss: 1.02; acc: 0.75
Batch: 700; loss: 0.7; acc: 0.84
Batch: 720; loss: 0.7; acc: 0.8
Batch: 740; loss: 0.66; acc: 0.86
Batch: 760; loss: 0.68; acc: 0.8
Batch: 780; loss: 0.8; acc: 0.75
Train Epoch over. train_loss: 0.78; train_accuracy: 0.78 

Batch: 0; loss: 0.76; acc: 0.86
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.65; acc: 0.88
Batch: 100; loss: 0.74; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.91
Val Epoch over. val_loss: 0.7293736249398274; val_accuracy: 0.8049363057324841 

Epoch 14 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.78; acc: 0.86
Batch: 20; loss: 0.79; acc: 0.77
Batch: 40; loss: 0.68; acc: 0.88
Batch: 60; loss: 1.12; acc: 0.62
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.77; acc: 0.8
Batch: 120; loss: 0.78; acc: 0.83
Batch: 140; loss: 0.66; acc: 0.83
Batch: 160; loss: 0.76; acc: 0.78
Batch: 180; loss: 0.78; acc: 0.84
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.77; acc: 0.8
Batch: 240; loss: 0.83; acc: 0.69
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.85; acc: 0.77
Batch: 320; loss: 0.82; acc: 0.72
Batch: 340; loss: 0.95; acc: 0.81
Batch: 360; loss: 0.74; acc: 0.86
Batch: 380; loss: 0.68; acc: 0.83
Batch: 400; loss: 0.61; acc: 0.86
Batch: 420; loss: 0.76; acc: 0.81
Batch: 440; loss: 0.71; acc: 0.77
Batch: 460; loss: 0.75; acc: 0.84
Batch: 480; loss: 0.94; acc: 0.72
Batch: 500; loss: 0.51; acc: 0.91
Batch: 520; loss: 0.87; acc: 0.77
Batch: 540; loss: 0.84; acc: 0.69
Batch: 560; loss: 0.65; acc: 0.83
Batch: 580; loss: 0.83; acc: 0.7
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.77; acc: 0.8
Batch: 640; loss: 0.81; acc: 0.78
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.69; acc: 0.8
Batch: 700; loss: 0.89; acc: 0.78
Batch: 720; loss: 0.53; acc: 0.88
Batch: 740; loss: 0.66; acc: 0.83
Batch: 760; loss: 0.75; acc: 0.78
Batch: 780; loss: 0.76; acc: 0.77
Train Epoch over. train_loss: 0.75; train_accuracy: 0.79 

Batch: 0; loss: 0.74; acc: 0.84
Batch: 20; loss: 0.84; acc: 0.7
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.67; acc: 0.8
Batch: 80; loss: 0.64; acc: 0.88
Batch: 100; loss: 0.72; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.46; acc: 0.91
Val Epoch over. val_loss: 0.7025278861735277; val_accuracy: 0.8100119426751592 

Epoch 15 start
The current lr is: 0.00016000000000000004
Batch: 0; loss: 0.92; acc: 0.72
Batch: 20; loss: 1.04; acc: 0.7
Batch: 40; loss: 0.67; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.78; acc: 0.75
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.84
Batch: 140; loss: 0.78; acc: 0.77
Batch: 160; loss: 0.76; acc: 0.75
Batch: 180; loss: 0.72; acc: 0.77
Batch: 200; loss: 0.89; acc: 0.78
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.61; acc: 0.84
Batch: 260; loss: 0.82; acc: 0.73
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.77; acc: 0.75
Batch: 340; loss: 0.7; acc: 0.88
Batch: 360; loss: 0.68; acc: 0.86
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.84; acc: 0.75
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.86; acc: 0.69
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 0.84; acc: 0.78
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.83; acc: 0.72
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.86; acc: 0.8
Batch: 640; loss: 0.64; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.84
Batch: 680; loss: 0.69; acc: 0.81
Batch: 700; loss: 0.74; acc: 0.8
Batch: 720; loss: 0.84; acc: 0.81
Batch: 740; loss: 0.68; acc: 0.84
Batch: 760; loss: 0.74; acc: 0.78
Batch: 780; loss: 0.97; acc: 0.75
Train Epoch over. train_loss: 0.73; train_accuracy: 0.8 

Batch: 0; loss: 0.72; acc: 0.86
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.62; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.61; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.43; acc: 0.92
Val Epoch over. val_loss: 0.677353945127718; val_accuracy: 0.816281847133758 

Epoch 16 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.74; acc: 0.8
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.81; acc: 0.73
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.89; acc: 0.69
Batch: 140; loss: 0.65; acc: 0.83
Batch: 160; loss: 0.68; acc: 0.84
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.91
Batch: 220; loss: 0.55; acc: 0.91
Batch: 240; loss: 0.69; acc: 0.8
Batch: 260; loss: 0.71; acc: 0.81
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.66; acc: 0.91
Batch: 320; loss: 0.9; acc: 0.73
Batch: 340; loss: 0.73; acc: 0.75
Batch: 360; loss: 0.71; acc: 0.77
Batch: 380; loss: 0.74; acc: 0.84
Batch: 400; loss: 0.97; acc: 0.73
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.71; acc: 0.83
Batch: 480; loss: 0.88; acc: 0.73
Batch: 500; loss: 0.63; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.89
Batch: 540; loss: 0.68; acc: 0.8
Batch: 560; loss: 0.58; acc: 0.91
Batch: 580; loss: 0.81; acc: 0.77
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.84; acc: 0.8
Batch: 640; loss: 0.75; acc: 0.83
Batch: 660; loss: 0.68; acc: 0.78
Batch: 680; loss: 0.59; acc: 0.78
Batch: 700; loss: 0.76; acc: 0.78
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.75; acc: 0.78
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.67; acc: 0.86
Train Epoch over. train_loss: 0.71; train_accuracy: 0.8 

Batch: 0; loss: 0.71; acc: 0.84
Batch: 20; loss: 0.8; acc: 0.73
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.6; acc: 0.89
Batch: 100; loss: 0.69; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.92
Val Epoch over. val_loss: 0.6667141491060804; val_accuracy: 0.8164808917197452 

Epoch 17 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.78; acc: 0.86
Batch: 40; loss: 0.86; acc: 0.69
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.84
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 0.58; acc: 0.88
Batch: 140; loss: 0.78; acc: 0.73
Batch: 160; loss: 0.8; acc: 0.83
Batch: 180; loss: 0.82; acc: 0.73
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.96; acc: 0.75
Batch: 240; loss: 0.83; acc: 0.75
Batch: 260; loss: 0.85; acc: 0.8
Batch: 280; loss: 0.75; acc: 0.83
Batch: 300; loss: 0.75; acc: 0.77
Batch: 320; loss: 0.78; acc: 0.75
Batch: 340; loss: 0.63; acc: 0.83
Batch: 360; loss: 0.84; acc: 0.77
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.89; acc: 0.72
Batch: 440; loss: 0.62; acc: 0.89
Batch: 460; loss: 0.86; acc: 0.75
Batch: 480; loss: 0.67; acc: 0.78
Batch: 500; loss: 0.62; acc: 0.84
Batch: 520; loss: 0.84; acc: 0.8
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.87; acc: 0.73
Batch: 580; loss: 0.72; acc: 0.81
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.81; acc: 0.72
Batch: 640; loss: 0.73; acc: 0.78
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.67; acc: 0.86
Batch: 700; loss: 0.63; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.88
Batch: 760; loss: 0.74; acc: 0.77
Batch: 780; loss: 0.79; acc: 0.75
Train Epoch over. train_loss: 0.7; train_accuracy: 0.81 

Batch: 0; loss: 0.7; acc: 0.88
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.59; acc: 0.89
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.92
Val Epoch over. val_loss: 0.6566747647182197; val_accuracy: 0.8214570063694268 

Epoch 18 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.78
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.84; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.88
Batch: 140; loss: 0.6; acc: 0.88
Batch: 160; loss: 0.71; acc: 0.77
Batch: 180; loss: 0.74; acc: 0.75
Batch: 200; loss: 0.78; acc: 0.75
Batch: 220; loss: 0.72; acc: 0.81
Batch: 240; loss: 0.91; acc: 0.69
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.95; acc: 0.75
Batch: 300; loss: 0.68; acc: 0.73
Batch: 320; loss: 0.76; acc: 0.8
Batch: 340; loss: 0.63; acc: 0.8
Batch: 360; loss: 0.71; acc: 0.75
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.71; acc: 0.8
Batch: 420; loss: 0.73; acc: 0.83
Batch: 440; loss: 0.62; acc: 0.83
Batch: 460; loss: 0.63; acc: 0.84
Batch: 480; loss: 0.75; acc: 0.84
Batch: 500; loss: 0.53; acc: 0.94
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.51; acc: 0.88
Batch: 560; loss: 0.8; acc: 0.77
Batch: 580; loss: 0.8; acc: 0.8
Batch: 600; loss: 0.64; acc: 0.78
Batch: 620; loss: 0.77; acc: 0.73
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.81
Batch: 680; loss: 0.76; acc: 0.8
Batch: 700; loss: 0.79; acc: 0.8
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.79; acc: 0.72
Batch: 780; loss: 0.54; acc: 0.92
Train Epoch over. train_loss: 0.69; train_accuracy: 0.81 

Batch: 0; loss: 0.69; acc: 0.88
Batch: 20; loss: 0.77; acc: 0.73
Batch: 40; loss: 0.6; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.78
Batch: 80; loss: 0.58; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.4; acc: 0.92
Val Epoch over. val_loss: 0.647058267122621; val_accuracy: 0.8230493630573248 

Epoch 19 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.67; acc: 0.84
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.57; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.81
Batch: 140; loss: 0.74; acc: 0.81
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.76; acc: 0.83
Batch: 200; loss: 0.73; acc: 0.81
Batch: 220; loss: 0.61; acc: 0.8
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.74; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 0.76; acc: 0.83
Batch: 380; loss: 0.64; acc: 0.83
Batch: 400; loss: 0.85; acc: 0.77
Batch: 420; loss: 0.71; acc: 0.86
Batch: 440; loss: 0.66; acc: 0.73
Batch: 460; loss: 0.62; acc: 0.8
Batch: 480; loss: 0.75; acc: 0.77
Batch: 500; loss: 0.7; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.78
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.8; acc: 0.75
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.81; acc: 0.8
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.73; acc: 0.78
Batch: 680; loss: 0.67; acc: 0.78
Batch: 700; loss: 0.71; acc: 0.77
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.81 

Batch: 0; loss: 0.68; acc: 0.86
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.57; acc: 0.89
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.38; acc: 0.95
Val Epoch over. val_loss: 0.6374576636560404; val_accuracy: 0.826234076433121 

Epoch 20 start
The current lr is: 6.400000000000001e-05
Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.74; acc: 0.72
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.76; acc: 0.8
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.49; acc: 0.95
Batch: 180; loss: 0.65; acc: 0.78
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.77; acc: 0.77
Batch: 260; loss: 0.8; acc: 0.75
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.68; acc: 0.86
Batch: 320; loss: 0.6; acc: 0.8
Batch: 340; loss: 0.85; acc: 0.7
Batch: 360; loss: 0.69; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.63; acc: 0.84
Batch: 420; loss: 0.66; acc: 0.84
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.72; acc: 0.78
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.62; acc: 0.8
Batch: 540; loss: 0.76; acc: 0.84
Batch: 560; loss: 0.96; acc: 0.77
Batch: 580; loss: 0.7; acc: 0.83
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 0.71; acc: 0.88
Batch: 640; loss: 0.66; acc: 0.78
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.73; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.72
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.77
Batch: 780; loss: 0.87; acc: 0.78
Train Epoch over. train_loss: 0.67; train_accuracy: 0.81 

Batch: 0; loss: 0.67; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.94
Val Epoch over. val_loss: 0.6283656347329449; val_accuracy: 0.8286226114649682 

Epoch 21 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 0.49; acc: 0.92
Batch: 80; loss: 0.85; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.84; acc: 0.73
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.82; acc: 0.81
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.75; acc: 0.73
Batch: 300; loss: 0.57; acc: 0.88
Batch: 320; loss: 0.7; acc: 0.78
Batch: 340; loss: 0.69; acc: 0.78
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.69; acc: 0.81
Batch: 440; loss: 0.66; acc: 0.84
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.79; acc: 0.73
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.6; acc: 0.8
Batch: 540; loss: 0.74; acc: 0.78
Batch: 560; loss: 0.68; acc: 0.84
Batch: 580; loss: 0.55; acc: 0.92
Batch: 600; loss: 0.81; acc: 0.78
Batch: 620; loss: 0.74; acc: 0.83
Batch: 640; loss: 0.69; acc: 0.8
Batch: 660; loss: 0.69; acc: 0.78
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.57; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 0.54; acc: 0.91
Batch: 760; loss: 0.63; acc: 0.77
Batch: 780; loss: 0.86; acc: 0.72
Train Epoch over. train_loss: 0.66; train_accuracy: 0.81 

Batch: 0; loss: 0.67; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.6249618222759028; val_accuracy: 0.8296178343949044 

Epoch 22 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 0.64; acc: 0.77
Batch: 40; loss: 0.67; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.88; acc: 0.75
Batch: 100; loss: 0.79; acc: 0.77
Batch: 120; loss: 1.09; acc: 0.69
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 0.79; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.89; acc: 0.72
Batch: 220; loss: 0.72; acc: 0.78
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.72; acc: 0.78
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.67; acc: 0.83
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.65; acc: 0.84
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.71; acc: 0.77
Batch: 420; loss: 0.73; acc: 0.81
Batch: 440; loss: 0.77; acc: 0.7
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.62; acc: 0.88
Batch: 500; loss: 0.58; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.84
Batch: 540; loss: 0.66; acc: 0.78
Batch: 560; loss: 0.74; acc: 0.77
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.62; acc: 0.84
Batch: 620; loss: 0.68; acc: 0.78
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.8
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.69; acc: 0.86
Batch: 760; loss: 0.6; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.89
Train Epoch over. train_loss: 0.66; train_accuracy: 0.82 

Batch: 0; loss: 0.67; acc: 0.89
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.6215927625537678; val_accuracy: 0.8309116242038217 

Epoch 23 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.89
Batch: 60; loss: 0.57; acc: 0.92
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.79; acc: 0.81
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 0.78; acc: 0.73
Batch: 180; loss: 0.75; acc: 0.77
Batch: 200; loss: 0.74; acc: 0.81
Batch: 220; loss: 0.91; acc: 0.72
Batch: 240; loss: 0.71; acc: 0.78
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.92
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.8
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.84
Batch: 380; loss: 0.7; acc: 0.86
Batch: 400; loss: 0.85; acc: 0.7
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.55; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.67; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.81
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.7; acc: 0.8
Batch: 560; loss: 0.87; acc: 0.77
Batch: 580; loss: 0.63; acc: 0.83
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.71; acc: 0.75
Batch: 640; loss: 0.73; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.61; acc: 0.86
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.62; acc: 0.8
Batch: 780; loss: 0.8; acc: 0.72
Train Epoch over. train_loss: 0.66; train_accuracy: 0.82 

Batch: 0; loss: 0.66; acc: 0.88
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.92
Val Epoch over. val_loss: 0.6180579127020137; val_accuracy: 0.8314092356687898 

Epoch 24 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.7; acc: 0.84
Batch: 20; loss: 0.78; acc: 0.81
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.72; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.79; acc: 0.73
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.8; acc: 0.77
Batch: 160; loss: 0.8; acc: 0.77
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.78
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.84
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.82; acc: 0.73
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.89
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.84
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.87; acc: 0.72
Batch: 460; loss: 0.66; acc: 0.83
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.44; acc: 0.94
Batch: 540; loss: 0.76; acc: 0.75
Batch: 560; loss: 0.74; acc: 0.8
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.52; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.89
Batch: 700; loss: 0.83; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.86; acc: 0.73
Batch: 760; loss: 0.74; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.84
Train Epoch over. train_loss: 0.65; train_accuracy: 0.82 

Batch: 0; loss: 0.66; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.89
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.6144254608139111; val_accuracy: 0.8336982484076433 

Epoch 25 start
The current lr is: 2.5600000000000006e-05
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.64; acc: 0.81
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.46; acc: 0.95
Batch: 200; loss: 0.61; acc: 0.77
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.58; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.86
Batch: 280; loss: 0.92; acc: 0.73
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.73; acc: 0.78
Batch: 340; loss: 0.82; acc: 0.78
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.81
Batch: 400; loss: 0.58; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.75; acc: 0.78
Batch: 480; loss: 0.55; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.85; acc: 0.78
Batch: 580; loss: 0.54; acc: 0.89
Batch: 600; loss: 0.72; acc: 0.8
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.63; acc: 0.81
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.67; acc: 0.75
Batch: 720; loss: 0.75; acc: 0.72
Batch: 740; loss: 0.71; acc: 0.83
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.65; train_accuracy: 0.82 

Batch: 0; loss: 0.66; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.84
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.92
Val Epoch over. val_loss: 0.6114155589395268; val_accuracy: 0.8326035031847133 

Epoch 26 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.79; acc: 0.78
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.88; acc: 0.7
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.81; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.88
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.66; acc: 0.83
Batch: 320; loss: 0.67; acc: 0.81
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.61; acc: 0.73
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.58; acc: 0.89
Batch: 420; loss: 0.68; acc: 0.81
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.88; acc: 0.8
Batch: 520; loss: 0.61; acc: 0.84
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.63; acc: 0.8
Batch: 600; loss: 0.82; acc: 0.78
Batch: 620; loss: 0.61; acc: 0.78
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.56; acc: 0.8
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.6; acc: 0.84
Batch: 720; loss: 0.58; acc: 0.89
Batch: 740; loss: 0.76; acc: 0.77
Batch: 760; loss: 0.74; acc: 0.81
Batch: 780; loss: 0.82; acc: 0.72
Train Epoch over. train_loss: 0.65; train_accuracy: 0.82 

Batch: 0; loss: 0.66; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.6097305136121762; val_accuracy: 0.834593949044586 

Epoch 27 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.77; acc: 0.7
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.83
Batch: 140; loss: 0.6; acc: 0.8
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.8
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.44; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.89
Batch: 280; loss: 0.77; acc: 0.8
Batch: 300; loss: 0.74; acc: 0.83
Batch: 320; loss: 0.92; acc: 0.64
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.57; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.68; acc: 0.83
Batch: 440; loss: 0.64; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.91
Batch: 480; loss: 0.61; acc: 0.86
Batch: 500; loss: 0.79; acc: 0.75
Batch: 520; loss: 0.75; acc: 0.81
Batch: 540; loss: 0.66; acc: 0.81
Batch: 560; loss: 0.58; acc: 0.8
Batch: 580; loss: 0.68; acc: 0.78
Batch: 600; loss: 0.56; acc: 0.92
Batch: 620; loss: 0.71; acc: 0.75
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.72; acc: 0.77
Batch: 680; loss: 0.63; acc: 0.83
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.78; acc: 0.7
Batch: 760; loss: 0.65; acc: 0.91
Batch: 780; loss: 0.83; acc: 0.7
Train Epoch over. train_loss: 0.65; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.6082771249637482; val_accuracy: 0.8347929936305732 

Epoch 28 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.78
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.76; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.84
Batch: 160; loss: 0.66; acc: 0.78
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.6; acc: 0.81
Batch: 220; loss: 0.74; acc: 0.72
Batch: 240; loss: 0.65; acc: 0.8
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.96; acc: 0.78
Batch: 300; loss: 0.62; acc: 0.84
Batch: 320; loss: 0.59; acc: 0.88
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.71; acc: 0.83
Batch: 380; loss: 0.75; acc: 0.81
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.71; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.84
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.89
Batch: 520; loss: 0.72; acc: 0.8
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.66; acc: 0.86
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.86; acc: 0.69
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.64; acc: 0.84
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.54; acc: 0.91
Batch: 760; loss: 0.57; acc: 0.81
Batch: 780; loss: 0.61; acc: 0.8
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.95
Val Epoch over. val_loss: 0.6069341138669639; val_accuracy: 0.8349920382165605 

Epoch 29 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.89; acc: 0.7
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.52; acc: 0.89
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.8
Batch: 140; loss: 0.63; acc: 0.75
Batch: 160; loss: 0.67; acc: 0.78
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.97; acc: 0.64
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.73; acc: 0.77
Batch: 300; loss: 0.68; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.55; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.53; acc: 0.92
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.77; acc: 0.78
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.72; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.7; acc: 0.81
Batch: 540; loss: 0.75; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.8; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.92
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.75; acc: 0.75
Batch: 680; loss: 0.62; acc: 0.86
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.83
Batch: 760; loss: 0.67; acc: 0.75
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.605716944879787; val_accuracy: 0.8350915605095541 

Epoch 30 start
The current lr is: 1.0240000000000004e-05
Batch: 0; loss: 0.64; acc: 0.86
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.59; acc: 0.84
Batch: 80; loss: 0.57; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.73; acc: 0.8
Batch: 140; loss: 0.79; acc: 0.77
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.58; acc: 0.78
Batch: 220; loss: 0.62; acc: 0.8
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.7; acc: 0.78
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.69; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.73
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.66; acc: 0.84
Batch: 380; loss: 0.73; acc: 0.78
Batch: 400; loss: 0.58; acc: 0.8
Batch: 420; loss: 0.68; acc: 0.81
Batch: 440; loss: 0.68; acc: 0.84
Batch: 460; loss: 0.67; acc: 0.84
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.73; acc: 0.78
Batch: 520; loss: 0.77; acc: 0.78
Batch: 540; loss: 0.77; acc: 0.75
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.88
Batch: 600; loss: 0.6; acc: 0.78
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.77; acc: 0.7
Batch: 660; loss: 0.48; acc: 0.92
Batch: 680; loss: 0.67; acc: 0.78
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.73; acc: 0.75
Batch: 740; loss: 0.67; acc: 0.81
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.69; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6043656504457924; val_accuracy: 0.8358877388535032 

Epoch 31 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.71; acc: 0.78
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 0.6; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.63; acc: 0.8
Batch: 160; loss: 0.76; acc: 0.73
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.68; acc: 0.8
Batch: 220; loss: 0.6; acc: 0.88
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.77; acc: 0.73
Batch: 400; loss: 0.9; acc: 0.77
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 0.72; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.66; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.77
Batch: 620; loss: 0.74; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.83
Batch: 660; loss: 0.64; acc: 0.83
Batch: 680; loss: 0.73; acc: 0.8
Batch: 700; loss: 0.6; acc: 0.88
Batch: 720; loss: 0.7; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.88
Batch: 760; loss: 0.82; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6038036242032506; val_accuracy: 0.8358877388535032 

Epoch 32 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.68; acc: 0.8
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.53; acc: 0.94
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.8; acc: 0.75
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.95
Batch: 140; loss: 0.75; acc: 0.77
Batch: 160; loss: 0.52; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.47; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.95
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.7; acc: 0.77
Batch: 280; loss: 0.62; acc: 0.78
Batch: 300; loss: 0.69; acc: 0.81
Batch: 320; loss: 0.74; acc: 0.77
Batch: 340; loss: 0.66; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.89
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.53; acc: 0.88
Batch: 440; loss: 0.64; acc: 0.78
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.67; acc: 0.81
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.7; acc: 0.78
Batch: 620; loss: 0.78; acc: 0.73
Batch: 640; loss: 0.9; acc: 0.7
Batch: 660; loss: 0.5; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.6; acc: 0.84
Batch: 720; loss: 0.75; acc: 0.75
Batch: 740; loss: 0.67; acc: 0.84
Batch: 760; loss: 0.7; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.54; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6032439914478618; val_accuracy: 0.8355891719745223 

Epoch 33 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.84; acc: 0.77
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.8
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.74; acc: 0.77
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.86
Batch: 260; loss: 0.77; acc: 0.78
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.81; acc: 0.72
Batch: 320; loss: 0.69; acc: 0.86
Batch: 340; loss: 0.75; acc: 0.8
Batch: 360; loss: 0.73; acc: 0.8
Batch: 380; loss: 0.64; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.78
Batch: 440; loss: 0.72; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.88
Batch: 580; loss: 0.74; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.8
Batch: 620; loss: 0.59; acc: 0.84
Batch: 640; loss: 0.67; acc: 0.83
Batch: 660; loss: 0.56; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.92
Batch: 700; loss: 0.77; acc: 0.8
Batch: 720; loss: 0.59; acc: 0.88
Batch: 740; loss: 0.65; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6027299803533371; val_accuracy: 0.8355891719745223 

Epoch 34 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.59; acc: 0.84
Batch: 20; loss: 0.72; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.8
Batch: 60; loss: 0.51; acc: 0.89
Batch: 80; loss: 0.64; acc: 0.84
Batch: 100; loss: 0.63; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.86
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.75; acc: 0.8
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.78
Batch: 260; loss: 0.71; acc: 0.77
Batch: 280; loss: 0.67; acc: 0.84
Batch: 300; loss: 0.66; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.92
Batch: 340; loss: 0.58; acc: 0.88
Batch: 360; loss: 0.73; acc: 0.81
Batch: 380; loss: 0.73; acc: 0.78
Batch: 400; loss: 0.65; acc: 0.8
Batch: 420; loss: 0.54; acc: 0.86
Batch: 440; loss: 0.62; acc: 0.77
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.64; acc: 0.78
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.84; acc: 0.7
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.79; acc: 0.8
Batch: 620; loss: 0.85; acc: 0.72
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.73; acc: 0.81
Batch: 720; loss: 0.68; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.84; acc: 0.75
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6021877390563868; val_accuracy: 0.8355891719745223 

Epoch 35 start
The current lr is: 4.096000000000002e-06
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.77
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.92
Batch: 200; loss: 0.56; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.84
Batch: 240; loss: 0.63; acc: 0.78
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.64; acc: 0.77
Batch: 320; loss: 0.74; acc: 0.81
Batch: 340; loss: 0.86; acc: 0.78
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.52; acc: 0.89
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.61; acc: 0.86
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.64; acc: 0.88
Batch: 520; loss: 0.76; acc: 0.73
Batch: 540; loss: 0.66; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.83
Batch: 580; loss: 0.85; acc: 0.8
Batch: 600; loss: 0.68; acc: 0.81
Batch: 620; loss: 0.59; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.7; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.77; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.88
Batch: 780; loss: 0.61; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6016482522912846; val_accuracy: 0.8354896496815286 

Epoch 36 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.53; acc: 0.86
Batch: 80; loss: 0.6; acc: 0.83
Batch: 100; loss: 0.62; acc: 0.84
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.92
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.7; acc: 0.83
Batch: 240; loss: 0.74; acc: 0.8
Batch: 260; loss: 0.58; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.7; acc: 0.84
Batch: 320; loss: 0.65; acc: 0.78
Batch: 340; loss: 0.8; acc: 0.75
Batch: 360; loss: 0.83; acc: 0.73
Batch: 380; loss: 0.91; acc: 0.69
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.76; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.82; acc: 0.75
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.54; acc: 0.88
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.8
Batch: 680; loss: 0.69; acc: 0.73
Batch: 700; loss: 0.57; acc: 0.84
Batch: 720; loss: 0.75; acc: 0.75
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.601424806626739; val_accuracy: 0.8354896496815286 

Epoch 37 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.62; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.69; acc: 0.81
Batch: 200; loss: 0.67; acc: 0.83
Batch: 220; loss: 0.92; acc: 0.73
Batch: 240; loss: 0.66; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.91
Batch: 340; loss: 0.76; acc: 0.72
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.98; acc: 0.75
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.86; acc: 0.7
Batch: 440; loss: 0.64; acc: 0.78
Batch: 460; loss: 0.61; acc: 0.84
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.78; acc: 0.77
Batch: 520; loss: 0.51; acc: 0.91
Batch: 540; loss: 0.69; acc: 0.78
Batch: 560; loss: 0.71; acc: 0.8
Batch: 580; loss: 0.67; acc: 0.78
Batch: 600; loss: 0.6; acc: 0.84
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.69; acc: 0.8
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.72; acc: 0.78
Batch: 720; loss: 0.73; acc: 0.8
Batch: 740; loss: 0.52; acc: 0.88
Batch: 760; loss: 0.87; acc: 0.81
Batch: 780; loss: 0.56; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6012113033586247; val_accuracy: 0.8355891719745223 

Epoch 38 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.86
Batch: 160; loss: 0.63; acc: 0.84
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.74; acc: 0.77
Batch: 220; loss: 0.61; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.83
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.71; acc: 0.75
Batch: 300; loss: 0.69; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.75
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.76; acc: 0.81
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 1.0; acc: 0.73
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.58; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.66; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.62; acc: 0.88
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.72; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.91
Batch: 720; loss: 0.89; acc: 0.75
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.69; acc: 0.86
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6009979599220737; val_accuracy: 0.8356886942675159 

Epoch 39 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.56; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.87; acc: 0.69
Batch: 160; loss: 0.73; acc: 0.77
Batch: 180; loss: 0.73; acc: 0.88
Batch: 200; loss: 0.57; acc: 0.84
Batch: 220; loss: 1.0; acc: 0.7
Batch: 240; loss: 0.62; acc: 0.8
Batch: 260; loss: 0.65; acc: 0.84
Batch: 280; loss: 0.71; acc: 0.78
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.56; acc: 0.88
Batch: 380; loss: 0.59; acc: 0.81
Batch: 400; loss: 0.64; acc: 0.78
Batch: 420; loss: 0.76; acc: 0.78
Batch: 440; loss: 0.71; acc: 0.75
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 0.5; acc: 0.94
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.68; acc: 0.81
Batch: 580; loss: 0.61; acc: 0.84
Batch: 600; loss: 0.66; acc: 0.81
Batch: 620; loss: 0.74; acc: 0.73
Batch: 640; loss: 0.66; acc: 0.81
Batch: 660; loss: 0.78; acc: 0.78
Batch: 680; loss: 0.77; acc: 0.8
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.61; acc: 0.81
Batch: 740; loss: 0.74; acc: 0.77
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.84
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6007811561891228; val_accuracy: 0.8357882165605095 

Epoch 40 start
The current lr is: 1.6384000000000007e-06
Batch: 0; loss: 0.48; acc: 0.89
Batch: 20; loss: 0.73; acc: 0.7
Batch: 40; loss: 0.73; acc: 0.81
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.72; acc: 0.83
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.72; acc: 0.83
Batch: 200; loss: 0.56; acc: 0.91
Batch: 220; loss: 0.87; acc: 0.72
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.59; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.66; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.7
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.76; acc: 0.73
Batch: 520; loss: 0.5; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.92
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.78; acc: 0.8
Batch: 600; loss: 0.73; acc: 0.78
Batch: 620; loss: 0.5; acc: 0.89
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.68; acc: 0.8
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.61; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.89; acc: 0.73
Batch: 760; loss: 0.55; acc: 0.88
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.72; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6005754863760274; val_accuracy: 0.8359872611464968 

Epoch 41 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.71; acc: 0.78
Batch: 20; loss: 0.76; acc: 0.75
Batch: 40; loss: 0.58; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 0.82; acc: 0.77
Batch: 140; loss: 0.68; acc: 0.8
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.61; acc: 0.77
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.69; acc: 0.78
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.75
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.63; acc: 0.8
Batch: 400; loss: 0.97; acc: 0.73
Batch: 420; loss: 0.48; acc: 0.91
Batch: 440; loss: 0.68; acc: 0.83
Batch: 460; loss: 0.75; acc: 0.78
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.68; acc: 0.78
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.72; acc: 0.8
Batch: 560; loss: 0.82; acc: 0.78
Batch: 580; loss: 0.64; acc: 0.81
Batch: 600; loss: 0.7; acc: 0.77
Batch: 620; loss: 0.68; acc: 0.8
Batch: 640; loss: 0.76; acc: 0.8
Batch: 660; loss: 0.62; acc: 0.83
Batch: 680; loss: 0.77; acc: 0.77
Batch: 700; loss: 0.44; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.71; acc: 0.81
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.65; acc: 0.78
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6004947718161686; val_accuracy: 0.8359872611464968 

Epoch 42 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.77; acc: 0.75
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.75; acc: 0.78
Batch: 100; loss: 0.52; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.63; acc: 0.8
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 0.65; acc: 0.86
Batch: 220; loss: 0.71; acc: 0.81
Batch: 240; loss: 0.86; acc: 0.78
Batch: 260; loss: 0.65; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.78; acc: 0.81
Batch: 340; loss: 0.67; acc: 0.8
Batch: 360; loss: 0.74; acc: 0.81
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.76; acc: 0.84
Batch: 420; loss: 0.55; acc: 0.89
Batch: 440; loss: 0.77; acc: 0.8
Batch: 460; loss: 0.69; acc: 0.77
Batch: 480; loss: 0.52; acc: 0.91
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.85; acc: 0.78
Batch: 540; loss: 0.6; acc: 0.84
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.59; acc: 0.83
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.8
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.81
Batch: 720; loss: 0.7; acc: 0.83
Batch: 740; loss: 0.66; acc: 0.78
Batch: 760; loss: 0.53; acc: 0.88
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.600413535051285; val_accuracy: 0.8359872611464968 

Epoch 43 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.81
Batch: 80; loss: 0.83; acc: 0.78
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.91; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.92; acc: 0.72
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.63; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.73
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 0.52; acc: 0.89
Batch: 320; loss: 0.79; acc: 0.73
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.6; acc: 0.84
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.81
Batch: 500; loss: 0.51; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.81
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.78
Batch: 600; loss: 0.71; acc: 0.75
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.71; acc: 0.81
Batch: 700; loss: 0.64; acc: 0.88
Batch: 720; loss: 0.68; acc: 0.81
Batch: 740; loss: 0.67; acc: 0.86
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.71; acc: 0.72
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6003337029818516; val_accuracy: 0.8359872611464968 

Epoch 44 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.58; acc: 0.83
Batch: 80; loss: 0.72; acc: 0.73
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.83
Batch: 140; loss: 0.56; acc: 0.81
Batch: 160; loss: 0.56; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.66; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.91
Batch: 280; loss: 0.69; acc: 0.8
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.61; acc: 0.78
Batch: 380; loss: 0.62; acc: 0.84
Batch: 400; loss: 0.75; acc: 0.75
Batch: 420; loss: 0.66; acc: 0.83
Batch: 440; loss: 0.66; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.83
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.88
Batch: 520; loss: 0.6; acc: 0.8
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.83
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.56; acc: 0.91
Batch: 640; loss: 0.63; acc: 0.78
Batch: 660; loss: 0.58; acc: 0.86
Batch: 680; loss: 0.71; acc: 0.8
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.63; acc: 0.78
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6002525379703303; val_accuracy: 0.8359872611464968 

Epoch 45 start
The current lr is: 6.553600000000004e-07
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.66; acc: 0.78
Batch: 60; loss: 0.48; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.94
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.55; acc: 0.89
Batch: 140; loss: 0.67; acc: 0.81
Batch: 160; loss: 0.58; acc: 0.89
Batch: 180; loss: 0.68; acc: 0.81
Batch: 200; loss: 0.47; acc: 0.92
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.54; acc: 0.84
Batch: 300; loss: 0.64; acc: 0.78
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.47; acc: 0.88
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.69; acc: 0.77
Batch: 420; loss: 0.69; acc: 0.73
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.77; acc: 0.78
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.51; acc: 0.83
Batch: 700; loss: 0.66; acc: 0.89
Batch: 720; loss: 0.78; acc: 0.7
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.81; acc: 0.73
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6001720925804915; val_accuracy: 0.8359872611464968 

Epoch 46 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.69; acc: 0.81
Batch: 60; loss: 0.74; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.75; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.8
Batch: 220; loss: 0.68; acc: 0.8
Batch: 240; loss: 0.53; acc: 0.91
Batch: 260; loss: 0.65; acc: 0.81
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.7; acc: 0.77
Batch: 320; loss: 0.75; acc: 0.8
Batch: 340; loss: 0.56; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.81; acc: 0.81
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.76; acc: 0.75
Batch: 460; loss: 0.7; acc: 0.8
Batch: 480; loss: 0.81; acc: 0.78
Batch: 500; loss: 0.78; acc: 0.75
Batch: 520; loss: 0.66; acc: 0.81
Batch: 540; loss: 0.84; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.86
Batch: 600; loss: 0.62; acc: 0.88
Batch: 620; loss: 0.82; acc: 0.81
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.55; acc: 0.88
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.51; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.91
Batch: 780; loss: 0.66; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.60014486711496; val_accuracy: 0.8359872611464968 

Epoch 47 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.52; acc: 0.83
Batch: 20; loss: 0.69; acc: 0.73
Batch: 40; loss: 0.85; acc: 0.75
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.74; acc: 0.72
Batch: 100; loss: 0.84; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.63; acc: 0.86
Batch: 160; loss: 0.74; acc: 0.8
Batch: 180; loss: 0.58; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.67; acc: 0.84
Batch: 260; loss: 0.81; acc: 0.73
Batch: 280; loss: 0.58; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.68; acc: 0.81
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.72; acc: 0.78
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.64; acc: 0.83
Batch: 480; loss: 0.6; acc: 0.8
Batch: 500; loss: 0.69; acc: 0.8
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.91
Batch: 560; loss: 0.66; acc: 0.81
Batch: 580; loss: 0.74; acc: 0.77
Batch: 600; loss: 0.63; acc: 0.8
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.75; acc: 0.77
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.78; acc: 0.81
Batch: 740; loss: 0.69; acc: 0.8
Batch: 760; loss: 0.63; acc: 0.81
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6001178570993387; val_accuracy: 0.8359872611464968 

Epoch 48 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.82; acc: 0.8
Batch: 20; loss: 0.66; acc: 0.84
Batch: 40; loss: 0.61; acc: 0.8
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.8; acc: 0.8
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.8; acc: 0.69
Batch: 160; loss: 0.57; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.63; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.83; acc: 0.8
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.59; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.86
Batch: 400; loss: 0.67; acc: 0.8
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.46; acc: 0.94
Batch: 460; loss: 0.82; acc: 0.77
Batch: 480; loss: 0.73; acc: 0.73
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.69; acc: 0.75
Batch: 560; loss: 0.64; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.92
Batch: 600; loss: 0.59; acc: 0.86
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 0.72; acc: 0.8
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.66; acc: 0.83
Batch: 740; loss: 0.78; acc: 0.78
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.54; acc: 0.88
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6000907647002275; val_accuracy: 0.8358877388535032 

Epoch 49 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.59; acc: 0.8
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.81; acc: 0.77
Batch: 60; loss: 0.6; acc: 0.88
Batch: 80; loss: 0.59; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.8
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.83
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 0.65; acc: 0.81
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.67; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.76; acc: 0.73
Batch: 360; loss: 0.84; acc: 0.73
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.88
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.63; acc: 0.81
Batch: 460; loss: 0.59; acc: 0.89
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.86
Batch: 520; loss: 0.8; acc: 0.75
Batch: 540; loss: 0.65; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.74; acc: 0.75
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.72; acc: 0.75
Batch: 660; loss: 0.61; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.58; acc: 0.86
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.69; acc: 0.83
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.600064148758627; val_accuracy: 0.8358877388535032 

Epoch 50 start
The current lr is: 2.6214400000000015e-07
Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.7; acc: 0.81
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.65; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.75; acc: 0.77
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.63; acc: 0.89
Batch: 200; loss: 0.68; acc: 0.83
Batch: 220; loss: 0.77; acc: 0.75
Batch: 240; loss: 0.58; acc: 0.8
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.84
Batch: 300; loss: 0.74; acc: 0.77
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.75
Batch: 420; loss: 0.58; acc: 0.88
Batch: 440; loss: 0.82; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.72; acc: 0.78
Batch: 500; loss: 0.81; acc: 0.78
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.67; acc: 0.77
Batch: 560; loss: 0.66; acc: 0.78
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.7; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.66; acc: 0.77
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.64; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.89
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.53; acc: 0.89
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.95
Val Epoch over. val_loss: 0.6000363729941617; val_accuracy: 0.8358877388535032 

plots/no_subspace_training/reg_lenet/2020-01-19 03:30:59/d_dim_1000_lr_0.001_gamma_0.4_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.21611541699452; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.0288564832347213; val_accuracy: 0.36892914012738853 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.39
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508558346207734; val_accuracy: 0.45710589171974525 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.38
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5094988566295358; val_accuracy: 0.568968949044586 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878710760432444; val_accuracy: 0.6331608280254777 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.0523909015260684; val_accuracy: 0.6972531847133758 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.8261767857393642; val_accuracy: 0.7794585987261147 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6589601875110797; val_accuracy: 0.8039410828025477 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.536056330059744; val_accuracy: 0.8443471337579618 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.4435340901658793; val_accuracy: 0.8715167197452229 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.72; acc: 0.72
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.3871797337463707; val_accuracy: 0.8840565286624203 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3392742941523813; val_accuracy: 0.9023686305732485 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.3062540131864274; val_accuracy: 0.9116242038216561 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.98
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.27934014488747166; val_accuracy: 0.9200835987261147 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.97
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.27112073603139564; val_accuracy: 0.9175955414012739 

Epoch 16 start
The current lr is: 0.0002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.3; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24305489021027163; val_accuracy: 0.9299363057324841 

Epoch 17 start
The current lr is: 0.0002
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.98
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.23899182767435245; val_accuracy: 0.931031050955414 

Epoch 18 start
The current lr is: 0.0002
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23517188712195225; val_accuracy: 0.931827229299363 

Epoch 19 start
The current lr is: 0.0002
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.8
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.98
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23216609867989638; val_accuracy: 0.9332205414012739 

Epoch 20 start
The current lr is: 0.0002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.14; acc: 1.0
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.88
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2278324454832988; val_accuracy: 0.9341162420382165 

Epoch 21 start
The current lr is: 0.0002
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.98
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.8
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22636799350570722; val_accuracy: 0.9343152866242038 

Epoch 22 start
The current lr is: 0.0002
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.84
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2222350030710363; val_accuracy: 0.9357085987261147 

Epoch 23 start
The current lr is: 0.0002
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.88
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2192070954686897; val_accuracy: 0.9366042993630573 

Epoch 24 start
The current lr is: 0.0002
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.95
Batch: 520; loss: 0.08; acc: 1.0
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.31; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21577666702259118; val_accuracy: 0.9379976114649682 

Epoch 25 start
The current lr is: 0.0002
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21345654462173486; val_accuracy: 0.9379976114649682 

Epoch 26 start
The current lr is: 0.0002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.07; acc: 1.0
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21046403954458084; val_accuracy: 0.939390923566879 

Epoch 27 start
The current lr is: 0.0002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.95
Batch: 40; loss: 0.18; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.09; acc: 0.98
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.43; acc: 0.88
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.20772163256718096; val_accuracy: 0.9391918789808917 

Epoch 28 start
The current lr is: 0.0002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.24; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.86
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.11; acc: 0.98
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.20573724611739444; val_accuracy: 0.9395899681528662 

Epoch 29 start
The current lr is: 0.0002
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.84
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.89
Batch: 240; loss: 0.46; acc: 0.81
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.08; acc: 0.98
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.1; acc: 1.0
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2043532342857616; val_accuracy: 0.9408837579617835 

Epoch 30 start
The current lr is: 0.0002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.49; acc: 0.78
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.20215455981291783; val_accuracy: 0.9404856687898089 

Epoch 31 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.22; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.1; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.24; acc: 0.91
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.20038761582913672; val_accuracy: 0.941281847133758 

Epoch 32 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.88
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19987575427457027; val_accuracy: 0.9408837579617835 

Epoch 33 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.22; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.84
Batch: 400; loss: 0.14; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1993675918383583; val_accuracy: 0.9414808917197452 

Epoch 34 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.16; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.09; acc: 1.0
Batch: 700; loss: 0.18; acc: 0.98
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1990485221242449; val_accuracy: 0.9413813694267515 

Epoch 35 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.1; acc: 1.0
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.1; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.86
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1986598545436267; val_accuracy: 0.9415804140127388 

Epoch 36 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.22; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.05; acc: 1.0
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.12; acc: 0.98
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.15; acc: 0.97
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.16; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19789372674029343; val_accuracy: 0.9415804140127388 

Epoch 37 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.19; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.97
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.94
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19781331013247466; val_accuracy: 0.942078025477707 

Epoch 38 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.16; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.4; acc: 0.83
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.19; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1973734445585187; val_accuracy: 0.9416799363057324 

Epoch 39 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.13; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.14; acc: 0.98
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.97
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.12; acc: 0.98
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.86
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.2; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1967435212936371; val_accuracy: 0.9416799363057324 

Epoch 40 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.98
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.1; acc: 1.0
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.09; acc: 0.97
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19644789331278223; val_accuracy: 0.9422770700636943 

Epoch 41 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.09; acc: 1.0
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1960904108728193; val_accuracy: 0.9421775477707006 

Epoch 42 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.24; acc: 0.94
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.94
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1954176097301541; val_accuracy: 0.9424761146496815 

Epoch 43 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.84
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.1; acc: 0.95
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.13; acc: 1.0
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.07; acc: 1.0
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.8
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1950194310088446; val_accuracy: 0.9425756369426752 

Epoch 44 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.88
Batch: 260; loss: 0.15; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.17; acc: 0.92
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.98
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.16; acc: 0.91
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.92
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19462518415348545; val_accuracy: 0.9426751592356688 

Epoch 45 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.08; acc: 0.98
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.98
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.86
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19437953305377323; val_accuracy: 0.9426751592356688 

Epoch 46 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.98
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.98
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1942069481844735; val_accuracy: 0.9428742038216561 

Epoch 47 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.4; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1940819134662865; val_accuracy: 0.9429737261146497 

Epoch 48 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.23; acc: 0.92
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.12; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.19402701561902738; val_accuracy: 0.9429737261146497 

Epoch 49 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.11; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.84
Batch: 380; loss: 0.16; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.97
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.17; acc: 0.98
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.22; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1939375734395662; val_accuracy: 0.9429737261146497 

Epoch 50 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.98
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.16; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.17; acc: 0.92
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.22; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.1938219441777202; val_accuracy: 0.9430732484076433 

plots/no_subspace_training/reg_lenet/2020-01-19 03:40:24/d_dim_1000_lr_0.001_gamma_0.2_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161153547323433; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.02886035336051; val_accuracy: 0.3688296178343949 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.39
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.750862498192271; val_accuracy: 0.45710589171974525 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5095042362334623; val_accuracy: 0.5688694267515924 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878706212256366; val_accuracy: 0.6331608280254777 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.0523914451811724; val_accuracy: 0.6973527070063694 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.8261860180052982; val_accuracy: 0.7792595541401274 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6589667070063816; val_accuracy: 0.8039410828025477 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5360525311178462; val_accuracy: 0.8444466560509554 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.44351649009118416; val_accuracy: 0.8715167197452229 

Epoch 11 start
The current lr is: 0.0002
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.61; acc: 0.78
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.44; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.97
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.73; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.76; acc: 0.7
Batch: 480; loss: 0.78; acc: 0.64
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.8
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.72; acc: 0.78
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.4266796862813318; val_accuracy: 0.8745023885350318 

Epoch 12 start
The current lr is: 0.0002
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.59; acc: 0.78
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.8
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.8
Batch: 500; loss: 0.66; acc: 0.8
Batch: 520; loss: 0.33; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.89
Batch: 600; loss: 0.49; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.51; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.65; acc: 0.77
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.46; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.4132166583636764; val_accuracy: 0.8793789808917197 

Epoch 13 start
The current lr is: 0.0002
Batch: 0; loss: 0.45; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.78
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.55; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.84
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.94
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.8
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.51; acc: 0.88
Batch: 680; loss: 0.62; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.83
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.400407770446911; val_accuracy: 0.8836584394904459 

Epoch 14 start
The current lr is: 0.0002
Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.37; acc: 0.92
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.5; acc: 0.81
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.92
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.6; acc: 0.81
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.95
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.51; acc: 0.81
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.49; acc: 0.8
Batch: 720; loss: 0.24; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.95
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.3895044172075903; val_accuracy: 0.8870421974522293 

Epoch 15 start
The current lr is: 0.0002
Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.64; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.81
Batch: 160; loss: 0.5; acc: 0.81
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.6; acc: 0.83
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.95
Batch: 260; loss: 0.53; acc: 0.81
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.8
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.83
Batch: 480; loss: 0.53; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.86
Batch: 520; loss: 0.52; acc: 0.81
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.92
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.94
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.91
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.3796349353851027; val_accuracy: 0.8896297770700637 

Epoch 16 start
The current lr is: 0.0002
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.63; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.78
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.35; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3679116258196011; val_accuracy: 0.8943073248407644 

Epoch 17 start
The current lr is: 0.0002
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.26; acc: 0.97
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.54; acc: 0.81
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.8
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.64; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.83
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35837763851615273; val_accuracy: 0.8954020700636943 

Epoch 18 start
The current lr is: 0.0002
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.92
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.83
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.29; acc: 0.94
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.97
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3486735549322359; val_accuracy: 0.8975915605095541 

Epoch 19 start
The current lr is: 0.0002
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.64; acc: 0.77
Batch: 360; loss: 0.42; acc: 0.91
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.83
Batch: 740; loss: 0.22; acc: 0.97
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.33990745009130735; val_accuracy: 0.9008757961783439 

Epoch 20 start
The current lr is: 0.0002
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.81
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.2; acc: 0.98
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.83
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.83
Batch: 720; loss: 0.54; acc: 0.81
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.33089727139586855; val_accuracy: 0.9045581210191083 

Epoch 21 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.51; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.328879050151178; val_accuracy: 0.9039609872611465 

Epoch 22 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.8
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.83
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3274487557874364; val_accuracy: 0.9047571656050956 

Epoch 23 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.81
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.94
Batch: 400; loss: 0.58; acc: 0.81
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.95
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.64; acc: 0.77
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.3258840094800967; val_accuracy: 0.903562898089172 

Epoch 24 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.81
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.84
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.28; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.98
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.32403505854545883; val_accuracy: 0.9060509554140127 

Epoch 25 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.83
Batch: 400; loss: 0.28; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.98
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3227608690311195; val_accuracy: 0.9053542993630573 

Epoch 26 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.84
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.8
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.321051107470397; val_accuracy: 0.9063495222929936 

Epoch 27 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.35; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.98
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.23; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.52; acc: 0.81
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.35; acc: 0.84
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.84
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.49; acc: 0.81
Batch: 760; loss: 0.38; acc: 0.92
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.3194102524856853; val_accuracy: 0.9070461783439491 

Epoch 28 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.86
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.47; acc: 0.8
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.31809061362295393; val_accuracy: 0.9057523885350318 

Epoch 29 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.42; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.81
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.14; acc: 0.98
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.73
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.316818608742231; val_accuracy: 0.9065485668789809 

Epoch 30 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.97
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.51; acc: 0.81
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.315398711260337; val_accuracy: 0.9080414012738853 

Epoch 31 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.4; acc: 0.83
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.95
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.31491137500021865; val_accuracy: 0.9076433121019108 

Epoch 32 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.97
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.94
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.97
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.84
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.25; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.83
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.3145336939650736; val_accuracy: 0.9075437898089171 

Epoch 33 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.46; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.31428944642186923; val_accuracy: 0.9071457006369427 

Epoch 34 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.95
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.95
Batch: 320; loss: 0.25; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.12; acc: 1.0
Val Epoch over. val_loss: 0.31399583080961446; val_accuracy: 0.9075437898089171 

Epoch 35 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.17; acc: 0.98
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.46; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.56; acc: 0.88
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31367593718941805; val_accuracy: 0.9074442675159236 

Epoch 36 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.34; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.15; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.97
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.34; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31334005486054023; val_accuracy: 0.9083399681528662 

Epoch 37 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.98
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.86
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.64; acc: 0.81
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.66; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.31309126953410493; val_accuracy: 0.9080414012738853 

Epoch 38 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.49; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.67; acc: 0.84
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.95
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.53; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.3128181756207138; val_accuracy: 0.9075437898089171 

Epoch 39 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.73; acc: 0.75
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.8
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31244537500059527; val_accuracy: 0.9078423566878981 

Epoch 40 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.2; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.83
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.46; acc: 0.83
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.22; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.95
Batch: 780; loss: 0.43; acc: 0.83
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.312247769847797; val_accuracy: 0.9080414012738853 

Epoch 41 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.83
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31217377870136004; val_accuracy: 0.9082404458598726 

Epoch 42 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.97
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31211010115161825; val_accuracy: 0.908140923566879 

Epoch 43 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.37; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.15; acc: 0.98
Batch: 280; loss: 0.36; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.81
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31204564029433923; val_accuracy: 0.908140923566879 

Epoch 44 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.95
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.27; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.88
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.84
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.98
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31197761587656225; val_accuracy: 0.9083399681528662 

Epoch 45 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.4; acc: 0.83
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31192052350112587; val_accuracy: 0.9083399681528662 

Epoch 46 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.41; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.88
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.43; acc: 0.81
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.97
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.3118691442024176; val_accuracy: 0.9083399681528662 

Epoch 47 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.54; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.5; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.39; acc: 0.83
Batch: 320; loss: 0.3; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.97
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.48; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.311810583160941; val_accuracy: 0.9083399681528662 

Epoch 48 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.8
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.31175454582568185; val_accuracy: 0.9083399681528662 

Epoch 49 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.5; acc: 0.78
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.49; acc: 0.81
Batch: 620; loss: 0.24; acc: 0.97
Batch: 640; loss: 0.4; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.3117021007143008; val_accuracy: 0.9083399681528662 

Epoch 50 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.33; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.31; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.41; acc: 0.94
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.19; acc: 0.97
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.5; acc: 0.81
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.78
Batch: 140; loss: 0.11; acc: 1.0
Val Epoch over. val_loss: 0.3116430293792372; val_accuracy: 0.9083399681528662 

plots/no_subspace_training/reg_lenet/2020-01-19 03:49:35/d_dim_1000_lr_0.001_gamma_0.2_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161154382547754; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.0288662356176195; val_accuracy: 0.3690286624203822 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.38
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.42
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508631511858315; val_accuracy: 0.4570063694267516 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5094987670327449; val_accuracy: 0.5688694267515924 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878652241579287; val_accuracy: 0.6332603503184714 

Epoch 6 start
The current lr is: 0.0002
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.56; acc: 0.52
Batch: 80; loss: 1.21; acc: 0.67
Batch: 100; loss: 1.46; acc: 0.52
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 1.42; acc: 0.58
Batch: 160; loss: 1.36; acc: 0.61
Batch: 180; loss: 1.31; acc: 0.64
Batch: 200; loss: 1.31; acc: 0.62
Batch: 220; loss: 1.27; acc: 0.61
Batch: 240; loss: 1.26; acc: 0.67
Batch: 260; loss: 1.28; acc: 0.58
Batch: 280; loss: 1.25; acc: 0.58
Batch: 300; loss: 1.27; acc: 0.72
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.37; acc: 0.58
Batch: 360; loss: 1.33; acc: 0.53
Batch: 380; loss: 1.3; acc: 0.61
Batch: 400; loss: 1.33; acc: 0.64
Batch: 420; loss: 1.17; acc: 0.72
Batch: 440; loss: 1.25; acc: 0.64
Batch: 460; loss: 1.16; acc: 0.78
Batch: 480; loss: 1.26; acc: 0.73
Batch: 500; loss: 1.47; acc: 0.56
Batch: 520; loss: 1.31; acc: 0.64
Batch: 540; loss: 1.21; acc: 0.67
Batch: 560; loss: 1.39; acc: 0.64
Batch: 580; loss: 1.36; acc: 0.61
Batch: 600; loss: 1.18; acc: 0.67
Batch: 620; loss: 1.18; acc: 0.67
Batch: 640; loss: 1.21; acc: 0.66
Batch: 660; loss: 1.34; acc: 0.59
Batch: 680; loss: 1.22; acc: 0.62
Batch: 700; loss: 1.24; acc: 0.58
Batch: 720; loss: 1.18; acc: 0.7
Batch: 740; loss: 1.32; acc: 0.69
Batch: 760; loss: 1.15; acc: 0.69
Batch: 780; loss: 1.33; acc: 0.58
Train Epoch over. train_loss: 1.28; train_accuracy: 0.63 

Batch: 0; loss: 1.23; acc: 0.69
Batch: 20; loss: 1.36; acc: 0.55
Batch: 40; loss: 1.03; acc: 0.7
Batch: 60; loss: 1.13; acc: 0.64
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.3; acc: 0.66
Batch: 140; loss: 1.06; acc: 0.78
Val Epoch over. val_loss: 1.2360918187791374; val_accuracy: 0.6582404458598726 

Epoch 7 start
The current lr is: 0.0002
Batch: 0; loss: 1.17; acc: 0.59
Batch: 20; loss: 1.2; acc: 0.69
Batch: 40; loss: 1.43; acc: 0.59
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 1.32; acc: 0.73
Batch: 100; loss: 1.33; acc: 0.67
Batch: 120; loss: 1.22; acc: 0.7
Batch: 140; loss: 1.32; acc: 0.67
Batch: 160; loss: 1.21; acc: 0.67
Batch: 180; loss: 1.16; acc: 0.7
Batch: 200; loss: 1.22; acc: 0.62
Batch: 220; loss: 1.48; acc: 0.53
Batch: 240; loss: 1.22; acc: 0.7
Batch: 260; loss: 1.25; acc: 0.61
Batch: 280; loss: 1.16; acc: 0.77
Batch: 300; loss: 1.27; acc: 0.62
Batch: 320; loss: 1.33; acc: 0.58
Batch: 340; loss: 1.53; acc: 0.53
Batch: 360; loss: 1.35; acc: 0.62
Batch: 380; loss: 1.25; acc: 0.7
Batch: 400; loss: 1.16; acc: 0.64
Batch: 420; loss: 1.25; acc: 0.61
Batch: 440; loss: 1.22; acc: 0.66
Batch: 460; loss: 1.16; acc: 0.66
Batch: 480; loss: 1.33; acc: 0.53
Batch: 500; loss: 1.26; acc: 0.66
Batch: 520; loss: 1.38; acc: 0.59
Batch: 540; loss: 1.39; acc: 0.64
Batch: 560; loss: 1.27; acc: 0.56
Batch: 580; loss: 1.39; acc: 0.64
Batch: 600; loss: 1.28; acc: 0.66
Batch: 620; loss: 1.3; acc: 0.59
Batch: 640; loss: 1.22; acc: 0.64
Batch: 660; loss: 1.23; acc: 0.59
Batch: 680; loss: 1.13; acc: 0.72
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.23; acc: 0.61
Batch: 740; loss: 1.26; acc: 0.58
Batch: 760; loss: 1.35; acc: 0.62
Batch: 780; loss: 1.15; acc: 0.62
Train Epoch over. train_loss: 1.24; train_accuracy: 0.65 

Batch: 0; loss: 1.18; acc: 0.72
Batch: 20; loss: 1.33; acc: 0.56
Batch: 40; loss: 0.98; acc: 0.73
Batch: 60; loss: 1.08; acc: 0.69
Batch: 80; loss: 1.12; acc: 0.72
Batch: 100; loss: 1.15; acc: 0.73
Batch: 120; loss: 1.26; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.188788958795511; val_accuracy: 0.678343949044586 

Epoch 8 start
The current lr is: 0.0002
Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 1.04; acc: 0.69
Batch: 40; loss: 1.14; acc: 0.72
Batch: 60; loss: 1.16; acc: 0.66
Batch: 80; loss: 1.36; acc: 0.5
Batch: 100; loss: 1.28; acc: 0.61
Batch: 120; loss: 1.04; acc: 0.8
Batch: 140; loss: 1.12; acc: 0.72
Batch: 160; loss: 1.23; acc: 0.64
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 1.19; acc: 0.66
Batch: 220; loss: 1.28; acc: 0.66
Batch: 240; loss: 1.19; acc: 0.58
Batch: 260; loss: 1.26; acc: 0.69
Batch: 280; loss: 1.0; acc: 0.69
Batch: 300; loss: 1.28; acc: 0.61
Batch: 320; loss: 1.17; acc: 0.69
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 1.1; acc: 0.7
Batch: 380; loss: 1.22; acc: 0.62
Batch: 400; loss: 1.32; acc: 0.61
Batch: 420; loss: 1.29; acc: 0.66
Batch: 440; loss: 1.16; acc: 0.67
Batch: 460; loss: 1.1; acc: 0.72
Batch: 480; loss: 1.04; acc: 0.78
Batch: 500; loss: 1.08; acc: 0.75
Batch: 520; loss: 1.11; acc: 0.72
Batch: 540; loss: 1.17; acc: 0.64
Batch: 560; loss: 1.09; acc: 0.64
Batch: 580; loss: 0.82; acc: 0.88
Batch: 600; loss: 1.31; acc: 0.59
Batch: 620; loss: 1.23; acc: 0.61
Batch: 640; loss: 1.22; acc: 0.67
Batch: 660; loss: 1.07; acc: 0.7
Batch: 680; loss: 1.29; acc: 0.56
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 0.97; acc: 0.8
Batch: 740; loss: 1.24; acc: 0.59
Batch: 760; loss: 1.23; acc: 0.69
Batch: 780; loss: 1.03; acc: 0.67
Train Epoch over. train_loss: 1.19; train_accuracy: 0.66 

Batch: 0; loss: 1.14; acc: 0.72
Batch: 20; loss: 1.28; acc: 0.56
Batch: 40; loss: 0.95; acc: 0.73
Batch: 60; loss: 1.04; acc: 0.7
Batch: 80; loss: 1.08; acc: 0.73
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.72
Batch: 140; loss: 0.95; acc: 0.83
Val Epoch over. val_loss: 1.1418623400341934; val_accuracy: 0.6919785031847133 

Epoch 9 start
The current lr is: 0.0002
Batch: 0; loss: 1.15; acc: 0.75
Batch: 20; loss: 1.04; acc: 0.75
Batch: 40; loss: 1.27; acc: 0.67
Batch: 60; loss: 1.31; acc: 0.66
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 1.2; acc: 0.62
Batch: 120; loss: 1.09; acc: 0.66
Batch: 140; loss: 1.18; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.75
Batch: 180; loss: 0.97; acc: 0.8
Batch: 200; loss: 1.14; acc: 0.62
Batch: 220; loss: 1.13; acc: 0.67
Batch: 240; loss: 1.28; acc: 0.58
Batch: 260; loss: 1.14; acc: 0.7
Batch: 280; loss: 1.11; acc: 0.7
Batch: 300; loss: 1.15; acc: 0.66
Batch: 320; loss: 1.09; acc: 0.69
Batch: 340; loss: 1.03; acc: 0.75
Batch: 360; loss: 1.02; acc: 0.7
Batch: 380; loss: 1.22; acc: 0.69
Batch: 400; loss: 1.12; acc: 0.8
Batch: 420; loss: 1.24; acc: 0.59
Batch: 440; loss: 1.19; acc: 0.67
Batch: 460; loss: 1.03; acc: 0.67
Batch: 480; loss: 1.14; acc: 0.66
Batch: 500; loss: 1.24; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.58
Batch: 540; loss: 0.98; acc: 0.7
Batch: 560; loss: 1.09; acc: 0.73
Batch: 580; loss: 1.25; acc: 0.59
Batch: 600; loss: 1.06; acc: 0.78
Batch: 620; loss: 1.3; acc: 0.61
Batch: 640; loss: 1.19; acc: 0.69
Batch: 660; loss: 1.08; acc: 0.67
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 1.1; acc: 0.66
Batch: 720; loss: 1.12; acc: 0.66
Batch: 740; loss: 1.05; acc: 0.72
Batch: 760; loss: 0.96; acc: 0.77
Batch: 780; loss: 0.92; acc: 0.75
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.24; acc: 0.59
Batch: 40; loss: 0.91; acc: 0.73
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 1.02; acc: 0.73
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.17; acc: 0.75
Batch: 140; loss: 0.89; acc: 0.81
Val Epoch over. val_loss: 1.0928210760377775; val_accuracy: 0.7093949044585988 

Epoch 10 start
The current lr is: 0.0002
Batch: 0; loss: 1.04; acc: 0.73
Batch: 20; loss: 1.1; acc: 0.75
Batch: 40; loss: 1.1; acc: 0.67
Batch: 60; loss: 1.03; acc: 0.75
Batch: 80; loss: 1.17; acc: 0.59
Batch: 100; loss: 0.9; acc: 0.73
Batch: 120; loss: 0.94; acc: 0.77
Batch: 140; loss: 1.0; acc: 0.7
Batch: 160; loss: 1.38; acc: 0.47
Batch: 180; loss: 1.18; acc: 0.55
Batch: 200; loss: 1.07; acc: 0.78
Batch: 220; loss: 1.1; acc: 0.66
Batch: 240; loss: 1.13; acc: 0.73
Batch: 260; loss: 1.04; acc: 0.73
Batch: 280; loss: 1.23; acc: 0.58
Batch: 300; loss: 1.09; acc: 0.7
Batch: 320; loss: 1.03; acc: 0.69
Batch: 340; loss: 1.01; acc: 0.72
Batch: 360; loss: 1.01; acc: 0.75
Batch: 380; loss: 1.13; acc: 0.7
Batch: 400; loss: 1.4; acc: 0.58
Batch: 420; loss: 1.13; acc: 0.64
Batch: 440; loss: 1.21; acc: 0.64
Batch: 460; loss: 1.21; acc: 0.67
Batch: 480; loss: 0.92; acc: 0.81
Batch: 500; loss: 1.22; acc: 0.61
Batch: 520; loss: 1.04; acc: 0.72
Batch: 540; loss: 1.04; acc: 0.77
Batch: 560; loss: 1.15; acc: 0.66
Batch: 580; loss: 1.27; acc: 0.61
Batch: 600; loss: 1.21; acc: 0.62
Batch: 620; loss: 1.2; acc: 0.62
Batch: 640; loss: 1.17; acc: 0.62
Batch: 660; loss: 0.84; acc: 0.8
Batch: 680; loss: 1.06; acc: 0.75
Batch: 700; loss: 1.13; acc: 0.62
Batch: 720; loss: 1.22; acc: 0.64
Batch: 740; loss: 0.95; acc: 0.73
Batch: 760; loss: 1.24; acc: 0.64
Batch: 780; loss: 1.0; acc: 0.8
Train Epoch over. train_loss: 1.1; train_accuracy: 0.69 

Batch: 0; loss: 1.07; acc: 0.75
Batch: 20; loss: 1.19; acc: 0.59
Batch: 40; loss: 0.88; acc: 0.77
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.99; acc: 0.73
Batch: 100; loss: 1.02; acc: 0.73
Batch: 120; loss: 1.14; acc: 0.72
Batch: 140; loss: 0.84; acc: 0.84
Val Epoch over. val_loss: 1.047020321059379; val_accuracy: 0.7180533439490446 

Epoch 11 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.98; acc: 0.77
Batch: 20; loss: 1.1; acc: 0.69
Batch: 40; loss: 1.15; acc: 0.66
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.39; acc: 0.56
Batch: 100; loss: 1.06; acc: 0.69
Batch: 120; loss: 1.13; acc: 0.67
Batch: 140; loss: 1.02; acc: 0.81
Batch: 160; loss: 1.19; acc: 0.69
Batch: 180; loss: 1.15; acc: 0.62
Batch: 200; loss: 1.03; acc: 0.73
Batch: 220; loss: 1.08; acc: 0.66
Batch: 240; loss: 0.98; acc: 0.72
Batch: 260; loss: 1.02; acc: 0.67
Batch: 280; loss: 1.06; acc: 0.67
Batch: 300; loss: 1.27; acc: 0.62
Batch: 320; loss: 0.94; acc: 0.78
Batch: 340; loss: 1.24; acc: 0.66
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.05; acc: 0.66
Batch: 400; loss: 1.06; acc: 0.69
Batch: 420; loss: 1.07; acc: 0.69
Batch: 440; loss: 1.12; acc: 0.66
Batch: 460; loss: 1.41; acc: 0.56
Batch: 480; loss: 1.46; acc: 0.52
Batch: 500; loss: 0.93; acc: 0.77
Batch: 520; loss: 1.19; acc: 0.61
Batch: 540; loss: 1.14; acc: 0.7
Batch: 560; loss: 0.98; acc: 0.7
Batch: 580; loss: 1.25; acc: 0.64
Batch: 600; loss: 1.11; acc: 0.67
Batch: 620; loss: 1.03; acc: 0.73
Batch: 640; loss: 0.96; acc: 0.72
Batch: 660; loss: 0.99; acc: 0.78
Batch: 680; loss: 1.14; acc: 0.64
Batch: 700; loss: 0.93; acc: 0.73
Batch: 720; loss: 1.04; acc: 0.78
Batch: 740; loss: 0.94; acc: 0.7
Batch: 760; loss: 1.12; acc: 0.69
Batch: 780; loss: 0.88; acc: 0.83
Train Epoch over. train_loss: 1.07; train_accuracy: 0.7 

Batch: 0; loss: 1.05; acc: 0.75
Batch: 20; loss: 1.18; acc: 0.58
Batch: 40; loss: 0.87; acc: 0.73
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 0.98; acc: 0.77
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.75
Batch: 140; loss: 0.82; acc: 0.83
Val Epoch over. val_loss: 1.0358948244410715; val_accuracy: 0.7222332802547771 

Epoch 12 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 0.99; acc: 0.72
Batch: 20; loss: 1.08; acc: 0.7
Batch: 40; loss: 0.87; acc: 0.73
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 1.15; acc: 0.62
Batch: 100; loss: 0.91; acc: 0.77
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 1.1; acc: 0.64
Batch: 160; loss: 1.13; acc: 0.62
Batch: 180; loss: 1.05; acc: 0.66
Batch: 200; loss: 1.04; acc: 0.66
Batch: 220; loss: 1.14; acc: 0.67
Batch: 240; loss: 0.86; acc: 0.78
Batch: 260; loss: 1.03; acc: 0.73
Batch: 280; loss: 1.08; acc: 0.72
Batch: 300; loss: 1.15; acc: 0.72
Batch: 320; loss: 1.03; acc: 0.73
Batch: 340; loss: 1.19; acc: 0.67
Batch: 360; loss: 1.16; acc: 0.69
Batch: 380; loss: 1.11; acc: 0.64
Batch: 400; loss: 1.07; acc: 0.72
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 1.08; acc: 0.7
Batch: 460; loss: 1.0; acc: 0.72
Batch: 480; loss: 1.03; acc: 0.72
Batch: 500; loss: 1.18; acc: 0.64
Batch: 520; loss: 0.88; acc: 0.77
Batch: 540; loss: 1.0; acc: 0.75
Batch: 560; loss: 1.01; acc: 0.72
Batch: 580; loss: 1.16; acc: 0.73
Batch: 600; loss: 1.17; acc: 0.59
Batch: 620; loss: 1.06; acc: 0.75
Batch: 640; loss: 1.1; acc: 0.69
Batch: 660; loss: 1.08; acc: 0.7
Batch: 680; loss: 1.13; acc: 0.73
Batch: 700; loss: 1.15; acc: 0.69
Batch: 720; loss: 1.0; acc: 0.73
Batch: 740; loss: 0.86; acc: 0.8
Batch: 760; loss: 1.22; acc: 0.66
Batch: 780; loss: 1.08; acc: 0.72
Train Epoch over. train_loss: 1.06; train_accuracy: 0.7 

Batch: 0; loss: 1.04; acc: 0.75
Batch: 20; loss: 1.17; acc: 0.61
Batch: 40; loss: 0.86; acc: 0.75
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 0.97; acc: 0.75
Batch: 100; loss: 1.01; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.73
Batch: 140; loss: 0.81; acc: 0.83
Val Epoch over. val_loss: 1.026289317258604; val_accuracy: 0.724422770700637 

Epoch 13 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.16; acc: 0.66
Batch: 40; loss: 1.09; acc: 0.66
Batch: 60; loss: 1.01; acc: 0.73
Batch: 80; loss: 0.97; acc: 0.77
Batch: 100; loss: 1.04; acc: 0.75
Batch: 120; loss: 0.9; acc: 0.77
Batch: 140; loss: 1.0; acc: 0.72
Batch: 160; loss: 1.16; acc: 0.59
Batch: 180; loss: 0.99; acc: 0.72
Batch: 200; loss: 1.2; acc: 0.72
Batch: 220; loss: 0.99; acc: 0.72
Batch: 240; loss: 1.1; acc: 0.73
Batch: 260; loss: 0.92; acc: 0.72
Batch: 280; loss: 0.9; acc: 0.73
Batch: 300; loss: 1.0; acc: 0.75
Batch: 320; loss: 0.95; acc: 0.77
Batch: 340; loss: 1.06; acc: 0.72
Batch: 360; loss: 1.2; acc: 0.72
Batch: 380; loss: 0.92; acc: 0.78
Batch: 400; loss: 0.99; acc: 0.77
Batch: 420; loss: 1.16; acc: 0.7
Batch: 440; loss: 1.01; acc: 0.69
Batch: 460; loss: 0.89; acc: 0.78
Batch: 480; loss: 0.99; acc: 0.73
Batch: 500; loss: 1.0; acc: 0.7
Batch: 520; loss: 1.01; acc: 0.7
Batch: 540; loss: 1.11; acc: 0.58
Batch: 560; loss: 1.12; acc: 0.69
Batch: 580; loss: 1.09; acc: 0.66
Batch: 600; loss: 1.06; acc: 0.69
Batch: 620; loss: 1.11; acc: 0.69
Batch: 640; loss: 0.94; acc: 0.81
Batch: 660; loss: 1.06; acc: 0.72
Batch: 680; loss: 1.32; acc: 0.61
Batch: 700; loss: 0.95; acc: 0.75
Batch: 720; loss: 0.94; acc: 0.7
Batch: 740; loss: 0.93; acc: 0.78
Batch: 760; loss: 0.92; acc: 0.77
Batch: 780; loss: 1.06; acc: 0.7
Train Epoch over. train_loss: 1.05; train_accuracy: 0.71 

Batch: 0; loss: 1.03; acc: 0.75
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.93; acc: 0.72
Batch: 80; loss: 0.95; acc: 0.75
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.73
Batch: 140; loss: 0.8; acc: 0.83
Val Epoch over. val_loss: 1.0164561476677088; val_accuracy: 0.7293988853503185 

Epoch 14 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 1.08; acc: 0.69
Batch: 20; loss: 1.04; acc: 0.67
Batch: 40; loss: 0.97; acc: 0.81
Batch: 60; loss: 1.38; acc: 0.55
Batch: 80; loss: 0.89; acc: 0.77
Batch: 100; loss: 1.09; acc: 0.77
Batch: 120; loss: 1.05; acc: 0.69
Batch: 140; loss: 0.97; acc: 0.69
Batch: 160; loss: 1.07; acc: 0.69
Batch: 180; loss: 1.08; acc: 0.73
Batch: 200; loss: 1.05; acc: 0.69
Batch: 220; loss: 1.07; acc: 0.69
Batch: 240; loss: 1.08; acc: 0.66
Batch: 260; loss: 0.96; acc: 0.69
Batch: 280; loss: 0.88; acc: 0.77
Batch: 300; loss: 1.18; acc: 0.64
Batch: 320; loss: 1.08; acc: 0.7
Batch: 340; loss: 1.25; acc: 0.64
Batch: 360; loss: 0.97; acc: 0.81
Batch: 380; loss: 0.98; acc: 0.77
Batch: 400; loss: 0.92; acc: 0.77
Batch: 420; loss: 1.03; acc: 0.75
Batch: 440; loss: 0.96; acc: 0.72
Batch: 460; loss: 1.03; acc: 0.75
Batch: 480; loss: 1.2; acc: 0.69
Batch: 500; loss: 0.81; acc: 0.84
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.09; acc: 0.61
Batch: 560; loss: 0.95; acc: 0.75
Batch: 580; loss: 1.12; acc: 0.66
Batch: 600; loss: 0.96; acc: 0.73
Batch: 620; loss: 1.09; acc: 0.7
Batch: 640; loss: 1.08; acc: 0.72
Batch: 660; loss: 0.99; acc: 0.7
Batch: 680; loss: 1.0; acc: 0.72
Batch: 700; loss: 1.2; acc: 0.64
Batch: 720; loss: 0.82; acc: 0.78
Batch: 740; loss: 0.95; acc: 0.78
Batch: 760; loss: 1.06; acc: 0.7
Batch: 780; loss: 1.14; acc: 0.58
Train Epoch over. train_loss: 1.04; train_accuracy: 0.71 

Batch: 0; loss: 1.02; acc: 0.75
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 0.85; acc: 0.78
Batch: 60; loss: 0.93; acc: 0.72
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 0.99; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.73
Batch: 140; loss: 0.79; acc: 0.83
Val Epoch over. val_loss: 1.007282093831688; val_accuracy: 0.7303941082802548 

Epoch 15 start
The current lr is: 4.000000000000001e-05
Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 1.34; acc: 0.59
Batch: 40; loss: 0.96; acc: 0.77
Batch: 60; loss: 1.04; acc: 0.75
Batch: 80; loss: 1.06; acc: 0.66
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 1.1; acc: 0.64
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 1.0; acc: 0.69
Batch: 200; loss: 1.16; acc: 0.7
Batch: 220; loss: 0.97; acc: 0.72
Batch: 240; loss: 0.93; acc: 0.78
Batch: 260; loss: 1.09; acc: 0.61
Batch: 280; loss: 0.95; acc: 0.75
Batch: 300; loss: 1.06; acc: 0.66
Batch: 320; loss: 1.06; acc: 0.7
Batch: 340; loss: 0.98; acc: 0.77
Batch: 360; loss: 0.97; acc: 0.67
Batch: 380; loss: 1.04; acc: 0.75
Batch: 400; loss: 1.02; acc: 0.75
Batch: 420; loss: 1.13; acc: 0.67
Batch: 440; loss: 1.01; acc: 0.7
Batch: 460; loss: 1.16; acc: 0.64
Batch: 480; loss: 1.15; acc: 0.66
Batch: 500; loss: 1.06; acc: 0.69
Batch: 520; loss: 1.14; acc: 0.67
Batch: 540; loss: 0.81; acc: 0.78
Batch: 560; loss: 1.05; acc: 0.72
Batch: 580; loss: 1.12; acc: 0.61
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 1.19; acc: 0.72
Batch: 640; loss: 0.97; acc: 0.77
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 1.02; acc: 0.73
Batch: 700; loss: 1.1; acc: 0.73
Batch: 720; loss: 1.2; acc: 0.67
Batch: 740; loss: 0.97; acc: 0.78
Batch: 760; loss: 1.08; acc: 0.73
Batch: 780; loss: 1.3; acc: 0.64
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

Batch: 0; loss: 1.01; acc: 0.77
Batch: 20; loss: 1.15; acc: 0.64
Batch: 40; loss: 0.84; acc: 0.78
Batch: 60; loss: 0.92; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.98; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.78; acc: 0.83
Val Epoch over. val_loss: 0.9974062594638509; val_accuracy: 0.7333797770700637 

Epoch 16 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 1.04; acc: 0.77
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 1.05; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.77
Batch: 80; loss: 1.11; acc: 0.67
Batch: 100; loss: 0.91; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.59
Batch: 140; loss: 1.01; acc: 0.7
Batch: 160; loss: 1.04; acc: 0.72
Batch: 180; loss: 1.08; acc: 0.7
Batch: 200; loss: 0.83; acc: 0.88
Batch: 220; loss: 0.9; acc: 0.8
Batch: 240; loss: 1.03; acc: 0.69
Batch: 260; loss: 1.0; acc: 0.73
Batch: 280; loss: 0.98; acc: 0.69
Batch: 300; loss: 0.95; acc: 0.81
Batch: 320; loss: 1.17; acc: 0.58
Batch: 340; loss: 1.04; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.75
Batch: 380; loss: 1.1; acc: 0.72
Batch: 400; loss: 1.25; acc: 0.64
Batch: 420; loss: 1.02; acc: 0.72
Batch: 440; loss: 0.9; acc: 0.81
Batch: 460; loss: 0.99; acc: 0.7
Batch: 480; loss: 1.19; acc: 0.64
Batch: 500; loss: 1.01; acc: 0.8
Batch: 520; loss: 0.86; acc: 0.83
Batch: 540; loss: 1.0; acc: 0.67
Batch: 560; loss: 0.88; acc: 0.73
Batch: 580; loss: 1.08; acc: 0.7
Batch: 600; loss: 0.99; acc: 0.77
Batch: 620; loss: 1.14; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.77
Batch: 660; loss: 0.98; acc: 0.73
Batch: 680; loss: 0.86; acc: 0.7
Batch: 700; loss: 1.19; acc: 0.7
Batch: 720; loss: 0.98; acc: 0.73
Batch: 740; loss: 1.03; acc: 0.72
Batch: 760; loss: 0.98; acc: 0.75
Batch: 780; loss: 1.0; acc: 0.78
Train Epoch over. train_loss: 1.03; train_accuracy: 0.71 

Batch: 0; loss: 1.01; acc: 0.77
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.8
Batch: 60; loss: 0.92; acc: 0.72
Batch: 80; loss: 0.94; acc: 0.77
Batch: 100; loss: 0.98; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.9956582632793742; val_accuracy: 0.73328025477707 

Epoch 17 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.94; acc: 0.8
Batch: 20; loss: 1.12; acc: 0.75
Batch: 40; loss: 1.2; acc: 0.66
Batch: 60; loss: 1.07; acc: 0.73
Batch: 80; loss: 1.01; acc: 0.72
Batch: 100; loss: 1.19; acc: 0.7
Batch: 120; loss: 0.96; acc: 0.78
Batch: 140; loss: 1.12; acc: 0.61
Batch: 160; loss: 1.11; acc: 0.75
Batch: 180; loss: 1.11; acc: 0.58
Batch: 200; loss: 0.98; acc: 0.8
Batch: 220; loss: 1.32; acc: 0.64
Batch: 240; loss: 1.19; acc: 0.7
Batch: 260; loss: 1.23; acc: 0.62
Batch: 280; loss: 1.05; acc: 0.78
Batch: 300; loss: 1.09; acc: 0.7
Batch: 320; loss: 1.09; acc: 0.61
Batch: 340; loss: 0.98; acc: 0.69
Batch: 360; loss: 1.15; acc: 0.67
Batch: 380; loss: 0.89; acc: 0.77
Batch: 400; loss: 0.92; acc: 0.84
Batch: 420; loss: 1.19; acc: 0.62
Batch: 440; loss: 0.98; acc: 0.72
Batch: 460; loss: 1.2; acc: 0.62
Batch: 480; loss: 0.95; acc: 0.72
Batch: 500; loss: 0.94; acc: 0.77
Batch: 520; loss: 1.08; acc: 0.69
Batch: 540; loss: 1.12; acc: 0.64
Batch: 560; loss: 1.19; acc: 0.58
Batch: 580; loss: 1.0; acc: 0.7
Batch: 600; loss: 0.9; acc: 0.77
Batch: 620; loss: 1.16; acc: 0.64
Batch: 640; loss: 1.02; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.73
Batch: 680; loss: 1.02; acc: 0.77
Batch: 700; loss: 1.01; acc: 0.83
Batch: 720; loss: 1.1; acc: 0.64
Batch: 740; loss: 1.0; acc: 0.72
Batch: 760; loss: 1.08; acc: 0.69
Batch: 780; loss: 1.1; acc: 0.62
Train Epoch over. train_loss: 1.03; train_accuracy: 0.72 

Batch: 0; loss: 1.01; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.98; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.993793963247044; val_accuracy: 0.7339769108280255 

Epoch 18 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 1.18; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.75
Batch: 40; loss: 1.03; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.7
Batch: 80; loss: 0.99; acc: 0.72
Batch: 100; loss: 1.19; acc: 0.66
Batch: 120; loss: 0.93; acc: 0.77
Batch: 140; loss: 0.95; acc: 0.78
Batch: 160; loss: 1.03; acc: 0.66
Batch: 180; loss: 1.09; acc: 0.66
Batch: 200; loss: 1.12; acc: 0.64
Batch: 220; loss: 0.99; acc: 0.66
Batch: 240; loss: 1.26; acc: 0.55
Batch: 260; loss: 1.0; acc: 0.73
Batch: 280; loss: 1.25; acc: 0.66
Batch: 300; loss: 0.99; acc: 0.64
Batch: 320; loss: 1.11; acc: 0.7
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 1.02; acc: 0.67
Batch: 380; loss: 0.9; acc: 0.72
Batch: 400; loss: 1.08; acc: 0.73
Batch: 420; loss: 1.07; acc: 0.69
Batch: 440; loss: 0.91; acc: 0.77
Batch: 460; loss: 0.98; acc: 0.77
Batch: 480; loss: 1.07; acc: 0.67
Batch: 500; loss: 0.86; acc: 0.84
Batch: 520; loss: 1.09; acc: 0.72
Batch: 540; loss: 0.86; acc: 0.81
Batch: 560; loss: 1.13; acc: 0.67
Batch: 580; loss: 1.12; acc: 0.62
Batch: 600; loss: 0.94; acc: 0.77
Batch: 620; loss: 1.17; acc: 0.64
Batch: 640; loss: 1.07; acc: 0.72
Batch: 660; loss: 1.06; acc: 0.75
Batch: 680; loss: 1.09; acc: 0.72
Batch: 700; loss: 1.22; acc: 0.67
Batch: 720; loss: 0.83; acc: 0.77
Batch: 740; loss: 1.04; acc: 0.67
Batch: 760; loss: 1.03; acc: 0.72
Batch: 780; loss: 0.91; acc: 0.78
Train Epoch over. train_loss: 1.03; train_accuracy: 0.72 

Batch: 0; loss: 1.01; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.7
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.98; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.9919148953097641; val_accuracy: 0.7340764331210191 

Epoch 19 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.99; acc: 0.72
Batch: 20; loss: 1.06; acc: 0.66
Batch: 40; loss: 1.19; acc: 0.61
Batch: 60; loss: 1.03; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 0.91; acc: 0.77
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 1.04; acc: 0.72
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 1.12; acc: 0.73
Batch: 200; loss: 1.01; acc: 0.72
Batch: 220; loss: 1.0; acc: 0.72
Batch: 240; loss: 0.89; acc: 0.8
Batch: 260; loss: 1.04; acc: 0.69
Batch: 280; loss: 1.05; acc: 0.67
Batch: 300; loss: 0.95; acc: 0.7
Batch: 320; loss: 1.02; acc: 0.72
Batch: 340; loss: 1.22; acc: 0.64
Batch: 360; loss: 1.12; acc: 0.67
Batch: 380; loss: 1.01; acc: 0.75
Batch: 400; loss: 1.21; acc: 0.64
Batch: 420; loss: 1.07; acc: 0.67
Batch: 440; loss: 0.93; acc: 0.75
Batch: 460; loss: 1.01; acc: 0.67
Batch: 480; loss: 1.09; acc: 0.72
Batch: 500; loss: 1.01; acc: 0.64
Batch: 520; loss: 1.04; acc: 0.64
Batch: 540; loss: 1.09; acc: 0.75
Batch: 560; loss: 0.98; acc: 0.7
Batch: 580; loss: 1.08; acc: 0.7
Batch: 600; loss: 0.95; acc: 0.77
Batch: 620; loss: 1.09; acc: 0.73
Batch: 640; loss: 1.0; acc: 0.64
Batch: 660; loss: 1.07; acc: 0.69
Batch: 680; loss: 0.99; acc: 0.7
Batch: 700; loss: 1.04; acc: 0.75
Batch: 720; loss: 1.02; acc: 0.72
Batch: 740; loss: 0.9; acc: 0.75
Batch: 760; loss: 1.01; acc: 0.69
Batch: 780; loss: 0.99; acc: 0.77
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.01; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.84; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.98; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.72
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.9900404526169893; val_accuracy: 0.7347730891719745 

Epoch 20 start
The current lr is: 8.000000000000001e-06
Batch: 0; loss: 0.94; acc: 0.73
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 1.04; acc: 0.69
Batch: 60; loss: 1.09; acc: 0.7
Batch: 80; loss: 1.12; acc: 0.66
Batch: 100; loss: 1.07; acc: 0.66
Batch: 120; loss: 1.11; acc: 0.66
Batch: 140; loss: 0.86; acc: 0.73
Batch: 160; loss: 0.89; acc: 0.8
Batch: 180; loss: 1.0; acc: 0.72
Batch: 200; loss: 0.89; acc: 0.77
Batch: 220; loss: 0.88; acc: 0.78
Batch: 240; loss: 1.13; acc: 0.64
Batch: 260; loss: 1.14; acc: 0.62
Batch: 280; loss: 0.83; acc: 0.8
Batch: 300; loss: 1.02; acc: 0.72
Batch: 320; loss: 0.98; acc: 0.73
Batch: 340; loss: 1.19; acc: 0.61
Batch: 360; loss: 1.08; acc: 0.73
Batch: 380; loss: 0.99; acc: 0.73
Batch: 400; loss: 0.96; acc: 0.8
Batch: 420; loss: 1.04; acc: 0.72
Batch: 440; loss: 1.0; acc: 0.73
Batch: 460; loss: 0.97; acc: 0.8
Batch: 480; loss: 1.03; acc: 0.66
Batch: 500; loss: 0.84; acc: 0.78
Batch: 520; loss: 1.0; acc: 0.7
Batch: 540; loss: 1.15; acc: 0.7
Batch: 560; loss: 1.25; acc: 0.64
Batch: 580; loss: 1.13; acc: 0.72
Batch: 600; loss: 1.09; acc: 0.69
Batch: 620; loss: 1.15; acc: 0.64
Batch: 640; loss: 0.96; acc: 0.72
Batch: 660; loss: 0.85; acc: 0.84
Batch: 680; loss: 1.09; acc: 0.77
Batch: 700; loss: 1.02; acc: 0.69
Batch: 720; loss: 1.17; acc: 0.58
Batch: 740; loss: 0.98; acc: 0.66
Batch: 760; loss: 1.02; acc: 0.7
Batch: 780; loss: 1.32; acc: 0.67
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.9880923606028222; val_accuracy: 0.7352707006369427 

Epoch 21 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 0.99; acc: 0.8
Batch: 40; loss: 1.16; acc: 0.66
Batch: 60; loss: 0.86; acc: 0.81
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.05; acc: 0.7
Batch: 120; loss: 1.03; acc: 0.73
Batch: 140; loss: 1.07; acc: 0.69
Batch: 160; loss: 0.89; acc: 0.83
Batch: 180; loss: 1.21; acc: 0.66
Batch: 200; loss: 0.86; acc: 0.81
Batch: 220; loss: 0.98; acc: 0.75
Batch: 240; loss: 0.86; acc: 0.78
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 1.1; acc: 0.64
Batch: 300; loss: 0.91; acc: 0.77
Batch: 320; loss: 1.11; acc: 0.73
Batch: 340; loss: 0.98; acc: 0.73
Batch: 360; loss: 0.95; acc: 0.77
Batch: 380; loss: 1.03; acc: 0.67
Batch: 400; loss: 0.8; acc: 0.84
Batch: 420; loss: 1.05; acc: 0.72
Batch: 440; loss: 1.02; acc: 0.75
Batch: 460; loss: 1.15; acc: 0.69
Batch: 480; loss: 1.15; acc: 0.69
Batch: 500; loss: 1.02; acc: 0.75
Batch: 520; loss: 0.95; acc: 0.75
Batch: 540; loss: 1.18; acc: 0.64
Batch: 560; loss: 0.99; acc: 0.8
Batch: 580; loss: 0.94; acc: 0.8
Batch: 600; loss: 1.13; acc: 0.62
Batch: 620; loss: 1.14; acc: 0.75
Batch: 640; loss: 1.02; acc: 0.7
Batch: 660; loss: 1.05; acc: 0.69
Batch: 680; loss: 0.91; acc: 0.75
Batch: 700; loss: 0.92; acc: 0.75
Batch: 720; loss: 0.9; acc: 0.77
Batch: 740; loss: 0.99; acc: 0.77
Batch: 760; loss: 1.02; acc: 0.7
Batch: 780; loss: 1.21; acc: 0.61
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.9877317502240467; val_accuracy: 0.73546974522293 

Epoch 22 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 1.23; acc: 0.66
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 1.01; acc: 0.66
Batch: 60; loss: 0.99; acc: 0.72
Batch: 80; loss: 1.23; acc: 0.64
Batch: 100; loss: 1.19; acc: 0.61
Batch: 120; loss: 1.38; acc: 0.56
Batch: 140; loss: 0.96; acc: 0.73
Batch: 160; loss: 1.1; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.7
Batch: 200; loss: 1.25; acc: 0.66
Batch: 220; loss: 1.15; acc: 0.66
Batch: 240; loss: 1.01; acc: 0.81
Batch: 260; loss: 1.04; acc: 0.67
Batch: 280; loss: 0.91; acc: 0.75
Batch: 300; loss: 1.11; acc: 0.64
Batch: 320; loss: 1.03; acc: 0.69
Batch: 340; loss: 0.95; acc: 0.78
Batch: 360; loss: 0.98; acc: 0.72
Batch: 380; loss: 0.96; acc: 0.81
Batch: 400; loss: 1.1; acc: 0.7
Batch: 420; loss: 1.15; acc: 0.69
Batch: 440; loss: 1.11; acc: 0.62
Batch: 460; loss: 0.95; acc: 0.73
Batch: 480; loss: 0.98; acc: 0.81
Batch: 500; loss: 0.94; acc: 0.66
Batch: 520; loss: 0.89; acc: 0.8
Batch: 540; loss: 1.02; acc: 0.69
Batch: 560; loss: 1.09; acc: 0.64
Batch: 580; loss: 0.98; acc: 0.77
Batch: 600; loss: 0.96; acc: 0.75
Batch: 620; loss: 1.03; acc: 0.72
Batch: 640; loss: 1.03; acc: 0.69
Batch: 660; loss: 1.16; acc: 0.7
Batch: 680; loss: 0.93; acc: 0.78
Batch: 700; loss: 1.04; acc: 0.75
Batch: 720; loss: 1.01; acc: 0.75
Batch: 740; loss: 1.06; acc: 0.73
Batch: 760; loss: 0.97; acc: 0.77
Batch: 780; loss: 0.81; acc: 0.73
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.77; acc: 0.83
Val Epoch over. val_loss: 0.9873692799525656; val_accuracy: 0.7353702229299363 

Epoch 23 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.99; acc: 0.7
Batch: 20; loss: 0.92; acc: 0.78
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 1.01; acc: 0.8
Batch: 80; loss: 0.9; acc: 0.75
Batch: 100; loss: 0.92; acc: 0.72
Batch: 120; loss: 1.09; acc: 0.7
Batch: 140; loss: 1.07; acc: 0.72
Batch: 160; loss: 1.12; acc: 0.64
Batch: 180; loss: 1.07; acc: 0.77
Batch: 200; loss: 1.1; acc: 0.7
Batch: 220; loss: 1.3; acc: 0.61
Batch: 240; loss: 1.07; acc: 0.73
Batch: 260; loss: 0.99; acc: 0.72
Batch: 280; loss: 0.93; acc: 0.83
Batch: 300; loss: 0.92; acc: 0.78
Batch: 320; loss: 1.09; acc: 0.59
Batch: 340; loss: 0.93; acc: 0.75
Batch: 360; loss: 0.98; acc: 0.77
Batch: 380; loss: 1.03; acc: 0.8
Batch: 400; loss: 1.2; acc: 0.64
Batch: 420; loss: 1.0; acc: 0.77
Batch: 440; loss: 0.97; acc: 0.78
Batch: 460; loss: 0.94; acc: 0.75
Batch: 480; loss: 0.97; acc: 0.73
Batch: 500; loss: 0.98; acc: 0.66
Batch: 520; loss: 0.91; acc: 0.75
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.16; acc: 0.69
Batch: 580; loss: 1.0; acc: 0.73
Batch: 600; loss: 0.86; acc: 0.78
Batch: 620; loss: 1.07; acc: 0.62
Batch: 640; loss: 1.11; acc: 0.72
Batch: 660; loss: 0.97; acc: 0.73
Batch: 680; loss: 0.9; acc: 0.75
Batch: 700; loss: 1.01; acc: 0.72
Batch: 720; loss: 1.05; acc: 0.73
Batch: 740; loss: 1.04; acc: 0.7
Batch: 760; loss: 1.02; acc: 0.7
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9870034797935728; val_accuracy: 0.7352707006369427 

Epoch 24 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 1.04; acc: 0.73
Batch: 60; loss: 1.13; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.66
Batch: 100; loss: 1.1; acc: 0.67
Batch: 120; loss: 0.97; acc: 0.73
Batch: 140; loss: 1.11; acc: 0.67
Batch: 160; loss: 1.26; acc: 0.66
Batch: 180; loss: 0.92; acc: 0.73
Batch: 200; loss: 1.03; acc: 0.7
Batch: 220; loss: 0.89; acc: 0.8
Batch: 240; loss: 0.91; acc: 0.8
Batch: 260; loss: 0.91; acc: 0.72
Batch: 280; loss: 0.95; acc: 0.72
Batch: 300; loss: 1.16; acc: 0.67
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 1.05; acc: 0.66
Batch: 380; loss: 1.06; acc: 0.77
Batch: 400; loss: 0.94; acc: 0.75
Batch: 420; loss: 0.91; acc: 0.7
Batch: 440; loss: 1.16; acc: 0.59
Batch: 460; loss: 1.09; acc: 0.73
Batch: 480; loss: 0.92; acc: 0.75
Batch: 500; loss: 0.91; acc: 0.7
Batch: 520; loss: 0.86; acc: 0.8
Batch: 540; loss: 1.12; acc: 0.59
Batch: 560; loss: 1.12; acc: 0.69
Batch: 580; loss: 0.83; acc: 0.77
Batch: 600; loss: 0.92; acc: 0.73
Batch: 620; loss: 0.97; acc: 0.78
Batch: 640; loss: 1.0; acc: 0.69
Batch: 660; loss: 1.15; acc: 0.72
Batch: 680; loss: 0.99; acc: 0.72
Batch: 700; loss: 1.16; acc: 0.67
Batch: 720; loss: 0.91; acc: 0.73
Batch: 740; loss: 1.24; acc: 0.64
Batch: 760; loss: 1.09; acc: 0.73
Batch: 780; loss: 1.13; acc: 0.7
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9866381553327961; val_accuracy: 0.7355692675159236 

Epoch 25 start
The current lr is: 1.6000000000000004e-06
Batch: 0; loss: 0.95; acc: 0.72
Batch: 20; loss: 1.03; acc: 0.72
Batch: 40; loss: 0.93; acc: 0.73
Batch: 60; loss: 0.91; acc: 0.73
Batch: 80; loss: 1.11; acc: 0.75
Batch: 100; loss: 1.08; acc: 0.56
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 1.01; acc: 0.73
Batch: 160; loss: 1.03; acc: 0.69
Batch: 180; loss: 0.9; acc: 0.78
Batch: 200; loss: 1.04; acc: 0.66
Batch: 220; loss: 1.0; acc: 0.72
Batch: 240; loss: 0.95; acc: 0.78
Batch: 260; loss: 0.94; acc: 0.75
Batch: 280; loss: 1.28; acc: 0.64
Batch: 300; loss: 0.85; acc: 0.8
Batch: 320; loss: 1.18; acc: 0.66
Batch: 340; loss: 1.2; acc: 0.66
Batch: 360; loss: 0.98; acc: 0.69
Batch: 380; loss: 0.99; acc: 0.73
Batch: 400; loss: 1.0; acc: 0.77
Batch: 420; loss: 0.85; acc: 0.77
Batch: 440; loss: 0.75; acc: 0.86
Batch: 460; loss: 1.1; acc: 0.72
Batch: 480; loss: 0.93; acc: 0.77
Batch: 500; loss: 1.01; acc: 0.73
Batch: 520; loss: 1.01; acc: 0.69
Batch: 540; loss: 0.87; acc: 0.77
Batch: 560; loss: 1.2; acc: 0.67
Batch: 580; loss: 0.89; acc: 0.77
Batch: 600; loss: 1.13; acc: 0.72
Batch: 620; loss: 0.94; acc: 0.75
Batch: 640; loss: 0.79; acc: 0.8
Batch: 660; loss: 1.01; acc: 0.73
Batch: 680; loss: 0.88; acc: 0.75
Batch: 700; loss: 1.08; acc: 0.62
Batch: 720; loss: 1.16; acc: 0.61
Batch: 740; loss: 1.13; acc: 0.67
Batch: 760; loss: 0.98; acc: 0.73
Batch: 780; loss: 0.99; acc: 0.72
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9862693810159233; val_accuracy: 0.7357683121019108 

Epoch 26 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.14; acc: 0.59
Batch: 40; loss: 1.04; acc: 0.67
Batch: 60; loss: 1.0; acc: 0.69
Batch: 80; loss: 0.92; acc: 0.73
Batch: 100; loss: 0.86; acc: 0.84
Batch: 120; loss: 1.05; acc: 0.67
Batch: 140; loss: 0.89; acc: 0.77
Batch: 160; loss: 1.33; acc: 0.59
Batch: 180; loss: 1.03; acc: 0.66
Batch: 200; loss: 0.9; acc: 0.83
Batch: 220; loss: 1.19; acc: 0.64
Batch: 240; loss: 1.09; acc: 0.7
Batch: 260; loss: 0.95; acc: 0.75
Batch: 280; loss: 1.04; acc: 0.69
Batch: 300; loss: 1.09; acc: 0.66
Batch: 320; loss: 1.02; acc: 0.69
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 0.96; acc: 0.73
Batch: 380; loss: 0.81; acc: 0.77
Batch: 400; loss: 0.99; acc: 0.7
Batch: 420; loss: 1.01; acc: 0.69
Batch: 440; loss: 0.94; acc: 0.8
Batch: 460; loss: 0.86; acc: 0.77
Batch: 480; loss: 1.06; acc: 0.77
Batch: 500; loss: 1.25; acc: 0.7
Batch: 520; loss: 1.01; acc: 0.75
Batch: 540; loss: 0.86; acc: 0.81
Batch: 560; loss: 0.94; acc: 0.72
Batch: 580; loss: 0.99; acc: 0.73
Batch: 600; loss: 1.18; acc: 0.7
Batch: 620; loss: 1.0; acc: 0.7
Batch: 640; loss: 1.06; acc: 0.72
Batch: 660; loss: 0.95; acc: 0.62
Batch: 680; loss: 0.85; acc: 0.75
Batch: 700; loss: 0.93; acc: 0.75
Batch: 720; loss: 1.03; acc: 0.81
Batch: 740; loss: 1.11; acc: 0.72
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9862064168711376; val_accuracy: 0.7358678343949044 

Epoch 27 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.2; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.61
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.04; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.73
Batch: 120; loss: 1.01; acc: 0.77
Batch: 140; loss: 1.02; acc: 0.72
Batch: 160; loss: 1.07; acc: 0.78
Batch: 180; loss: 1.04; acc: 0.7
Batch: 200; loss: 0.95; acc: 0.69
Batch: 220; loss: 0.85; acc: 0.78
Batch: 240; loss: 0.78; acc: 0.84
Batch: 260; loss: 0.91; acc: 0.8
Batch: 280; loss: 1.23; acc: 0.62
Batch: 300; loss: 1.12; acc: 0.69
Batch: 320; loss: 1.26; acc: 0.56
Batch: 340; loss: 0.96; acc: 0.8
Batch: 360; loss: 0.98; acc: 0.73
Batch: 380; loss: 0.97; acc: 0.69
Batch: 400; loss: 1.11; acc: 0.67
Batch: 420; loss: 1.04; acc: 0.7
Batch: 440; loss: 1.01; acc: 0.75
Batch: 460; loss: 0.96; acc: 0.81
Batch: 480; loss: 1.03; acc: 0.73
Batch: 500; loss: 1.16; acc: 0.62
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.04; acc: 0.73
Batch: 560; loss: 1.02; acc: 0.72
Batch: 580; loss: 0.98; acc: 0.72
Batch: 600; loss: 0.88; acc: 0.84
Batch: 620; loss: 1.04; acc: 0.67
Batch: 640; loss: 1.17; acc: 0.64
Batch: 660; loss: 1.05; acc: 0.64
Batch: 680; loss: 1.05; acc: 0.7
Batch: 700; loss: 0.88; acc: 0.78
Batch: 720; loss: 0.94; acc: 0.78
Batch: 740; loss: 1.07; acc: 0.64
Batch: 760; loss: 1.02; acc: 0.77
Batch: 780; loss: 1.15; acc: 0.62
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9861425685275133; val_accuracy: 0.7358678343949044 

Epoch 28 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 1.07; acc: 0.73
Batch: 20; loss: 1.02; acc: 0.66
Batch: 40; loss: 1.02; acc: 0.67
Batch: 60; loss: 1.04; acc: 0.7
Batch: 80; loss: 0.93; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.72
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.98; acc: 0.78
Batch: 160; loss: 1.01; acc: 0.72
Batch: 180; loss: 0.98; acc: 0.72
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 1.2; acc: 0.59
Batch: 240; loss: 1.01; acc: 0.7
Batch: 260; loss: 0.93; acc: 0.75
Batch: 280; loss: 1.27; acc: 0.69
Batch: 300; loss: 0.99; acc: 0.73
Batch: 320; loss: 1.0; acc: 0.7
Batch: 340; loss: 0.95; acc: 0.8
Batch: 360; loss: 1.01; acc: 0.75
Batch: 380; loss: 1.1; acc: 0.67
Batch: 400; loss: 1.03; acc: 0.72
Batch: 420; loss: 1.13; acc: 0.78
Batch: 440; loss: 1.1; acc: 0.67
Batch: 460; loss: 1.02; acc: 0.75
Batch: 480; loss: 0.99; acc: 0.78
Batch: 500; loss: 0.91; acc: 0.78
Batch: 520; loss: 1.14; acc: 0.64
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 1.05; acc: 0.8
Batch: 580; loss: 1.04; acc: 0.69
Batch: 600; loss: 0.82; acc: 0.78
Batch: 620; loss: 1.22; acc: 0.62
Batch: 640; loss: 0.87; acc: 0.78
Batch: 660; loss: 0.99; acc: 0.75
Batch: 680; loss: 1.07; acc: 0.75
Batch: 700; loss: 1.26; acc: 0.56
Batch: 720; loss: 0.87; acc: 0.83
Batch: 740; loss: 0.98; acc: 0.75
Batch: 760; loss: 0.93; acc: 0.81
Batch: 780; loss: 0.92; acc: 0.73
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9860788553383699; val_accuracy: 0.7359673566878981 

Epoch 29 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 1.27; acc: 0.62
Batch: 20; loss: 1.06; acc: 0.67
Batch: 40; loss: 1.15; acc: 0.69
Batch: 60; loss: 0.92; acc: 0.81
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 1.03; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.7
Batch: 140; loss: 1.02; acc: 0.7
Batch: 160; loss: 1.06; acc: 0.72
Batch: 180; loss: 0.86; acc: 0.78
Batch: 200; loss: 1.08; acc: 0.75
Batch: 220; loss: 1.08; acc: 0.7
Batch: 240; loss: 1.27; acc: 0.58
Batch: 260; loss: 0.92; acc: 0.78
Batch: 280; loss: 1.15; acc: 0.64
Batch: 300; loss: 1.06; acc: 0.75
Batch: 320; loss: 0.93; acc: 0.78
Batch: 340; loss: 0.89; acc: 0.81
Batch: 360; loss: 0.84; acc: 0.81
Batch: 380; loss: 0.94; acc: 0.81
Batch: 400; loss: 0.99; acc: 0.73
Batch: 420; loss: 0.94; acc: 0.78
Batch: 440; loss: 1.11; acc: 0.62
Batch: 460; loss: 1.0; acc: 0.72
Batch: 480; loss: 1.06; acc: 0.67
Batch: 500; loss: 0.85; acc: 0.8
Batch: 520; loss: 1.11; acc: 0.64
Batch: 540; loss: 1.13; acc: 0.66
Batch: 560; loss: 0.97; acc: 0.7
Batch: 580; loss: 1.15; acc: 0.66
Batch: 600; loss: 0.83; acc: 0.77
Batch: 620; loss: 0.94; acc: 0.77
Batch: 640; loss: 0.88; acc: 0.73
Batch: 660; loss: 1.15; acc: 0.72
Batch: 680; loss: 0.99; acc: 0.73
Batch: 700; loss: 0.97; acc: 0.69
Batch: 720; loss: 0.92; acc: 0.77
Batch: 740; loss: 0.99; acc: 0.72
Batch: 760; loss: 1.13; acc: 0.7
Batch: 780; loss: 0.97; acc: 0.72
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9860157226301303; val_accuracy: 0.7359673566878981 

Epoch 30 start
The current lr is: 3.200000000000001e-07
Batch: 0; loss: 1.01; acc: 0.69
Batch: 20; loss: 1.05; acc: 0.64
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 1.05; acc: 0.67
Batch: 80; loss: 0.93; acc: 0.75
Batch: 100; loss: 0.95; acc: 0.75
Batch: 120; loss: 1.1; acc: 0.72
Batch: 140; loss: 1.17; acc: 0.7
Batch: 160; loss: 0.87; acc: 0.81
Batch: 180; loss: 0.99; acc: 0.72
Batch: 200; loss: 0.95; acc: 0.7
Batch: 220; loss: 1.01; acc: 0.67
Batch: 240; loss: 0.99; acc: 0.75
Batch: 260; loss: 1.05; acc: 0.69
Batch: 280; loss: 1.05; acc: 0.7
Batch: 300; loss: 1.11; acc: 0.66
Batch: 320; loss: 1.05; acc: 0.67
Batch: 340; loss: 1.06; acc: 0.73
Batch: 360; loss: 1.02; acc: 0.69
Batch: 380; loss: 1.12; acc: 0.67
Batch: 400; loss: 0.96; acc: 0.64
Batch: 420; loss: 1.0; acc: 0.7
Batch: 440; loss: 1.08; acc: 0.73
Batch: 460; loss: 1.02; acc: 0.7
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 1.17; acc: 0.61
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.17; acc: 0.69
Batch: 560; loss: 1.0; acc: 0.75
Batch: 580; loss: 1.01; acc: 0.69
Batch: 600; loss: 1.0; acc: 0.67
Batch: 620; loss: 1.03; acc: 0.73
Batch: 640; loss: 1.11; acc: 0.64
Batch: 660; loss: 0.93; acc: 0.8
Batch: 680; loss: 1.07; acc: 0.61
Batch: 700; loss: 0.96; acc: 0.66
Batch: 720; loss: 1.06; acc: 0.64
Batch: 740; loss: 0.95; acc: 0.8
Batch: 760; loss: 0.93; acc: 0.69
Batch: 780; loss: 0.99; acc: 0.72
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859527600039343; val_accuracy: 0.7359673566878981 

Epoch 31 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.91; acc: 0.77
Batch: 20; loss: 1.08; acc: 0.73
Batch: 40; loss: 1.08; acc: 0.7
Batch: 60; loss: 0.92; acc: 0.73
Batch: 80; loss: 1.2; acc: 0.64
Batch: 100; loss: 0.98; acc: 0.72
Batch: 120; loss: 1.08; acc: 0.69
Batch: 140; loss: 0.98; acc: 0.75
Batch: 160; loss: 1.12; acc: 0.66
Batch: 180; loss: 0.97; acc: 0.67
Batch: 200; loss: 1.08; acc: 0.7
Batch: 220; loss: 0.98; acc: 0.72
Batch: 240; loss: 0.93; acc: 0.73
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 1.04; acc: 0.66
Batch: 300; loss: 1.03; acc: 0.77
Batch: 320; loss: 0.86; acc: 0.88
Batch: 340; loss: 0.93; acc: 0.78
Batch: 360; loss: 0.91; acc: 0.78
Batch: 380; loss: 1.12; acc: 0.62
Batch: 400; loss: 1.29; acc: 0.62
Batch: 420; loss: 1.09; acc: 0.67
Batch: 440; loss: 1.1; acc: 0.73
Batch: 460; loss: 1.03; acc: 0.73
Batch: 480; loss: 1.07; acc: 0.75
Batch: 500; loss: 1.04; acc: 0.7
Batch: 520; loss: 0.9; acc: 0.72
Batch: 540; loss: 0.92; acc: 0.78
Batch: 560; loss: 0.89; acc: 0.69
Batch: 580; loss: 1.03; acc: 0.73
Batch: 600; loss: 1.04; acc: 0.72
Batch: 620; loss: 1.1; acc: 0.69
Batch: 640; loss: 0.92; acc: 0.7
Batch: 660; loss: 0.99; acc: 0.69
Batch: 680; loss: 1.15; acc: 0.67
Batch: 700; loss: 0.94; acc: 0.8
Batch: 720; loss: 1.09; acc: 0.73
Batch: 740; loss: 1.0; acc: 0.75
Batch: 760; loss: 1.22; acc: 0.61
Batch: 780; loss: 1.12; acc: 0.69
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859451712316768; val_accuracy: 0.7359673566878981 

Epoch 32 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 1.06; acc: 0.69
Batch: 20; loss: 0.96; acc: 0.73
Batch: 40; loss: 0.92; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.66
Batch: 80; loss: 1.17; acc: 0.66
Batch: 100; loss: 1.19; acc: 0.69
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 1.13; acc: 0.64
Batch: 160; loss: 0.93; acc: 0.73
Batch: 180; loss: 0.73; acc: 0.84
Batch: 200; loss: 0.82; acc: 0.81
Batch: 220; loss: 0.88; acc: 0.83
Batch: 240; loss: 0.97; acc: 0.77
Batch: 260; loss: 1.11; acc: 0.69
Batch: 280; loss: 1.02; acc: 0.7
Batch: 300; loss: 1.04; acc: 0.7
Batch: 320; loss: 1.09; acc: 0.7
Batch: 340; loss: 1.04; acc: 0.69
Batch: 360; loss: 0.9; acc: 0.73
Batch: 380; loss: 1.03; acc: 0.72
Batch: 400; loss: 0.95; acc: 0.67
Batch: 420; loss: 0.9; acc: 0.81
Batch: 440; loss: 1.02; acc: 0.7
Batch: 460; loss: 0.94; acc: 0.77
Batch: 480; loss: 1.0; acc: 0.73
Batch: 500; loss: 1.0; acc: 0.73
Batch: 520; loss: 0.85; acc: 0.77
Batch: 540; loss: 1.07; acc: 0.69
Batch: 560; loss: 1.09; acc: 0.75
Batch: 580; loss: 1.06; acc: 0.72
Batch: 600; loss: 1.09; acc: 0.67
Batch: 620; loss: 1.11; acc: 0.72
Batch: 640; loss: 1.26; acc: 0.58
Batch: 660; loss: 0.9; acc: 0.77
Batch: 680; loss: 0.93; acc: 0.75
Batch: 700; loss: 1.01; acc: 0.7
Batch: 720; loss: 1.02; acc: 0.72
Batch: 740; loss: 1.04; acc: 0.75
Batch: 760; loss: 1.05; acc: 0.7
Batch: 780; loss: 1.11; acc: 0.72
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.985937446165996; val_accuracy: 0.7359673566878981 

Epoch 33 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 1.13; acc: 0.69
Batch: 20; loss: 1.0; acc: 0.78
Batch: 40; loss: 1.11; acc: 0.67
Batch: 60; loss: 0.95; acc: 0.77
Batch: 80; loss: 0.89; acc: 0.78
Batch: 100; loss: 1.08; acc: 0.69
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 0.89; acc: 0.8
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 1.05; acc: 0.66
Batch: 200; loss: 1.04; acc: 0.77
Batch: 220; loss: 0.92; acc: 0.81
Batch: 240; loss: 1.08; acc: 0.78
Batch: 260; loss: 1.14; acc: 0.73
Batch: 280; loss: 0.95; acc: 0.8
Batch: 300; loss: 1.1; acc: 0.67
Batch: 320; loss: 1.17; acc: 0.66
Batch: 340; loss: 1.17; acc: 0.7
Batch: 360; loss: 1.1; acc: 0.66
Batch: 380; loss: 0.98; acc: 0.75
Batch: 400; loss: 0.97; acc: 0.75
Batch: 420; loss: 1.02; acc: 0.72
Batch: 440; loss: 1.11; acc: 0.72
Batch: 460; loss: 0.94; acc: 0.77
Batch: 480; loss: 0.96; acc: 0.73
Batch: 500; loss: 0.92; acc: 0.8
Batch: 520; loss: 0.95; acc: 0.75
Batch: 540; loss: 0.89; acc: 0.77
Batch: 560; loss: 0.91; acc: 0.8
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 1.07; acc: 0.7
Batch: 620; loss: 0.96; acc: 0.77
Batch: 640; loss: 1.05; acc: 0.73
Batch: 660; loss: 0.97; acc: 0.75
Batch: 680; loss: 0.84; acc: 0.81
Batch: 700; loss: 1.05; acc: 0.78
Batch: 720; loss: 0.97; acc: 0.72
Batch: 740; loss: 1.08; acc: 0.73
Batch: 760; loss: 1.1; acc: 0.67
Batch: 780; loss: 0.86; acc: 0.8
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859297871589661; val_accuracy: 0.7359673566878981 

Epoch 34 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.97; acc: 0.77
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 1.05; acc: 0.69
Batch: 60; loss: 0.85; acc: 0.78
Batch: 80; loss: 1.06; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.75
Batch: 120; loss: 1.02; acc: 0.73
Batch: 140; loss: 0.99; acc: 0.77
Batch: 160; loss: 0.82; acc: 0.8
Batch: 180; loss: 0.88; acc: 0.75
Batch: 200; loss: 1.13; acc: 0.64
Batch: 220; loss: 0.88; acc: 0.7
Batch: 240; loss: 0.93; acc: 0.7
Batch: 260; loss: 1.03; acc: 0.7
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 1.04; acc: 0.78
Batch: 320; loss: 0.93; acc: 0.78
Batch: 340; loss: 1.01; acc: 0.73
Batch: 360; loss: 1.08; acc: 0.7
Batch: 380; loss: 1.14; acc: 0.58
Batch: 400; loss: 1.01; acc: 0.72
Batch: 420; loss: 0.88; acc: 0.81
Batch: 440; loss: 1.03; acc: 0.66
Batch: 460; loss: 1.01; acc: 0.69
Batch: 480; loss: 1.08; acc: 0.67
Batch: 500; loss: 0.78; acc: 0.77
Batch: 520; loss: 1.09; acc: 0.75
Batch: 540; loss: 1.28; acc: 0.66
Batch: 560; loss: 0.81; acc: 0.81
Batch: 580; loss: 1.02; acc: 0.72
Batch: 600; loss: 1.14; acc: 0.67
Batch: 620; loss: 1.19; acc: 0.61
Batch: 640; loss: 1.05; acc: 0.7
Batch: 660; loss: 0.91; acc: 0.77
Batch: 680; loss: 0.82; acc: 0.84
Batch: 700; loss: 1.26; acc: 0.61
Batch: 720; loss: 1.06; acc: 0.77
Batch: 740; loss: 1.02; acc: 0.7
Batch: 760; loss: 0.96; acc: 0.75
Batch: 780; loss: 1.22; acc: 0.62
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859221551069028; val_accuracy: 0.7359673566878981 

Epoch 35 start
The current lr is: 6.400000000000003e-08
Batch: 0; loss: 0.92; acc: 0.7
Batch: 20; loss: 0.97; acc: 0.73
Batch: 40; loss: 1.04; acc: 0.7
Batch: 60; loss: 1.09; acc: 0.69
Batch: 80; loss: 1.07; acc: 0.77
Batch: 100; loss: 1.01; acc: 0.73
Batch: 120; loss: 1.04; acc: 0.69
Batch: 140; loss: 1.2; acc: 0.64
Batch: 160; loss: 0.89; acc: 0.8
Batch: 180; loss: 0.76; acc: 0.86
Batch: 200; loss: 0.89; acc: 0.73
Batch: 220; loss: 1.05; acc: 0.77
Batch: 240; loss: 1.04; acc: 0.7
Batch: 260; loss: 0.86; acc: 0.78
Batch: 280; loss: 0.96; acc: 0.69
Batch: 300; loss: 1.08; acc: 0.7
Batch: 320; loss: 1.19; acc: 0.69
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 0.98; acc: 0.75
Batch: 380; loss: 1.05; acc: 0.75
Batch: 400; loss: 0.84; acc: 0.81
Batch: 420; loss: 1.02; acc: 0.72
Batch: 440; loss: 0.9; acc: 0.75
Batch: 460; loss: 0.93; acc: 0.69
Batch: 480; loss: 1.13; acc: 0.72
Batch: 500; loss: 0.99; acc: 0.7
Batch: 520; loss: 1.12; acc: 0.67
Batch: 540; loss: 1.1; acc: 0.7
Batch: 560; loss: 0.95; acc: 0.77
Batch: 580; loss: 1.24; acc: 0.69
Batch: 600; loss: 1.0; acc: 0.67
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 0.85; acc: 0.73
Batch: 660; loss: 1.13; acc: 0.7
Batch: 680; loss: 1.04; acc: 0.7
Batch: 700; loss: 1.01; acc: 0.67
Batch: 720; loss: 1.04; acc: 0.67
Batch: 740; loss: 1.26; acc: 0.7
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 0.94; acc: 0.69
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859145644364083; val_accuracy: 0.7359673566878981 

Epoch 36 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.74; acc: 0.84
Batch: 20; loss: 0.96; acc: 0.75
Batch: 40; loss: 0.9; acc: 0.72
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.91; acc: 0.78
Batch: 120; loss: 1.02; acc: 0.66
Batch: 140; loss: 0.82; acc: 0.86
Batch: 160; loss: 1.01; acc: 0.75
Batch: 180; loss: 0.94; acc: 0.75
Batch: 200; loss: 1.02; acc: 0.69
Batch: 220; loss: 1.04; acc: 0.75
Batch: 240; loss: 1.06; acc: 0.67
Batch: 260; loss: 0.9; acc: 0.73
Batch: 280; loss: 1.02; acc: 0.69
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 1.01; acc: 0.7
Batch: 340; loss: 1.22; acc: 0.59
Batch: 360; loss: 1.21; acc: 0.64
Batch: 380; loss: 1.34; acc: 0.61
Batch: 400; loss: 0.92; acc: 0.77
Batch: 420; loss: 1.11; acc: 0.7
Batch: 440; loss: 0.96; acc: 0.83
Batch: 460; loss: 1.06; acc: 0.67
Batch: 480; loss: 1.1; acc: 0.69
Batch: 500; loss: 0.93; acc: 0.8
Batch: 520; loss: 1.18; acc: 0.62
Batch: 540; loss: 0.97; acc: 0.72
Batch: 560; loss: 0.96; acc: 0.72
Batch: 580; loss: 0.91; acc: 0.8
Batch: 600; loss: 1.0; acc: 0.75
Batch: 620; loss: 1.07; acc: 0.72
Batch: 640; loss: 0.91; acc: 0.72
Batch: 660; loss: 1.08; acc: 0.69
Batch: 680; loss: 1.07; acc: 0.69
Batch: 700; loss: 0.95; acc: 0.75
Batch: 720; loss: 1.15; acc: 0.66
Batch: 740; loss: 0.98; acc: 0.72
Batch: 760; loss: 0.99; acc: 0.77
Batch: 780; loss: 1.01; acc: 0.7
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859141388516517; val_accuracy: 0.7359673566878981 

Epoch 37 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.94; acc: 0.75
Batch: 20; loss: 0.86; acc: 0.78
Batch: 40; loss: 0.96; acc: 0.69
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.02; acc: 0.72
Batch: 100; loss: 0.9; acc: 0.78
Batch: 120; loss: 0.89; acc: 0.77
Batch: 140; loss: 0.94; acc: 0.77
Batch: 160; loss: 0.99; acc: 0.78
Batch: 180; loss: 1.06; acc: 0.73
Batch: 200; loss: 1.04; acc: 0.75
Batch: 220; loss: 1.32; acc: 0.58
Batch: 240; loss: 1.02; acc: 0.69
Batch: 260; loss: 1.08; acc: 0.78
Batch: 280; loss: 0.86; acc: 0.78
Batch: 300; loss: 0.92; acc: 0.78
Batch: 320; loss: 0.88; acc: 0.84
Batch: 340; loss: 1.12; acc: 0.62
Batch: 360; loss: 0.86; acc: 0.78
Batch: 380; loss: 1.39; acc: 0.61
Batch: 400; loss: 0.92; acc: 0.78
Batch: 420; loss: 1.23; acc: 0.59
Batch: 440; loss: 0.98; acc: 0.67
Batch: 460; loss: 1.0; acc: 0.7
Batch: 480; loss: 1.09; acc: 0.75
Batch: 500; loss: 1.07; acc: 0.69
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 1.01; acc: 0.69
Batch: 560; loss: 1.13; acc: 0.64
Batch: 580; loss: 1.02; acc: 0.72
Batch: 600; loss: 0.99; acc: 0.72
Batch: 620; loss: 0.98; acc: 0.67
Batch: 640; loss: 1.12; acc: 0.69
Batch: 660; loss: 1.04; acc: 0.7
Batch: 680; loss: 0.95; acc: 0.72
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 1.13; acc: 0.64
Batch: 740; loss: 0.93; acc: 0.72
Batch: 760; loss: 1.21; acc: 0.7
Batch: 780; loss: 0.94; acc: 0.8
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859137307306763; val_accuracy: 0.7359673566878981 

Epoch 38 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 1.04; acc: 0.69
Batch: 20; loss: 1.05; acc: 0.67
Batch: 40; loss: 0.96; acc: 0.72
Batch: 60; loss: 0.82; acc: 0.75
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.11; acc: 0.72
Batch: 120; loss: 1.26; acc: 0.66
Batch: 140; loss: 0.93; acc: 0.78
Batch: 160; loss: 0.97; acc: 0.72
Batch: 180; loss: 0.99; acc: 0.7
Batch: 200; loss: 1.17; acc: 0.66
Batch: 220; loss: 0.97; acc: 0.77
Batch: 240; loss: 1.06; acc: 0.72
Batch: 260; loss: 0.97; acc: 0.75
Batch: 280; loss: 1.09; acc: 0.67
Batch: 300; loss: 1.09; acc: 0.73
Batch: 320; loss: 0.97; acc: 0.7
Batch: 340; loss: 0.97; acc: 0.77
Batch: 360; loss: 1.1; acc: 0.69
Batch: 380; loss: 1.07; acc: 0.66
Batch: 400; loss: 1.39; acc: 0.58
Batch: 420; loss: 0.85; acc: 0.73
Batch: 440; loss: 0.99; acc: 0.77
Batch: 460; loss: 1.0; acc: 0.69
Batch: 480; loss: 0.74; acc: 0.84
Batch: 500; loss: 1.05; acc: 0.75
Batch: 520; loss: 0.96; acc: 0.69
Batch: 540; loss: 0.99; acc: 0.66
Batch: 560; loss: 1.05; acc: 0.7
Batch: 580; loss: 1.04; acc: 0.77
Batch: 600; loss: 0.92; acc: 0.78
Batch: 620; loss: 0.97; acc: 0.73
Batch: 640; loss: 1.01; acc: 0.73
Batch: 660; loss: 1.01; acc: 0.75
Batch: 680; loss: 1.11; acc: 0.66
Batch: 700; loss: 0.91; acc: 0.81
Batch: 720; loss: 1.3; acc: 0.58
Batch: 740; loss: 0.88; acc: 0.77
Batch: 760; loss: 1.06; acc: 0.69
Batch: 780; loss: 1.04; acc: 0.66
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859133450088987; val_accuracy: 0.7359673566878981 

Epoch 39 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 1.04; acc: 0.73
Batch: 20; loss: 0.91; acc: 0.78
Batch: 40; loss: 1.0; acc: 0.73
Batch: 60; loss: 0.96; acc: 0.67
Batch: 80; loss: 0.92; acc: 0.7
Batch: 100; loss: 0.79; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.66
Batch: 140; loss: 1.2; acc: 0.64
Batch: 160; loss: 1.09; acc: 0.67
Batch: 180; loss: 1.19; acc: 0.7
Batch: 200; loss: 1.02; acc: 0.72
Batch: 220; loss: 1.3; acc: 0.62
Batch: 240; loss: 1.03; acc: 0.77
Batch: 260; loss: 0.97; acc: 0.78
Batch: 280; loss: 1.09; acc: 0.72
Batch: 300; loss: 1.02; acc: 0.66
Batch: 320; loss: 0.99; acc: 0.72
Batch: 340; loss: 0.95; acc: 0.66
Batch: 360; loss: 0.99; acc: 0.73
Batch: 380; loss: 1.02; acc: 0.77
Batch: 400; loss: 1.0; acc: 0.69
Batch: 420; loss: 1.05; acc: 0.7
Batch: 440; loss: 1.07; acc: 0.64
Batch: 460; loss: 0.95; acc: 0.69
Batch: 480; loss: 1.07; acc: 0.66
Batch: 500; loss: 0.98; acc: 0.8
Batch: 520; loss: 0.91; acc: 0.8
Batch: 540; loss: 1.07; acc: 0.7
Batch: 560; loss: 1.14; acc: 0.72
Batch: 580; loss: 1.0; acc: 0.72
Batch: 600; loss: 1.02; acc: 0.72
Batch: 620; loss: 1.14; acc: 0.61
Batch: 640; loss: 1.05; acc: 0.7
Batch: 660; loss: 1.17; acc: 0.67
Batch: 680; loss: 1.28; acc: 0.58
Batch: 700; loss: 1.09; acc: 0.69
Batch: 720; loss: 1.02; acc: 0.72
Batch: 740; loss: 1.07; acc: 0.72
Batch: 760; loss: 0.93; acc: 0.72
Batch: 780; loss: 1.06; acc: 0.61
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859129513145253; val_accuracy: 0.7359673566878981 

Epoch 40 start
The current lr is: 1.2800000000000005e-08
Batch: 0; loss: 0.91; acc: 0.8
Batch: 20; loss: 1.06; acc: 0.69
Batch: 40; loss: 1.16; acc: 0.67
Batch: 60; loss: 1.08; acc: 0.7
Batch: 80; loss: 1.11; acc: 0.69
Batch: 100; loss: 1.0; acc: 0.78
Batch: 120; loss: 0.89; acc: 0.83
Batch: 140; loss: 1.0; acc: 0.75
Batch: 160; loss: 1.01; acc: 0.77
Batch: 180; loss: 1.09; acc: 0.66
Batch: 200; loss: 0.94; acc: 0.81
Batch: 220; loss: 1.17; acc: 0.61
Batch: 240; loss: 1.12; acc: 0.59
Batch: 260; loss: 1.09; acc: 0.73
Batch: 280; loss: 0.92; acc: 0.8
Batch: 300; loss: 0.94; acc: 0.77
Batch: 320; loss: 1.16; acc: 0.66
Batch: 340; loss: 0.94; acc: 0.81
Batch: 360; loss: 1.09; acc: 0.69
Batch: 380; loss: 0.98; acc: 0.7
Batch: 400; loss: 1.04; acc: 0.7
Batch: 420; loss: 0.88; acc: 0.77
Batch: 440; loss: 0.92; acc: 0.81
Batch: 460; loss: 1.17; acc: 0.66
Batch: 480; loss: 0.97; acc: 0.78
Batch: 500; loss: 1.15; acc: 0.66
Batch: 520; loss: 0.91; acc: 0.72
Batch: 540; loss: 0.89; acc: 0.78
Batch: 560; loss: 0.83; acc: 0.83
Batch: 580; loss: 1.17; acc: 0.67
Batch: 600; loss: 1.06; acc: 0.7
Batch: 620; loss: 0.87; acc: 0.78
Batch: 640; loss: 0.97; acc: 0.7
Batch: 660; loss: 1.03; acc: 0.7
Batch: 680; loss: 0.85; acc: 0.77
Batch: 700; loss: 1.0; acc: 0.73
Batch: 720; loss: 0.92; acc: 0.75
Batch: 740; loss: 1.24; acc: 0.59
Batch: 760; loss: 0.95; acc: 0.78
Batch: 780; loss: 1.04; acc: 0.72
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859125299058902; val_accuracy: 0.7359673566878981 

Epoch 41 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 1.06; acc: 0.69
Batch: 20; loss: 1.18; acc: 0.62
Batch: 40; loss: 0.96; acc: 0.8
Batch: 60; loss: 0.88; acc: 0.8
Batch: 80; loss: 1.16; acc: 0.67
Batch: 100; loss: 0.97; acc: 0.77
Batch: 120; loss: 1.13; acc: 0.66
Batch: 140; loss: 1.09; acc: 0.73
Batch: 160; loss: 0.85; acc: 0.83
Batch: 180; loss: 1.01; acc: 0.69
Batch: 200; loss: 1.03; acc: 0.72
Batch: 220; loss: 1.0; acc: 0.7
Batch: 240; loss: 0.99; acc: 0.73
Batch: 260; loss: 1.09; acc: 0.72
Batch: 280; loss: 0.92; acc: 0.75
Batch: 300; loss: 1.1; acc: 0.69
Batch: 320; loss: 1.02; acc: 0.7
Batch: 340; loss: 1.19; acc: 0.67
Batch: 360; loss: 0.9; acc: 0.83
Batch: 380; loss: 0.92; acc: 0.72
Batch: 400; loss: 1.25; acc: 0.67
Batch: 420; loss: 0.93; acc: 0.75
Batch: 440; loss: 1.12; acc: 0.7
Batch: 460; loss: 1.09; acc: 0.67
Batch: 480; loss: 0.94; acc: 0.77
Batch: 500; loss: 1.09; acc: 0.67
Batch: 520; loss: 0.83; acc: 0.75
Batch: 540; loss: 1.18; acc: 0.69
Batch: 560; loss: 1.18; acc: 0.66
Batch: 580; loss: 1.03; acc: 0.73
Batch: 600; loss: 1.04; acc: 0.72
Batch: 620; loss: 1.08; acc: 0.66
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.04; acc: 0.69
Batch: 680; loss: 1.19; acc: 0.72
Batch: 700; loss: 0.81; acc: 0.84
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 1.1; acc: 0.7
Batch: 760; loss: 1.06; acc: 0.67
Batch: 780; loss: 1.01; acc: 0.72
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859125154792883; val_accuracy: 0.7359673566878981 

Epoch 42 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.94; acc: 0.72
Batch: 20; loss: 0.92; acc: 0.81
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.04; acc: 0.72
Batch: 80; loss: 1.12; acc: 0.69
Batch: 100; loss: 0.89; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 1.04; acc: 0.77
Batch: 160; loss: 1.04; acc: 0.69
Batch: 180; loss: 0.99; acc: 0.7
Batch: 200; loss: 1.01; acc: 0.77
Batch: 220; loss: 1.05; acc: 0.72
Batch: 240; loss: 1.24; acc: 0.62
Batch: 260; loss: 1.12; acc: 0.67
Batch: 280; loss: 0.92; acc: 0.75
Batch: 300; loss: 0.97; acc: 0.75
Batch: 320; loss: 1.16; acc: 0.72
Batch: 340; loss: 1.07; acc: 0.67
Batch: 360; loss: 1.13; acc: 0.67
Batch: 380; loss: 0.94; acc: 0.77
Batch: 400; loss: 1.11; acc: 0.66
Batch: 420; loss: 0.98; acc: 0.75
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.62
Batch: 480; loss: 0.98; acc: 0.75
Batch: 500; loss: 0.96; acc: 0.7
Batch: 520; loss: 1.19; acc: 0.72
Batch: 540; loss: 0.99; acc: 0.75
Batch: 560; loss: 0.92; acc: 0.72
Batch: 580; loss: 1.0; acc: 0.72
Batch: 600; loss: 0.98; acc: 0.77
Batch: 620; loss: 0.96; acc: 0.72
Batch: 640; loss: 0.91; acc: 0.73
Batch: 660; loss: 0.85; acc: 0.73
Batch: 680; loss: 0.99; acc: 0.64
Batch: 700; loss: 1.03; acc: 0.75
Batch: 720; loss: 1.13; acc: 0.58
Batch: 740; loss: 1.07; acc: 0.72
Batch: 760; loss: 0.89; acc: 0.75
Batch: 780; loss: 1.02; acc: 0.7
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859124991544492; val_accuracy: 0.7359673566878981 

Epoch 43 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.86; acc: 0.78
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.89; acc: 0.81
Batch: 60; loss: 0.99; acc: 0.73
Batch: 80; loss: 1.15; acc: 0.7
Batch: 100; loss: 0.86; acc: 0.83
Batch: 120; loss: 1.25; acc: 0.64
Batch: 140; loss: 0.86; acc: 0.77
Batch: 160; loss: 0.95; acc: 0.73
Batch: 180; loss: 1.2; acc: 0.61
Batch: 200; loss: 0.8; acc: 0.77
Batch: 220; loss: 1.05; acc: 0.7
Batch: 240; loss: 1.14; acc: 0.61
Batch: 260; loss: 0.83; acc: 0.86
Batch: 280; loss: 1.1; acc: 0.66
Batch: 300; loss: 0.92; acc: 0.73
Batch: 320; loss: 1.12; acc: 0.59
Batch: 340; loss: 0.93; acc: 0.75
Batch: 360; loss: 1.05; acc: 0.69
Batch: 380; loss: 0.98; acc: 0.73
Batch: 400; loss: 0.95; acc: 0.8
Batch: 420; loss: 1.06; acc: 0.7
Batch: 440; loss: 0.95; acc: 0.78
Batch: 460; loss: 0.99; acc: 0.66
Batch: 480; loss: 1.0; acc: 0.7
Batch: 500; loss: 0.81; acc: 0.77
Batch: 520; loss: 1.0; acc: 0.7
Batch: 540; loss: 1.01; acc: 0.72
Batch: 560; loss: 0.91; acc: 0.81
Batch: 580; loss: 1.06; acc: 0.66
Batch: 600; loss: 1.11; acc: 0.64
Batch: 620; loss: 0.97; acc: 0.75
Batch: 640; loss: 0.92; acc: 0.7
Batch: 660; loss: 1.01; acc: 0.73
Batch: 680; loss: 1.07; acc: 0.75
Batch: 700; loss: 1.07; acc: 0.75
Batch: 720; loss: 1.06; acc: 0.64
Batch: 740; loss: 1.06; acc: 0.75
Batch: 760; loss: 1.06; acc: 0.7
Batch: 780; loss: 1.05; acc: 0.67
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859124900429113; val_accuracy: 0.7359673566878981 

Epoch 44 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 0.96; acc: 0.75
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 1.06; acc: 0.7
Batch: 60; loss: 0.99; acc: 0.75
Batch: 80; loss: 1.08; acc: 0.69
Batch: 100; loss: 0.8; acc: 0.86
Batch: 120; loss: 1.03; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.75
Batch: 160; loss: 0.92; acc: 0.75
Batch: 180; loss: 0.81; acc: 0.8
Batch: 200; loss: 0.95; acc: 0.77
Batch: 220; loss: 1.0; acc: 0.72
Batch: 240; loss: 1.01; acc: 0.73
Batch: 260; loss: 0.97; acc: 0.8
Batch: 280; loss: 1.05; acc: 0.73
Batch: 300; loss: 1.02; acc: 0.66
Batch: 320; loss: 0.94; acc: 0.7
Batch: 340; loss: 0.99; acc: 0.73
Batch: 360; loss: 1.0; acc: 0.66
Batch: 380; loss: 1.01; acc: 0.77
Batch: 400; loss: 1.13; acc: 0.67
Batch: 420; loss: 1.02; acc: 0.73
Batch: 440; loss: 1.11; acc: 0.69
Batch: 460; loss: 0.92; acc: 0.77
Batch: 480; loss: 0.95; acc: 0.75
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 1.0; acc: 0.67
Batch: 540; loss: 0.86; acc: 0.73
Batch: 560; loss: 0.92; acc: 0.75
Batch: 580; loss: 0.84; acc: 0.75
Batch: 600; loss: 1.07; acc: 0.72
Batch: 620; loss: 0.98; acc: 0.78
Batch: 640; loss: 0.95; acc: 0.72
Batch: 660; loss: 0.91; acc: 0.77
Batch: 680; loss: 1.0; acc: 0.72
Batch: 700; loss: 0.86; acc: 0.81
Batch: 720; loss: 0.87; acc: 0.84
Batch: 740; loss: 0.96; acc: 0.7
Batch: 760; loss: 0.91; acc: 0.7
Batch: 780; loss: 0.76; acc: 0.7
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859124873853793; val_accuracy: 0.7359673566878981 

Epoch 45 start
The current lr is: 2.5600000000000015e-09
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 1.01; acc: 0.67
Batch: 40; loss: 1.1; acc: 0.66
Batch: 60; loss: 0.82; acc: 0.84
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 1.11; acc: 0.58
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 1.02; acc: 0.75
Batch: 160; loss: 1.0; acc: 0.77
Batch: 180; loss: 1.08; acc: 0.75
Batch: 200; loss: 0.8; acc: 0.8
Batch: 220; loss: 1.06; acc: 0.69
Batch: 240; loss: 0.99; acc: 0.69
Batch: 260; loss: 1.06; acc: 0.69
Batch: 280; loss: 0.89; acc: 0.77
Batch: 300; loss: 0.97; acc: 0.72
Batch: 320; loss: 0.78; acc: 0.84
Batch: 340; loss: 0.95; acc: 0.69
Batch: 360; loss: 0.87; acc: 0.77
Batch: 380; loss: 0.93; acc: 0.77
Batch: 400; loss: 1.09; acc: 0.72
Batch: 420; loss: 1.02; acc: 0.67
Batch: 440; loss: 1.03; acc: 0.73
Batch: 460; loss: 1.15; acc: 0.64
Batch: 480; loss: 0.94; acc: 0.77
Batch: 500; loss: 0.94; acc: 0.75
Batch: 520; loss: 0.94; acc: 0.7
Batch: 540; loss: 0.88; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.75
Batch: 580; loss: 0.87; acc: 0.84
Batch: 600; loss: 1.1; acc: 0.75
Batch: 620; loss: 0.91; acc: 0.78
Batch: 640; loss: 0.81; acc: 0.78
Batch: 660; loss: 0.85; acc: 0.78
Batch: 680; loss: 0.85; acc: 0.75
Batch: 700; loss: 1.07; acc: 0.72
Batch: 720; loss: 1.11; acc: 0.67
Batch: 740; loss: 0.92; acc: 0.72
Batch: 760; loss: 1.21; acc: 0.62
Batch: 780; loss: 0.86; acc: 0.77
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859124771348989; val_accuracy: 0.7359673566878981 

Epoch 46 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.93; acc: 0.75
Batch: 20; loss: 0.92; acc: 0.8
Batch: 40; loss: 1.11; acc: 0.72
Batch: 60; loss: 1.09; acc: 0.73
Batch: 80; loss: 1.08; acc: 0.73
Batch: 100; loss: 1.16; acc: 0.67
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.97; acc: 0.75
Batch: 160; loss: 0.87; acc: 0.73
Batch: 180; loss: 0.85; acc: 0.84
Batch: 200; loss: 1.05; acc: 0.66
Batch: 220; loss: 1.11; acc: 0.64
Batch: 240; loss: 1.0; acc: 0.78
Batch: 260; loss: 1.0; acc: 0.69
Batch: 280; loss: 0.96; acc: 0.78
Batch: 300; loss: 1.09; acc: 0.66
Batch: 320; loss: 1.06; acc: 0.73
Batch: 340; loss: 1.02; acc: 0.75
Batch: 360; loss: 1.15; acc: 0.67
Batch: 380; loss: 1.02; acc: 0.67
Batch: 400; loss: 1.24; acc: 0.69
Batch: 420; loss: 1.03; acc: 0.67
Batch: 440; loss: 1.17; acc: 0.64
Batch: 460; loss: 1.07; acc: 0.69
Batch: 480; loss: 1.18; acc: 0.64
Batch: 500; loss: 1.18; acc: 0.61
Batch: 520; loss: 0.98; acc: 0.73
Batch: 540; loss: 1.3; acc: 0.59
Batch: 560; loss: 1.08; acc: 0.78
Batch: 580; loss: 1.11; acc: 0.7
Batch: 600; loss: 1.02; acc: 0.77
Batch: 620; loss: 1.13; acc: 0.72
Batch: 640; loss: 0.91; acc: 0.75
Batch: 660; loss: 0.96; acc: 0.78
Batch: 680; loss: 0.87; acc: 0.7
Batch: 700; loss: 0.96; acc: 0.78
Batch: 720; loss: 1.0; acc: 0.69
Batch: 740; loss: 0.93; acc: 0.78
Batch: 760; loss: 0.94; acc: 0.78
Batch: 780; loss: 1.07; acc: 0.7
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.985912481690668; val_accuracy: 0.7359673566878981 

Epoch 47 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.91; acc: 0.73
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 1.18; acc: 0.66
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 1.25; acc: 0.61
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 1.05; acc: 0.7
Batch: 160; loss: 1.13; acc: 0.69
Batch: 180; loss: 0.99; acc: 0.73
Batch: 200; loss: 0.95; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.78
Batch: 240; loss: 1.07; acc: 0.77
Batch: 260; loss: 1.18; acc: 0.59
Batch: 280; loss: 0.95; acc: 0.73
Batch: 300; loss: 0.9; acc: 0.72
Batch: 320; loss: 0.91; acc: 0.75
Batch: 340; loss: 1.15; acc: 0.66
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 0.93; acc: 0.78
Batch: 400; loss: 0.85; acc: 0.75
Batch: 420; loss: 1.12; acc: 0.75
Batch: 440; loss: 1.04; acc: 0.67
Batch: 460; loss: 0.97; acc: 0.72
Batch: 480; loss: 1.02; acc: 0.73
Batch: 500; loss: 1.09; acc: 0.7
Batch: 520; loss: 1.0; acc: 0.69
Batch: 540; loss: 0.84; acc: 0.8
Batch: 560; loss: 1.09; acc: 0.73
Batch: 580; loss: 1.2; acc: 0.69
Batch: 600; loss: 1.06; acc: 0.67
Batch: 620; loss: 0.93; acc: 0.75
Batch: 640; loss: 0.97; acc: 0.7
Batch: 660; loss: 0.91; acc: 0.81
Batch: 680; loss: 1.17; acc: 0.67
Batch: 700; loss: 1.02; acc: 0.69
Batch: 720; loss: 1.13; acc: 0.7
Batch: 740; loss: 1.07; acc: 0.66
Batch: 760; loss: 0.95; acc: 0.72
Batch: 780; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859124756163093; val_accuracy: 0.7359673566878981 

Epoch 48 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 1.11; acc: 0.7
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 0.94; acc: 0.8
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 1.22; acc: 0.64
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 0.98; acc: 0.69
Batch: 140; loss: 1.14; acc: 0.61
Batch: 160; loss: 1.01; acc: 0.73
Batch: 180; loss: 0.93; acc: 0.78
Batch: 200; loss: 0.94; acc: 0.78
Batch: 220; loss: 1.06; acc: 0.72
Batch: 240; loss: 1.04; acc: 0.73
Batch: 260; loss: 1.03; acc: 0.72
Batch: 280; loss: 0.85; acc: 0.78
Batch: 300; loss: 1.22; acc: 0.64
Batch: 320; loss: 1.07; acc: 0.64
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 1.03; acc: 0.72
Batch: 380; loss: 1.0; acc: 0.69
Batch: 400; loss: 1.1; acc: 0.67
Batch: 420; loss: 1.07; acc: 0.73
Batch: 440; loss: 0.86; acc: 0.8
Batch: 460; loss: 1.15; acc: 0.7
Batch: 480; loss: 1.08; acc: 0.64
Batch: 500; loss: 1.0; acc: 0.83
Batch: 520; loss: 0.93; acc: 0.78
Batch: 540; loss: 1.11; acc: 0.7
Batch: 560; loss: 1.06; acc: 0.72
Batch: 580; loss: 0.88; acc: 0.83
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 1.07; acc: 0.66
Batch: 640; loss: 1.1; acc: 0.66
Batch: 660; loss: 1.12; acc: 0.72
Batch: 680; loss: 0.94; acc: 0.75
Batch: 700; loss: 1.02; acc: 0.67
Batch: 720; loss: 1.08; acc: 0.73
Batch: 740; loss: 1.16; acc: 0.61
Batch: 760; loss: 0.85; acc: 0.75
Batch: 780; loss: 0.94; acc: 0.78
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859124706808928; val_accuracy: 0.7359673566878981 

Epoch 49 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 0.89; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.73
Batch: 40; loss: 1.21; acc: 0.67
Batch: 60; loss: 0.87; acc: 0.8
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.0; acc: 0.75
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 1.01; acc: 0.69
Batch: 160; loss: 1.06; acc: 0.72
Batch: 180; loss: 0.9; acc: 0.77
Batch: 200; loss: 1.04; acc: 0.66
Batch: 220; loss: 0.96; acc: 0.77
Batch: 240; loss: 1.01; acc: 0.73
Batch: 260; loss: 1.04; acc: 0.73
Batch: 280; loss: 0.9; acc: 0.83
Batch: 300; loss: 1.05; acc: 0.7
Batch: 320; loss: 0.93; acc: 0.81
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 0.96; acc: 0.7
Batch: 400; loss: 0.93; acc: 0.78
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 0.98; acc: 0.73
Batch: 460; loss: 1.02; acc: 0.7
Batch: 480; loss: 1.0; acc: 0.78
Batch: 500; loss: 0.94; acc: 0.83
Batch: 520; loss: 1.15; acc: 0.61
Batch: 540; loss: 1.05; acc: 0.69
Batch: 560; loss: 1.12; acc: 0.67
Batch: 580; loss: 0.99; acc: 0.8
Batch: 600; loss: 1.09; acc: 0.72
Batch: 620; loss: 0.92; acc: 0.78
Batch: 640; loss: 1.06; acc: 0.62
Batch: 660; loss: 0.98; acc: 0.78
Batch: 680; loss: 0.83; acc: 0.78
Batch: 700; loss: 0.97; acc: 0.73
Batch: 720; loss: 1.1; acc: 0.66
Batch: 740; loss: 1.04; acc: 0.67
Batch: 760; loss: 1.07; acc: 0.72
Batch: 780; loss: 1.08; acc: 0.67
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859124691623031; val_accuracy: 0.7359673566878981 

Epoch 50 start
The current lr is: 5.120000000000003e-10
Batch: 0; loss: 1.0; acc: 0.72
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 1.07; acc: 0.67
Batch: 60; loss: 1.08; acc: 0.69
Batch: 80; loss: 1.03; acc: 0.67
Batch: 100; loss: 0.93; acc: 0.75
Batch: 120; loss: 0.99; acc: 0.7
Batch: 140; loss: 1.14; acc: 0.66
Batch: 160; loss: 0.99; acc: 0.77
Batch: 180; loss: 0.99; acc: 0.8
Batch: 200; loss: 1.06; acc: 0.73
Batch: 220; loss: 1.17; acc: 0.56
Batch: 240; loss: 0.98; acc: 0.69
Batch: 260; loss: 0.95; acc: 0.75
Batch: 280; loss: 1.09; acc: 0.64
Batch: 300; loss: 1.08; acc: 0.64
Batch: 320; loss: 0.96; acc: 0.7
Batch: 340; loss: 0.96; acc: 0.77
Batch: 360; loss: 0.85; acc: 0.8
Batch: 380; loss: 0.95; acc: 0.72
Batch: 400; loss: 1.14; acc: 0.64
Batch: 420; loss: 0.98; acc: 0.78
Batch: 440; loss: 1.21; acc: 0.64
Batch: 460; loss: 1.04; acc: 0.77
Batch: 480; loss: 1.11; acc: 0.62
Batch: 500; loss: 1.19; acc: 0.66
Batch: 520; loss: 1.01; acc: 0.67
Batch: 540; loss: 1.1; acc: 0.73
Batch: 560; loss: 1.03; acc: 0.69
Batch: 580; loss: 0.94; acc: 0.73
Batch: 600; loss: 1.08; acc: 0.7
Batch: 620; loss: 0.88; acc: 0.75
Batch: 640; loss: 0.91; acc: 0.81
Batch: 660; loss: 0.92; acc: 0.8
Batch: 680; loss: 1.14; acc: 0.69
Batch: 700; loss: 1.05; acc: 0.67
Batch: 720; loss: 1.0; acc: 0.72
Batch: 740; loss: 1.03; acc: 0.73
Batch: 760; loss: 1.05; acc: 0.75
Batch: 780; loss: 0.75; acc: 0.8
Train Epoch over. train_loss: 1.02; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.75
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 0.83; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.77
Batch: 100; loss: 0.97; acc: 0.75
Batch: 120; loss: 1.09; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.83
Val Epoch over. val_loss: 0.9859124676437135; val_accuracy: 0.7359673566878981 

plots/no_subspace_training/reg_lenet/2020-01-19 03:58:59/d_dim_1000_lr_0.001_gamma_0.2_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161154261060583; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.0288658696375075; val_accuracy: 0.3690286624203822 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.38
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.42
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508678709625438; val_accuracy: 0.4570063694267516 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5094989188917123; val_accuracy: 0.5688694267515924 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878710790804238; val_accuracy: 0.6332603503184714 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.21; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.62
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.0524158105728731; val_accuracy: 0.6973527070063694 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.81
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.8262192796749673; val_accuracy: 0.7791600318471338 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6590044659793757; val_accuracy: 0.8039410828025477 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5360987657194685; val_accuracy: 0.8444466560509554 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.44355986832053795; val_accuracy: 0.8714171974522293 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.72; acc: 0.72
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.38722235657227266; val_accuracy: 0.8839570063694268 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.33928126993642493; val_accuracy: 0.9021695859872612 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.30628211141391926; val_accuracy: 0.9116242038216561 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.98
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.2793804246244157; val_accuracy: 0.9198845541401274 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.2711375002173861; val_accuracy: 0.9175955414012739 

Epoch 16 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.3; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24390292561547772; val_accuracy: 0.9296377388535032 

Epoch 17 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.98
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24101749824205781; val_accuracy: 0.9307324840764332 

Epoch 18 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.23870473165231146; val_accuracy: 0.9309315286624203 

Epoch 19 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.8
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.98
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.88
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23633458712108577; val_accuracy: 0.9324243630573248 

Epoch 20 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.98
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.15; acc: 1.0
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23332434138105174; val_accuracy: 0.9326234076433121 

Epoch 21 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.98
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23199909026170992; val_accuracy: 0.9332205414012739 

Epoch 22 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.83
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2295539018454825; val_accuracy: 0.9331210191082803 

Epoch 23 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22724025929050082; val_accuracy: 0.9346138535031847 

Epoch 24 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22481232660875958; val_accuracy: 0.9352109872611465 

Epoch 25 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.17; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2230106871693757; val_accuracy: 0.9355095541401274 

Epoch 26 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.08; acc: 1.0
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.95
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2209568064019179; val_accuracy: 0.9361066878980892 

Epoch 27 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.1; acc: 1.0
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21890991603492932; val_accuracy: 0.9362062101910829 

Epoch 28 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.3; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.97
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21733637842213271; val_accuracy: 0.93640525477707 

Epoch 29 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.78
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.16; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.11; acc: 1.0
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21600973413913113; val_accuracy: 0.9373009554140127 

Epoch 30 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.88
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.1; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21448952358239776; val_accuracy: 0.9379976114649682 

Epoch 31 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.88
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21351779722104405; val_accuracy: 0.9374004777070064 

Epoch 32 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.21; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21311991626195087; val_accuracy: 0.9375995222929936 

Epoch 33 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21287580259191763; val_accuracy: 0.9379976114649682 

Epoch 34 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.1; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.95
Batch: 160; loss: 0.12; acc: 0.98
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.97
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.13; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.2; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.19; acc: 0.97
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21270034993719902; val_accuracy: 0.9379976114649682 

Epoch 35 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.11; acc: 1.0
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.98
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.97
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.18; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2124206649412395; val_accuracy: 0.9377985668789809 

Epoch 36 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.28; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2121305276349092; val_accuracy: 0.9382961783439491 

Epoch 37 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.21; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.45; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.212015901449; val_accuracy: 0.9382961783439491 

Epoch 38 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.97
Batch: 320; loss: 0.42; acc: 0.83
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.36; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.53; acc: 0.89
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.16; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21180174611271566; val_accuracy: 0.9380971337579618 

Epoch 39 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.58; acc: 0.83
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.97
Batch: 340; loss: 0.2; acc: 0.97
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.14; acc: 0.98
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.1; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.84
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21145047695868335; val_accuracy: 0.9380971337579618 

Epoch 40 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.21; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.98
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.86
Batch: 620; loss: 0.1; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21136075028093756; val_accuracy: 0.9386942675159236 

Epoch 41 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.91
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.1; acc: 1.0
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2110994832863094; val_accuracy: 0.9381966560509554 

Epoch 42 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.98
Batch: 660; loss: 0.17; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21082602614525017; val_accuracy: 0.9386942675159236 

Epoch 43 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.25; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.14; acc: 1.0
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.97
Batch: 340; loss: 0.14; acc: 0.98
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.91
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.19; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21056629779042713; val_accuracy: 0.9387937898089171 

Epoch 44 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.88
Batch: 260; loss: 0.17; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.97
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.18; acc: 0.91
Batch: 720; loss: 0.11; acc: 0.98
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2103068052203792; val_accuracy: 0.93859474522293 

Epoch 45 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.28; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.91
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.98
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2101627326789935; val_accuracy: 0.9386942675159236 

Epoch 46 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.09; acc: 0.98
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.11; acc: 0.98
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.14; acc: 0.98
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21013727226549653; val_accuracy: 0.9387937898089171 

Epoch 47 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.94
Batch: 480; loss: 0.15; acc: 0.98
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2101117520338031; val_accuracy: 0.9387937898089171 

Epoch 48 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21008817455286433; val_accuracy: 0.9388933121019108 

Epoch 49 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.37; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.81
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.97
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.38; acc: 0.84
Batch: 620; loss: 0.18; acc: 0.98
Batch: 640; loss: 0.26; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2100650356596063; val_accuracy: 0.9389928343949044 

Epoch 50 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.98
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.13; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21003292548428676; val_accuracy: 0.9388933121019108 

plots/no_subspace_training/reg_lenet/2020-01-19 04:08:20/d_dim_1000_lr_0.001_gamma_0.13_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161154655893895; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.028858893236537; val_accuracy: 0.36892914012738853 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.39
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.750854894613764; val_accuracy: 0.4570063694267516 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.39
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.509495070785474; val_accuracy: 0.5688694267515924 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.28787545261869; val_accuracy: 0.6331608280254777 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.052383605461971; val_accuracy: 0.6972531847133758 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.83
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.8261687049440517; val_accuracy: 0.7795581210191083 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.6589442075817449; val_accuracy: 0.8039410828025477 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5360276429516495; val_accuracy: 0.8443471337579618 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.44349975836504796; val_accuracy: 0.8713176751592356 

Epoch 11 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.61; acc: 0.78
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.81
Batch: 140; loss: 0.45; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.97
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.74; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.77; acc: 0.7
Batch: 480; loss: 0.79; acc: 0.64
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.8
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.72; acc: 0.78
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.430602369414773; val_accuracy: 0.8749004777070064 

Epoch 12 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.6; acc: 0.78
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.8
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.67; acc: 0.78
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.44; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.42169398591396917; val_accuracy: 0.8767914012738853 

Epoch 13 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.65; acc: 0.78
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.53; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.63; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.98
Val Epoch over. val_loss: 0.4128528150023928; val_accuracy: 0.8800756369426752 

Epoch 14 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.8
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.8
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.62; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.94
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.4; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.52; acc: 0.8
Batch: 720; loss: 0.25; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.4052879310147777; val_accuracy: 0.8834593949044586 

Epoch 15 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.81
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.48; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.81
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.95
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.8
Batch: 440; loss: 0.36; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.8
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.54; acc: 0.81
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.59; acc: 0.81
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.397861539273505; val_accuracy: 0.8846536624203821 

Epoch 16 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.65; acc: 0.83
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.45; acc: 0.84
Batch: 440; loss: 0.29; acc: 0.94
Batch: 460; loss: 0.43; acc: 0.91
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.84
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.8
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.39007476579611466; val_accuracy: 0.8875398089171974 

Epoch 17 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.49; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.29; acc: 0.97
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.81
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.46; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.8
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.55; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.36; acc: 0.94
Batch: 460; loss: 0.55; acc: 0.81
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.42; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.42; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.38276140249458845; val_accuracy: 0.8889331210191083 

Epoch 18 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.81
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.8
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.64; acc: 0.83
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.97
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.52; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.37558468796645; val_accuracy: 0.8911226114649682 

Epoch 19 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.41; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.3; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.73
Batch: 360; loss: 0.45; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.58; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.94
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.88
Batch: 720; loss: 0.4; acc: 0.84
Batch: 740; loss: 0.25; acc: 0.97
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.36862340208831107; val_accuracy: 0.8924164012738853 

Epoch 20 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.78
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.98
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.57; acc: 0.8
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.34; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.71; acc: 0.78
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.42; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.81
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3617185264066526; val_accuracy: 0.895203025477707 

Epoch 21 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.26; acc: 0.94
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.45; acc: 0.83
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.53; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.38; acc: 0.84
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3606478161872572; val_accuracy: 0.8947054140127388 

Epoch 22 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.81
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.55; acc: 0.84
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.37; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.44; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3599016944504088; val_accuracy: 0.8946058917197452 

Epoch 23 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.47; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.62; acc: 0.8
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.94
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.359118384350637; val_accuracy: 0.8948049363057324 

Epoch 24 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.77
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.52; acc: 0.81
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.59; acc: 0.8
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.54; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.52; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3581303111307181; val_accuracy: 0.8954020700636943 

Epoch 25 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.98
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.42; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.59; acc: 0.84
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.47; acc: 0.81
Batch: 400; loss: 0.31; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.65; acc: 0.84
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.17; acc: 0.98
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3575138830265422; val_accuracy: 0.8953025477707006 

Epoch 26 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.86
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.41; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.48; acc: 0.83
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.86
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.35; acc: 0.84
Batch: 640; loss: 0.41; acc: 0.84
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.51; acc: 0.8
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3565866667184101; val_accuracy: 0.8958001592356688 

Epoch 27 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.58; acc: 0.8
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.83
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.25; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.51; acc: 0.78
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.81
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.54; acc: 0.8
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.59; acc: 0.78
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35573817096698057; val_accuracy: 0.8963972929936306 

Epoch 28 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.68; acc: 0.84
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.51; acc: 0.8
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.55; acc: 0.81
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.84
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3549937055369092; val_accuracy: 0.8963972929936306 

Epoch 29 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.69; acc: 0.72
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.81
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.86
Batch: 600; loss: 0.22; acc: 0.97
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3543765736613304; val_accuracy: 0.8963972929936306 

Epoch 30 start
The current lr is: 1.69e-05
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.48; acc: 0.91
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.83
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.78
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.26; acc: 0.94
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3535339912980985; val_accuracy: 0.8966958598726115 

Epoch 31 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.81
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.36; acc: 0.83
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.83
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.97
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3533915948526115; val_accuracy: 0.8966958598726115 

Epoch 32 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.97
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.83
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.84
Batch: 640; loss: 0.6; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35326275447751304; val_accuracy: 0.8964968152866242 

Epoch 33 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.63; acc: 0.81
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.92
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.36; acc: 0.94
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.84
Batch: 400; loss: 0.29; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.52; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3531569648700155; val_accuracy: 0.8964968152866242 

Epoch 34 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.39; acc: 0.95
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3530504949343432; val_accuracy: 0.8966958598726115 

Epoch 35 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.55; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.52; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.6; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.83
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35294023469375196; val_accuracy: 0.8965963375796179 

Epoch 36 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.92
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.94
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.62; acc: 0.78
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3528216584662723; val_accuracy: 0.8968949044585988 

Epoch 37 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.69; acc: 0.81
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.58; acc: 0.8
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.81
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.69; acc: 0.84
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35272066200235086; val_accuracy: 0.897093949044586 

Epoch 38 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.88
Batch: 260; loss: 0.32; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.72; acc: 0.8
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.95
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35261296855795915; val_accuracy: 0.897093949044586 

Epoch 39 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.58; acc: 0.77
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.78; acc: 0.73
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.84
Batch: 400; loss: 0.4; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.8
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.81
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3525005098740766; val_accuracy: 0.897093949044586 

Epoch 40 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.61; acc: 0.78
Batch: 240; loss: 0.52; acc: 0.8
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.83
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.49; acc: 0.78
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35240336759075236; val_accuracy: 0.8971934713375797 

Epoch 41 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.7; acc: 0.81
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.58; acc: 0.83
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3523924593712873; val_accuracy: 0.8971934713375797 

Epoch 42 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.97
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.97
Batch: 80; loss: 0.51; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.51; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35238154251484355; val_accuracy: 0.8971934713375797 

Epoch 43 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.6; acc: 0.81
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.65; acc: 0.77
Batch: 200; loss: 0.21; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.97
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.94
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.81
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35237039559206385; val_accuracy: 0.8971934713375797 

Epoch 44 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.29; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.53; acc: 0.81
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.84
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.97
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.56; acc: 0.8
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.98
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3523589581440968; val_accuracy: 0.897093949044586 

Epoch 45 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.97
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.8
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3523483030545484; val_accuracy: 0.897093949044586 

Epoch 46 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.86
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.3; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.81
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.51; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.97
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.53; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.352337695610751; val_accuracy: 0.897093949044586 

Epoch 47 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.77
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.83
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.83
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.95
Batch: 500; loss: 0.44; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.83
Batch: 640; loss: 0.4; acc: 0.84
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.94
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.35232732221958746; val_accuracy: 0.897093949044586 

Epoch 48 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.78
Batch: 160; loss: 0.33; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.83
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.58; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.34; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.83
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.44; acc: 0.84
Batch: 720; loss: 0.41; acc: 0.94
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3523166033492726; val_accuracy: 0.897093949044586 

Epoch 49 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.92
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.56; acc: 0.78
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.95
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.53; acc: 0.8
Batch: 620; loss: 0.27; acc: 0.97
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3523063653024139; val_accuracy: 0.897093949044586 

Epoch 50 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.54; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.5; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.84
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.3522953964342737; val_accuracy: 0.897093949044586 

plots/no_subspace_training/reg_lenet/2020-01-19 04:17:32/d_dim_1000_lr_0.001_gamma_0.13_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.216115347139395; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.028857907671837; val_accuracy: 0.3690286624203822 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.39
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508597920654685; val_accuracy: 0.45720541401273884 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.38
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.509506789742002; val_accuracy: 0.5687699044585988 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878692150115967; val_accuracy: 0.6331608280254777 

Epoch 6 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.56; acc: 0.5
Batch: 80; loss: 1.21; acc: 0.66
Batch: 100; loss: 1.47; acc: 0.5
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 1.43; acc: 0.58
Batch: 160; loss: 1.36; acc: 0.61
Batch: 180; loss: 1.31; acc: 0.64
Batch: 200; loss: 1.31; acc: 0.62
Batch: 220; loss: 1.28; acc: 0.61
Batch: 240; loss: 1.26; acc: 0.69
Batch: 260; loss: 1.28; acc: 0.58
Batch: 280; loss: 1.25; acc: 0.58
Batch: 300; loss: 1.28; acc: 0.72
Batch: 320; loss: 1.21; acc: 0.7
Batch: 340; loss: 1.37; acc: 0.58
Batch: 360; loss: 1.33; acc: 0.53
Batch: 380; loss: 1.3; acc: 0.61
Batch: 400; loss: 1.34; acc: 0.64
Batch: 420; loss: 1.18; acc: 0.72
Batch: 440; loss: 1.26; acc: 0.64
Batch: 460; loss: 1.17; acc: 0.78
Batch: 480; loss: 1.27; acc: 0.73
Batch: 500; loss: 1.47; acc: 0.58
Batch: 520; loss: 1.32; acc: 0.64
Batch: 540; loss: 1.22; acc: 0.69
Batch: 560; loss: 1.4; acc: 0.62
Batch: 580; loss: 1.37; acc: 0.59
Batch: 600; loss: 1.19; acc: 0.67
Batch: 620; loss: 1.19; acc: 0.69
Batch: 640; loss: 1.23; acc: 0.64
Batch: 660; loss: 1.35; acc: 0.58
Batch: 680; loss: 1.23; acc: 0.62
Batch: 700; loss: 1.25; acc: 0.55
Batch: 720; loss: 1.19; acc: 0.7
Batch: 740; loss: 1.35; acc: 0.7
Batch: 760; loss: 1.16; acc: 0.69
Batch: 780; loss: 1.34; acc: 0.58
Train Epoch over. train_loss: 1.29; train_accuracy: 0.63 

Batch: 0; loss: 1.24; acc: 0.67
Batch: 20; loss: 1.38; acc: 0.53
Batch: 40; loss: 1.04; acc: 0.69
Batch: 60; loss: 1.14; acc: 0.62
Batch: 80; loss: 1.19; acc: 0.69
Batch: 100; loss: 1.21; acc: 0.69
Batch: 120; loss: 1.31; acc: 0.64
Batch: 140; loss: 1.08; acc: 0.78
Val Epoch over. val_loss: 1.2520898553975828; val_accuracy: 0.6555533439490446 

Epoch 7 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 1.19; acc: 0.59
Batch: 20; loss: 1.22; acc: 0.69
Batch: 40; loss: 1.44; acc: 0.59
Batch: 60; loss: 1.18; acc: 0.67
Batch: 80; loss: 1.33; acc: 0.7
Batch: 100; loss: 1.34; acc: 0.67
Batch: 120; loss: 1.24; acc: 0.7
Batch: 140; loss: 1.34; acc: 0.66
Batch: 160; loss: 1.23; acc: 0.67
Batch: 180; loss: 1.17; acc: 0.69
Batch: 200; loss: 1.24; acc: 0.64
Batch: 220; loss: 1.49; acc: 0.52
Batch: 240; loss: 1.24; acc: 0.66
Batch: 260; loss: 1.26; acc: 0.61
Batch: 280; loss: 1.19; acc: 0.77
Batch: 300; loss: 1.29; acc: 0.62
Batch: 320; loss: 1.35; acc: 0.56
Batch: 340; loss: 1.55; acc: 0.53
Batch: 360; loss: 1.37; acc: 0.64
Batch: 380; loss: 1.28; acc: 0.69
Batch: 400; loss: 1.18; acc: 0.64
Batch: 420; loss: 1.28; acc: 0.61
Batch: 440; loss: 1.24; acc: 0.66
Batch: 460; loss: 1.19; acc: 0.64
Batch: 480; loss: 1.35; acc: 0.53
Batch: 500; loss: 1.29; acc: 0.66
Batch: 520; loss: 1.4; acc: 0.59
Batch: 540; loss: 1.41; acc: 0.64
Batch: 560; loss: 1.3; acc: 0.53
Batch: 580; loss: 1.42; acc: 0.64
Batch: 600; loss: 1.31; acc: 0.66
Batch: 620; loss: 1.33; acc: 0.55
Batch: 640; loss: 1.25; acc: 0.64
Batch: 660; loss: 1.26; acc: 0.59
Batch: 680; loss: 1.16; acc: 0.72
Batch: 700; loss: 1.17; acc: 0.75
Batch: 720; loss: 1.26; acc: 0.61
Batch: 740; loss: 1.3; acc: 0.56
Batch: 760; loss: 1.39; acc: 0.59
Batch: 780; loss: 1.18; acc: 0.59
Train Epoch over. train_loss: 1.26; train_accuracy: 0.64 

Batch: 0; loss: 1.21; acc: 0.73
Batch: 20; loss: 1.35; acc: 0.56
Batch: 40; loss: 1.01; acc: 0.73
Batch: 60; loss: 1.11; acc: 0.67
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.18; acc: 0.7
Batch: 120; loss: 1.28; acc: 0.67
Batch: 140; loss: 1.04; acc: 0.8
Val Epoch over. val_loss: 1.2215419662226537; val_accuracy: 0.6675955414012739 

Epoch 8 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 1.05; acc: 0.69
Batch: 20; loss: 1.07; acc: 0.69
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 1.19; acc: 0.64
Batch: 80; loss: 1.38; acc: 0.48
Batch: 100; loss: 1.31; acc: 0.61
Batch: 120; loss: 1.07; acc: 0.73
Batch: 140; loss: 1.16; acc: 0.72
Batch: 160; loss: 1.27; acc: 0.64
Batch: 180; loss: 1.34; acc: 0.56
Batch: 200; loss: 1.22; acc: 0.66
Batch: 220; loss: 1.31; acc: 0.66
Batch: 240; loss: 1.23; acc: 0.58
Batch: 260; loss: 1.3; acc: 0.67
Batch: 280; loss: 1.04; acc: 0.67
Batch: 300; loss: 1.32; acc: 0.62
Batch: 320; loss: 1.2; acc: 0.69
Batch: 340; loss: 1.28; acc: 0.61
Batch: 360; loss: 1.14; acc: 0.69
Batch: 380; loss: 1.25; acc: 0.61
Batch: 400; loss: 1.35; acc: 0.61
Batch: 420; loss: 1.33; acc: 0.66
Batch: 440; loss: 1.21; acc: 0.66
Batch: 460; loss: 1.15; acc: 0.72
Batch: 480; loss: 1.08; acc: 0.77
Batch: 500; loss: 1.13; acc: 0.75
Batch: 520; loss: 1.15; acc: 0.7
Batch: 540; loss: 1.22; acc: 0.62
Batch: 560; loss: 1.13; acc: 0.64
Batch: 580; loss: 0.86; acc: 0.86
Batch: 600; loss: 1.35; acc: 0.59
Batch: 620; loss: 1.27; acc: 0.61
Batch: 640; loss: 1.26; acc: 0.67
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 1.33; acc: 0.56
Batch: 700; loss: 1.04; acc: 0.69
Batch: 720; loss: 1.02; acc: 0.75
Batch: 740; loss: 1.28; acc: 0.55
Batch: 760; loss: 1.28; acc: 0.67
Batch: 780; loss: 1.08; acc: 0.66
Train Epoch over. train_loss: 1.23; train_accuracy: 0.65 

Batch: 0; loss: 1.18; acc: 0.72
Batch: 20; loss: 1.32; acc: 0.55
Batch: 40; loss: 0.99; acc: 0.73
Batch: 60; loss: 1.09; acc: 0.69
Batch: 80; loss: 1.13; acc: 0.72
Batch: 100; loss: 1.15; acc: 0.72
Batch: 120; loss: 1.26; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.1912709000004325; val_accuracy: 0.6777468152866242 

Epoch 9 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 1.19; acc: 0.72
Batch: 20; loss: 1.09; acc: 0.75
Batch: 40; loss: 1.31; acc: 0.66
Batch: 60; loss: 1.36; acc: 0.64
Batch: 80; loss: 1.12; acc: 0.66
Batch: 100; loss: 1.24; acc: 0.59
Batch: 120; loss: 1.14; acc: 0.66
Batch: 140; loss: 1.23; acc: 0.7
Batch: 160; loss: 1.13; acc: 0.73
Batch: 180; loss: 1.03; acc: 0.77
Batch: 200; loss: 1.19; acc: 0.61
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.33; acc: 0.58
Batch: 260; loss: 1.19; acc: 0.67
Batch: 280; loss: 1.17; acc: 0.69
Batch: 300; loss: 1.19; acc: 0.66
Batch: 320; loss: 1.14; acc: 0.67
Batch: 340; loss: 1.09; acc: 0.77
Batch: 360; loss: 1.06; acc: 0.69
Batch: 380; loss: 1.26; acc: 0.67
Batch: 400; loss: 1.18; acc: 0.77
Batch: 420; loss: 1.29; acc: 0.55
Batch: 440; loss: 1.25; acc: 0.62
Batch: 460; loss: 1.08; acc: 0.67
Batch: 480; loss: 1.21; acc: 0.66
Batch: 500; loss: 1.3; acc: 0.62
Batch: 520; loss: 1.21; acc: 0.56
Batch: 540; loss: 1.03; acc: 0.69
Batch: 560; loss: 1.15; acc: 0.73
Batch: 580; loss: 1.31; acc: 0.56
Batch: 600; loss: 1.13; acc: 0.73
Batch: 620; loss: 1.35; acc: 0.64
Batch: 640; loss: 1.24; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.66
Batch: 680; loss: 1.15; acc: 0.67
Batch: 700; loss: 1.16; acc: 0.62
Batch: 720; loss: 1.18; acc: 0.64
Batch: 740; loss: 1.1; acc: 0.7
Batch: 760; loss: 1.02; acc: 0.73
Batch: 780; loss: 0.98; acc: 0.67
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.15; acc: 0.77
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.97; acc: 0.78
Val Epoch over. val_loss: 1.1593149634683209; val_accuracy: 0.6866042993630573 

Epoch 10 start
The current lr is: 0.00013000000000000002
Batch: 0; loss: 1.1; acc: 0.72
Batch: 20; loss: 1.17; acc: 0.73
Batch: 40; loss: 1.18; acc: 0.67
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 1.24; acc: 0.59
Batch: 100; loss: 0.97; acc: 0.72
Batch: 120; loss: 1.01; acc: 0.73
Batch: 140; loss: 1.07; acc: 0.62
Batch: 160; loss: 1.44; acc: 0.45
Batch: 180; loss: 1.25; acc: 0.55
Batch: 200; loss: 1.14; acc: 0.77
Batch: 220; loss: 1.18; acc: 0.61
Batch: 240; loss: 1.19; acc: 0.72
Batch: 260; loss: 1.1; acc: 0.69
Batch: 280; loss: 1.29; acc: 0.56
Batch: 300; loss: 1.17; acc: 0.69
Batch: 320; loss: 1.09; acc: 0.66
Batch: 340; loss: 1.07; acc: 0.69
Batch: 360; loss: 1.08; acc: 0.72
Batch: 380; loss: 1.22; acc: 0.66
Batch: 400; loss: 1.47; acc: 0.55
Batch: 420; loss: 1.21; acc: 0.59
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.28; acc: 0.64
Batch: 480; loss: 0.99; acc: 0.77
Batch: 500; loss: 1.31; acc: 0.56
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.13; acc: 0.73
Batch: 560; loss: 1.22; acc: 0.64
Batch: 580; loss: 1.34; acc: 0.59
Batch: 600; loss: 1.28; acc: 0.62
Batch: 620; loss: 1.3; acc: 0.59
Batch: 640; loss: 1.23; acc: 0.62
Batch: 660; loss: 0.92; acc: 0.75
Batch: 680; loss: 1.16; acc: 0.7
Batch: 700; loss: 1.2; acc: 0.61
Batch: 720; loss: 1.29; acc: 0.62
Batch: 740; loss: 1.02; acc: 0.7
Batch: 760; loss: 1.32; acc: 0.59
Batch: 780; loss: 1.09; acc: 0.8
Train Epoch over. train_loss: 1.17; train_accuracy: 0.67 

Batch: 0; loss: 1.14; acc: 0.75
Batch: 20; loss: 1.27; acc: 0.56
Batch: 40; loss: 0.94; acc: 0.75
Batch: 60; loss: 1.03; acc: 0.69
Batch: 80; loss: 1.07; acc: 0.73
Batch: 100; loss: 1.1; acc: 0.73
Batch: 120; loss: 1.21; acc: 0.7
Batch: 140; loss: 0.93; acc: 0.83
Val Epoch over. val_loss: 1.1299606189606295; val_accuracy: 0.6946656050955414 

Epoch 11 start
The current lr is: 1.69e-05
Batch: 0; loss: 1.07; acc: 0.75
Batch: 20; loss: 1.19; acc: 0.66
Batch: 40; loss: 1.23; acc: 0.62
Batch: 60; loss: 1.13; acc: 0.67
Batch: 80; loss: 1.47; acc: 0.55
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.2; acc: 0.66
Batch: 140; loss: 1.1; acc: 0.75
Batch: 160; loss: 1.28; acc: 0.67
Batch: 180; loss: 1.24; acc: 0.62
Batch: 200; loss: 1.13; acc: 0.69
Batch: 220; loss: 1.16; acc: 0.62
Batch: 240; loss: 1.08; acc: 0.7
Batch: 260; loss: 1.1; acc: 0.64
Batch: 280; loss: 1.15; acc: 0.66
Batch: 300; loss: 1.35; acc: 0.62
Batch: 320; loss: 1.02; acc: 0.78
Batch: 340; loss: 1.31; acc: 0.62
Batch: 360; loss: 1.23; acc: 0.67
Batch: 380; loss: 1.14; acc: 0.64
Batch: 400; loss: 1.14; acc: 0.61
Batch: 420; loss: 1.15; acc: 0.67
Batch: 440; loss: 1.2; acc: 0.62
Batch: 460; loss: 1.48; acc: 0.55
Batch: 480; loss: 1.54; acc: 0.5
Batch: 500; loss: 1.01; acc: 0.75
Batch: 520; loss: 1.27; acc: 0.59
Batch: 540; loss: 1.23; acc: 0.69
Batch: 560; loss: 1.07; acc: 0.67
Batch: 580; loss: 1.31; acc: 0.61
Batch: 600; loss: 1.19; acc: 0.64
Batch: 620; loss: 1.13; acc: 0.67
Batch: 640; loss: 1.05; acc: 0.73
Batch: 660; loss: 1.07; acc: 0.75
Batch: 680; loss: 1.23; acc: 0.62
Batch: 700; loss: 1.02; acc: 0.72
Batch: 720; loss: 1.13; acc: 0.75
Batch: 740; loss: 1.02; acc: 0.69
Batch: 760; loss: 1.21; acc: 0.64
Batch: 780; loss: 0.96; acc: 0.8
Train Epoch over. train_loss: 1.16; train_accuracy: 0.68 

Batch: 0; loss: 1.13; acc: 0.73
Batch: 20; loss: 1.26; acc: 0.55
Batch: 40; loss: 0.94; acc: 0.73
Batch: 60; loss: 1.03; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.2; acc: 0.7
Batch: 140; loss: 0.92; acc: 0.81
Val Epoch over. val_loss: 1.1248672479277204; val_accuracy: 0.6949641719745223 

Epoch 12 start
The current lr is: 1.69e-05
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 0.93; acc: 0.67
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 1.23; acc: 0.59
Batch: 100; loss: 0.99; acc: 0.77
Batch: 120; loss: 1.07; acc: 0.72
Batch: 140; loss: 1.18; acc: 0.62
Batch: 160; loss: 1.22; acc: 0.56
Batch: 180; loss: 1.13; acc: 0.64
Batch: 200; loss: 1.13; acc: 0.62
Batch: 220; loss: 1.25; acc: 0.59
Batch: 240; loss: 0.94; acc: 0.78
Batch: 260; loss: 1.12; acc: 0.72
Batch: 280; loss: 1.16; acc: 0.7
Batch: 300; loss: 1.23; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.72
Batch: 340; loss: 1.28; acc: 0.59
Batch: 360; loss: 1.23; acc: 0.69
Batch: 380; loss: 1.19; acc: 0.62
Batch: 400; loss: 1.16; acc: 0.72
Batch: 420; loss: 1.28; acc: 0.61
Batch: 440; loss: 1.16; acc: 0.62
Batch: 460; loss: 1.09; acc: 0.67
Batch: 480; loss: 1.11; acc: 0.67
Batch: 500; loss: 1.26; acc: 0.62
Batch: 520; loss: 0.97; acc: 0.72
Batch: 540; loss: 1.09; acc: 0.72
Batch: 560; loss: 1.11; acc: 0.67
Batch: 580; loss: 1.26; acc: 0.72
Batch: 600; loss: 1.27; acc: 0.58
Batch: 620; loss: 1.16; acc: 0.67
Batch: 640; loss: 1.2; acc: 0.69
Batch: 660; loss: 1.17; acc: 0.66
Batch: 680; loss: 1.22; acc: 0.72
Batch: 700; loss: 1.25; acc: 0.62
Batch: 720; loss: 1.08; acc: 0.66
Batch: 740; loss: 0.95; acc: 0.78
Batch: 760; loss: 1.3; acc: 0.62
Batch: 780; loss: 1.17; acc: 0.66
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

Batch: 0; loss: 1.12; acc: 0.72
Batch: 20; loss: 1.26; acc: 0.56
Batch: 40; loss: 0.93; acc: 0.73
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 1.09; acc: 0.72
Batch: 120; loss: 1.2; acc: 0.7
Batch: 140; loss: 0.92; acc: 0.81
Val Epoch over. val_loss: 1.1207159753817661; val_accuracy: 0.6978503184713376 

Epoch 13 start
The current lr is: 1.69e-05
Batch: 0; loss: 1.16; acc: 0.67
Batch: 20; loss: 1.25; acc: 0.66
Batch: 40; loss: 1.2; acc: 0.59
Batch: 60; loss: 1.1; acc: 0.72
Batch: 80; loss: 1.05; acc: 0.75
Batch: 100; loss: 1.12; acc: 0.73
Batch: 120; loss: 0.99; acc: 0.72
Batch: 140; loss: 1.08; acc: 0.69
Batch: 160; loss: 1.26; acc: 0.58
Batch: 180; loss: 1.06; acc: 0.72
Batch: 200; loss: 1.28; acc: 0.67
Batch: 220; loss: 1.07; acc: 0.7
Batch: 240; loss: 1.2; acc: 0.72
Batch: 260; loss: 1.0; acc: 0.72
Batch: 280; loss: 1.01; acc: 0.72
Batch: 300; loss: 1.07; acc: 0.73
Batch: 320; loss: 1.04; acc: 0.75
Batch: 340; loss: 1.16; acc: 0.7
Batch: 360; loss: 1.31; acc: 0.69
Batch: 380; loss: 1.02; acc: 0.73
Batch: 400; loss: 1.07; acc: 0.69
Batch: 420; loss: 1.25; acc: 0.7
Batch: 440; loss: 1.11; acc: 0.69
Batch: 460; loss: 0.98; acc: 0.75
Batch: 480; loss: 1.09; acc: 0.73
Batch: 500; loss: 1.12; acc: 0.67
Batch: 520; loss: 1.1; acc: 0.69
Batch: 540; loss: 1.21; acc: 0.52
Batch: 560; loss: 1.2; acc: 0.66
Batch: 580; loss: 1.19; acc: 0.62
Batch: 600; loss: 1.17; acc: 0.67
Batch: 620; loss: 1.19; acc: 0.66
Batch: 640; loss: 1.04; acc: 0.8
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.41; acc: 0.58
Batch: 700; loss: 1.04; acc: 0.75
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.03; acc: 0.72
Batch: 760; loss: 1.01; acc: 0.77
Batch: 780; loss: 1.14; acc: 0.69
Train Epoch over. train_loss: 1.15; train_accuracy: 0.68 

Batch: 0; loss: 1.12; acc: 0.77
Batch: 20; loss: 1.26; acc: 0.55
Batch: 40; loss: 0.93; acc: 0.73
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.72
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.91; acc: 0.81
Val Epoch over. val_loss: 1.1165559652504649; val_accuracy: 0.699343152866242 

Epoch 14 start
The current lr is: 1.69e-05
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 1.07; acc: 0.75
Batch: 60; loss: 1.46; acc: 0.52
Batch: 80; loss: 0.99; acc: 0.69
Batch: 100; loss: 1.19; acc: 0.75
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 1.07; acc: 0.67
Batch: 160; loss: 1.18; acc: 0.64
Batch: 180; loss: 1.17; acc: 0.72
Batch: 200; loss: 1.14; acc: 0.64
Batch: 220; loss: 1.17; acc: 0.64
Batch: 240; loss: 1.17; acc: 0.56
Batch: 260; loss: 1.05; acc: 0.7
Batch: 280; loss: 0.97; acc: 0.77
Batch: 300; loss: 1.29; acc: 0.61
Batch: 320; loss: 1.18; acc: 0.67
Batch: 340; loss: 1.35; acc: 0.61
Batch: 360; loss: 1.05; acc: 0.78
Batch: 380; loss: 1.09; acc: 0.75
Batch: 400; loss: 1.03; acc: 0.75
Batch: 420; loss: 1.12; acc: 0.69
Batch: 440; loss: 1.05; acc: 0.72
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.29; acc: 0.61
Batch: 500; loss: 0.91; acc: 0.8
Batch: 520; loss: 1.22; acc: 0.61
Batch: 540; loss: 1.17; acc: 0.59
Batch: 560; loss: 1.06; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.58
Batch: 600; loss: 1.05; acc: 0.72
Batch: 620; loss: 1.19; acc: 0.67
Batch: 640; loss: 1.16; acc: 0.73
Batch: 660; loss: 1.11; acc: 0.7
Batch: 680; loss: 1.11; acc: 0.69
Batch: 700; loss: 1.3; acc: 0.58
Batch: 720; loss: 0.93; acc: 0.75
Batch: 740; loss: 1.04; acc: 0.77
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.26; acc: 0.55
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.55
Batch: 40; loss: 0.93; acc: 0.73
Batch: 60; loss: 1.02; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.91; acc: 0.81
Val Epoch over. val_loss: 1.1126371899228187; val_accuracy: 0.7005374203821656 

Epoch 15 start
The current lr is: 1.69e-05
Batch: 0; loss: 1.26; acc: 0.61
Batch: 20; loss: 1.43; acc: 0.58
Batch: 40; loss: 1.06; acc: 0.73
Batch: 60; loss: 1.14; acc: 0.73
Batch: 80; loss: 1.16; acc: 0.64
Batch: 100; loss: 0.99; acc: 0.77
Batch: 120; loss: 1.1; acc: 0.67
Batch: 140; loss: 1.2; acc: 0.64
Batch: 160; loss: 1.05; acc: 0.69
Batch: 180; loss: 1.1; acc: 0.64
Batch: 200; loss: 1.25; acc: 0.67
Batch: 220; loss: 1.07; acc: 0.72
Batch: 240; loss: 1.04; acc: 0.75
Batch: 260; loss: 1.18; acc: 0.58
Batch: 280; loss: 1.07; acc: 0.69
Batch: 300; loss: 1.18; acc: 0.62
Batch: 320; loss: 1.16; acc: 0.67
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 1.07; acc: 0.64
Batch: 380; loss: 1.14; acc: 0.72
Batch: 400; loss: 1.14; acc: 0.67
Batch: 420; loss: 1.21; acc: 0.64
Batch: 440; loss: 1.12; acc: 0.69
Batch: 460; loss: 1.29; acc: 0.55
Batch: 480; loss: 1.25; acc: 0.64
Batch: 500; loss: 1.16; acc: 0.67
Batch: 520; loss: 1.24; acc: 0.62
Batch: 540; loss: 0.91; acc: 0.77
Batch: 560; loss: 1.16; acc: 0.69
Batch: 580; loss: 1.2; acc: 0.59
Batch: 600; loss: 1.03; acc: 0.75
Batch: 620; loss: 1.3; acc: 0.69
Batch: 640; loss: 1.08; acc: 0.73
Batch: 660; loss: 1.08; acc: 0.7
Batch: 680; loss: 1.13; acc: 0.67
Batch: 700; loss: 1.21; acc: 0.7
Batch: 720; loss: 1.31; acc: 0.62
Batch: 740; loss: 1.08; acc: 0.75
Batch: 760; loss: 1.19; acc: 0.72
Batch: 780; loss: 1.39; acc: 0.62
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.55
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.83
Val Epoch over. val_loss: 1.1083858901528036; val_accuracy: 0.7013335987261147 

Epoch 16 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 1.12; acc: 0.73
Batch: 20; loss: 1.07; acc: 0.72
Batch: 40; loss: 1.16; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 1.22; acc: 0.66
Batch: 100; loss: 1.0; acc: 0.72
Batch: 120; loss: 1.34; acc: 0.61
Batch: 140; loss: 1.13; acc: 0.66
Batch: 160; loss: 1.17; acc: 0.69
Batch: 180; loss: 1.19; acc: 0.69
Batch: 200; loss: 0.95; acc: 0.83
Batch: 220; loss: 1.02; acc: 0.78
Batch: 240; loss: 1.14; acc: 0.69
Batch: 260; loss: 1.11; acc: 0.72
Batch: 280; loss: 1.09; acc: 0.61
Batch: 300; loss: 1.05; acc: 0.75
Batch: 320; loss: 1.25; acc: 0.55
Batch: 340; loss: 1.14; acc: 0.59
Batch: 360; loss: 1.15; acc: 0.7
Batch: 380; loss: 1.2; acc: 0.67
Batch: 400; loss: 1.34; acc: 0.62
Batch: 420; loss: 1.13; acc: 0.66
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 1.09; acc: 0.7
Batch: 480; loss: 1.28; acc: 0.61
Batch: 500; loss: 1.13; acc: 0.73
Batch: 520; loss: 0.98; acc: 0.77
Batch: 540; loss: 1.11; acc: 0.59
Batch: 560; loss: 0.99; acc: 0.73
Batch: 580; loss: 1.17; acc: 0.69
Batch: 600; loss: 1.12; acc: 0.7
Batch: 620; loss: 1.24; acc: 0.69
Batch: 640; loss: 1.19; acc: 0.73
Batch: 660; loss: 1.08; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.67
Batch: 700; loss: 1.32; acc: 0.64
Batch: 720; loss: 1.09; acc: 0.66
Batch: 740; loss: 1.13; acc: 0.66
Batch: 760; loss: 1.09; acc: 0.73
Batch: 780; loss: 1.11; acc: 0.75
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.55
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1078887688126533; val_accuracy: 0.7013335987261147 

Epoch 17 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 1.04; acc: 0.77
Batch: 20; loss: 1.22; acc: 0.73
Batch: 40; loss: 1.31; acc: 0.61
Batch: 60; loss: 1.18; acc: 0.67
Batch: 80; loss: 1.11; acc: 0.72
Batch: 100; loss: 1.29; acc: 0.69
Batch: 120; loss: 1.09; acc: 0.77
Batch: 140; loss: 1.22; acc: 0.56
Batch: 160; loss: 1.22; acc: 0.72
Batch: 180; loss: 1.21; acc: 0.55
Batch: 200; loss: 1.1; acc: 0.75
Batch: 220; loss: 1.43; acc: 0.62
Batch: 240; loss: 1.29; acc: 0.62
Batch: 260; loss: 1.36; acc: 0.56
Batch: 280; loss: 1.14; acc: 0.72
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.2; acc: 0.56
Batch: 340; loss: 1.1; acc: 0.62
Batch: 360; loss: 1.27; acc: 0.62
Batch: 380; loss: 1.01; acc: 0.73
Batch: 400; loss: 1.04; acc: 0.78
Batch: 420; loss: 1.28; acc: 0.56
Batch: 440; loss: 1.1; acc: 0.67
Batch: 460; loss: 1.31; acc: 0.62
Batch: 480; loss: 1.04; acc: 0.69
Batch: 500; loss: 1.05; acc: 0.69
Batch: 520; loss: 1.16; acc: 0.67
Batch: 540; loss: 1.22; acc: 0.58
Batch: 560; loss: 1.29; acc: 0.53
Batch: 580; loss: 1.09; acc: 0.64
Batch: 600; loss: 1.02; acc: 0.7
Batch: 620; loss: 1.27; acc: 0.59
Batch: 640; loss: 1.11; acc: 0.64
Batch: 660; loss: 1.18; acc: 0.7
Batch: 680; loss: 1.13; acc: 0.72
Batch: 700; loss: 1.14; acc: 0.75
Batch: 720; loss: 1.24; acc: 0.59
Batch: 740; loss: 1.12; acc: 0.67
Batch: 760; loss: 1.2; acc: 0.66
Batch: 780; loss: 1.2; acc: 0.61
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.55
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1073827675193737; val_accuracy: 0.7017316878980892 

Epoch 18 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 1.26; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.72
Batch: 40; loss: 1.14; acc: 0.7
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.11; acc: 0.67
Batch: 100; loss: 1.3; acc: 0.62
Batch: 120; loss: 1.05; acc: 0.7
Batch: 140; loss: 1.08; acc: 0.75
Batch: 160; loss: 1.14; acc: 0.61
Batch: 180; loss: 1.21; acc: 0.62
Batch: 200; loss: 1.23; acc: 0.62
Batch: 220; loss: 1.07; acc: 0.66
Batch: 240; loss: 1.37; acc: 0.55
Batch: 260; loss: 1.14; acc: 0.66
Batch: 280; loss: 1.34; acc: 0.62
Batch: 300; loss: 1.09; acc: 0.62
Batch: 320; loss: 1.21; acc: 0.64
Batch: 340; loss: 1.04; acc: 0.7
Batch: 360; loss: 1.13; acc: 0.64
Batch: 380; loss: 1.01; acc: 0.69
Batch: 400; loss: 1.21; acc: 0.66
Batch: 420; loss: 1.19; acc: 0.67
Batch: 440; loss: 1.02; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.7
Batch: 480; loss: 1.16; acc: 0.66
Batch: 500; loss: 0.99; acc: 0.81
Batch: 520; loss: 1.2; acc: 0.67
Batch: 540; loss: 0.97; acc: 0.78
Batch: 560; loss: 1.24; acc: 0.67
Batch: 580; loss: 1.22; acc: 0.59
Batch: 600; loss: 1.04; acc: 0.77
Batch: 620; loss: 1.29; acc: 0.56
Batch: 640; loss: 1.2; acc: 0.67
Batch: 660; loss: 1.18; acc: 0.69
Batch: 680; loss: 1.19; acc: 0.69
Batch: 700; loss: 1.34; acc: 0.64
Batch: 720; loss: 0.94; acc: 0.75
Batch: 740; loss: 1.16; acc: 0.61
Batch: 760; loss: 1.11; acc: 0.7
Batch: 780; loss: 1.03; acc: 0.77
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1068742878877433; val_accuracy: 0.7019307324840764 

Epoch 19 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.17; acc: 0.66
Batch: 40; loss: 1.3; acc: 0.56
Batch: 60; loss: 1.14; acc: 0.64
Batch: 80; loss: 1.02; acc: 0.73
Batch: 100; loss: 1.03; acc: 0.75
Batch: 120; loss: 1.13; acc: 0.7
Batch: 140; loss: 1.14; acc: 0.7
Batch: 160; loss: 1.16; acc: 0.62
Batch: 180; loss: 1.23; acc: 0.66
Batch: 200; loss: 1.1; acc: 0.69
Batch: 220; loss: 1.11; acc: 0.7
Batch: 240; loss: 0.99; acc: 0.77
Batch: 260; loss: 1.14; acc: 0.64
Batch: 280; loss: 1.17; acc: 0.58
Batch: 300; loss: 1.05; acc: 0.69
Batch: 320; loss: 1.13; acc: 0.7
Batch: 340; loss: 1.31; acc: 0.62
Batch: 360; loss: 1.24; acc: 0.64
Batch: 380; loss: 1.13; acc: 0.75
Batch: 400; loss: 1.32; acc: 0.64
Batch: 420; loss: 1.2; acc: 0.64
Batch: 440; loss: 1.03; acc: 0.73
Batch: 460; loss: 1.14; acc: 0.62
Batch: 480; loss: 1.2; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.64
Batch: 520; loss: 1.15; acc: 0.61
Batch: 540; loss: 1.21; acc: 0.72
Batch: 560; loss: 1.09; acc: 0.66
Batch: 580; loss: 1.17; acc: 0.7
Batch: 600; loss: 1.07; acc: 0.72
Batch: 620; loss: 1.18; acc: 0.73
Batch: 640; loss: 1.11; acc: 0.66
Batch: 660; loss: 1.18; acc: 0.67
Batch: 680; loss: 1.09; acc: 0.69
Batch: 700; loss: 1.14; acc: 0.72
Batch: 720; loss: 1.14; acc: 0.67
Batch: 740; loss: 1.02; acc: 0.72
Batch: 760; loss: 1.1; acc: 0.69
Batch: 780; loss: 1.09; acc: 0.75
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.73
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.106363530371599; val_accuracy: 0.70203025477707 

Epoch 20 start
The current lr is: 2.1970000000000003e-06
Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 1.14; acc: 0.64
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.22; acc: 0.64
Batch: 100; loss: 1.16; acc: 0.66
Batch: 120; loss: 1.2; acc: 0.62
Batch: 140; loss: 0.98; acc: 0.75
Batch: 160; loss: 1.02; acc: 0.77
Batch: 180; loss: 1.11; acc: 0.69
Batch: 200; loss: 1.0; acc: 0.7
Batch: 220; loss: 0.99; acc: 0.75
Batch: 240; loss: 1.24; acc: 0.64
Batch: 260; loss: 1.24; acc: 0.61
Batch: 280; loss: 0.97; acc: 0.75
Batch: 300; loss: 1.13; acc: 0.69
Batch: 320; loss: 1.09; acc: 0.67
Batch: 340; loss: 1.3; acc: 0.59
Batch: 360; loss: 1.2; acc: 0.66
Batch: 380; loss: 1.11; acc: 0.75
Batch: 400; loss: 1.08; acc: 0.75
Batch: 420; loss: 1.15; acc: 0.72
Batch: 440; loss: 1.12; acc: 0.69
Batch: 460; loss: 1.08; acc: 0.77
Batch: 480; loss: 1.13; acc: 0.62
Batch: 500; loss: 0.96; acc: 0.73
Batch: 520; loss: 1.12; acc: 0.67
Batch: 540; loss: 1.26; acc: 0.66
Batch: 560; loss: 1.34; acc: 0.59
Batch: 580; loss: 1.25; acc: 0.67
Batch: 600; loss: 1.2; acc: 0.67
Batch: 620; loss: 1.27; acc: 0.62
Batch: 640; loss: 1.06; acc: 0.69
Batch: 660; loss: 0.96; acc: 0.84
Batch: 680; loss: 1.21; acc: 0.77
Batch: 700; loss: 1.14; acc: 0.67
Batch: 720; loss: 1.27; acc: 0.52
Batch: 740; loss: 1.1; acc: 0.66
Batch: 760; loss: 1.12; acc: 0.67
Batch: 780; loss: 1.44; acc: 0.61
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.77
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1058428014159962; val_accuracy: 0.7025278662420382 

Epoch 21 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 1.16; acc: 0.66
Batch: 20; loss: 1.12; acc: 0.72
Batch: 40; loss: 1.27; acc: 0.61
Batch: 60; loss: 0.99; acc: 0.75
Batch: 80; loss: 1.3; acc: 0.64
Batch: 100; loss: 1.16; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.69
Batch: 140; loss: 1.14; acc: 0.66
Batch: 160; loss: 1.02; acc: 0.81
Batch: 180; loss: 1.32; acc: 0.62
Batch: 200; loss: 0.99; acc: 0.78
Batch: 220; loss: 1.12; acc: 0.73
Batch: 240; loss: 0.98; acc: 0.7
Batch: 260; loss: 1.02; acc: 0.75
Batch: 280; loss: 1.2; acc: 0.61
Batch: 300; loss: 1.02; acc: 0.77
Batch: 320; loss: 1.24; acc: 0.64
Batch: 340; loss: 1.08; acc: 0.7
Batch: 360; loss: 1.09; acc: 0.73
Batch: 380; loss: 1.16; acc: 0.66
Batch: 400; loss: 0.91; acc: 0.83
Batch: 420; loss: 1.16; acc: 0.67
Batch: 440; loss: 1.14; acc: 0.75
Batch: 460; loss: 1.26; acc: 0.64
Batch: 480; loss: 1.26; acc: 0.66
Batch: 500; loss: 1.13; acc: 0.7
Batch: 520; loss: 1.07; acc: 0.7
Batch: 540; loss: 1.29; acc: 0.59
Batch: 560; loss: 1.1; acc: 0.78
Batch: 580; loss: 1.06; acc: 0.77
Batch: 600; loss: 1.22; acc: 0.58
Batch: 620; loss: 1.27; acc: 0.72
Batch: 640; loss: 1.13; acc: 0.72
Batch: 660; loss: 1.16; acc: 0.61
Batch: 680; loss: 1.03; acc: 0.73
Batch: 700; loss: 1.04; acc: 0.69
Batch: 720; loss: 1.0; acc: 0.73
Batch: 740; loss: 1.13; acc: 0.73
Batch: 760; loss: 1.13; acc: 0.67
Batch: 780; loss: 1.32; acc: 0.62
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.77
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1057878163210146; val_accuracy: 0.7025278662420382 

Epoch 22 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 1.33; acc: 0.67
Batch: 20; loss: 1.07; acc: 0.66
Batch: 40; loss: 1.12; acc: 0.62
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.34; acc: 0.61
Batch: 100; loss: 1.31; acc: 0.61
Batch: 120; loss: 1.46; acc: 0.55
Batch: 140; loss: 1.08; acc: 0.72
Batch: 160; loss: 1.2; acc: 0.69
Batch: 180; loss: 1.15; acc: 0.64
Batch: 200; loss: 1.35; acc: 0.62
Batch: 220; loss: 1.28; acc: 0.62
Batch: 240; loss: 1.13; acc: 0.75
Batch: 260; loss: 1.16; acc: 0.64
Batch: 280; loss: 1.02; acc: 0.73
Batch: 300; loss: 1.24; acc: 0.59
Batch: 320; loss: 1.14; acc: 0.67
Batch: 340; loss: 1.06; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.7
Batch: 380; loss: 1.07; acc: 0.8
Batch: 400; loss: 1.22; acc: 0.66
Batch: 420; loss: 1.28; acc: 0.66
Batch: 440; loss: 1.22; acc: 0.61
Batch: 460; loss: 1.06; acc: 0.69
Batch: 480; loss: 1.09; acc: 0.72
Batch: 500; loss: 1.06; acc: 0.64
Batch: 520; loss: 1.0; acc: 0.73
Batch: 540; loss: 1.14; acc: 0.64
Batch: 560; loss: 1.19; acc: 0.61
Batch: 580; loss: 1.11; acc: 0.75
Batch: 600; loss: 1.06; acc: 0.72
Batch: 620; loss: 1.14; acc: 0.69
Batch: 640; loss: 1.15; acc: 0.67
Batch: 660; loss: 1.29; acc: 0.64
Batch: 680; loss: 1.04; acc: 0.75
Batch: 700; loss: 1.16; acc: 0.72
Batch: 720; loss: 1.14; acc: 0.72
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.09; acc: 0.75
Batch: 780; loss: 0.92; acc: 0.69
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.77
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1057324603105048; val_accuracy: 0.7025278662420382 

Epoch 23 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 1.04; acc: 0.73
Batch: 40; loss: 0.99; acc: 0.73
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.01; acc: 0.7
Batch: 100; loss: 1.02; acc: 0.72
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 1.18; acc: 0.66
Batch: 160; loss: 1.24; acc: 0.62
Batch: 180; loss: 1.16; acc: 0.72
Batch: 200; loss: 1.2; acc: 0.69
Batch: 220; loss: 1.42; acc: 0.55
Batch: 240; loss: 1.18; acc: 0.69
Batch: 260; loss: 1.1; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.77
Batch: 300; loss: 1.04; acc: 0.77
Batch: 320; loss: 1.19; acc: 0.61
Batch: 340; loss: 1.08; acc: 0.73
Batch: 360; loss: 1.1; acc: 0.77
Batch: 380; loss: 1.12; acc: 0.78
Batch: 400; loss: 1.31; acc: 0.61
Batch: 420; loss: 1.1; acc: 0.72
Batch: 440; loss: 1.11; acc: 0.73
Batch: 460; loss: 1.07; acc: 0.67
Batch: 480; loss: 1.07; acc: 0.69
Batch: 500; loss: 1.09; acc: 0.61
Batch: 520; loss: 1.02; acc: 0.73
Batch: 540; loss: 1.16; acc: 0.69
Batch: 560; loss: 1.25; acc: 0.64
Batch: 580; loss: 1.12; acc: 0.7
Batch: 600; loss: 0.97; acc: 0.77
Batch: 620; loss: 1.17; acc: 0.61
Batch: 640; loss: 1.23; acc: 0.69
Batch: 660; loss: 1.1; acc: 0.67
Batch: 680; loss: 1.0; acc: 0.7
Batch: 700; loss: 1.14; acc: 0.66
Batch: 720; loss: 1.17; acc: 0.69
Batch: 740; loss: 1.16; acc: 0.69
Batch: 760; loss: 1.15; acc: 0.67
Batch: 780; loss: 1.29; acc: 0.62
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.105677199591497; val_accuracy: 0.7023288216560509 

Epoch 24 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.31; acc: 0.59
Batch: 40; loss: 1.14; acc: 0.7
Batch: 60; loss: 1.25; acc: 0.66
Batch: 80; loss: 1.16; acc: 0.62
Batch: 100; loss: 1.19; acc: 0.62
Batch: 120; loss: 1.09; acc: 0.7
Batch: 140; loss: 1.2; acc: 0.66
Batch: 160; loss: 1.4; acc: 0.59
Batch: 180; loss: 1.04; acc: 0.69
Batch: 200; loss: 1.15; acc: 0.67
Batch: 220; loss: 1.01; acc: 0.78
Batch: 240; loss: 1.02; acc: 0.77
Batch: 260; loss: 1.01; acc: 0.69
Batch: 280; loss: 1.07; acc: 0.66
Batch: 300; loss: 1.27; acc: 0.59
Batch: 320; loss: 1.0; acc: 0.69
Batch: 340; loss: 1.06; acc: 0.66
Batch: 360; loss: 1.18; acc: 0.61
Batch: 380; loss: 1.18; acc: 0.75
Batch: 400; loss: 1.07; acc: 0.75
Batch: 420; loss: 1.03; acc: 0.67
Batch: 440; loss: 1.25; acc: 0.56
Batch: 460; loss: 1.22; acc: 0.72
Batch: 480; loss: 1.03; acc: 0.69
Batch: 500; loss: 1.02; acc: 0.69
Batch: 520; loss: 0.99; acc: 0.73
Batch: 540; loss: 1.23; acc: 0.53
Batch: 560; loss: 1.23; acc: 0.66
Batch: 580; loss: 0.94; acc: 0.7
Batch: 600; loss: 1.04; acc: 0.7
Batch: 620; loss: 1.1; acc: 0.77
Batch: 640; loss: 1.13; acc: 0.66
Batch: 660; loss: 1.28; acc: 0.64
Batch: 680; loss: 1.13; acc: 0.69
Batch: 700; loss: 1.27; acc: 0.64
Batch: 720; loss: 1.04; acc: 0.72
Batch: 740; loss: 1.34; acc: 0.59
Batch: 760; loss: 1.22; acc: 0.69
Batch: 780; loss: 1.26; acc: 0.62
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1056222247469956; val_accuracy: 0.7023288216560509 

Epoch 25 start
The current lr is: 2.856100000000001e-07
Batch: 0; loss: 1.05; acc: 0.7
Batch: 20; loss: 1.12; acc: 0.62
Batch: 40; loss: 1.05; acc: 0.7
Batch: 60; loss: 1.03; acc: 0.72
Batch: 80; loss: 1.23; acc: 0.67
Batch: 100; loss: 1.2; acc: 0.56
Batch: 120; loss: 1.13; acc: 0.73
Batch: 140; loss: 1.12; acc: 0.7
Batch: 160; loss: 1.17; acc: 0.62
Batch: 180; loss: 1.04; acc: 0.73
Batch: 200; loss: 1.17; acc: 0.66
Batch: 220; loss: 1.13; acc: 0.7
Batch: 240; loss: 1.07; acc: 0.72
Batch: 260; loss: 1.04; acc: 0.69
Batch: 280; loss: 1.38; acc: 0.62
Batch: 300; loss: 0.97; acc: 0.75
Batch: 320; loss: 1.31; acc: 0.61
Batch: 340; loss: 1.33; acc: 0.64
Batch: 360; loss: 1.09; acc: 0.64
Batch: 380; loss: 1.09; acc: 0.7
Batch: 400; loss: 1.12; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.75
Batch: 440; loss: 0.87; acc: 0.81
Batch: 460; loss: 1.2; acc: 0.7
Batch: 480; loss: 1.06; acc: 0.72
Batch: 500; loss: 1.15; acc: 0.7
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.0; acc: 0.72
Batch: 560; loss: 1.31; acc: 0.62
Batch: 580; loss: 1.01; acc: 0.73
Batch: 600; loss: 1.26; acc: 0.69
Batch: 620; loss: 1.04; acc: 0.7
Batch: 640; loss: 0.91; acc: 0.77
Batch: 660; loss: 1.12; acc: 0.59
Batch: 680; loss: 0.98; acc: 0.72
Batch: 700; loss: 1.2; acc: 0.62
Batch: 720; loss: 1.28; acc: 0.56
Batch: 740; loss: 1.24; acc: 0.66
Batch: 760; loss: 1.09; acc: 0.69
Batch: 780; loss: 1.1; acc: 0.67
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055666747366546; val_accuracy: 0.7023288216560509 

Epoch 26 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 1.16; acc: 0.64
Batch: 20; loss: 1.24; acc: 0.56
Batch: 40; loss: 1.14; acc: 0.66
Batch: 60; loss: 1.11; acc: 0.69
Batch: 80; loss: 1.05; acc: 0.72
Batch: 100; loss: 0.98; acc: 0.83
Batch: 120; loss: 1.15; acc: 0.66
Batch: 140; loss: 1.0; acc: 0.75
Batch: 160; loss: 1.46; acc: 0.56
Batch: 180; loss: 1.13; acc: 0.66
Batch: 200; loss: 1.01; acc: 0.83
Batch: 220; loss: 1.3; acc: 0.58
Batch: 240; loss: 1.23; acc: 0.66
Batch: 260; loss: 1.06; acc: 0.72
Batch: 280; loss: 1.15; acc: 0.67
Batch: 300; loss: 1.21; acc: 0.66
Batch: 320; loss: 1.13; acc: 0.67
Batch: 340; loss: 1.18; acc: 0.66
Batch: 360; loss: 1.08; acc: 0.67
Batch: 380; loss: 0.92; acc: 0.75
Batch: 400; loss: 1.11; acc: 0.67
Batch: 420; loss: 1.12; acc: 0.67
Batch: 440; loss: 1.05; acc: 0.7
Batch: 460; loss: 0.98; acc: 0.7
Batch: 480; loss: 1.19; acc: 0.73
Batch: 500; loss: 1.35; acc: 0.62
Batch: 520; loss: 1.13; acc: 0.72
Batch: 540; loss: 0.99; acc: 0.7
Batch: 560; loss: 1.04; acc: 0.72
Batch: 580; loss: 1.11; acc: 0.7
Batch: 600; loss: 1.28; acc: 0.66
Batch: 620; loss: 1.13; acc: 0.7
Batch: 640; loss: 1.16; acc: 0.7
Batch: 660; loss: 1.06; acc: 0.59
Batch: 680; loss: 0.95; acc: 0.75
Batch: 700; loss: 1.02; acc: 0.72
Batch: 720; loss: 1.17; acc: 0.77
Batch: 740; loss: 1.23; acc: 0.66
Batch: 760; loss: 1.15; acc: 0.7
Batch: 780; loss: 1.28; acc: 0.61
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055637590444771; val_accuracy: 0.7023288216560509 

Epoch 27 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 1.2; acc: 0.64
Batch: 20; loss: 1.32; acc: 0.61
Batch: 40; loss: 1.16; acc: 0.61
Batch: 60; loss: 1.23; acc: 0.66
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 1.03; acc: 0.7
Batch: 120; loss: 1.14; acc: 0.72
Batch: 140; loss: 1.15; acc: 0.7
Batch: 160; loss: 1.17; acc: 0.75
Batch: 180; loss: 1.16; acc: 0.69
Batch: 200; loss: 1.06; acc: 0.66
Batch: 220; loss: 0.97; acc: 0.75
Batch: 240; loss: 0.9; acc: 0.83
Batch: 260; loss: 1.03; acc: 0.78
Batch: 280; loss: 1.35; acc: 0.58
Batch: 300; loss: 1.23; acc: 0.64
Batch: 320; loss: 1.34; acc: 0.53
Batch: 340; loss: 1.1; acc: 0.78
Batch: 360; loss: 1.08; acc: 0.66
Batch: 380; loss: 1.09; acc: 0.64
Batch: 400; loss: 1.23; acc: 0.64
Batch: 420; loss: 1.14; acc: 0.66
Batch: 440; loss: 1.13; acc: 0.72
Batch: 460; loss: 1.11; acc: 0.75
Batch: 480; loss: 1.16; acc: 0.7
Batch: 500; loss: 1.27; acc: 0.58
Batch: 520; loss: 1.25; acc: 0.64
Batch: 540; loss: 1.15; acc: 0.72
Batch: 560; loss: 1.14; acc: 0.7
Batch: 580; loss: 1.08; acc: 0.67
Batch: 600; loss: 0.99; acc: 0.77
Batch: 620; loss: 1.15; acc: 0.66
Batch: 640; loss: 1.26; acc: 0.61
Batch: 660; loss: 1.14; acc: 0.62
Batch: 680; loss: 1.17; acc: 0.69
Batch: 700; loss: 0.99; acc: 0.77
Batch: 720; loss: 1.06; acc: 0.73
Batch: 740; loss: 1.16; acc: 0.64
Batch: 760; loss: 1.14; acc: 0.73
Batch: 780; loss: 1.24; acc: 0.61
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055605779787538; val_accuracy: 0.7023288216560509 

Epoch 28 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 1.19; acc: 0.7
Batch: 20; loss: 1.15; acc: 0.59
Batch: 40; loss: 1.14; acc: 0.62
Batch: 60; loss: 1.17; acc: 0.67
Batch: 80; loss: 1.03; acc: 0.72
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.0; acc: 0.67
Batch: 140; loss: 1.1; acc: 0.73
Batch: 160; loss: 1.12; acc: 0.67
Batch: 180; loss: 1.09; acc: 0.67
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.34; acc: 0.56
Batch: 240; loss: 1.13; acc: 0.69
Batch: 260; loss: 1.04; acc: 0.7
Batch: 280; loss: 1.37; acc: 0.64
Batch: 300; loss: 1.09; acc: 0.72
Batch: 320; loss: 1.12; acc: 0.67
Batch: 340; loss: 1.08; acc: 0.78
Batch: 360; loss: 1.11; acc: 0.72
Batch: 380; loss: 1.21; acc: 0.66
Batch: 400; loss: 1.14; acc: 0.7
Batch: 420; loss: 1.25; acc: 0.72
Batch: 440; loss: 1.21; acc: 0.64
Batch: 460; loss: 1.13; acc: 0.67
Batch: 480; loss: 1.09; acc: 0.7
Batch: 500; loss: 1.04; acc: 0.73
Batch: 520; loss: 1.27; acc: 0.59
Batch: 540; loss: 1.2; acc: 0.7
Batch: 560; loss: 1.18; acc: 0.77
Batch: 580; loss: 1.17; acc: 0.66
Batch: 600; loss: 0.95; acc: 0.77
Batch: 620; loss: 1.31; acc: 0.61
Batch: 640; loss: 0.98; acc: 0.77
Batch: 660; loss: 1.1; acc: 0.73
Batch: 680; loss: 1.2; acc: 0.7
Batch: 700; loss: 1.37; acc: 0.56
Batch: 720; loss: 0.99; acc: 0.78
Batch: 740; loss: 1.11; acc: 0.73
Batch: 760; loss: 1.04; acc: 0.73
Batch: 780; loss: 1.02; acc: 0.73
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055575897739192; val_accuracy: 0.7023288216560509 

Epoch 29 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 1.37; acc: 0.55
Batch: 20; loss: 1.16; acc: 0.61
Batch: 40; loss: 1.27; acc: 0.64
Batch: 60; loss: 1.05; acc: 0.75
Batch: 80; loss: 1.13; acc: 0.62
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 1.14; acc: 0.73
Batch: 160; loss: 1.16; acc: 0.67
Batch: 180; loss: 0.99; acc: 0.73
Batch: 200; loss: 1.19; acc: 0.7
Batch: 220; loss: 1.22; acc: 0.62
Batch: 240; loss: 1.35; acc: 0.56
Batch: 260; loss: 1.04; acc: 0.73
Batch: 280; loss: 1.27; acc: 0.62
Batch: 300; loss: 1.18; acc: 0.72
Batch: 320; loss: 1.05; acc: 0.72
Batch: 340; loss: 1.0; acc: 0.81
Batch: 360; loss: 0.96; acc: 0.75
Batch: 380; loss: 1.06; acc: 0.73
Batch: 400; loss: 1.1; acc: 0.75
Batch: 420; loss: 1.06; acc: 0.72
Batch: 440; loss: 1.22; acc: 0.62
Batch: 460; loss: 1.12; acc: 0.66
Batch: 480; loss: 1.17; acc: 0.66
Batch: 500; loss: 0.98; acc: 0.78
Batch: 520; loss: 1.23; acc: 0.62
Batch: 540; loss: 1.23; acc: 0.61
Batch: 560; loss: 1.09; acc: 0.7
Batch: 580; loss: 1.25; acc: 0.61
Batch: 600; loss: 0.95; acc: 0.77
Batch: 620; loss: 1.06; acc: 0.73
Batch: 640; loss: 0.98; acc: 0.7
Batch: 660; loss: 1.25; acc: 0.69
Batch: 680; loss: 1.1; acc: 0.72
Batch: 700; loss: 1.08; acc: 0.64
Batch: 720; loss: 1.04; acc: 0.69
Batch: 740; loss: 1.11; acc: 0.69
Batch: 760; loss: 1.27; acc: 0.64
Batch: 780; loss: 1.09; acc: 0.69
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055545434830294; val_accuracy: 0.7023288216560509 

Epoch 30 start
The current lr is: 3.7129300000000004e-08
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 0.98; acc: 0.72
Batch: 60; loss: 1.18; acc: 0.66
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.07; acc: 0.7
Batch: 120; loss: 1.22; acc: 0.7
Batch: 140; loss: 1.3; acc: 0.64
Batch: 160; loss: 0.99; acc: 0.77
Batch: 180; loss: 1.1; acc: 0.72
Batch: 200; loss: 1.06; acc: 0.7
Batch: 220; loss: 1.12; acc: 0.62
Batch: 240; loss: 1.1; acc: 0.72
Batch: 260; loss: 1.17; acc: 0.64
Batch: 280; loss: 1.17; acc: 0.64
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 1.16; acc: 0.69
Batch: 340; loss: 1.17; acc: 0.7
Batch: 360; loss: 1.13; acc: 0.69
Batch: 380; loss: 1.24; acc: 0.66
Batch: 400; loss: 1.08; acc: 0.62
Batch: 420; loss: 1.11; acc: 0.67
Batch: 440; loss: 1.19; acc: 0.69
Batch: 460; loss: 1.14; acc: 0.69
Batch: 480; loss: 1.22; acc: 0.62
Batch: 500; loss: 1.29; acc: 0.59
Batch: 520; loss: 1.26; acc: 0.61
Batch: 540; loss: 1.3; acc: 0.67
Batch: 560; loss: 1.13; acc: 0.73
Batch: 580; loss: 1.14; acc: 0.62
Batch: 600; loss: 1.13; acc: 0.67
Batch: 620; loss: 1.15; acc: 0.7
Batch: 640; loss: 1.21; acc: 0.61
Batch: 660; loss: 1.08; acc: 0.77
Batch: 680; loss: 1.19; acc: 0.58
Batch: 700; loss: 1.09; acc: 0.62
Batch: 720; loss: 1.15; acc: 0.61
Batch: 740; loss: 1.04; acc: 0.77
Batch: 760; loss: 1.05; acc: 0.66
Batch: 780; loss: 1.1; acc: 0.67
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055515473055992; val_accuracy: 0.7023288216560509 

Epoch 31 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 1.03; acc: 0.73
Batch: 20; loss: 1.18; acc: 0.7
Batch: 40; loss: 1.2; acc: 0.66
Batch: 60; loss: 1.03; acc: 0.72
Batch: 80; loss: 1.31; acc: 0.56
Batch: 100; loss: 1.1; acc: 0.7
Batch: 120; loss: 1.2; acc: 0.61
Batch: 140; loss: 1.09; acc: 0.72
Batch: 160; loss: 1.24; acc: 0.58
Batch: 180; loss: 1.09; acc: 0.67
Batch: 200; loss: 1.2; acc: 0.66
Batch: 220; loss: 1.1; acc: 0.67
Batch: 240; loss: 1.04; acc: 0.75
Batch: 260; loss: 1.07; acc: 0.72
Batch: 280; loss: 1.18; acc: 0.62
Batch: 300; loss: 1.14; acc: 0.73
Batch: 320; loss: 0.98; acc: 0.84
Batch: 340; loss: 1.07; acc: 0.73
Batch: 360; loss: 1.03; acc: 0.75
Batch: 380; loss: 1.21; acc: 0.58
Batch: 400; loss: 1.4; acc: 0.61
Batch: 420; loss: 1.21; acc: 0.64
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.16; acc: 0.67
Batch: 480; loss: 1.19; acc: 0.7
Batch: 500; loss: 1.16; acc: 0.66
Batch: 520; loss: 1.03; acc: 0.67
Batch: 540; loss: 1.03; acc: 0.75
Batch: 560; loss: 0.99; acc: 0.7
Batch: 580; loss: 1.13; acc: 0.72
Batch: 600; loss: 1.15; acc: 0.7
Batch: 620; loss: 1.2; acc: 0.67
Batch: 640; loss: 1.01; acc: 0.69
Batch: 660; loss: 1.1; acc: 0.67
Batch: 680; loss: 1.26; acc: 0.61
Batch: 700; loss: 1.05; acc: 0.75
Batch: 720; loss: 1.21; acc: 0.69
Batch: 740; loss: 1.12; acc: 0.67
Batch: 760; loss: 1.33; acc: 0.61
Batch: 780; loss: 1.24; acc: 0.69
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055514877009545; val_accuracy: 0.7023288216560509 

Epoch 32 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.72
Batch: 40; loss: 1.05; acc: 0.8
Batch: 60; loss: 1.2; acc: 0.64
Batch: 80; loss: 1.26; acc: 0.64
Batch: 100; loss: 1.33; acc: 0.69
Batch: 120; loss: 0.98; acc: 0.75
Batch: 140; loss: 1.24; acc: 0.64
Batch: 160; loss: 1.06; acc: 0.69
Batch: 180; loss: 0.82; acc: 0.81
Batch: 200; loss: 0.94; acc: 0.77
Batch: 220; loss: 1.01; acc: 0.83
Batch: 240; loss: 1.08; acc: 0.7
Batch: 260; loss: 1.24; acc: 0.67
Batch: 280; loss: 1.15; acc: 0.72
Batch: 300; loss: 1.16; acc: 0.64
Batch: 320; loss: 1.2; acc: 0.66
Batch: 340; loss: 1.15; acc: 0.66
Batch: 360; loss: 1.02; acc: 0.69
Batch: 380; loss: 1.15; acc: 0.69
Batch: 400; loss: 1.05; acc: 0.66
Batch: 420; loss: 1.02; acc: 0.8
Batch: 440; loss: 1.15; acc: 0.66
Batch: 460; loss: 1.06; acc: 0.75
Batch: 480; loss: 1.12; acc: 0.69
Batch: 500; loss: 1.12; acc: 0.67
Batch: 520; loss: 0.97; acc: 0.77
Batch: 540; loss: 1.19; acc: 0.7
Batch: 560; loss: 1.2; acc: 0.73
Batch: 580; loss: 1.19; acc: 0.7
Batch: 600; loss: 1.21; acc: 0.62
Batch: 620; loss: 1.2; acc: 0.69
Batch: 640; loss: 1.35; acc: 0.58
Batch: 660; loss: 1.03; acc: 0.72
Batch: 680; loss: 1.08; acc: 0.7
Batch: 700; loss: 1.14; acc: 0.69
Batch: 720; loss: 1.11; acc: 0.67
Batch: 740; loss: 1.15; acc: 0.72
Batch: 760; loss: 1.17; acc: 0.64
Batch: 780; loss: 1.22; acc: 0.64
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055514531530393; val_accuracy: 0.7023288216560509 

Epoch 33 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 1.22; acc: 0.64
Batch: 20; loss: 1.12; acc: 0.77
Batch: 40; loss: 1.26; acc: 0.62
Batch: 60; loss: 1.07; acc: 0.75
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.64
Batch: 140; loss: 1.02; acc: 0.75
Batch: 160; loss: 1.25; acc: 0.62
Batch: 180; loss: 1.17; acc: 0.66
Batch: 200; loss: 1.18; acc: 0.66
Batch: 220; loss: 1.03; acc: 0.77
Batch: 240; loss: 1.21; acc: 0.72
Batch: 260; loss: 1.24; acc: 0.73
Batch: 280; loss: 1.09; acc: 0.8
Batch: 300; loss: 1.18; acc: 0.66
Batch: 320; loss: 1.3; acc: 0.61
Batch: 340; loss: 1.29; acc: 0.67
Batch: 360; loss: 1.2; acc: 0.62
Batch: 380; loss: 1.08; acc: 0.69
Batch: 400; loss: 1.09; acc: 0.7
Batch: 420; loss: 1.13; acc: 0.69
Batch: 440; loss: 1.22; acc: 0.72
Batch: 460; loss: 1.06; acc: 0.67
Batch: 480; loss: 1.09; acc: 0.67
Batch: 500; loss: 1.05; acc: 0.77
Batch: 520; loss: 1.08; acc: 0.69
Batch: 540; loss: 1.0; acc: 0.77
Batch: 560; loss: 1.02; acc: 0.78
Batch: 580; loss: 1.23; acc: 0.62
Batch: 600; loss: 1.19; acc: 0.61
Batch: 620; loss: 1.07; acc: 0.7
Batch: 640; loss: 1.17; acc: 0.7
Batch: 660; loss: 1.1; acc: 0.77
Batch: 680; loss: 0.96; acc: 0.73
Batch: 700; loss: 1.13; acc: 0.7
Batch: 720; loss: 1.08; acc: 0.67
Batch: 740; loss: 1.2; acc: 0.66
Batch: 760; loss: 1.23; acc: 0.58
Batch: 780; loss: 0.97; acc: 0.73
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.105551402280285; val_accuracy: 0.7023288216560509 

Epoch 34 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 1.09; acc: 0.73
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 1.16; acc: 0.59
Batch: 60; loss: 0.96; acc: 0.73
Batch: 80; loss: 1.18; acc: 0.69
Batch: 100; loss: 1.24; acc: 0.67
Batch: 120; loss: 1.15; acc: 0.66
Batch: 140; loss: 1.11; acc: 0.75
Batch: 160; loss: 0.95; acc: 0.73
Batch: 180; loss: 1.0; acc: 0.7
Batch: 200; loss: 1.24; acc: 0.61
Batch: 220; loss: 0.98; acc: 0.69
Batch: 240; loss: 1.02; acc: 0.64
Batch: 260; loss: 1.13; acc: 0.67
Batch: 280; loss: 1.12; acc: 0.64
Batch: 300; loss: 1.15; acc: 0.75
Batch: 320; loss: 1.06; acc: 0.73
Batch: 340; loss: 1.14; acc: 0.62
Batch: 360; loss: 1.19; acc: 0.67
Batch: 380; loss: 1.25; acc: 0.56
Batch: 400; loss: 1.12; acc: 0.7
Batch: 420; loss: 1.01; acc: 0.78
Batch: 440; loss: 1.17; acc: 0.62
Batch: 460; loss: 1.12; acc: 0.67
Batch: 480; loss: 1.21; acc: 0.67
Batch: 500; loss: 0.88; acc: 0.7
Batch: 520; loss: 1.22; acc: 0.73
Batch: 540; loss: 1.4; acc: 0.59
Batch: 560; loss: 0.94; acc: 0.78
Batch: 580; loss: 1.14; acc: 0.69
Batch: 600; loss: 1.22; acc: 0.62
Batch: 620; loss: 1.29; acc: 0.61
Batch: 640; loss: 1.18; acc: 0.7
Batch: 660; loss: 0.99; acc: 0.7
Batch: 680; loss: 0.96; acc: 0.8
Batch: 700; loss: 1.39; acc: 0.58
Batch: 720; loss: 1.17; acc: 0.73
Batch: 740; loss: 1.13; acc: 0.59
Batch: 760; loss: 1.09; acc: 0.72
Batch: 780; loss: 1.33; acc: 0.64
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513529261207; val_accuracy: 0.7023288216560509 

Epoch 35 start
The current lr is: 4.826809000000001e-09
Batch: 0; loss: 1.03; acc: 0.7
Batch: 20; loss: 1.08; acc: 0.73
Batch: 40; loss: 1.16; acc: 0.7
Batch: 60; loss: 1.19; acc: 0.66
Batch: 80; loss: 1.18; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.7
Batch: 120; loss: 1.18; acc: 0.61
Batch: 140; loss: 1.34; acc: 0.58
Batch: 160; loss: 1.01; acc: 0.75
Batch: 180; loss: 0.88; acc: 0.8
Batch: 200; loss: 1.0; acc: 0.72
Batch: 220; loss: 1.18; acc: 0.72
Batch: 240; loss: 1.16; acc: 0.69
Batch: 260; loss: 0.99; acc: 0.77
Batch: 280; loss: 1.07; acc: 0.64
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.33; acc: 0.61
Batch: 340; loss: 1.35; acc: 0.56
Batch: 360; loss: 1.11; acc: 0.7
Batch: 380; loss: 1.18; acc: 0.73
Batch: 400; loss: 0.94; acc: 0.7
Batch: 420; loss: 1.13; acc: 0.69
Batch: 440; loss: 1.02; acc: 0.69
Batch: 460; loss: 1.03; acc: 0.67
Batch: 480; loss: 1.25; acc: 0.64
Batch: 500; loss: 1.08; acc: 0.67
Batch: 520; loss: 1.23; acc: 0.66
Batch: 540; loss: 1.22; acc: 0.66
Batch: 560; loss: 1.08; acc: 0.75
Batch: 580; loss: 1.36; acc: 0.62
Batch: 600; loss: 1.1; acc: 0.66
Batch: 620; loss: 1.12; acc: 0.73
Batch: 640; loss: 0.95; acc: 0.72
Batch: 660; loss: 1.26; acc: 0.64
Batch: 680; loss: 1.17; acc: 0.67
Batch: 700; loss: 1.13; acc: 0.64
Batch: 720; loss: 1.15; acc: 0.67
Batch: 740; loss: 1.39; acc: 0.66
Batch: 760; loss: 1.12; acc: 0.77
Batch: 780; loss: 1.04; acc: 0.66
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.105551329767628; val_accuracy: 0.7023288216560509 

Epoch 36 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 0.87; acc: 0.83
Batch: 20; loss: 1.06; acc: 0.75
Batch: 40; loss: 0.99; acc: 0.69
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 1.04; acc: 0.75
Batch: 100; loss: 1.01; acc: 0.77
Batch: 120; loss: 1.15; acc: 0.64
Batch: 140; loss: 0.94; acc: 0.77
Batch: 160; loss: 1.14; acc: 0.7
Batch: 180; loss: 1.07; acc: 0.69
Batch: 200; loss: 1.15; acc: 0.66
Batch: 220; loss: 1.16; acc: 0.72
Batch: 240; loss: 1.16; acc: 0.62
Batch: 260; loss: 1.0; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.67
Batch: 300; loss: 1.23; acc: 0.69
Batch: 320; loss: 1.13; acc: 0.66
Batch: 340; loss: 1.33; acc: 0.56
Batch: 360; loss: 1.32; acc: 0.62
Batch: 380; loss: 1.46; acc: 0.58
Batch: 400; loss: 1.07; acc: 0.73
Batch: 420; loss: 1.24; acc: 0.67
Batch: 440; loss: 1.08; acc: 0.8
Batch: 460; loss: 1.22; acc: 0.67
Batch: 480; loss: 1.2; acc: 0.59
Batch: 500; loss: 1.05; acc: 0.72
Batch: 520; loss: 1.27; acc: 0.58
Batch: 540; loss: 1.09; acc: 0.66
Batch: 560; loss: 1.07; acc: 0.67
Batch: 580; loss: 1.03; acc: 0.75
Batch: 600; loss: 1.11; acc: 0.7
Batch: 620; loss: 1.21; acc: 0.66
Batch: 640; loss: 1.03; acc: 0.7
Batch: 660; loss: 1.18; acc: 0.69
Batch: 680; loss: 1.18; acc: 0.66
Batch: 700; loss: 1.08; acc: 0.66
Batch: 720; loss: 1.28; acc: 0.64
Batch: 740; loss: 1.07; acc: 0.69
Batch: 760; loss: 1.1; acc: 0.75
Batch: 780; loss: 1.09; acc: 0.69
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513168596158; val_accuracy: 0.7023288216560509 

Epoch 37 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 1.06; acc: 0.73
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 1.08; acc: 0.64
Batch: 60; loss: 1.21; acc: 0.66
Batch: 80; loss: 1.15; acc: 0.67
Batch: 100; loss: 0.99; acc: 0.7
Batch: 120; loss: 1.01; acc: 0.72
Batch: 140; loss: 1.06; acc: 0.72
Batch: 160; loss: 1.1; acc: 0.75
Batch: 180; loss: 1.18; acc: 0.7
Batch: 200; loss: 1.15; acc: 0.75
Batch: 220; loss: 1.42; acc: 0.55
Batch: 240; loss: 1.12; acc: 0.67
Batch: 260; loss: 1.21; acc: 0.73
Batch: 280; loss: 0.97; acc: 0.75
Batch: 300; loss: 1.04; acc: 0.72
Batch: 320; loss: 0.99; acc: 0.78
Batch: 340; loss: 1.22; acc: 0.59
Batch: 360; loss: 0.99; acc: 0.77
Batch: 380; loss: 1.49; acc: 0.58
Batch: 400; loss: 1.02; acc: 0.77
Batch: 420; loss: 1.33; acc: 0.56
Batch: 440; loss: 1.08; acc: 0.67
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 1.2; acc: 0.7
Batch: 500; loss: 1.17; acc: 0.7
Batch: 520; loss: 0.96; acc: 0.78
Batch: 540; loss: 1.11; acc: 0.62
Batch: 560; loss: 1.25; acc: 0.62
Batch: 580; loss: 1.12; acc: 0.67
Batch: 600; loss: 1.11; acc: 0.64
Batch: 620; loss: 1.09; acc: 0.64
Batch: 640; loss: 1.26; acc: 0.66
Batch: 660; loss: 1.16; acc: 0.67
Batch: 680; loss: 1.07; acc: 0.69
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.24; acc: 0.61
Batch: 740; loss: 1.05; acc: 0.67
Batch: 760; loss: 1.32; acc: 0.66
Batch: 780; loss: 1.05; acc: 0.8
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513221746798; val_accuracy: 0.7023288216560509 

Epoch 38 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.17; acc: 0.61
Batch: 40; loss: 1.07; acc: 0.7
Batch: 60; loss: 0.91; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 1.22; acc: 0.72
Batch: 120; loss: 1.36; acc: 0.62
Batch: 140; loss: 1.04; acc: 0.77
Batch: 160; loss: 1.07; acc: 0.66
Batch: 180; loss: 1.11; acc: 0.69
Batch: 200; loss: 1.29; acc: 0.62
Batch: 220; loss: 1.08; acc: 0.73
Batch: 240; loss: 1.18; acc: 0.72
Batch: 260; loss: 1.1; acc: 0.75
Batch: 280; loss: 1.21; acc: 0.66
Batch: 300; loss: 1.2; acc: 0.72
Batch: 320; loss: 1.07; acc: 0.72
Batch: 340; loss: 1.09; acc: 0.73
Batch: 360; loss: 1.22; acc: 0.64
Batch: 380; loss: 1.2; acc: 0.61
Batch: 400; loss: 1.51; acc: 0.59
Batch: 420; loss: 0.98; acc: 0.7
Batch: 440; loss: 1.12; acc: 0.72
Batch: 460; loss: 1.13; acc: 0.69
Batch: 480; loss: 0.85; acc: 0.81
Batch: 500; loss: 1.18; acc: 0.72
Batch: 520; loss: 1.08; acc: 0.62
Batch: 540; loss: 1.1; acc: 0.58
Batch: 560; loss: 1.16; acc: 0.64
Batch: 580; loss: 1.16; acc: 0.75
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.08; acc: 0.69
Batch: 640; loss: 1.13; acc: 0.72
Batch: 660; loss: 1.12; acc: 0.66
Batch: 680; loss: 1.23; acc: 0.59
Batch: 700; loss: 1.02; acc: 0.77
Batch: 720; loss: 1.41; acc: 0.58
Batch: 740; loss: 0.99; acc: 0.69
Batch: 760; loss: 1.17; acc: 0.66
Batch: 780; loss: 1.17; acc: 0.62
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513198967952; val_accuracy: 0.7023288216560509 

Epoch 39 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 1.03; acc: 0.72
Batch: 40; loss: 1.13; acc: 0.7
Batch: 60; loss: 1.05; acc: 0.67
Batch: 80; loss: 1.03; acc: 0.69
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 1.11; acc: 0.64
Batch: 140; loss: 1.31; acc: 0.62
Batch: 160; loss: 1.19; acc: 0.59
Batch: 180; loss: 1.32; acc: 0.7
Batch: 200; loss: 1.16; acc: 0.67
Batch: 220; loss: 1.38; acc: 0.58
Batch: 240; loss: 1.17; acc: 0.7
Batch: 260; loss: 1.07; acc: 0.75
Batch: 280; loss: 1.21; acc: 0.72
Batch: 300; loss: 1.16; acc: 0.64
Batch: 320; loss: 1.12; acc: 0.72
Batch: 340; loss: 1.05; acc: 0.64
Batch: 360; loss: 1.11; acc: 0.7
Batch: 380; loss: 1.15; acc: 0.72
Batch: 400; loss: 1.13; acc: 0.67
Batch: 420; loss: 1.14; acc: 0.67
Batch: 440; loss: 1.18; acc: 0.61
Batch: 460; loss: 1.05; acc: 0.69
Batch: 480; loss: 1.19; acc: 0.61
Batch: 500; loss: 1.12; acc: 0.75
Batch: 520; loss: 1.02; acc: 0.77
Batch: 540; loss: 1.17; acc: 0.66
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.12; acc: 0.73
Batch: 600; loss: 1.13; acc: 0.66
Batch: 620; loss: 1.25; acc: 0.59
Batch: 640; loss: 1.15; acc: 0.69
Batch: 660; loss: 1.29; acc: 0.66
Batch: 680; loss: 1.42; acc: 0.56
Batch: 700; loss: 1.2; acc: 0.62
Batch: 720; loss: 1.14; acc: 0.69
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.05; acc: 0.72
Batch: 780; loss: 1.18; acc: 0.58
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.105551329767628; val_accuracy: 0.7023288216560509 

Epoch 40 start
The current lr is: 6.274851700000002e-10
Batch: 0; loss: 1.06; acc: 0.77
Batch: 20; loss: 1.16; acc: 0.69
Batch: 40; loss: 1.28; acc: 0.64
Batch: 60; loss: 1.19; acc: 0.64
Batch: 80; loss: 1.22; acc: 0.62
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.01; acc: 0.81
Batch: 140; loss: 1.14; acc: 0.66
Batch: 160; loss: 1.13; acc: 0.72
Batch: 180; loss: 1.22; acc: 0.66
Batch: 200; loss: 1.06; acc: 0.77
Batch: 220; loss: 1.26; acc: 0.59
Batch: 240; loss: 1.22; acc: 0.61
Batch: 260; loss: 1.21; acc: 0.69
Batch: 280; loss: 1.05; acc: 0.77
Batch: 300; loss: 1.06; acc: 0.75
Batch: 320; loss: 1.27; acc: 0.64
Batch: 340; loss: 1.09; acc: 0.75
Batch: 360; loss: 1.22; acc: 0.67
Batch: 380; loss: 1.09; acc: 0.64
Batch: 400; loss: 1.16; acc: 0.64
Batch: 420; loss: 0.99; acc: 0.73
Batch: 440; loss: 1.06; acc: 0.77
Batch: 460; loss: 1.29; acc: 0.64
Batch: 480; loss: 1.08; acc: 0.7
Batch: 500; loss: 1.26; acc: 0.62
Batch: 520; loss: 1.04; acc: 0.69
Batch: 540; loss: 1.02; acc: 0.73
Batch: 560; loss: 0.94; acc: 0.77
Batch: 580; loss: 1.29; acc: 0.67
Batch: 600; loss: 1.17; acc: 0.7
Batch: 620; loss: 1.0; acc: 0.75
Batch: 640; loss: 1.1; acc: 0.67
Batch: 660; loss: 1.15; acc: 0.64
Batch: 680; loss: 0.96; acc: 0.73
Batch: 700; loss: 1.12; acc: 0.64
Batch: 720; loss: 1.06; acc: 0.72
Batch: 740; loss: 1.33; acc: 0.56
Batch: 760; loss: 1.09; acc: 0.72
Batch: 780; loss: 1.16; acc: 0.66
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513145817313; val_accuracy: 0.7023288216560509 

Epoch 41 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 1.07; acc: 0.75
Batch: 60; loss: 0.98; acc: 0.8
Batch: 80; loss: 1.29; acc: 0.61
Batch: 100; loss: 1.06; acc: 0.77
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 1.22; acc: 0.66
Batch: 160; loss: 0.96; acc: 0.78
Batch: 180; loss: 1.13; acc: 0.67
Batch: 200; loss: 1.14; acc: 0.69
Batch: 220; loss: 1.1; acc: 0.66
Batch: 240; loss: 1.11; acc: 0.72
Batch: 260; loss: 1.21; acc: 0.64
Batch: 280; loss: 1.05; acc: 0.69
Batch: 300; loss: 1.21; acc: 0.66
Batch: 320; loss: 1.13; acc: 0.64
Batch: 340; loss: 1.33; acc: 0.62
Batch: 360; loss: 1.02; acc: 0.78
Batch: 380; loss: 1.01; acc: 0.72
Batch: 400; loss: 1.33; acc: 0.61
Batch: 420; loss: 1.07; acc: 0.7
Batch: 440; loss: 1.24; acc: 0.66
Batch: 460; loss: 1.2; acc: 0.66
Batch: 480; loss: 1.06; acc: 0.7
Batch: 500; loss: 1.2; acc: 0.59
Batch: 520; loss: 0.96; acc: 0.73
Batch: 540; loss: 1.29; acc: 0.61
Batch: 560; loss: 1.29; acc: 0.62
Batch: 580; loss: 1.13; acc: 0.69
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.21; acc: 0.64
Batch: 640; loss: 1.27; acc: 0.61
Batch: 660; loss: 1.17; acc: 0.7
Batch: 680; loss: 1.3; acc: 0.67
Batch: 700; loss: 0.94; acc: 0.8
Batch: 720; loss: 0.98; acc: 0.7
Batch: 740; loss: 1.23; acc: 0.66
Batch: 760; loss: 1.16; acc: 0.67
Batch: 780; loss: 1.11; acc: 0.66
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513130631416; val_accuracy: 0.7023288216560509 

Epoch 42 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.06; acc: 0.8
Batch: 40; loss: 1.32; acc: 0.56
Batch: 60; loss: 1.16; acc: 0.62
Batch: 80; loss: 1.25; acc: 0.66
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 1.16; acc: 0.73
Batch: 160; loss: 1.15; acc: 0.64
Batch: 180; loss: 1.09; acc: 0.67
Batch: 200; loss: 1.13; acc: 0.72
Batch: 220; loss: 1.14; acc: 0.66
Batch: 240; loss: 1.35; acc: 0.58
Batch: 260; loss: 1.25; acc: 0.59
Batch: 280; loss: 1.04; acc: 0.73
Batch: 300; loss: 1.11; acc: 0.69
Batch: 320; loss: 1.27; acc: 0.67
Batch: 340; loss: 1.19; acc: 0.62
Batch: 360; loss: 1.25; acc: 0.62
Batch: 380; loss: 1.04; acc: 0.7
Batch: 400; loss: 1.22; acc: 0.66
Batch: 420; loss: 1.11; acc: 0.7
Batch: 440; loss: 1.33; acc: 0.67
Batch: 460; loss: 1.23; acc: 0.61
Batch: 480; loss: 1.12; acc: 0.69
Batch: 500; loss: 1.09; acc: 0.67
Batch: 520; loss: 1.29; acc: 0.61
Batch: 540; loss: 1.11; acc: 0.77
Batch: 560; loss: 1.03; acc: 0.7
Batch: 580; loss: 1.1; acc: 0.67
Batch: 600; loss: 1.11; acc: 0.75
Batch: 620; loss: 1.07; acc: 0.67
Batch: 640; loss: 1.03; acc: 0.72
Batch: 660; loss: 0.95; acc: 0.67
Batch: 680; loss: 1.12; acc: 0.62
Batch: 700; loss: 1.15; acc: 0.67
Batch: 720; loss: 1.26; acc: 0.56
Batch: 740; loss: 1.19; acc: 0.64
Batch: 760; loss: 1.01; acc: 0.72
Batch: 780; loss: 1.15; acc: 0.67
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513130631416; val_accuracy: 0.7023288216560509 

Epoch 43 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 0.95; acc: 0.75
Batch: 20; loss: 1.02; acc: 0.67
Batch: 40; loss: 1.01; acc: 0.78
Batch: 60; loss: 1.08; acc: 0.72
Batch: 80; loss: 1.25; acc: 0.7
Batch: 100; loss: 0.99; acc: 0.77
Batch: 120; loss: 1.36; acc: 0.59
Batch: 140; loss: 0.98; acc: 0.77
Batch: 160; loss: 1.08; acc: 0.69
Batch: 180; loss: 1.28; acc: 0.61
Batch: 200; loss: 0.92; acc: 0.73
Batch: 220; loss: 1.17; acc: 0.69
Batch: 240; loss: 1.25; acc: 0.56
Batch: 260; loss: 0.97; acc: 0.81
Batch: 280; loss: 1.21; acc: 0.64
Batch: 300; loss: 1.04; acc: 0.66
Batch: 320; loss: 1.22; acc: 0.55
Batch: 340; loss: 1.05; acc: 0.7
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.09; acc: 0.7
Batch: 400; loss: 1.06; acc: 0.75
Batch: 420; loss: 1.18; acc: 0.67
Batch: 440; loss: 1.06; acc: 0.72
Batch: 460; loss: 1.09; acc: 0.66
Batch: 480; loss: 1.11; acc: 0.69
Batch: 500; loss: 0.92; acc: 0.73
Batch: 520; loss: 1.11; acc: 0.69
Batch: 540; loss: 1.13; acc: 0.67
Batch: 560; loss: 1.01; acc: 0.77
Batch: 580; loss: 1.17; acc: 0.64
Batch: 600; loss: 1.23; acc: 0.58
Batch: 620; loss: 1.07; acc: 0.73
Batch: 640; loss: 1.04; acc: 0.69
Batch: 660; loss: 1.12; acc: 0.72
Batch: 680; loss: 1.18; acc: 0.72
Batch: 700; loss: 1.21; acc: 0.75
Batch: 720; loss: 1.18; acc: 0.61
Batch: 740; loss: 1.17; acc: 0.7
Batch: 760; loss: 1.18; acc: 0.67
Batch: 780; loss: 1.15; acc: 0.64
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513145817313; val_accuracy: 0.7023288216560509 

Epoch 44 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 1.08; acc: 0.72
Batch: 20; loss: 1.09; acc: 0.69
Batch: 40; loss: 1.18; acc: 0.62
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 1.18; acc: 0.64
Batch: 100; loss: 0.89; acc: 0.81
Batch: 120; loss: 1.13; acc: 0.67
Batch: 140; loss: 1.07; acc: 0.72
Batch: 160; loss: 1.04; acc: 0.73
Batch: 180; loss: 0.91; acc: 0.77
Batch: 200; loss: 1.06; acc: 0.72
Batch: 220; loss: 1.09; acc: 0.69
Batch: 240; loss: 1.11; acc: 0.7
Batch: 260; loss: 1.1; acc: 0.72
Batch: 280; loss: 1.17; acc: 0.72
Batch: 300; loss: 1.14; acc: 0.61
Batch: 320; loss: 1.04; acc: 0.69
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.62
Batch: 380; loss: 1.13; acc: 0.72
Batch: 400; loss: 1.23; acc: 0.62
Batch: 420; loss: 1.13; acc: 0.69
Batch: 440; loss: 1.24; acc: 0.61
Batch: 460; loss: 1.02; acc: 0.73
Batch: 480; loss: 1.08; acc: 0.72
Batch: 500; loss: 1.08; acc: 0.69
Batch: 520; loss: 1.13; acc: 0.61
Batch: 540; loss: 0.95; acc: 0.72
Batch: 560; loss: 1.02; acc: 0.72
Batch: 580; loss: 0.92; acc: 0.72
Batch: 600; loss: 1.17; acc: 0.69
Batch: 620; loss: 1.12; acc: 0.72
Batch: 640; loss: 1.04; acc: 0.72
Batch: 660; loss: 1.01; acc: 0.72
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 0.97; acc: 0.77
Batch: 720; loss: 1.0; acc: 0.83
Batch: 740; loss: 1.06; acc: 0.69
Batch: 760; loss: 1.02; acc: 0.69
Batch: 780; loss: 0.87; acc: 0.69
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513145817313; val_accuracy: 0.7023288216560509 

Epoch 45 start
The current lr is: 8.157307210000002e-11
Batch: 0; loss: 1.2; acc: 0.66
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 1.24; acc: 0.61
Batch: 60; loss: 0.92; acc: 0.83
Batch: 80; loss: 0.89; acc: 0.84
Batch: 100; loss: 1.23; acc: 0.53
Batch: 120; loss: 1.08; acc: 0.7
Batch: 140; loss: 1.14; acc: 0.64
Batch: 160; loss: 1.13; acc: 0.69
Batch: 180; loss: 1.2; acc: 0.7
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 1.18; acc: 0.64
Batch: 240; loss: 1.12; acc: 0.64
Batch: 260; loss: 1.18; acc: 0.61
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 1.08; acc: 0.69
Batch: 320; loss: 0.91; acc: 0.77
Batch: 340; loss: 1.07; acc: 0.69
Batch: 360; loss: 0.99; acc: 0.72
Batch: 380; loss: 1.03; acc: 0.72
Batch: 400; loss: 1.2; acc: 0.7
Batch: 420; loss: 1.12; acc: 0.62
Batch: 440; loss: 1.15; acc: 0.7
Batch: 460; loss: 1.26; acc: 0.64
Batch: 480; loss: 1.06; acc: 0.7
Batch: 500; loss: 1.07; acc: 0.75
Batch: 520; loss: 1.04; acc: 0.66
Batch: 540; loss: 1.0; acc: 0.72
Batch: 560; loss: 1.12; acc: 0.7
Batch: 580; loss: 0.99; acc: 0.83
Batch: 600; loss: 1.25; acc: 0.66
Batch: 620; loss: 1.03; acc: 0.77
Batch: 640; loss: 0.91; acc: 0.75
Batch: 660; loss: 0.95; acc: 0.73
Batch: 680; loss: 0.95; acc: 0.73
Batch: 700; loss: 1.19; acc: 0.67
Batch: 720; loss: 1.23; acc: 0.64
Batch: 740; loss: 1.03; acc: 0.7
Batch: 760; loss: 1.31; acc: 0.59
Batch: 780; loss: 0.97; acc: 0.72
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513145817313; val_accuracy: 0.7023288216560509 

Epoch 46 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 1.05; acc: 0.67
Batch: 20; loss: 1.06; acc: 0.73
Batch: 40; loss: 1.23; acc: 0.72
Batch: 60; loss: 1.19; acc: 0.72
Batch: 80; loss: 1.2; acc: 0.69
Batch: 100; loss: 1.28; acc: 0.66
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 1.1; acc: 0.72
Batch: 160; loss: 0.98; acc: 0.72
Batch: 180; loss: 0.96; acc: 0.81
Batch: 200; loss: 1.17; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.61
Batch: 240; loss: 1.14; acc: 0.72
Batch: 260; loss: 1.11; acc: 0.66
Batch: 280; loss: 1.07; acc: 0.72
Batch: 300; loss: 1.2; acc: 0.66
Batch: 320; loss: 1.16; acc: 0.72
Batch: 340; loss: 1.15; acc: 0.69
Batch: 360; loss: 1.29; acc: 0.61
Batch: 380; loss: 1.14; acc: 0.64
Batch: 400; loss: 1.36; acc: 0.67
Batch: 420; loss: 1.15; acc: 0.64
Batch: 440; loss: 1.29; acc: 0.59
Batch: 460; loss: 1.16; acc: 0.66
Batch: 480; loss: 1.29; acc: 0.61
Batch: 500; loss: 1.29; acc: 0.56
Batch: 520; loss: 1.1; acc: 0.7
Batch: 540; loss: 1.41; acc: 0.56
Batch: 560; loss: 1.21; acc: 0.67
Batch: 580; loss: 1.27; acc: 0.67
Batch: 600; loss: 1.14; acc: 0.69
Batch: 620; loss: 1.22; acc: 0.67
Batch: 640; loss: 1.03; acc: 0.7
Batch: 660; loss: 1.06; acc: 0.72
Batch: 680; loss: 0.96; acc: 0.72
Batch: 700; loss: 1.09; acc: 0.72
Batch: 720; loss: 1.12; acc: 0.62
Batch: 740; loss: 1.06; acc: 0.75
Batch: 760; loss: 1.05; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.7
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513145817313; val_accuracy: 0.7023288216560509 

Epoch 47 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 1.04; acc: 0.7
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 1.27; acc: 0.64
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.13; acc: 0.69
Batch: 100; loss: 1.35; acc: 0.55
Batch: 120; loss: 1.18; acc: 0.62
Batch: 140; loss: 1.17; acc: 0.69
Batch: 160; loss: 1.24; acc: 0.61
Batch: 180; loss: 1.12; acc: 0.67
Batch: 200; loss: 1.07; acc: 0.77
Batch: 220; loss: 1.04; acc: 0.72
Batch: 240; loss: 1.19; acc: 0.75
Batch: 260; loss: 1.29; acc: 0.58
Batch: 280; loss: 1.08; acc: 0.69
Batch: 300; loss: 1.0; acc: 0.64
Batch: 320; loss: 1.02; acc: 0.73
Batch: 340; loss: 1.27; acc: 0.66
Batch: 360; loss: 1.15; acc: 0.66
Batch: 380; loss: 1.04; acc: 0.73
Batch: 400; loss: 0.96; acc: 0.73
Batch: 420; loss: 1.23; acc: 0.73
Batch: 440; loss: 1.15; acc: 0.67
Batch: 460; loss: 1.07; acc: 0.72
Batch: 480; loss: 1.15; acc: 0.66
Batch: 500; loss: 1.21; acc: 0.64
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 0.94; acc: 0.77
Batch: 560; loss: 1.21; acc: 0.69
Batch: 580; loss: 1.34; acc: 0.59
Batch: 600; loss: 1.18; acc: 0.64
Batch: 620; loss: 1.04; acc: 0.7
Batch: 640; loss: 1.06; acc: 0.69
Batch: 660; loss: 1.03; acc: 0.77
Batch: 680; loss: 1.29; acc: 0.64
Batch: 700; loss: 1.13; acc: 0.66
Batch: 720; loss: 1.24; acc: 0.7
Batch: 740; loss: 1.18; acc: 0.61
Batch: 760; loss: 1.05; acc: 0.69
Batch: 780; loss: 1.18; acc: 0.62
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513145817313; val_accuracy: 0.7023288216560509 

Epoch 48 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 1.19; acc: 0.67
Batch: 20; loss: 1.2; acc: 0.7
Batch: 40; loss: 1.05; acc: 0.73
Batch: 60; loss: 1.14; acc: 0.7
Batch: 80; loss: 1.34; acc: 0.56
Batch: 100; loss: 1.19; acc: 0.66
Batch: 120; loss: 1.09; acc: 0.66
Batch: 140; loss: 1.26; acc: 0.59
Batch: 160; loss: 1.14; acc: 0.69
Batch: 180; loss: 1.06; acc: 0.67
Batch: 200; loss: 1.07; acc: 0.77
Batch: 220; loss: 1.18; acc: 0.67
Batch: 240; loss: 1.15; acc: 0.7
Batch: 260; loss: 1.15; acc: 0.66
Batch: 280; loss: 0.98; acc: 0.75
Batch: 300; loss: 1.32; acc: 0.61
Batch: 320; loss: 1.18; acc: 0.62
Batch: 340; loss: 1.04; acc: 0.69
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.11; acc: 0.64
Batch: 400; loss: 1.22; acc: 0.64
Batch: 420; loss: 1.18; acc: 0.7
Batch: 440; loss: 0.99; acc: 0.78
Batch: 460; loss: 1.24; acc: 0.72
Batch: 480; loss: 1.18; acc: 0.58
Batch: 500; loss: 1.13; acc: 0.75
Batch: 520; loss: 1.06; acc: 0.72
Batch: 540; loss: 1.23; acc: 0.69
Batch: 560; loss: 1.2; acc: 0.72
Batch: 580; loss: 1.02; acc: 0.8
Batch: 600; loss: 1.04; acc: 0.7
Batch: 620; loss: 1.2; acc: 0.58
Batch: 640; loss: 1.18; acc: 0.61
Batch: 660; loss: 1.23; acc: 0.67
Batch: 680; loss: 1.08; acc: 0.73
Batch: 700; loss: 1.11; acc: 0.66
Batch: 720; loss: 1.22; acc: 0.7
Batch: 740; loss: 1.27; acc: 0.61
Batch: 760; loss: 0.97; acc: 0.73
Batch: 780; loss: 1.06; acc: 0.69
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513145817313; val_accuracy: 0.7023288216560509 

Epoch 49 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 0.98; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.72
Batch: 40; loss: 1.32; acc: 0.59
Batch: 60; loss: 0.96; acc: 0.72
Batch: 80; loss: 1.09; acc: 0.67
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.16; acc: 0.75
Batch: 140; loss: 1.1; acc: 0.66
Batch: 160; loss: 1.18; acc: 0.69
Batch: 180; loss: 1.0; acc: 0.75
Batch: 200; loss: 1.15; acc: 0.62
Batch: 220; loss: 1.07; acc: 0.7
Batch: 240; loss: 1.12; acc: 0.73
Batch: 260; loss: 1.16; acc: 0.69
Batch: 280; loss: 1.03; acc: 0.8
Batch: 300; loss: 1.17; acc: 0.67
Batch: 320; loss: 1.06; acc: 0.75
Batch: 340; loss: 1.2; acc: 0.64
Batch: 360; loss: 1.3; acc: 0.62
Batch: 380; loss: 1.09; acc: 0.69
Batch: 400; loss: 1.04; acc: 0.7
Batch: 420; loss: 1.32; acc: 0.55
Batch: 440; loss: 1.08; acc: 0.67
Batch: 460; loss: 1.15; acc: 0.67
Batch: 480; loss: 1.12; acc: 0.73
Batch: 500; loss: 1.05; acc: 0.78
Batch: 520; loss: 1.24; acc: 0.61
Batch: 540; loss: 1.17; acc: 0.64
Batch: 560; loss: 1.25; acc: 0.61
Batch: 580; loss: 1.12; acc: 0.72
Batch: 600; loss: 1.21; acc: 0.7
Batch: 620; loss: 1.05; acc: 0.73
Batch: 640; loss: 1.17; acc: 0.59
Batch: 660; loss: 1.09; acc: 0.69
Batch: 680; loss: 0.94; acc: 0.77
Batch: 700; loss: 1.09; acc: 0.73
Batch: 720; loss: 1.23; acc: 0.62
Batch: 740; loss: 1.16; acc: 0.64
Batch: 760; loss: 1.17; acc: 0.69
Batch: 780; loss: 1.17; acc: 0.67
Train Epoch over. train_loss: 1.14; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513145817313; val_accuracy: 0.7023288216560509 

Epoch 50 start
The current lr is: 1.0604499373000003e-11
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.7
Batch: 40; loss: 1.17; acc: 0.64
Batch: 60; loss: 1.22; acc: 0.66
Batch: 80; loss: 1.15; acc: 0.62
Batch: 100; loss: 1.03; acc: 0.7
Batch: 120; loss: 1.1; acc: 0.64
Batch: 140; loss: 1.24; acc: 0.66
Batch: 160; loss: 1.11; acc: 0.73
Batch: 180; loss: 1.1; acc: 0.7
Batch: 200; loss: 1.17; acc: 0.72
Batch: 220; loss: 1.28; acc: 0.53
Batch: 240; loss: 1.11; acc: 0.66
Batch: 260; loss: 1.05; acc: 0.75
Batch: 280; loss: 1.21; acc: 0.61
Batch: 300; loss: 1.18; acc: 0.59
Batch: 320; loss: 1.06; acc: 0.67
Batch: 340; loss: 1.05; acc: 0.73
Batch: 360; loss: 0.97; acc: 0.77
Batch: 380; loss: 1.04; acc: 0.67
Batch: 400; loss: 1.24; acc: 0.58
Batch: 420; loss: 1.11; acc: 0.72
Batch: 440; loss: 1.32; acc: 0.59
Batch: 460; loss: 1.15; acc: 0.73
Batch: 480; loss: 1.23; acc: 0.59
Batch: 500; loss: 1.29; acc: 0.61
Batch: 520; loss: 1.1; acc: 0.66
Batch: 540; loss: 1.22; acc: 0.7
Batch: 560; loss: 1.14; acc: 0.67
Batch: 580; loss: 1.05; acc: 0.72
Batch: 600; loss: 1.2; acc: 0.61
Batch: 620; loss: 1.0; acc: 0.7
Batch: 640; loss: 1.03; acc: 0.78
Batch: 660; loss: 1.03; acc: 0.75
Batch: 680; loss: 1.27; acc: 0.66
Batch: 700; loss: 1.17; acc: 0.66
Batch: 720; loss: 1.1; acc: 0.69
Batch: 740; loss: 1.15; acc: 0.7
Batch: 760; loss: 1.16; acc: 0.67
Batch: 780; loss: 0.85; acc: 0.77
Train Epoch over. train_loss: 1.13; train_accuracy: 0.68 

Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 1.25; acc: 0.56
Batch: 40; loss: 0.92; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.19; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.81
Val Epoch over. val_loss: 1.1055513145817313; val_accuracy: 0.7023288216560509 

plots/no_subspace_training/reg_lenet/2020-01-19 04:26:44/d_dim_1000_lr_0.001_gamma_0.13_sched_freq_5_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.216115310693243; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.0288756334098283; val_accuracy: 0.3688296178343949 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.39
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508777076271689; val_accuracy: 0.4570063694267516 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.38
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5095196628266838; val_accuracy: 0.5688694267515924 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878806963088407; val_accuracy: 0.6332603503184714 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.0523852018793678; val_accuracy: 0.6972531847133758 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.83
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.8261542149410126; val_accuracy: 0.779359076433121 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.65892821588334; val_accuracy: 0.804140127388535 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5360139900711691; val_accuracy: 0.8442476114649682 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.44348721680747477; val_accuracy: 0.8714171974522293 

Epoch 11 start
The current lr is: 0.001
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.91
Batch: 40; loss: 0.63; acc: 0.8
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.71; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.91
Batch: 460; loss: 0.72; acc: 0.72
Batch: 480; loss: 0.73; acc: 0.77
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.78
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.78
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.38714269762206227; val_accuracy: 0.8838574840764332 

Epoch 12 start
The current lr is: 0.001
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.38; acc: 0.95
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.57; acc: 0.81
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.91
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.27; acc: 0.97
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.45; acc: 0.86
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.41; acc: 0.91
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3392352444256187; val_accuracy: 0.9021695859872612 

Epoch 13 start
The current lr is: 0.001
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.47; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.97
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.44; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.92
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.30624969654781803; val_accuracy: 0.9118232484076433 

Epoch 14 start
The current lr is: 0.001
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.53; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.24; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.98
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.55; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.25; acc: 0.95
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.279337538323205; val_accuracy: 0.9198845541401274 

Epoch 15 start
The current lr is: 0.001
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.97
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.88
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.17; acc: 0.97
Batch: 660; loss: 0.21; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.09; acc: 1.0
Val Epoch over. val_loss: 0.2711310274187167; val_accuracy: 0.9176950636942676 

Epoch 16 start
The current lr is: 0.0001
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.3; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.15; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24420123357491888; val_accuracy: 0.9295382165605095 

Epoch 17 start
The current lr is: 0.0001
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.98
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.42; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.95
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.98
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24186581655577488; val_accuracy: 0.9306329617834395 

Epoch 18 start
The current lr is: 0.0001
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.97
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.24020986235255648; val_accuracy: 0.9308320063694268 

Epoch 19 start
The current lr is: 0.0001
Batch: 0; loss: 0.26; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.8
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.26; acc: 0.97
Batch: 560; loss: 0.16; acc: 0.98
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.88
Batch: 740; loss: 0.12; acc: 0.98
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.2381504695554068; val_accuracy: 0.931827229299363 

Epoch 20 start
The current lr is: 0.0001
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.81
Batch: 60; loss: 0.42; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.89
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.32; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.84
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.95
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.15; acc: 1.0
Batch: 520; loss: 0.31; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.51; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.18; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.3; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.23581902777693073; val_accuracy: 0.931827229299363 

Epoch 21 start
The current lr is: 0.0001
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.97
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.98
Batch: 380; loss: 0.3; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.31; acc: 0.94
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.23465315409147056; val_accuracy: 0.9325238853503185 

Epoch 22 start
The current lr is: 0.0001
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.3; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.83
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.26; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.97
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.23289093173518302; val_accuracy: 0.9324243630573248 

Epoch 23 start
The current lr is: 0.0001
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.97
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.34; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2309954440470334; val_accuracy: 0.933718152866242 

Epoch 24 start
The current lr is: 0.0001
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.94
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22904129015507213; val_accuracy: 0.9341162420382165 

Epoch 25 start
The current lr is: 0.0001
Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.18; acc: 0.98
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.92
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.16; acc: 0.94
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.16; acc: 0.98
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.25; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.227584302116921; val_accuracy: 0.9344148089171974 

Epoch 26 start
The current lr is: 0.0001
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.35; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.91
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.08; acc: 1.0
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.84
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.2258947414529931; val_accuracy: 0.9335191082802548 

Epoch 27 start
The current lr is: 0.0001
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.97
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.1; acc: 1.0
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.22; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.98
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.48; acc: 0.84
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22427200699213204; val_accuracy: 0.9347133757961783 

Epoch 28 start
The current lr is: 0.0001
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.97
Batch: 240; loss: 0.26; acc: 0.88
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.92
Batch: 360; loss: 0.31; acc: 0.84
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.13; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.22; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22293357259243918; val_accuracy: 0.9350119426751592 

Epoch 29 start
The current lr is: 0.0001
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.84
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.07; acc: 1.0
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.27; acc: 0.88
Batch: 240; loss: 0.49; acc: 0.78
Batch: 260; loss: 0.25; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.12; acc: 1.0
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.22174748147179366; val_accuracy: 0.9357085987261147 

Epoch 30 start
The current lr is: 0.0001
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.11; acc: 0.97
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.22049691641976119; val_accuracy: 0.9363057324840764 

Epoch 31 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.95
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.38; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21989344931237256; val_accuracy: 0.9358081210191083 

Epoch 32 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.89
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21957650011891772; val_accuracy: 0.9355095541401274 

Epoch 33 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.46; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.97
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.91
Batch: 320; loss: 0.19; acc: 0.97
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.21; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21939517906422068; val_accuracy: 0.9357085987261147 

Epoch 34 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.97
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.28; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.11; acc: 0.97
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.86
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.98
Batch: 700; loss: 0.2; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.06; acc: 1.0
Val Epoch over. val_loss: 0.21926189750243144; val_accuracy: 0.935609076433121 

Epoch 35 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.98
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.95
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2190547552742776; val_accuracy: 0.9360071656050956 

Epoch 36 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.84
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.97
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21887774210257135; val_accuracy: 0.9358081210191083 

Epoch 37 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.98
Batch: 140; loss: 0.12; acc: 0.98
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.97
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.95
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.29; acc: 0.89
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.2; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.58; acc: 0.86
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21878404955670333; val_accuracy: 0.9359076433121019 

Epoch 38 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.43; acc: 0.83
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.54; acc: 0.89
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.27; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21864015853423982; val_accuracy: 0.9362062101910829 

Epoch 39 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.59; acc: 0.83
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.21; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.97
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.84
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21842422836999983; val_accuracy: 0.9359076433121019 

Epoch 40 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.98
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.14; acc: 0.97
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.95
Batch: 380; loss: 0.1; acc: 0.98
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.3; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.11; acc: 0.97
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21835890080139136; val_accuracy: 0.9362062101910829 

Epoch 41 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.14; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.1; acc: 1.0
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.17; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.94
Batch: 760; loss: 0.31; acc: 0.92
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21818387776518325; val_accuracy: 0.9360071656050956 

Epoch 42 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.33; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.24; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21803278243465787; val_accuracy: 0.9362062101910829 

Epoch 43 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.81
Batch: 100; loss: 0.09; acc: 0.98
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.11; acc: 0.97
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.08; acc: 1.0
Batch: 280; loss: 0.23; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.2; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.98
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.17; acc: 0.97
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.97
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.2; acc: 0.91
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21785589184161205; val_accuracy: 0.9363057324840764 

Epoch 44 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.97
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.18; acc: 0.97
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.17; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.95
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.17; acc: 0.97
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.23; acc: 0.95
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.24; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.18; acc: 0.91
Batch: 720; loss: 0.12; acc: 0.98
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2176703350369338; val_accuracy: 0.9362062101910829 

Epoch 45 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.3; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.97
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.2; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.38; acc: 0.84
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2175777718472253; val_accuracy: 0.9361066878980892 

Epoch 46 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.16; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.15; acc: 0.98
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21756917316537755; val_accuracy: 0.9360071656050956 

Epoch 47 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.13; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.86
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.94
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.98
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.33; acc: 0.91
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21755944121225623; val_accuracy: 0.9362062101910829 

Epoch 48 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.97
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.91
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.16; acc: 0.98
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.84
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2175494105003442; val_accuracy: 0.9363057324840764 

Epoch 49 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.14; acc: 0.94
Batch: 340; loss: 0.27; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.81
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.25; acc: 0.95
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.39; acc: 0.84
Batch: 620; loss: 0.18; acc: 0.98
Batch: 640; loss: 0.27; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21754016338070487; val_accuracy: 0.9363057324840764 

Epoch 50 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.27; acc: 0.95
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.98
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.97
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.37; acc: 0.86
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.19; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.95
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.77
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.21752757590952193; val_accuracy: 0.9363057324840764 

plots/no_subspace_training/reg_lenet/2020-01-19 04:36:01/d_dim_1000_lr_0.001_gamma_0.1_sched_freq_15_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161154200316995; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.028860976741572; val_accuracy: 0.3688296178343949 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.39
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.41
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.7508647905033865; val_accuracy: 0.45710589171974525 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.38
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.58
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.5095081754550812; val_accuracy: 0.568968949044586 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.2878719188605146; val_accuracy: 0.6330613057324841 

Epoch 6 start
The current lr is: 0.001
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.7
Batch: 40; loss: 1.22; acc: 0.66
Batch: 60; loss: 1.55; acc: 0.48
Batch: 80; loss: 1.19; acc: 0.64
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.24; acc: 0.64
Batch: 140; loss: 1.38; acc: 0.59
Batch: 160; loss: 1.32; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.67
Batch: 200; loss: 1.26; acc: 0.66
Batch: 220; loss: 1.23; acc: 0.64
Batch: 240; loss: 1.18; acc: 0.64
Batch: 260; loss: 1.22; acc: 0.59
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.72
Batch: 320; loss: 1.13; acc: 0.75
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.26; acc: 0.52
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.7
Batch: 420; loss: 1.05; acc: 0.77
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 1.14; acc: 0.73
Batch: 500; loss: 1.36; acc: 0.72
Batch: 520; loss: 1.18; acc: 0.7
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.27; acc: 0.64
Batch: 580; loss: 1.2; acc: 0.67
Batch: 600; loss: 1.05; acc: 0.73
Batch: 620; loss: 1.0; acc: 0.77
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.1; acc: 0.66
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.03; acc: 0.72
Batch: 740; loss: 1.11; acc: 0.77
Batch: 760; loss: 0.99; acc: 0.78
Batch: 780; loss: 1.19; acc: 0.55
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.06; acc: 0.72
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 0.89; acc: 0.69
Batch: 60; loss: 0.97; acc: 0.7
Batch: 80; loss: 1.0; acc: 0.77
Batch: 100; loss: 1.03; acc: 0.69
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 0.83; acc: 0.81
Val Epoch over. val_loss: 1.0523684806884475; val_accuracy: 0.6972531847133758 

Epoch 7 start
The current lr is: 0.001
Batch: 0; loss: 0.97; acc: 0.72
Batch: 20; loss: 1.01; acc: 0.73
Batch: 40; loss: 1.27; acc: 0.72
Batch: 60; loss: 0.98; acc: 0.73
Batch: 80; loss: 1.1; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.99; acc: 0.75
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.72
Batch: 200; loss: 1.01; acc: 0.7
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.87; acc: 0.83
Batch: 300; loss: 1.01; acc: 0.77
Batch: 320; loss: 1.04; acc: 0.66
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.72
Batch: 380; loss: 1.01; acc: 0.78
Batch: 400; loss: 0.91; acc: 0.73
Batch: 420; loss: 0.97; acc: 0.69
Batch: 440; loss: 1.0; acc: 0.75
Batch: 460; loss: 0.89; acc: 0.77
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.93; acc: 0.72
Batch: 520; loss: 1.12; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.07; acc: 0.73
Batch: 600; loss: 0.93; acc: 0.75
Batch: 620; loss: 0.98; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.72
Batch: 660; loss: 0.87; acc: 0.73
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.84; acc: 0.81
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.91; acc: 0.73
Batch: 760; loss: 0.95; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.97; train_accuracy: 0.73 

Batch: 0; loss: 0.86; acc: 0.83
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.88
Batch: 100; loss: 0.82; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.73
Batch: 140; loss: 0.6; acc: 0.86
Val Epoch over. val_loss: 0.8261588413244599; val_accuracy: 0.779359076433121 

Epoch 8 start
The current lr is: 0.001
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.82; acc: 0.77
Batch: 60; loss: 0.85; acc: 0.77
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 0.97; acc: 0.69
Batch: 120; loss: 0.73; acc: 0.84
Batch: 140; loss: 0.73; acc: 0.89
Batch: 160; loss: 0.86; acc: 0.77
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.8
Batch: 240; loss: 0.8; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.86; acc: 0.77
Batch: 320; loss: 0.77; acc: 0.8
Batch: 340; loss: 0.86; acc: 0.73
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.87; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.65; acc: 0.84
Batch: 480; loss: 0.67; acc: 0.81
Batch: 500; loss: 0.6; acc: 0.84
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.8
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.94
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.86; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.56; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.74; acc: 0.81
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.77; train_accuracy: 0.78 

Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.65; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.658930354437251; val_accuracy: 0.804140127388535 

Epoch 9 start
The current lr is: 0.001
Batch: 0; loss: 0.72; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.8
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.83
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.81
Batch: 280; loss: 0.58; acc: 0.8
Batch: 300; loss: 0.71; acc: 0.75
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.49; acc: 0.91
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.72; acc: 0.75
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.61; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.88
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.62; acc: 0.83
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.55; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.62; train_accuracy: 0.82 

Batch: 0; loss: 0.57; acc: 0.92
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5360211796441655; val_accuracy: 0.8442476114649682 

Epoch 10 start
The current lr is: 0.001
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.55; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.89
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.5; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.63; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.68; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.92
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.98
Val Epoch over. val_loss: 0.44350021441651; val_accuracy: 0.8715167197452229 

Epoch 11 start
The current lr is: 0.0001
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.61; acc: 0.78
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.8
Batch: 140; loss: 0.45; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.95
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.74; acc: 0.83
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.45; acc: 0.86
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.77; acc: 0.7
Batch: 480; loss: 0.79; acc: 0.64
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.54; acc: 0.8
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.72; acc: 0.78
Batch: 600; loss: 0.5; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.98
Val Epoch over. val_loss: 0.43235726417249937; val_accuracy: 0.8743033439490446 

Epoch 12 start
The current lr is: 0.0001
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.6; acc: 0.78
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.53; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.88
Batch: 360; loss: 0.57; acc: 0.8
Batch: 380; loss: 0.55; acc: 0.83
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.8
Batch: 500; loss: 0.67; acc: 0.78
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.89
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.41; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.66; acc: 0.8
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.92
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.97
Val Epoch over. val_loss: 0.425521107995586; val_accuracy: 0.8754976114649682 

Epoch 13 start
The current lr is: 0.0001
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.65; acc: 0.78
Batch: 180; loss: 0.58; acc: 0.81
Batch: 200; loss: 0.57; acc: 0.86
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.84
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.38; acc: 0.91
Batch: 400; loss: 0.55; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.94
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.64; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.49; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.98
Val Epoch over. val_loss: 0.41848103132597203; val_accuracy: 0.8788813694267515 

Epoch 14 start
The current lr is: 0.0001
Batch: 0; loss: 0.41; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.73
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.45; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.52; acc: 0.78
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.92
Batch: 320; loss: 0.53; acc: 0.8
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.84
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.63; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.53; acc: 0.81
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.8
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.39; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.41249827613496476; val_accuracy: 0.8812699044585988 

Epoch 15 start
The current lr is: 0.0001
Batch: 0; loss: 0.67; acc: 0.78
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.94
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.78
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.55; acc: 0.81
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.35; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.38; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.4064903340901539; val_accuracy: 0.8824641719745223 

Epoch 16 start
The current lr is: 0.0001
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.48; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.39; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.66; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.67; acc: 0.75
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.3; acc: 0.94
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.94
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.28; acc: 0.94
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.83
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.8
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.40043197562740107; val_accuracy: 0.8837579617834395 

Epoch 17 start
The current lr is: 0.0001
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.3; acc: 0.95
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.56; acc: 0.89
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.39; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.84
Batch: 240; loss: 0.5; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.51; acc: 0.8
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.56; acc: 0.83
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.8
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.43; train_accuracy: 0.88 

Batch: 0; loss: 0.44; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.39432254566508496; val_accuracy: 0.8859474522292994 

Epoch 18 start
The current lr is: 0.0001
Batch: 0; loss: 0.63; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.81
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.38; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.86
Batch: 420; loss: 0.44; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.97
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.95
Batch: 560; loss: 0.53; acc: 0.88
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.29; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.43; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.98
Val Epoch over. val_loss: 0.38848941739957044; val_accuracy: 0.887937898089172 

Epoch 19 start
The current lr is: 0.0001
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.51; acc: 0.86
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.7; acc: 0.73
Batch: 360; loss: 0.47; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.83
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.59; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.94
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.43; acc: 0.92
Train Epoch over. train_loss: 0.42; train_accuracy: 0.88 

Batch: 0; loss: 0.42; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.38258965750029134; val_accuracy: 0.8888335987261147 

Epoch 20 start
The current lr is: 0.0001
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.78
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.81
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.98
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.52; acc: 0.8
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.58; acc: 0.8
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.73; acc: 0.77
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.8
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.81
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.8
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.37688574099996286; val_accuracy: 0.8910230891719745 

Epoch 21 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.81
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.46; acc: 0.83
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.39; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.44; acc: 0.83
Batch: 520; loss: 0.38; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.83
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.3761943751459668; val_accuracy: 0.8909235668789809 

Epoch 22 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.8
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.92
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.52; acc: 0.81
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.98
Val Epoch over. val_loss: 0.37571637038212674; val_accuracy: 0.8912221337579618 

Epoch 23 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.95
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.81
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.95
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.64; acc: 0.8
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.46; acc: 0.83
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.43; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.83
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.37; acc: 0.92
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3752132317252979; val_accuracy: 0.8911226114649682 

Epoch 24 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.54; acc: 0.77
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.54; acc: 0.81
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.39; acc: 0.92
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.2; acc: 0.97
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.56; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.55; acc: 0.83
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37458408524276343; val_accuracy: 0.89171974522293 

Epoch 25 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.45; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.32; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.48; acc: 0.81
Batch: 400; loss: 0.33; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.18; acc: 0.98
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3741653003510396; val_accuracy: 0.8916202229299363 

Epoch 26 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.81
Batch: 180; loss: 0.5; acc: 0.83
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.52; acc: 0.84
Batch: 360; loss: 0.36; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.41; acc: 0.84
Batch: 600; loss: 0.53; acc: 0.81
Batch: 620; loss: 0.37; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.53; acc: 0.78
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3735966733686484; val_accuracy: 0.8921178343949044 

Epoch 27 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.92
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.8
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.95
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.6; acc: 0.8
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.45; acc: 0.81
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.53; acc: 0.78
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.66; acc: 0.8
Batch: 660; loss: 0.49; acc: 0.81
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.43; acc: 0.92
Batch: 780; loss: 0.61; acc: 0.78
Train Epoch over. train_loss: 0.41; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3730383180319124; val_accuracy: 0.8922173566878981 

Epoch 28 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.33; acc: 0.94
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.71; acc: 0.84
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.53; acc: 0.8
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.57; acc: 0.78
Batch: 640; loss: 0.44; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.94
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37253916804577897; val_accuracy: 0.8926154458598726 

Epoch 29 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.61; acc: 0.8
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.64; acc: 0.81
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.19; acc: 0.97
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.71; acc: 0.73
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.29; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.97
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.32; acc: 0.94
Batch: 480; loss: 0.47; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.46; acc: 0.81
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3721393445494828; val_accuracy: 0.8921178343949044 

Epoch 30 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.43; acc: 0.89
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.4; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.42; acc: 0.84
Batch: 520; loss: 0.57; acc: 0.78
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.24; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.28; acc: 0.94
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3715951291808657; val_accuracy: 0.8931130573248408 

Epoch 31 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.31; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.49; acc: 0.81
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.37; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.62; acc: 0.83
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.52; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.81
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.46; acc: 0.84
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.94
Batch: 760; loss: 0.51; acc: 0.81
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.371530760910101; val_accuracy: 0.8931130573248408 

Epoch 32 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.95
Batch: 240; loss: 0.29; acc: 0.95
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.97
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.83
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.47; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3714703985839892; val_accuracy: 0.8930135350318471 

Epoch 33 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.43; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.92
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.38; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.48; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.48; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3714156150817871; val_accuracy: 0.8929140127388535 

Epoch 34 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.95
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.5; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.94
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.3; acc: 0.92
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.44; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.42; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.56; acc: 0.78
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.37; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3713614422424584; val_accuracy: 0.8929140127388535 

Epoch 35 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.31; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.33; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.84
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37130696986131606; val_accuracy: 0.8930135350318471 

Epoch 36 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.91
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.43; acc: 0.91
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.84
Batch: 380; loss: 0.64; acc: 0.77
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.49; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.84
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.81
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3712484538555145; val_accuracy: 0.8930135350318471 

Epoch 37 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.26; acc: 0.97
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.47; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.63; acc: 0.81
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.71; acc: 0.8
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.8
Batch: 440; loss: 0.42; acc: 0.83
Batch: 460; loss: 0.39; acc: 0.88
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.6; acc: 0.78
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.7; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3711969126371821; val_accuracy: 0.8929140127388535 

Epoch 38 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.83
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.91
Batch: 320; loss: 0.61; acc: 0.8
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.55; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.95
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3711421110068157; val_accuracy: 0.8929140127388535 

Epoch 39 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.6; acc: 0.77
Batch: 160; loss: 0.52; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.8; acc: 0.73
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.84
Batch: 400; loss: 0.42; acc: 0.81
Batch: 420; loss: 0.53; acc: 0.78
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37108851883821425; val_accuracy: 0.8929140127388535 

Epoch 40 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.35; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.64; acc: 0.78
Batch: 240; loss: 0.54; acc: 0.8
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.27; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.52; acc: 0.77
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.81
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3710371477968374; val_accuracy: 0.8929140127388535 

Epoch 41 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.73; acc: 0.81
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.83
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.83
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37103404569777715; val_accuracy: 0.8929140127388535 

Epoch 42 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.97
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.53; acc: 0.88
Batch: 100; loss: 0.31; acc: 0.88
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.61; acc: 0.81
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.53; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.53; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.91
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3710308874108989; val_accuracy: 0.8929140127388535 

Epoch 43 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.67; acc: 0.75
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.2; acc: 0.97
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.41; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.8
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37102768204774067; val_accuracy: 0.8929140127388535 

Epoch 44 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.31; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.94
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.83
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.57; acc: 0.8
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.98
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37102444337052143; val_accuracy: 0.8929140127388535 

Epoch 45 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.97
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.78
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.94
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.46; acc: 0.88
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.28; acc: 0.94
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37102141425867746; val_accuracy: 0.8929140127388535 

Epoch 46 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.32; acc: 0.92
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.39; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.58; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.94
Batch: 440; loss: 0.5; acc: 0.78
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.3; acc: 0.97
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.55; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.35; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3710184216878976; val_accuracy: 0.8929140127388535 

Epoch 47 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.56; acc: 0.77
Batch: 100; loss: 0.55; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.38; acc: 0.91
Batch: 160; loss: 0.47; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.5; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.95
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.4; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.83
Batch: 660; loss: 0.31; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.88
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.91
Batch: 780; loss: 0.37; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37101566943393394; val_accuracy: 0.8929140127388535 

Epoch 48 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.61; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.56; acc: 0.78
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.48; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.47; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.83
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.46; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.3710127132143944; val_accuracy: 0.8929140127388535 

Epoch 49 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.58; acc: 0.77
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.95
Batch: 480; loss: 0.43; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.55; acc: 0.8
Batch: 620; loss: 0.29; acc: 0.95
Batch: 640; loss: 0.48; acc: 0.81
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.91
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.48; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37100973421600975; val_accuracy: 0.8929140127388535 

Epoch 50 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.81
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.57; acc: 0.81
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.42; acc: 0.84
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.35; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.91
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.83
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.41; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.39; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.37100679071465875; val_accuracy: 0.8929140127388535 

plots/no_subspace_training/reg_lenet/2020-01-19 04:45:20/d_dim_1000_lr_0.001_gamma_0.1_sched_freq_10_seed_1_epochs_50_batchsize_64
Epoch 1 start
The current lr is: 0.001
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.35; acc: 0.11
Batch: 20; loss: 2.32; acc: 0.12
Batch: 40; loss: 2.36; acc: 0.06
Batch: 60; loss: 2.34; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.03
Batch: 100; loss: 2.26; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.16
Batch: 160; loss: 2.29; acc: 0.16
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.3; acc: 0.11
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.25; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.33
Batch: 360; loss: 2.27; acc: 0.23
Batch: 380; loss: 2.29; acc: 0.06
Batch: 400; loss: 2.25; acc: 0.25
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.11
Batch: 580; loss: 2.23; acc: 0.25
Batch: 600; loss: 2.24; acc: 0.19
Batch: 620; loss: 2.24; acc: 0.27
Batch: 640; loss: 2.23; acc: 0.25
Batch: 660; loss: 2.24; acc: 0.22
Batch: 680; loss: 2.25; acc: 0.23
Batch: 700; loss: 2.23; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.25
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.2
Batch: 780; loss: 2.21; acc: 0.28
Train Epoch over. train_loss: 2.27; train_accuracy: 0.18 

Batch: 0; loss: 2.22; acc: 0.25
Batch: 20; loss: 2.25; acc: 0.25
Batch: 40; loss: 2.18; acc: 0.36
Batch: 60; loss: 2.2; acc: 0.3
Batch: 80; loss: 2.2; acc: 0.25
Batch: 100; loss: 2.23; acc: 0.28
Batch: 120; loss: 2.23; acc: 0.3
Batch: 140; loss: 2.21; acc: 0.31
Val Epoch over. val_loss: 2.2161154200316995; val_accuracy: 0.2753781847133758 

Epoch 2 start
The current lr is: 0.001
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 2.2; acc: 0.3
Batch: 40; loss: 2.23; acc: 0.19
Batch: 60; loss: 2.22; acc: 0.27
Batch: 80; loss: 2.18; acc: 0.31
Batch: 100; loss: 2.19; acc: 0.27
Batch: 120; loss: 2.2; acc: 0.25
Batch: 140; loss: 2.18; acc: 0.31
Batch: 160; loss: 2.22; acc: 0.27
Batch: 180; loss: 2.18; acc: 0.3
Batch: 200; loss: 2.19; acc: 0.28
Batch: 220; loss: 2.16; acc: 0.31
Batch: 240; loss: 2.15; acc: 0.33
Batch: 260; loss: 2.16; acc: 0.34
Batch: 280; loss: 2.19; acc: 0.27
Batch: 300; loss: 2.13; acc: 0.31
Batch: 320; loss: 2.15; acc: 0.23
Batch: 340; loss: 2.16; acc: 0.33
Batch: 360; loss: 2.14; acc: 0.3
Batch: 380; loss: 2.1; acc: 0.3
Batch: 400; loss: 2.15; acc: 0.25
Batch: 420; loss: 2.16; acc: 0.23
Batch: 440; loss: 2.11; acc: 0.31
Batch: 460; loss: 2.13; acc: 0.36
Batch: 480; loss: 2.13; acc: 0.3
Batch: 500; loss: 2.11; acc: 0.34
Batch: 520; loss: 2.05; acc: 0.41
Batch: 540; loss: 2.21; acc: 0.2
Batch: 560; loss: 2.1; acc: 0.38
Batch: 580; loss: 2.14; acc: 0.28
Batch: 600; loss: 2.08; acc: 0.34
Batch: 620; loss: 2.09; acc: 0.3
Batch: 640; loss: 2.08; acc: 0.34
Batch: 660; loss: 2.06; acc: 0.23
Batch: 680; loss: 2.03; acc: 0.34
Batch: 700; loss: 2.03; acc: 0.38
Batch: 720; loss: 2.09; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.47
Batch: 760; loss: 2.02; acc: 0.33
Batch: 780; loss: 2.04; acc: 0.38
Train Epoch over. train_loss: 2.13; train_accuracy: 0.31 

Batch: 0; loss: 2.02; acc: 0.41
Batch: 20; loss: 2.08; acc: 0.28
Batch: 40; loss: 1.92; acc: 0.48
Batch: 60; loss: 1.96; acc: 0.39
Batch: 80; loss: 1.99; acc: 0.36
Batch: 100; loss: 2.06; acc: 0.44
Batch: 120; loss: 2.03; acc: 0.38
Batch: 140; loss: 2.0; acc: 0.5
Val Epoch over. val_loss: 2.0288700191837967; val_accuracy: 0.36892914012738853 

Epoch 3 start
The current lr is: 0.001
Batch: 0; loss: 1.96; acc: 0.48
Batch: 20; loss: 2.02; acc: 0.39
Batch: 40; loss: 2.03; acc: 0.31
Batch: 60; loss: 1.92; acc: 0.41
Batch: 80; loss: 1.97; acc: 0.41
Batch: 100; loss: 2.01; acc: 0.31
Batch: 120; loss: 2.02; acc: 0.39
Batch: 140; loss: 1.95; acc: 0.33
Batch: 160; loss: 2.01; acc: 0.39
Batch: 180; loss: 2.01; acc: 0.28
Batch: 200; loss: 1.93; acc: 0.38
Batch: 220; loss: 1.86; acc: 0.42
Batch: 240; loss: 2.04; acc: 0.28
Batch: 260; loss: 2.05; acc: 0.28
Batch: 280; loss: 1.92; acc: 0.33
Batch: 300; loss: 1.87; acc: 0.36
Batch: 320; loss: 1.99; acc: 0.36
Batch: 340; loss: 1.99; acc: 0.48
Batch: 360; loss: 1.88; acc: 0.38
Batch: 380; loss: 1.84; acc: 0.47
Batch: 400; loss: 1.98; acc: 0.45
Batch: 420; loss: 1.93; acc: 0.41
Batch: 440; loss: 1.87; acc: 0.42
Batch: 460; loss: 1.98; acc: 0.36
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.87; acc: 0.42
Batch: 520; loss: 1.87; acc: 0.33
Batch: 540; loss: 1.87; acc: 0.47
Batch: 560; loss: 1.76; acc: 0.52
Batch: 580; loss: 1.77; acc: 0.44
Batch: 600; loss: 1.91; acc: 0.42
Batch: 620; loss: 1.68; acc: 0.47
Batch: 640; loss: 1.79; acc: 0.42
Batch: 660; loss: 1.76; acc: 0.38
Batch: 680; loss: 1.78; acc: 0.44
Batch: 700; loss: 1.63; acc: 0.53
Batch: 720; loss: 1.73; acc: 0.44
Batch: 740; loss: 1.75; acc: 0.41
Batch: 760; loss: 1.79; acc: 0.44
Batch: 780; loss: 1.8; acc: 0.36
Train Epoch over. train_loss: 1.89; train_accuracy: 0.39 

Batch: 0; loss: 1.72; acc: 0.45
Batch: 20; loss: 1.81; acc: 0.39
Batch: 40; loss: 1.56; acc: 0.55
Batch: 60; loss: 1.63; acc: 0.45
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.76; acc: 0.55
Batch: 120; loss: 1.74; acc: 0.48
Batch: 140; loss: 1.69; acc: 0.58
Val Epoch over. val_loss: 1.75087817079702; val_accuracy: 0.4570063694267516 

Epoch 4 start
The current lr is: 0.001
Batch: 0; loss: 1.86; acc: 0.44
Batch: 20; loss: 1.69; acc: 0.53
Batch: 40; loss: 1.83; acc: 0.41
Batch: 60; loss: 1.76; acc: 0.38
Batch: 80; loss: 1.49; acc: 0.5
Batch: 100; loss: 1.78; acc: 0.42
Batch: 120; loss: 1.57; acc: 0.58
Batch: 140; loss: 1.75; acc: 0.38
Batch: 160; loss: 1.74; acc: 0.5
Batch: 180; loss: 1.7; acc: 0.53
Batch: 200; loss: 1.73; acc: 0.47
Batch: 220; loss: 1.82; acc: 0.44
Batch: 240; loss: 1.69; acc: 0.44
Batch: 260; loss: 1.76; acc: 0.38
Batch: 280; loss: 1.76; acc: 0.41
Batch: 300; loss: 1.43; acc: 0.55
Batch: 320; loss: 1.6; acc: 0.48
Batch: 340; loss: 1.67; acc: 0.48
Batch: 360; loss: 1.58; acc: 0.55
Batch: 380; loss: 1.68; acc: 0.48
Batch: 400; loss: 1.51; acc: 0.47
Batch: 420; loss: 1.8; acc: 0.42
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.57; acc: 0.55
Batch: 480; loss: 1.72; acc: 0.39
Batch: 500; loss: 1.62; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.44
Batch: 540; loss: 1.68; acc: 0.44
Batch: 560; loss: 1.45; acc: 0.59
Batch: 580; loss: 1.77; acc: 0.3
Batch: 600; loss: 1.63; acc: 0.52
Batch: 620; loss: 1.66; acc: 0.39
Batch: 640; loss: 1.48; acc: 0.59
Batch: 660; loss: 1.65; acc: 0.56
Batch: 680; loss: 1.45; acc: 0.56
Batch: 700; loss: 1.45; acc: 0.59
Batch: 720; loss: 1.7; acc: 0.5
Batch: 740; loss: 1.47; acc: 0.52
Batch: 760; loss: 1.6; acc: 0.55
Batch: 780; loss: 1.63; acc: 0.58
Train Epoch over. train_loss: 1.64; train_accuracy: 0.5 

Batch: 0; loss: 1.48; acc: 0.59
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.29; acc: 0.67
Batch: 60; loss: 1.37; acc: 0.52
Batch: 80; loss: 1.45; acc: 0.61
Batch: 100; loss: 1.47; acc: 0.64
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.39; acc: 0.7
Val Epoch over. val_loss: 1.509505577148146; val_accuracy: 0.5688694267515924 

Epoch 5 start
The current lr is: 0.001
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 1.53; acc: 0.55
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.31; acc: 0.72
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.44; acc: 0.56
Batch: 140; loss: 1.47; acc: 0.47
Batch: 160; loss: 1.47; acc: 0.52
Batch: 180; loss: 1.61; acc: 0.5
Batch: 200; loss: 1.55; acc: 0.53
Batch: 220; loss: 1.42; acc: 0.59
Batch: 240; loss: 1.43; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.59
Batch: 280; loss: 1.54; acc: 0.52
Batch: 300; loss: 1.35; acc: 0.69
Batch: 320; loss: 1.38; acc: 0.61
Batch: 340; loss: 1.54; acc: 0.48
Batch: 360; loss: 1.37; acc: 0.62
Batch: 380; loss: 1.38; acc: 0.56
Batch: 400; loss: 1.25; acc: 0.75
Batch: 420; loss: 1.46; acc: 0.55
Batch: 440; loss: 1.47; acc: 0.61
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.29; acc: 0.66
Batch: 500; loss: 1.49; acc: 0.53
Batch: 520; loss: 1.41; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.66
Batch: 580; loss: 1.33; acc: 0.52
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.33; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.46; acc: 0.58
Batch: 700; loss: 1.34; acc: 0.62
Batch: 720; loss: 1.38; acc: 0.64
Batch: 740; loss: 1.38; acc: 0.61
Batch: 760; loss: 1.25; acc: 0.64
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.42; train_accuracy: 0.58 

Batch: 0; loss: 1.28; acc: 0.64
Batch: 20; loss: 1.43; acc: 0.5
Batch: 40; loss: 1.06; acc: 0.77
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.73
Batch: 100; loss: 1.26; acc: 0.7
Batch: 120; loss: 1.35; acc: 0.64
Batch: 140; loss: 1.13; acc: 0.73
Val Epoch over. val_loss: 1.287880595322627; val_accuracy: 0.6332603503184714 

Epoch 6 start
The current lr is: 0.0001
Batch: 0; loss: 1.22; acc: 0.7
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.56; acc: 0.5
Batch: 80; loss: 1.21; acc: 0.69
Batch: 100; loss: 1.47; acc: 0.55
Batch: 120; loss: 1.27; acc: 0.66
Batch: 140; loss: 1.43; acc: 0.61
Batch: 160; loss: 1.36; acc: 0.61
Batch: 180; loss: 1.31; acc: 0.64
Batch: 200; loss: 1.31; acc: 0.62
Batch: 220; loss: 1.28; acc: 0.61
Batch: 240; loss: 1.27; acc: 0.67
Batch: 260; loss: 1.28; acc: 0.58
Batch: 280; loss: 1.26; acc: 0.56
Batch: 300; loss: 1.28; acc: 0.72
Batch: 320; loss: 1.21; acc: 0.7
Batch: 340; loss: 1.38; acc: 0.58
Batch: 360; loss: 1.34; acc: 0.53
Batch: 380; loss: 1.3; acc: 0.61
Batch: 400; loss: 1.34; acc: 0.64
Batch: 420; loss: 1.18; acc: 0.72
Batch: 440; loss: 1.26; acc: 0.64
Batch: 460; loss: 1.17; acc: 0.78
Batch: 480; loss: 1.27; acc: 0.73
Batch: 500; loss: 1.48; acc: 0.58
Batch: 520; loss: 1.33; acc: 0.62
Batch: 540; loss: 1.22; acc: 0.69
Batch: 560; loss: 1.4; acc: 0.62
Batch: 580; loss: 1.38; acc: 0.59
Batch: 600; loss: 1.2; acc: 0.67
Batch: 620; loss: 1.2; acc: 0.7
Batch: 640; loss: 1.23; acc: 0.64
Batch: 660; loss: 1.35; acc: 0.56
Batch: 680; loss: 1.23; acc: 0.62
Batch: 700; loss: 1.26; acc: 0.55
Batch: 720; loss: 1.2; acc: 0.7
Batch: 740; loss: 1.35; acc: 0.7
Batch: 760; loss: 1.17; acc: 0.67
Batch: 780; loss: 1.35; acc: 0.58
Train Epoch over. train_loss: 1.29; train_accuracy: 0.63 

Batch: 0; loss: 1.25; acc: 0.67
Batch: 20; loss: 1.38; acc: 0.53
Batch: 40; loss: 1.05; acc: 0.69
Batch: 60; loss: 1.15; acc: 0.62
Batch: 80; loss: 1.2; acc: 0.69
Batch: 100; loss: 1.22; acc: 0.69
Batch: 120; loss: 1.32; acc: 0.64
Batch: 140; loss: 1.08; acc: 0.78
Val Epoch over. val_loss: 1.258927899940758; val_accuracy: 0.6541600318471338 

Epoch 7 start
The current lr is: 0.0001
Batch: 0; loss: 1.19; acc: 0.59
Batch: 20; loss: 1.23; acc: 0.7
Batch: 40; loss: 1.45; acc: 0.59
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 1.34; acc: 0.7
Batch: 100; loss: 1.35; acc: 0.67
Batch: 120; loss: 1.25; acc: 0.7
Batch: 140; loss: 1.35; acc: 0.64
Batch: 160; loss: 1.24; acc: 0.67
Batch: 180; loss: 1.18; acc: 0.69
Batch: 200; loss: 1.25; acc: 0.64
Batch: 220; loss: 1.5; acc: 0.52
Batch: 240; loss: 1.25; acc: 0.64
Batch: 260; loss: 1.27; acc: 0.61
Batch: 280; loss: 1.2; acc: 0.75
Batch: 300; loss: 1.3; acc: 0.61
Batch: 320; loss: 1.36; acc: 0.55
Batch: 340; loss: 1.56; acc: 0.53
Batch: 360; loss: 1.38; acc: 0.64
Batch: 380; loss: 1.29; acc: 0.69
Batch: 400; loss: 1.19; acc: 0.64
Batch: 420; loss: 1.29; acc: 0.61
Batch: 440; loss: 1.25; acc: 0.66
Batch: 460; loss: 1.2; acc: 0.64
Batch: 480; loss: 1.36; acc: 0.53
Batch: 500; loss: 1.3; acc: 0.62
Batch: 520; loss: 1.41; acc: 0.59
Batch: 540; loss: 1.43; acc: 0.64
Batch: 560; loss: 1.31; acc: 0.53
Batch: 580; loss: 1.43; acc: 0.64
Batch: 600; loss: 1.33; acc: 0.66
Batch: 620; loss: 1.34; acc: 0.53
Batch: 640; loss: 1.26; acc: 0.64
Batch: 660; loss: 1.27; acc: 0.58
Batch: 680; loss: 1.17; acc: 0.72
Batch: 700; loss: 1.18; acc: 0.75
Batch: 720; loss: 1.27; acc: 0.61
Batch: 740; loss: 1.31; acc: 0.55
Batch: 760; loss: 1.4; acc: 0.59
Batch: 780; loss: 1.19; acc: 0.59
Train Epoch over. train_loss: 1.27; train_accuracy: 0.64 

Batch: 0; loss: 1.22; acc: 0.73
Batch: 20; loss: 1.37; acc: 0.56
Batch: 40; loss: 1.02; acc: 0.73
Batch: 60; loss: 1.12; acc: 0.67
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 1.19; acc: 0.7
Batch: 120; loss: 1.3; acc: 0.67
Batch: 140; loss: 1.06; acc: 0.8
Val Epoch over. val_loss: 1.2354936937617649; val_accuracy: 0.6644108280254777 

Epoch 8 start
The current lr is: 0.0001
Batch: 0; loss: 1.06; acc: 0.7
Batch: 20; loss: 1.08; acc: 0.69
Batch: 40; loss: 1.18; acc: 0.72
Batch: 60; loss: 1.21; acc: 0.62
Batch: 80; loss: 1.39; acc: 0.48
Batch: 100; loss: 1.33; acc: 0.59
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 1.18; acc: 0.72
Batch: 160; loss: 1.28; acc: 0.64
Batch: 180; loss: 1.36; acc: 0.56
Batch: 200; loss: 1.24; acc: 0.64
Batch: 220; loss: 1.32; acc: 0.66
Batch: 240; loss: 1.25; acc: 0.58
Batch: 260; loss: 1.32; acc: 0.67
Batch: 280; loss: 1.06; acc: 0.64
Batch: 300; loss: 1.34; acc: 0.64
Batch: 320; loss: 1.22; acc: 0.69
Batch: 340; loss: 1.3; acc: 0.59
Batch: 360; loss: 1.16; acc: 0.69
Batch: 380; loss: 1.27; acc: 0.62
Batch: 400; loss: 1.37; acc: 0.59
Batch: 420; loss: 1.35; acc: 0.64
Batch: 440; loss: 1.22; acc: 0.66
Batch: 460; loss: 1.17; acc: 0.69
Batch: 480; loss: 1.1; acc: 0.77
Batch: 500; loss: 1.15; acc: 0.72
Batch: 520; loss: 1.17; acc: 0.69
Batch: 540; loss: 1.24; acc: 0.62
Batch: 560; loss: 1.15; acc: 0.66
Batch: 580; loss: 0.88; acc: 0.86
Batch: 600; loss: 1.36; acc: 0.59
Batch: 620; loss: 1.29; acc: 0.58
Batch: 640; loss: 1.27; acc: 0.69
Batch: 660; loss: 1.13; acc: 0.69
Batch: 680; loss: 1.35; acc: 0.56
Batch: 700; loss: 1.06; acc: 0.69
Batch: 720; loss: 1.04; acc: 0.73
Batch: 740; loss: 1.3; acc: 0.55
Batch: 760; loss: 1.3; acc: 0.67
Batch: 780; loss: 1.1; acc: 0.66
Train Epoch over. train_loss: 1.25; train_accuracy: 0.64 

Batch: 0; loss: 1.2; acc: 0.72
Batch: 20; loss: 1.34; acc: 0.55
Batch: 40; loss: 1.0; acc: 0.73
Batch: 60; loss: 1.1; acc: 0.69
Batch: 80; loss: 1.15; acc: 0.7
Batch: 100; loss: 1.17; acc: 0.7
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 1.03; acc: 0.78
Val Epoch over. val_loss: 1.2122738505624662; val_accuracy: 0.6700835987261147 

Epoch 9 start
The current lr is: 0.0001
Batch: 0; loss: 1.21; acc: 0.7
Batch: 20; loss: 1.11; acc: 0.73
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.38; acc: 0.64
Batch: 80; loss: 1.14; acc: 0.66
Batch: 100; loss: 1.26; acc: 0.59
Batch: 120; loss: 1.16; acc: 0.66
Batch: 140; loss: 1.26; acc: 0.69
Batch: 160; loss: 1.16; acc: 0.73
Batch: 180; loss: 1.06; acc: 0.73
Batch: 200; loss: 1.21; acc: 0.59
Batch: 220; loss: 1.2; acc: 0.67
Batch: 240; loss: 1.35; acc: 0.58
Batch: 260; loss: 1.22; acc: 0.67
Batch: 280; loss: 1.19; acc: 0.67
Batch: 300; loss: 1.21; acc: 0.64
Batch: 320; loss: 1.16; acc: 0.66
Batch: 340; loss: 1.12; acc: 0.75
Batch: 360; loss: 1.09; acc: 0.69
Batch: 380; loss: 1.28; acc: 0.67
Batch: 400; loss: 1.21; acc: 0.75
Batch: 420; loss: 1.3; acc: 0.55
Batch: 440; loss: 1.27; acc: 0.62
Batch: 460; loss: 1.1; acc: 0.67
Batch: 480; loss: 1.23; acc: 0.62
Batch: 500; loss: 1.32; acc: 0.62
Batch: 520; loss: 1.23; acc: 0.56
Batch: 540; loss: 1.06; acc: 0.69
Batch: 560; loss: 1.18; acc: 0.73
Batch: 580; loss: 1.34; acc: 0.56
Batch: 600; loss: 1.15; acc: 0.72
Batch: 620; loss: 1.37; acc: 0.62
Batch: 640; loss: 1.27; acc: 0.66
Batch: 660; loss: 1.17; acc: 0.66
Batch: 680; loss: 1.17; acc: 0.66
Batch: 700; loss: 1.19; acc: 0.62
Batch: 720; loss: 1.2; acc: 0.64
Batch: 740; loss: 1.13; acc: 0.7
Batch: 760; loss: 1.04; acc: 0.72
Batch: 780; loss: 1.0; acc: 0.67
Train Epoch over. train_loss: 1.23; train_accuracy: 0.65 

Batch: 0; loss: 1.18; acc: 0.75
Batch: 20; loss: 1.32; acc: 0.58
Batch: 40; loss: 0.98; acc: 0.73
Batch: 60; loss: 1.08; acc: 0.69
Batch: 80; loss: 1.12; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.72
Batch: 120; loss: 1.25; acc: 0.69
Batch: 140; loss: 1.0; acc: 0.8
Val Epoch over. val_loss: 1.187767348851368; val_accuracy: 0.6773487261146497 

Epoch 10 start
The current lr is: 0.0001
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 1.2; acc: 0.72
Batch: 40; loss: 1.21; acc: 0.67
Batch: 60; loss: 1.13; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.0; acc: 0.72
Batch: 120; loss: 1.04; acc: 0.72
Batch: 140; loss: 1.1; acc: 0.61
Batch: 160; loss: 1.47; acc: 0.45
Batch: 180; loss: 1.28; acc: 0.53
Batch: 200; loss: 1.16; acc: 0.75
Batch: 220; loss: 1.21; acc: 0.61
Batch: 240; loss: 1.21; acc: 0.72
Batch: 260; loss: 1.13; acc: 0.69
Batch: 280; loss: 1.32; acc: 0.55
Batch: 300; loss: 1.2; acc: 0.64
Batch: 320; loss: 1.12; acc: 0.64
Batch: 340; loss: 1.1; acc: 0.69
Batch: 360; loss: 1.11; acc: 0.73
Batch: 380; loss: 1.25; acc: 0.64
Batch: 400; loss: 1.5; acc: 0.55
Batch: 420; loss: 1.24; acc: 0.55
Batch: 440; loss: 1.3; acc: 0.59
Batch: 460; loss: 1.31; acc: 0.61
Batch: 480; loss: 1.02; acc: 0.75
Batch: 500; loss: 1.34; acc: 0.56
Batch: 520; loss: 1.15; acc: 0.67
Batch: 540; loss: 1.16; acc: 0.72
Batch: 560; loss: 1.25; acc: 0.62
Batch: 580; loss: 1.37; acc: 0.59
Batch: 600; loss: 1.31; acc: 0.61
Batch: 620; loss: 1.34; acc: 0.59
Batch: 640; loss: 1.26; acc: 0.62
Batch: 660; loss: 0.96; acc: 0.73
Batch: 680; loss: 1.2; acc: 0.7
Batch: 700; loss: 1.24; acc: 0.59
Batch: 720; loss: 1.33; acc: 0.58
Batch: 740; loss: 1.05; acc: 0.7
Batch: 760; loss: 1.36; acc: 0.58
Batch: 780; loss: 1.13; acc: 0.77
Train Epoch over. train_loss: 1.2; train_accuracy: 0.66 

Batch: 0; loss: 1.17; acc: 0.72
Batch: 20; loss: 1.3; acc: 0.56
Batch: 40; loss: 0.97; acc: 0.75
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.13; acc: 0.73
Batch: 120; loss: 1.24; acc: 0.67
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 1.1653652498676519; val_accuracy: 0.6833200636942676 

Epoch 11 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 1.1; acc: 0.72
Batch: 20; loss: 1.23; acc: 0.66
Batch: 40; loss: 1.27; acc: 0.62
Batch: 60; loss: 1.17; acc: 0.64
Batch: 80; loss: 1.5; acc: 0.55
Batch: 100; loss: 1.2; acc: 0.7
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 1.14; acc: 0.72
Batch: 160; loss: 1.32; acc: 0.66
Batch: 180; loss: 1.27; acc: 0.58
Batch: 200; loss: 1.17; acc: 0.67
Batch: 220; loss: 1.19; acc: 0.62
Batch: 240; loss: 1.12; acc: 0.66
Batch: 260; loss: 1.14; acc: 0.62
Batch: 280; loss: 1.18; acc: 0.66
Batch: 300; loss: 1.39; acc: 0.61
Batch: 320; loss: 1.05; acc: 0.78
Batch: 340; loss: 1.34; acc: 0.61
Batch: 360; loss: 1.27; acc: 0.67
Batch: 380; loss: 1.17; acc: 0.64
Batch: 400; loss: 1.17; acc: 0.59
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 1.23; acc: 0.62
Batch: 460; loss: 1.5; acc: 0.55
Batch: 480; loss: 1.57; acc: 0.5
Batch: 500; loss: 1.04; acc: 0.75
Batch: 520; loss: 1.31; acc: 0.58
Batch: 540; loss: 1.27; acc: 0.67
Batch: 560; loss: 1.11; acc: 0.67
Batch: 580; loss: 1.34; acc: 0.59
Batch: 600; loss: 1.22; acc: 0.64
Batch: 620; loss: 1.17; acc: 0.66
Batch: 640; loss: 1.08; acc: 0.73
Batch: 660; loss: 1.11; acc: 0.75
Batch: 680; loss: 1.26; acc: 0.62
Batch: 700; loss: 1.06; acc: 0.7
Batch: 720; loss: 1.17; acc: 0.73
Batch: 740; loss: 1.06; acc: 0.7
Batch: 760; loss: 1.25; acc: 0.61
Batch: 780; loss: 0.99; acc: 0.8
Train Epoch over. train_loss: 1.19; train_accuracy: 0.66 

Batch: 0; loss: 1.16; acc: 0.73
Batch: 20; loss: 1.3; acc: 0.55
Batch: 40; loss: 0.97; acc: 0.73
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.13; acc: 0.72
Batch: 120; loss: 1.24; acc: 0.69
Batch: 140; loss: 0.97; acc: 0.8
Val Epoch over. val_loss: 1.1622856513709778; val_accuracy: 0.6857085987261147 

Epoch 12 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.64
Batch: 40; loss: 0.96; acc: 0.69
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 1.26; acc: 0.56
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 1.22; acc: 0.61
Batch: 160; loss: 1.26; acc: 0.55
Batch: 180; loss: 1.16; acc: 0.61
Batch: 200; loss: 1.17; acc: 0.59
Batch: 220; loss: 1.29; acc: 0.59
Batch: 240; loss: 0.97; acc: 0.78
Batch: 260; loss: 1.15; acc: 0.69
Batch: 280; loss: 1.2; acc: 0.7
Batch: 300; loss: 1.27; acc: 0.69
Batch: 320; loss: 1.16; acc: 0.72
Batch: 340; loss: 1.31; acc: 0.58
Batch: 360; loss: 1.26; acc: 0.67
Batch: 380; loss: 1.22; acc: 0.61
Batch: 400; loss: 1.2; acc: 0.72
Batch: 420; loss: 1.32; acc: 0.59
Batch: 440; loss: 1.19; acc: 0.62
Batch: 460; loss: 1.12; acc: 0.66
Batch: 480; loss: 1.15; acc: 0.67
Batch: 500; loss: 1.29; acc: 0.62
Batch: 520; loss: 1.01; acc: 0.72
Batch: 540; loss: 1.13; acc: 0.7
Batch: 560; loss: 1.15; acc: 0.66
Batch: 580; loss: 1.3; acc: 0.69
Batch: 600; loss: 1.31; acc: 0.58
Batch: 620; loss: 1.2; acc: 0.67
Batch: 640; loss: 1.24; acc: 0.67
Batch: 660; loss: 1.21; acc: 0.62
Batch: 680; loss: 1.26; acc: 0.69
Batch: 700; loss: 1.3; acc: 0.61
Batch: 720; loss: 1.11; acc: 0.64
Batch: 740; loss: 0.99; acc: 0.78
Batch: 760; loss: 1.33; acc: 0.62
Batch: 780; loss: 1.21; acc: 0.66
Train Epoch over. train_loss: 1.19; train_accuracy: 0.66 

Batch: 0; loss: 1.16; acc: 0.73
Batch: 20; loss: 1.3; acc: 0.56
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.1597615833495074; val_accuracy: 0.6857085987261147 

Epoch 13 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 1.2; acc: 0.67
Batch: 20; loss: 1.28; acc: 0.66
Batch: 40; loss: 1.24; acc: 0.59
Batch: 60; loss: 1.14; acc: 0.72
Batch: 80; loss: 1.09; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.02; acc: 0.72
Batch: 140; loss: 1.11; acc: 0.69
Batch: 160; loss: 1.3; acc: 0.53
Batch: 180; loss: 1.09; acc: 0.69
Batch: 200; loss: 1.32; acc: 0.69
Batch: 220; loss: 1.11; acc: 0.7
Batch: 240; loss: 1.24; acc: 0.72
Batch: 260; loss: 1.04; acc: 0.72
Batch: 280; loss: 1.06; acc: 0.72
Batch: 300; loss: 1.1; acc: 0.75
Batch: 320; loss: 1.08; acc: 0.73
Batch: 340; loss: 1.2; acc: 0.7
Batch: 360; loss: 1.35; acc: 0.66
Batch: 380; loss: 1.06; acc: 0.72
Batch: 400; loss: 1.1; acc: 0.69
Batch: 420; loss: 1.29; acc: 0.69
Batch: 440; loss: 1.15; acc: 0.69
Batch: 460; loss: 1.02; acc: 0.72
Batch: 480; loss: 1.13; acc: 0.72
Batch: 500; loss: 1.17; acc: 0.64
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.25; acc: 0.52
Batch: 560; loss: 1.24; acc: 0.64
Batch: 580; loss: 1.23; acc: 0.62
Batch: 600; loss: 1.21; acc: 0.69
Batch: 620; loss: 1.22; acc: 0.66
Batch: 640; loss: 1.07; acc: 0.75
Batch: 660; loss: 1.19; acc: 0.7
Batch: 680; loss: 1.45; acc: 0.58
Batch: 700; loss: 1.08; acc: 0.72
Batch: 720; loss: 1.06; acc: 0.66
Batch: 740; loss: 1.07; acc: 0.69
Batch: 760; loss: 1.05; acc: 0.75
Batch: 780; loss: 1.17; acc: 0.69
Train Epoch over. train_loss: 1.19; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.56
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.06; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.1572897164684952; val_accuracy: 0.6865047770700637 

Epoch 14 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 1.22; acc: 0.66
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 1.12; acc: 0.73
Batch: 60; loss: 1.49; acc: 0.5
Batch: 80; loss: 1.03; acc: 0.69
Batch: 100; loss: 1.23; acc: 0.73
Batch: 120; loss: 1.18; acc: 0.66
Batch: 140; loss: 1.11; acc: 0.67
Batch: 160; loss: 1.22; acc: 0.64
Batch: 180; loss: 1.21; acc: 0.7
Batch: 200; loss: 1.17; acc: 0.64
Batch: 220; loss: 1.21; acc: 0.64
Batch: 240; loss: 1.21; acc: 0.55
Batch: 260; loss: 1.09; acc: 0.69
Batch: 280; loss: 1.01; acc: 0.73
Batch: 300; loss: 1.34; acc: 0.61
Batch: 320; loss: 1.22; acc: 0.64
Batch: 340; loss: 1.38; acc: 0.58
Batch: 360; loss: 1.09; acc: 0.77
Batch: 380; loss: 1.13; acc: 0.72
Batch: 400; loss: 1.07; acc: 0.75
Batch: 420; loss: 1.16; acc: 0.69
Batch: 440; loss: 1.09; acc: 0.7
Batch: 460; loss: 1.16; acc: 0.69
Batch: 480; loss: 1.32; acc: 0.61
Batch: 500; loss: 0.96; acc: 0.8
Batch: 520; loss: 1.26; acc: 0.61
Batch: 540; loss: 1.21; acc: 0.61
Batch: 560; loss: 1.1; acc: 0.72
Batch: 580; loss: 1.24; acc: 0.58
Batch: 600; loss: 1.09; acc: 0.72
Batch: 620; loss: 1.23; acc: 0.66
Batch: 640; loss: 1.2; acc: 0.72
Batch: 660; loss: 1.15; acc: 0.7
Batch: 680; loss: 1.15; acc: 0.66
Batch: 700; loss: 1.34; acc: 0.56
Batch: 720; loss: 0.97; acc: 0.75
Batch: 740; loss: 1.08; acc: 0.77
Batch: 760; loss: 1.2; acc: 0.66
Batch: 780; loss: 1.31; acc: 0.53
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.1549468439096098; val_accuracy: 0.6863057324840764 

Epoch 15 start
The current lr is: 1.0000000000000003e-05
Batch: 0; loss: 1.29; acc: 0.61
Batch: 20; loss: 1.47; acc: 0.58
Batch: 40; loss: 1.11; acc: 0.7
Batch: 60; loss: 1.18; acc: 0.73
Batch: 80; loss: 1.2; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.77
Batch: 120; loss: 1.14; acc: 0.67
Batch: 140; loss: 1.24; acc: 0.64
Batch: 160; loss: 1.09; acc: 0.67
Batch: 180; loss: 1.14; acc: 0.64
Batch: 200; loss: 1.29; acc: 0.67
Batch: 220; loss: 1.11; acc: 0.7
Batch: 240; loss: 1.09; acc: 0.72
Batch: 260; loss: 1.22; acc: 0.58
Batch: 280; loss: 1.12; acc: 0.69
Batch: 300; loss: 1.22; acc: 0.61
Batch: 320; loss: 1.2; acc: 0.66
Batch: 340; loss: 1.11; acc: 0.69
Batch: 360; loss: 1.12; acc: 0.64
Batch: 380; loss: 1.18; acc: 0.69
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 1.24; acc: 0.62
Batch: 440; loss: 1.16; acc: 0.64
Batch: 460; loss: 1.33; acc: 0.53
Batch: 480; loss: 1.29; acc: 0.64
Batch: 500; loss: 1.2; acc: 0.67
Batch: 520; loss: 1.28; acc: 0.61
Batch: 540; loss: 0.95; acc: 0.77
Batch: 560; loss: 1.2; acc: 0.66
Batch: 580; loss: 1.24; acc: 0.56
Batch: 600; loss: 1.07; acc: 0.73
Batch: 620; loss: 1.34; acc: 0.67
Batch: 640; loss: 1.12; acc: 0.73
Batch: 660; loss: 1.11; acc: 0.69
Batch: 680; loss: 1.18; acc: 0.66
Batch: 700; loss: 1.25; acc: 0.69
Batch: 720; loss: 1.35; acc: 0.62
Batch: 740; loss: 1.12; acc: 0.75
Batch: 760; loss: 1.24; acc: 0.72
Batch: 780; loss: 1.42; acc: 0.59
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.96; acc: 0.8
Val Epoch over. val_loss: 1.1524686175546828; val_accuracy: 0.6869028662420382 

Epoch 16 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 1.16; acc: 0.73
Batch: 20; loss: 1.11; acc: 0.67
Batch: 40; loss: 1.21; acc: 0.69
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.26; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.69
Batch: 120; loss: 1.38; acc: 0.59
Batch: 140; loss: 1.18; acc: 0.66
Batch: 160; loss: 1.21; acc: 0.69
Batch: 180; loss: 1.24; acc: 0.7
Batch: 200; loss: 0.99; acc: 0.81
Batch: 220; loss: 1.07; acc: 0.73
Batch: 240; loss: 1.18; acc: 0.69
Batch: 260; loss: 1.15; acc: 0.72
Batch: 280; loss: 1.14; acc: 0.61
Batch: 300; loss: 1.1; acc: 0.73
Batch: 320; loss: 1.29; acc: 0.55
Batch: 340; loss: 1.18; acc: 0.55
Batch: 360; loss: 1.19; acc: 0.69
Batch: 380; loss: 1.24; acc: 0.64
Batch: 400; loss: 1.37; acc: 0.59
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 1.04; acc: 0.75
Batch: 460; loss: 1.13; acc: 0.7
Batch: 480; loss: 1.32; acc: 0.59
Batch: 500; loss: 1.18; acc: 0.67
Batch: 520; loss: 1.03; acc: 0.77
Batch: 540; loss: 1.16; acc: 0.59
Batch: 560; loss: 1.04; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.17; acc: 0.64
Batch: 620; loss: 1.28; acc: 0.69
Batch: 640; loss: 1.24; acc: 0.7
Batch: 660; loss: 1.12; acc: 0.7
Batch: 680; loss: 0.99; acc: 0.67
Batch: 700; loss: 1.36; acc: 0.61
Batch: 720; loss: 1.13; acc: 0.64
Batch: 740; loss: 1.17; acc: 0.66
Batch: 760; loss: 1.13; acc: 0.72
Batch: 780; loss: 1.15; acc: 0.75
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.152240304430579; val_accuracy: 0.6872014331210191 

Epoch 17 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 1.08; acc: 0.75
Batch: 20; loss: 1.26; acc: 0.72
Batch: 40; loss: 1.35; acc: 0.61
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 1.15; acc: 0.7
Batch: 100; loss: 1.34; acc: 0.66
Batch: 120; loss: 1.14; acc: 0.77
Batch: 140; loss: 1.26; acc: 0.56
Batch: 160; loss: 1.26; acc: 0.72
Batch: 180; loss: 1.25; acc: 0.58
Batch: 200; loss: 1.14; acc: 0.72
Batch: 220; loss: 1.47; acc: 0.59
Batch: 240; loss: 1.33; acc: 0.61
Batch: 260; loss: 1.4; acc: 0.56
Batch: 280; loss: 1.18; acc: 0.7
Batch: 300; loss: 1.25; acc: 0.62
Batch: 320; loss: 1.24; acc: 0.55
Batch: 340; loss: 1.15; acc: 0.62
Batch: 360; loss: 1.31; acc: 0.61
Batch: 380; loss: 1.05; acc: 0.72
Batch: 400; loss: 1.08; acc: 0.72
Batch: 420; loss: 1.32; acc: 0.55
Batch: 440; loss: 1.14; acc: 0.67
Batch: 460; loss: 1.36; acc: 0.62
Batch: 480; loss: 1.07; acc: 0.67
Batch: 500; loss: 1.09; acc: 0.64
Batch: 520; loss: 1.2; acc: 0.66
Batch: 540; loss: 1.26; acc: 0.58
Batch: 560; loss: 1.33; acc: 0.53
Batch: 580; loss: 1.13; acc: 0.62
Batch: 600; loss: 1.07; acc: 0.7
Batch: 620; loss: 1.31; acc: 0.58
Batch: 640; loss: 1.14; acc: 0.61
Batch: 660; loss: 1.22; acc: 0.66
Batch: 680; loss: 1.18; acc: 0.7
Batch: 700; loss: 1.18; acc: 0.7
Batch: 720; loss: 1.29; acc: 0.59
Batch: 740; loss: 1.17; acc: 0.67
Batch: 760; loss: 1.24; acc: 0.66
Batch: 780; loss: 1.24; acc: 0.62
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1520099852495134; val_accuracy: 0.6873009554140127 

Epoch 18 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 1.29; acc: 0.67
Batch: 20; loss: 1.16; acc: 0.7
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 1.15; acc: 0.62
Batch: 100; loss: 1.35; acc: 0.62
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 1.13; acc: 0.75
Batch: 160; loss: 1.18; acc: 0.61
Batch: 180; loss: 1.26; acc: 0.62
Batch: 200; loss: 1.27; acc: 0.61
Batch: 220; loss: 1.1; acc: 0.64
Batch: 240; loss: 1.41; acc: 0.53
Batch: 260; loss: 1.19; acc: 0.64
Batch: 280; loss: 1.38; acc: 0.61
Batch: 300; loss: 1.13; acc: 0.64
Batch: 320; loss: 1.25; acc: 0.62
Batch: 340; loss: 1.07; acc: 0.7
Batch: 360; loss: 1.17; acc: 0.64
Batch: 380; loss: 1.05; acc: 0.66
Batch: 400; loss: 1.26; acc: 0.64
Batch: 420; loss: 1.23; acc: 0.66
Batch: 440; loss: 1.06; acc: 0.69
Batch: 460; loss: 1.14; acc: 0.67
Batch: 480; loss: 1.2; acc: 0.61
Batch: 500; loss: 1.03; acc: 0.81
Batch: 520; loss: 1.24; acc: 0.67
Batch: 540; loss: 1.02; acc: 0.75
Batch: 560; loss: 1.28; acc: 0.67
Batch: 580; loss: 1.25; acc: 0.59
Batch: 600; loss: 1.08; acc: 0.73
Batch: 620; loss: 1.34; acc: 0.53
Batch: 640; loss: 1.24; acc: 0.67
Batch: 660; loss: 1.23; acc: 0.67
Batch: 680; loss: 1.23; acc: 0.64
Batch: 700; loss: 1.39; acc: 0.62
Batch: 720; loss: 0.99; acc: 0.73
Batch: 740; loss: 1.21; acc: 0.56
Batch: 760; loss: 1.14; acc: 0.7
Batch: 780; loss: 1.08; acc: 0.77
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1517796732817487; val_accuracy: 0.6874004777070064 

Epoch 19 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 1.15; acc: 0.67
Batch: 20; loss: 1.21; acc: 0.66
Batch: 40; loss: 1.34; acc: 0.56
Batch: 60; loss: 1.18; acc: 0.64
Batch: 80; loss: 1.06; acc: 0.7
Batch: 100; loss: 1.07; acc: 0.75
Batch: 120; loss: 1.17; acc: 0.69
Batch: 140; loss: 1.18; acc: 0.64
Batch: 160; loss: 1.21; acc: 0.61
Batch: 180; loss: 1.28; acc: 0.66
Batch: 200; loss: 1.13; acc: 0.67
Batch: 220; loss: 1.16; acc: 0.72
Batch: 240; loss: 1.03; acc: 0.73
Batch: 260; loss: 1.18; acc: 0.59
Batch: 280; loss: 1.22; acc: 0.56
Batch: 300; loss: 1.09; acc: 0.69
Batch: 320; loss: 1.17; acc: 0.7
Batch: 340; loss: 1.35; acc: 0.61
Batch: 360; loss: 1.28; acc: 0.62
Batch: 380; loss: 1.18; acc: 0.73
Batch: 400; loss: 1.36; acc: 0.61
Batch: 420; loss: 1.25; acc: 0.64
Batch: 440; loss: 1.07; acc: 0.72
Batch: 460; loss: 1.2; acc: 0.61
Batch: 480; loss: 1.25; acc: 0.64
Batch: 500; loss: 1.16; acc: 0.62
Batch: 520; loss: 1.19; acc: 0.59
Batch: 540; loss: 1.26; acc: 0.69
Batch: 560; loss: 1.13; acc: 0.66
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.11; acc: 0.7
Batch: 620; loss: 1.21; acc: 0.72
Batch: 640; loss: 1.15; acc: 0.64
Batch: 660; loss: 1.23; acc: 0.69
Batch: 680; loss: 1.13; acc: 0.69
Batch: 700; loss: 1.19; acc: 0.7
Batch: 720; loss: 1.18; acc: 0.66
Batch: 740; loss: 1.06; acc: 0.72
Batch: 760; loss: 1.14; acc: 0.67
Batch: 780; loss: 1.13; acc: 0.75
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.23; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1515490997369122; val_accuracy: 0.6874004777070064 

Epoch 20 start
The current lr is: 1.0000000000000002e-06
Batch: 0; loss: 1.11; acc: 0.7
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 1.17; acc: 0.61
Batch: 60; loss: 1.23; acc: 0.67
Batch: 80; loss: 1.26; acc: 0.62
Batch: 100; loss: 1.2; acc: 0.66
Batch: 120; loss: 1.23; acc: 0.59
Batch: 140; loss: 1.02; acc: 0.75
Batch: 160; loss: 1.06; acc: 0.75
Batch: 180; loss: 1.15; acc: 0.64
Batch: 200; loss: 1.04; acc: 0.67
Batch: 220; loss: 1.03; acc: 0.73
Batch: 240; loss: 1.28; acc: 0.64
Batch: 260; loss: 1.28; acc: 0.59
Batch: 280; loss: 1.02; acc: 0.73
Batch: 300; loss: 1.18; acc: 0.69
Batch: 320; loss: 1.13; acc: 0.64
Batch: 340; loss: 1.33; acc: 0.56
Batch: 360; loss: 1.25; acc: 0.61
Batch: 380; loss: 1.16; acc: 0.73
Batch: 400; loss: 1.13; acc: 0.73
Batch: 420; loss: 1.19; acc: 0.7
Batch: 440; loss: 1.16; acc: 0.67
Batch: 460; loss: 1.13; acc: 0.77
Batch: 480; loss: 1.16; acc: 0.62
Batch: 500; loss: 1.01; acc: 0.73
Batch: 520; loss: 1.16; acc: 0.64
Batch: 540; loss: 1.29; acc: 0.66
Batch: 560; loss: 1.37; acc: 0.56
Batch: 580; loss: 1.3; acc: 0.66
Batch: 600; loss: 1.24; acc: 0.66
Batch: 620; loss: 1.32; acc: 0.62
Batch: 640; loss: 1.1; acc: 0.69
Batch: 660; loss: 1.0; acc: 0.8
Batch: 680; loss: 1.25; acc: 0.77
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 1.31; acc: 0.52
Batch: 740; loss: 1.14; acc: 0.64
Batch: 760; loss: 1.16; acc: 0.64
Batch: 780; loss: 1.49; acc: 0.58
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1513166017593093; val_accuracy: 0.6875 

Epoch 21 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 1.2; acc: 0.62
Batch: 20; loss: 1.17; acc: 0.69
Batch: 40; loss: 1.31; acc: 0.59
Batch: 60; loss: 1.05; acc: 0.75
Batch: 80; loss: 1.34; acc: 0.62
Batch: 100; loss: 1.21; acc: 0.67
Batch: 120; loss: 1.19; acc: 0.67
Batch: 140; loss: 1.17; acc: 0.64
Batch: 160; loss: 1.08; acc: 0.81
Batch: 180; loss: 1.37; acc: 0.59
Batch: 200; loss: 1.03; acc: 0.78
Batch: 220; loss: 1.17; acc: 0.69
Batch: 240; loss: 1.03; acc: 0.7
Batch: 260; loss: 1.07; acc: 0.73
Batch: 280; loss: 1.24; acc: 0.58
Batch: 300; loss: 1.06; acc: 0.75
Batch: 320; loss: 1.29; acc: 0.62
Batch: 340; loss: 1.12; acc: 0.7
Batch: 360; loss: 1.14; acc: 0.72
Batch: 380; loss: 1.21; acc: 0.64
Batch: 400; loss: 0.96; acc: 0.8
Batch: 420; loss: 1.2; acc: 0.67
Batch: 440; loss: 1.19; acc: 0.75
Batch: 460; loss: 1.3; acc: 0.64
Batch: 480; loss: 1.3; acc: 0.62
Batch: 500; loss: 1.16; acc: 0.67
Batch: 520; loss: 1.12; acc: 0.7
Batch: 540; loss: 1.33; acc: 0.58
Batch: 560; loss: 1.14; acc: 0.77
Batch: 580; loss: 1.11; acc: 0.75
Batch: 600; loss: 1.25; acc: 0.58
Batch: 620; loss: 1.32; acc: 0.7
Batch: 640; loss: 1.17; acc: 0.7
Batch: 660; loss: 1.2; acc: 0.61
Batch: 680; loss: 1.07; acc: 0.72
Batch: 700; loss: 1.09; acc: 0.67
Batch: 720; loss: 1.03; acc: 0.73
Batch: 740; loss: 1.18; acc: 0.72
Batch: 760; loss: 1.17; acc: 0.67
Batch: 780; loss: 1.36; acc: 0.62
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.151302548350802; val_accuracy: 0.6875 

Epoch 22 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 1.37; acc: 0.66
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 1.17; acc: 0.61
Batch: 60; loss: 1.15; acc: 0.69
Batch: 80; loss: 1.38; acc: 0.56
Batch: 100; loss: 1.35; acc: 0.61
Batch: 120; loss: 1.5; acc: 0.55
Batch: 140; loss: 1.13; acc: 0.7
Batch: 160; loss: 1.24; acc: 0.69
Batch: 180; loss: 1.19; acc: 0.59
Batch: 200; loss: 1.39; acc: 0.61
Batch: 220; loss: 1.32; acc: 0.61
Batch: 240; loss: 1.18; acc: 0.73
Batch: 260; loss: 1.2; acc: 0.62
Batch: 280; loss: 1.07; acc: 0.7
Batch: 300; loss: 1.29; acc: 0.59
Batch: 320; loss: 1.18; acc: 0.66
Batch: 340; loss: 1.1; acc: 0.67
Batch: 360; loss: 1.16; acc: 0.69
Batch: 380; loss: 1.11; acc: 0.77
Batch: 400; loss: 1.27; acc: 0.64
Batch: 420; loss: 1.32; acc: 0.62
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 1.14; acc: 0.72
Batch: 500; loss: 1.1; acc: 0.64
Batch: 520; loss: 1.04; acc: 0.7
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.23; acc: 0.59
Batch: 580; loss: 1.17; acc: 0.73
Batch: 600; loss: 1.1; acc: 0.72
Batch: 620; loss: 1.18; acc: 0.67
Batch: 640; loss: 1.19; acc: 0.64
Batch: 660; loss: 1.34; acc: 0.62
Batch: 680; loss: 1.08; acc: 0.75
Batch: 700; loss: 1.2; acc: 0.7
Batch: 720; loss: 1.18; acc: 0.7
Batch: 740; loss: 1.21; acc: 0.66
Batch: 760; loss: 1.14; acc: 0.73
Batch: 780; loss: 0.96; acc: 0.7
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512885199990242; val_accuracy: 0.6875 

Epoch 23 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 1.12; acc: 0.64
Batch: 20; loss: 1.09; acc: 0.73
Batch: 40; loss: 1.04; acc: 0.69
Batch: 60; loss: 1.19; acc: 0.67
Batch: 80; loss: 1.06; acc: 0.67
Batch: 100; loss: 1.06; acc: 0.7
Batch: 120; loss: 1.23; acc: 0.64
Batch: 140; loss: 1.22; acc: 0.61
Batch: 160; loss: 1.28; acc: 0.64
Batch: 180; loss: 1.2; acc: 0.7
Batch: 200; loss: 1.24; acc: 0.66
Batch: 220; loss: 1.46; acc: 0.55
Batch: 240; loss: 1.22; acc: 0.67
Batch: 260; loss: 1.14; acc: 0.62
Batch: 280; loss: 1.12; acc: 0.73
Batch: 300; loss: 1.09; acc: 0.69
Batch: 320; loss: 1.23; acc: 0.59
Batch: 340; loss: 1.13; acc: 0.72
Batch: 360; loss: 1.15; acc: 0.72
Batch: 380; loss: 1.16; acc: 0.78
Batch: 400; loss: 1.35; acc: 0.59
Batch: 420; loss: 1.14; acc: 0.72
Batch: 440; loss: 1.16; acc: 0.72
Batch: 460; loss: 1.11; acc: 0.67
Batch: 480; loss: 1.11; acc: 0.67
Batch: 500; loss: 1.13; acc: 0.59
Batch: 520; loss: 1.07; acc: 0.7
Batch: 540; loss: 1.2; acc: 0.67
Batch: 560; loss: 1.29; acc: 0.59
Batch: 580; loss: 1.16; acc: 0.66
Batch: 600; loss: 1.02; acc: 0.75
Batch: 620; loss: 1.2; acc: 0.58
Batch: 640; loss: 1.28; acc: 0.64
Batch: 660; loss: 1.15; acc: 0.67
Batch: 680; loss: 1.04; acc: 0.72
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 1.22; acc: 0.64
Batch: 740; loss: 1.21; acc: 0.69
Batch: 760; loss: 1.2; acc: 0.67
Batch: 780; loss: 1.34; acc: 0.59
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512743917999753; val_accuracy: 0.6875 

Epoch 24 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 1.26; acc: 0.62
Batch: 20; loss: 1.36; acc: 0.59
Batch: 40; loss: 1.18; acc: 0.69
Batch: 60; loss: 1.29; acc: 0.62
Batch: 80; loss: 1.21; acc: 0.62
Batch: 100; loss: 1.23; acc: 0.61
Batch: 120; loss: 1.14; acc: 0.7
Batch: 140; loss: 1.24; acc: 0.64
Batch: 160; loss: 1.45; acc: 0.56
Batch: 180; loss: 1.08; acc: 0.66
Batch: 200; loss: 1.19; acc: 0.67
Batch: 220; loss: 1.06; acc: 0.78
Batch: 240; loss: 1.06; acc: 0.75
Batch: 260; loss: 1.05; acc: 0.67
Batch: 280; loss: 1.12; acc: 0.64
Batch: 300; loss: 1.31; acc: 0.59
Batch: 320; loss: 1.04; acc: 0.69
Batch: 340; loss: 1.11; acc: 0.64
Batch: 360; loss: 1.22; acc: 0.58
Batch: 380; loss: 1.23; acc: 0.75
Batch: 400; loss: 1.13; acc: 0.75
Batch: 420; loss: 1.07; acc: 0.67
Batch: 440; loss: 1.28; acc: 0.53
Batch: 460; loss: 1.27; acc: 0.69
Batch: 480; loss: 1.07; acc: 0.67
Batch: 500; loss: 1.07; acc: 0.67
Batch: 520; loss: 1.04; acc: 0.7
Batch: 540; loss: 1.27; acc: 0.55
Batch: 560; loss: 1.27; acc: 0.66
Batch: 580; loss: 0.98; acc: 0.69
Batch: 600; loss: 1.08; acc: 0.67
Batch: 620; loss: 1.15; acc: 0.7
Batch: 640; loss: 1.18; acc: 0.66
Batch: 660; loss: 1.33; acc: 0.62
Batch: 680; loss: 1.18; acc: 0.67
Batch: 700; loss: 1.31; acc: 0.64
Batch: 720; loss: 1.09; acc: 0.72
Batch: 740; loss: 1.37; acc: 0.58
Batch: 760; loss: 1.27; acc: 0.67
Batch: 780; loss: 1.3; acc: 0.61
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.15126037635621; val_accuracy: 0.6875 

Epoch 25 start
The current lr is: 1.0000000000000002e-07
Batch: 0; loss: 1.09; acc: 0.69
Batch: 20; loss: 1.16; acc: 0.62
Batch: 40; loss: 1.1; acc: 0.67
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 1.27; acc: 0.67
Batch: 100; loss: 1.24; acc: 0.56
Batch: 120; loss: 1.18; acc: 0.72
Batch: 140; loss: 1.17; acc: 0.7
Batch: 160; loss: 1.22; acc: 0.62
Batch: 180; loss: 1.09; acc: 0.73
Batch: 200; loss: 1.22; acc: 0.66
Batch: 220; loss: 1.17; acc: 0.69
Batch: 240; loss: 1.12; acc: 0.72
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 1.42; acc: 0.59
Batch: 300; loss: 1.01; acc: 0.73
Batch: 320; loss: 1.36; acc: 0.61
Batch: 340; loss: 1.38; acc: 0.61
Batch: 360; loss: 1.13; acc: 0.62
Batch: 380; loss: 1.13; acc: 0.69
Batch: 400; loss: 1.17; acc: 0.7
Batch: 420; loss: 1.02; acc: 0.75
Batch: 440; loss: 0.92; acc: 0.81
Batch: 460; loss: 1.24; acc: 0.7
Batch: 480; loss: 1.11; acc: 0.69
Batch: 500; loss: 1.21; acc: 0.7
Batch: 520; loss: 1.17; acc: 0.64
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 1.36; acc: 0.61
Batch: 580; loss: 1.06; acc: 0.73
Batch: 600; loss: 1.31; acc: 0.62
Batch: 620; loss: 1.08; acc: 0.69
Batch: 640; loss: 0.96; acc: 0.77
Batch: 660; loss: 1.16; acc: 0.59
Batch: 680; loss: 1.02; acc: 0.72
Batch: 700; loss: 1.24; acc: 0.62
Batch: 720; loss: 1.32; acc: 0.55
Batch: 740; loss: 1.29; acc: 0.66
Batch: 760; loss: 1.13; acc: 0.67
Batch: 780; loss: 1.14; acc: 0.67
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512460856680657; val_accuracy: 0.6875 

Epoch 26 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 1.2; acc: 0.64
Batch: 20; loss: 1.28; acc: 0.55
Batch: 40; loss: 1.18; acc: 0.64
Batch: 60; loss: 1.15; acc: 0.67
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 1.03; acc: 0.83
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 1.04; acc: 0.75
Batch: 160; loss: 1.5; acc: 0.52
Batch: 180; loss: 1.17; acc: 0.64
Batch: 200; loss: 1.05; acc: 0.8
Batch: 220; loss: 1.34; acc: 0.58
Batch: 240; loss: 1.28; acc: 0.64
Batch: 260; loss: 1.11; acc: 0.67
Batch: 280; loss: 1.19; acc: 0.66
Batch: 300; loss: 1.25; acc: 0.64
Batch: 320; loss: 1.17; acc: 0.66
Batch: 340; loss: 1.22; acc: 0.66
Batch: 360; loss: 1.13; acc: 0.67
Batch: 380; loss: 0.96; acc: 0.73
Batch: 400; loss: 1.16; acc: 0.66
Batch: 420; loss: 1.16; acc: 0.66
Batch: 440; loss: 1.09; acc: 0.69
Batch: 460; loss: 1.03; acc: 0.7
Batch: 480; loss: 1.25; acc: 0.72
Batch: 500; loss: 1.39; acc: 0.62
Batch: 520; loss: 1.18; acc: 0.73
Batch: 540; loss: 1.04; acc: 0.69
Batch: 560; loss: 1.08; acc: 0.69
Batch: 580; loss: 1.16; acc: 0.7
Batch: 600; loss: 1.32; acc: 0.64
Batch: 620; loss: 1.18; acc: 0.69
Batch: 640; loss: 1.2; acc: 0.7
Batch: 660; loss: 1.11; acc: 0.59
Batch: 680; loss: 0.99; acc: 0.72
Batch: 700; loss: 1.06; acc: 0.7
Batch: 720; loss: 1.22; acc: 0.75
Batch: 740; loss: 1.27; acc: 0.66
Batch: 760; loss: 1.19; acc: 0.7
Batch: 780; loss: 1.32; acc: 0.59
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.151245875723043; val_accuracy: 0.6875 

Epoch 27 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 1.25; acc: 0.62
Batch: 20; loss: 1.36; acc: 0.59
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.27; acc: 0.66
Batch: 80; loss: 1.2; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.69
Batch: 120; loss: 1.19; acc: 0.7
Batch: 140; loss: 1.2; acc: 0.66
Batch: 160; loss: 1.2; acc: 0.75
Batch: 180; loss: 1.2; acc: 0.67
Batch: 200; loss: 1.11; acc: 0.64
Batch: 220; loss: 1.02; acc: 0.75
Batch: 240; loss: 0.95; acc: 0.8
Batch: 260; loss: 1.08; acc: 0.75
Batch: 280; loss: 1.39; acc: 0.55
Batch: 300; loss: 1.28; acc: 0.64
Batch: 320; loss: 1.37; acc: 0.52
Batch: 340; loss: 1.15; acc: 0.72
Batch: 360; loss: 1.12; acc: 0.62
Batch: 380; loss: 1.14; acc: 0.61
Batch: 400; loss: 1.28; acc: 0.64
Batch: 420; loss: 1.18; acc: 0.66
Batch: 440; loss: 1.17; acc: 0.7
Batch: 460; loss: 1.17; acc: 0.72
Batch: 480; loss: 1.21; acc: 0.69
Batch: 500; loss: 1.31; acc: 0.58
Batch: 520; loss: 1.29; acc: 0.62
Batch: 540; loss: 1.2; acc: 0.72
Batch: 560; loss: 1.19; acc: 0.7
Batch: 580; loss: 1.12; acc: 0.67
Batch: 600; loss: 1.03; acc: 0.72
Batch: 620; loss: 1.19; acc: 0.66
Batch: 640; loss: 1.3; acc: 0.59
Batch: 660; loss: 1.17; acc: 0.59
Batch: 680; loss: 1.22; acc: 0.66
Batch: 700; loss: 1.03; acc: 0.75
Batch: 720; loss: 1.1; acc: 0.73
Batch: 740; loss: 1.2; acc: 0.66
Batch: 760; loss: 1.18; acc: 0.69
Batch: 780; loss: 1.28; acc: 0.59
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512455890892417; val_accuracy: 0.6875 

Epoch 28 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 1.24; acc: 0.69
Batch: 20; loss: 1.2; acc: 0.58
Batch: 40; loss: 1.19; acc: 0.62
Batch: 60; loss: 1.22; acc: 0.67
Batch: 80; loss: 1.08; acc: 0.7
Batch: 100; loss: 1.25; acc: 0.67
Batch: 120; loss: 1.05; acc: 0.67
Batch: 140; loss: 1.15; acc: 0.72
Batch: 160; loss: 1.16; acc: 0.67
Batch: 180; loss: 1.13; acc: 0.67
Batch: 200; loss: 1.05; acc: 0.7
Batch: 220; loss: 1.39; acc: 0.56
Batch: 240; loss: 1.17; acc: 0.67
Batch: 260; loss: 1.08; acc: 0.7
Batch: 280; loss: 1.4; acc: 0.64
Batch: 300; loss: 1.13; acc: 0.7
Batch: 320; loss: 1.17; acc: 0.67
Batch: 340; loss: 1.13; acc: 0.78
Batch: 360; loss: 1.14; acc: 0.72
Batch: 380; loss: 1.25; acc: 0.62
Batch: 400; loss: 1.18; acc: 0.67
Batch: 420; loss: 1.3; acc: 0.7
Batch: 440; loss: 1.26; acc: 0.62
Batch: 460; loss: 1.17; acc: 0.66
Batch: 480; loss: 1.12; acc: 0.69
Batch: 500; loss: 1.09; acc: 0.72
Batch: 520; loss: 1.31; acc: 0.58
Batch: 540; loss: 1.24; acc: 0.69
Batch: 560; loss: 1.23; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.67
Batch: 600; loss: 1.0; acc: 0.77
Batch: 620; loss: 1.35; acc: 0.59
Batch: 640; loss: 1.02; acc: 0.75
Batch: 660; loss: 1.14; acc: 0.73
Batch: 680; loss: 1.25; acc: 0.66
Batch: 700; loss: 1.42; acc: 0.55
Batch: 720; loss: 1.03; acc: 0.77
Batch: 740; loss: 1.16; acc: 0.72
Batch: 760; loss: 1.08; acc: 0.73
Batch: 780; loss: 1.06; acc: 0.72
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512453601618482; val_accuracy: 0.6875 

Epoch 29 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 1.41; acc: 0.53
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 1.32; acc: 0.64
Batch: 60; loss: 1.1; acc: 0.73
Batch: 80; loss: 1.17; acc: 0.61
Batch: 100; loss: 1.21; acc: 0.7
Batch: 120; loss: 1.34; acc: 0.66
Batch: 140; loss: 1.18; acc: 0.72
Batch: 160; loss: 1.2; acc: 0.64
Batch: 180; loss: 1.04; acc: 0.69
Batch: 200; loss: 1.23; acc: 0.67
Batch: 220; loss: 1.27; acc: 0.62
Batch: 240; loss: 1.38; acc: 0.56
Batch: 260; loss: 1.08; acc: 0.72
Batch: 280; loss: 1.31; acc: 0.62
Batch: 300; loss: 1.23; acc: 0.72
Batch: 320; loss: 1.1; acc: 0.72
Batch: 340; loss: 1.05; acc: 0.78
Batch: 360; loss: 1.0; acc: 0.73
Batch: 380; loss: 1.11; acc: 0.73
Batch: 400; loss: 1.15; acc: 0.73
Batch: 420; loss: 1.11; acc: 0.67
Batch: 440; loss: 1.26; acc: 0.62
Batch: 460; loss: 1.17; acc: 0.64
Batch: 480; loss: 1.21; acc: 0.66
Batch: 500; loss: 1.04; acc: 0.8
Batch: 520; loss: 1.28; acc: 0.62
Batch: 540; loss: 1.27; acc: 0.58
Batch: 560; loss: 1.14; acc: 0.67
Batch: 580; loss: 1.29; acc: 0.61
Batch: 600; loss: 1.0; acc: 0.77
Batch: 620; loss: 1.11; acc: 0.73
Batch: 640; loss: 1.02; acc: 0.67
Batch: 660; loss: 1.29; acc: 0.66
Batch: 680; loss: 1.14; acc: 0.66
Batch: 700; loss: 1.12; acc: 0.61
Batch: 720; loss: 1.09; acc: 0.69
Batch: 740; loss: 1.16; acc: 0.67
Batch: 760; loss: 1.32; acc: 0.64
Batch: 780; loss: 1.14; acc: 0.69
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512451190857371; val_accuracy: 0.6875 

Epoch 30 start
The current lr is: 1.0000000000000004e-08
Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.2; acc: 0.59
Batch: 40; loss: 1.02; acc: 0.7
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 1.1; acc: 0.72
Batch: 100; loss: 1.12; acc: 0.7
Batch: 120; loss: 1.27; acc: 0.67
Batch: 140; loss: 1.34; acc: 0.62
Batch: 160; loss: 1.04; acc: 0.75
Batch: 180; loss: 1.14; acc: 0.72
Batch: 200; loss: 1.11; acc: 0.7
Batch: 220; loss: 1.17; acc: 0.62
Batch: 240; loss: 1.15; acc: 0.69
Batch: 260; loss: 1.21; acc: 0.62
Batch: 280; loss: 1.21; acc: 0.62
Batch: 300; loss: 1.29; acc: 0.62
Batch: 320; loss: 1.2; acc: 0.67
Batch: 340; loss: 1.2; acc: 0.66
Batch: 360; loss: 1.17; acc: 0.66
Batch: 380; loss: 1.28; acc: 0.64
Batch: 400; loss: 1.13; acc: 0.64
Batch: 420; loss: 1.15; acc: 0.69
Batch: 440; loss: 1.23; acc: 0.69
Batch: 460; loss: 1.18; acc: 0.69
Batch: 480; loss: 1.28; acc: 0.59
Batch: 500; loss: 1.34; acc: 0.61
Batch: 520; loss: 1.3; acc: 0.58
Batch: 540; loss: 1.35; acc: 0.61
Batch: 560; loss: 1.17; acc: 0.72
Batch: 580; loss: 1.18; acc: 0.61
Batch: 600; loss: 1.17; acc: 0.66
Batch: 620; loss: 1.2; acc: 0.66
Batch: 640; loss: 1.26; acc: 0.59
Batch: 660; loss: 1.14; acc: 0.69
Batch: 680; loss: 1.23; acc: 0.58
Batch: 700; loss: 1.13; acc: 0.62
Batch: 720; loss: 1.18; acc: 0.61
Batch: 740; loss: 1.07; acc: 0.75
Batch: 760; loss: 1.1; acc: 0.66
Batch: 780; loss: 1.14; acc: 0.67
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448867415166; val_accuracy: 0.6875 

Epoch 31 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 1.08; acc: 0.7
Batch: 20; loss: 1.22; acc: 0.69
Batch: 40; loss: 1.24; acc: 0.64
Batch: 60; loss: 1.07; acc: 0.72
Batch: 80; loss: 1.35; acc: 0.55
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 1.24; acc: 0.56
Batch: 140; loss: 1.13; acc: 0.67
Batch: 160; loss: 1.29; acc: 0.58
Batch: 180; loss: 1.14; acc: 0.66
Batch: 200; loss: 1.25; acc: 0.64
Batch: 220; loss: 1.14; acc: 0.67
Batch: 240; loss: 1.08; acc: 0.73
Batch: 260; loss: 1.11; acc: 0.7
Batch: 280; loss: 1.23; acc: 0.61
Batch: 300; loss: 1.18; acc: 0.69
Batch: 320; loss: 1.03; acc: 0.83
Batch: 340; loss: 1.12; acc: 0.7
Batch: 360; loss: 1.08; acc: 0.72
Batch: 380; loss: 1.25; acc: 0.56
Batch: 400; loss: 1.44; acc: 0.58
Batch: 420; loss: 1.25; acc: 0.62
Batch: 440; loss: 1.25; acc: 0.67
Batch: 460; loss: 1.21; acc: 0.64
Batch: 480; loss: 1.24; acc: 0.66
Batch: 500; loss: 1.21; acc: 0.62
Batch: 520; loss: 1.07; acc: 0.66
Batch: 540; loss: 1.08; acc: 0.75
Batch: 560; loss: 1.02; acc: 0.7
Batch: 580; loss: 1.17; acc: 0.7
Batch: 600; loss: 1.19; acc: 0.7
Batch: 620; loss: 1.25; acc: 0.64
Batch: 640; loss: 1.05; acc: 0.69
Batch: 660; loss: 1.14; acc: 0.66
Batch: 680; loss: 1.31; acc: 0.59
Batch: 700; loss: 1.09; acc: 0.7
Batch: 720; loss: 1.25; acc: 0.69
Batch: 740; loss: 1.17; acc: 0.64
Batch: 760; loss: 1.37; acc: 0.59
Batch: 780; loss: 1.28; acc: 0.62
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448856025745; val_accuracy: 0.6875 

Epoch 32 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 1.21; acc: 0.62
Batch: 20; loss: 1.15; acc: 0.7
Batch: 40; loss: 1.1; acc: 0.8
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.3; acc: 0.62
Batch: 100; loss: 1.39; acc: 0.67
Batch: 120; loss: 1.02; acc: 0.73
Batch: 140; loss: 1.28; acc: 0.62
Batch: 160; loss: 1.11; acc: 0.67
Batch: 180; loss: 0.86; acc: 0.81
Batch: 200; loss: 0.98; acc: 0.78
Batch: 220; loss: 1.06; acc: 0.8
Batch: 240; loss: 1.12; acc: 0.69
Batch: 260; loss: 1.29; acc: 0.66
Batch: 280; loss: 1.2; acc: 0.67
Batch: 300; loss: 1.21; acc: 0.61
Batch: 320; loss: 1.25; acc: 0.62
Batch: 340; loss: 1.19; acc: 0.69
Batch: 360; loss: 1.07; acc: 0.67
Batch: 380; loss: 1.2; acc: 0.69
Batch: 400; loss: 1.09; acc: 0.64
Batch: 420; loss: 1.07; acc: 0.78
Batch: 440; loss: 1.19; acc: 0.62
Batch: 460; loss: 1.1; acc: 0.73
Batch: 480; loss: 1.16; acc: 0.67
Batch: 500; loss: 1.17; acc: 0.66
Batch: 520; loss: 1.02; acc: 0.75
Batch: 540; loss: 1.24; acc: 0.66
Batch: 560; loss: 1.24; acc: 0.72
Batch: 580; loss: 1.24; acc: 0.7
Batch: 600; loss: 1.25; acc: 0.59
Batch: 620; loss: 1.24; acc: 0.67
Batch: 640; loss: 1.38; acc: 0.58
Batch: 660; loss: 1.08; acc: 0.69
Batch: 680; loss: 1.13; acc: 0.7
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 1.15; acc: 0.66
Batch: 740; loss: 1.19; acc: 0.7
Batch: 760; loss: 1.21; acc: 0.64
Batch: 780; loss: 1.26; acc: 0.59
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448742131518; val_accuracy: 0.6875 

Epoch 33 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 1.26; acc: 0.61
Batch: 20; loss: 1.17; acc: 0.73
Batch: 40; loss: 1.31; acc: 0.59
Batch: 60; loss: 1.12; acc: 0.73
Batch: 80; loss: 1.08; acc: 0.72
Batch: 100; loss: 1.24; acc: 0.7
Batch: 120; loss: 1.19; acc: 0.61
Batch: 140; loss: 1.07; acc: 0.75
Batch: 160; loss: 1.3; acc: 0.62
Batch: 180; loss: 1.21; acc: 0.61
Batch: 200; loss: 1.23; acc: 0.64
Batch: 220; loss: 1.08; acc: 0.77
Batch: 240; loss: 1.25; acc: 0.66
Batch: 260; loss: 1.28; acc: 0.73
Batch: 280; loss: 1.14; acc: 0.8
Batch: 300; loss: 1.22; acc: 0.64
Batch: 320; loss: 1.35; acc: 0.55
Batch: 340; loss: 1.34; acc: 0.66
Batch: 360; loss: 1.24; acc: 0.62
Batch: 380; loss: 1.12; acc: 0.67
Batch: 400; loss: 1.14; acc: 0.69
Batch: 420; loss: 1.17; acc: 0.69
Batch: 440; loss: 1.26; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.67
Batch: 480; loss: 1.14; acc: 0.67
Batch: 500; loss: 1.1; acc: 0.75
Batch: 520; loss: 1.13; acc: 0.66
Batch: 540; loss: 1.05; acc: 0.72
Batch: 560; loss: 1.07; acc: 0.75
Batch: 580; loss: 1.27; acc: 0.58
Batch: 600; loss: 1.23; acc: 0.62
Batch: 620; loss: 1.12; acc: 0.69
Batch: 640; loss: 1.22; acc: 0.69
Batch: 660; loss: 1.15; acc: 0.77
Batch: 680; loss: 1.01; acc: 0.72
Batch: 700; loss: 1.16; acc: 0.69
Batch: 720; loss: 1.13; acc: 0.64
Batch: 740; loss: 1.25; acc: 0.61
Batch: 760; loss: 1.28; acc: 0.58
Batch: 780; loss: 1.01; acc: 0.72
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448893990486; val_accuracy: 0.6875 

Epoch 34 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 1.14; acc: 0.72
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 1.19; acc: 0.58
Batch: 60; loss: 1.0; acc: 0.66
Batch: 80; loss: 1.22; acc: 0.64
Batch: 100; loss: 1.29; acc: 0.64
Batch: 120; loss: 1.2; acc: 0.64
Batch: 140; loss: 1.15; acc: 0.73
Batch: 160; loss: 0.99; acc: 0.72
Batch: 180; loss: 1.04; acc: 0.7
Batch: 200; loss: 1.28; acc: 0.61
Batch: 220; loss: 1.02; acc: 0.67
Batch: 240; loss: 1.06; acc: 0.64
Batch: 260; loss: 1.16; acc: 0.66
Batch: 280; loss: 1.16; acc: 0.64
Batch: 300; loss: 1.19; acc: 0.73
Batch: 320; loss: 1.11; acc: 0.73
Batch: 340; loss: 1.19; acc: 0.62
Batch: 360; loss: 1.23; acc: 0.69
Batch: 380; loss: 1.29; acc: 0.55
Batch: 400; loss: 1.16; acc: 0.69
Batch: 420; loss: 1.06; acc: 0.75
Batch: 440; loss: 1.22; acc: 0.62
Batch: 460; loss: 1.17; acc: 0.66
Batch: 480; loss: 1.26; acc: 0.66
Batch: 500; loss: 0.92; acc: 0.7
Batch: 520; loss: 1.27; acc: 0.7
Batch: 540; loss: 1.44; acc: 0.53
Batch: 560; loss: 0.99; acc: 0.75
Batch: 580; loss: 1.18; acc: 0.66
Batch: 600; loss: 1.26; acc: 0.62
Batch: 620; loss: 1.33; acc: 0.61
Batch: 640; loss: 1.23; acc: 0.69
Batch: 660; loss: 1.02; acc: 0.7
Batch: 680; loss: 1.02; acc: 0.78
Batch: 700; loss: 1.43; acc: 0.55
Batch: 720; loss: 1.21; acc: 0.69
Batch: 740; loss: 1.18; acc: 0.59
Batch: 760; loss: 1.14; acc: 0.7
Batch: 780; loss: 1.37; acc: 0.59
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448825653951; val_accuracy: 0.6875 

Epoch 35 start
The current lr is: 1.0000000000000005e-09
Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 1.12; acc: 0.73
Batch: 40; loss: 1.2; acc: 0.69
Batch: 60; loss: 1.23; acc: 0.67
Batch: 80; loss: 1.22; acc: 0.67
Batch: 100; loss: 1.16; acc: 0.7
Batch: 120; loss: 1.24; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.58
Batch: 160; loss: 1.06; acc: 0.7
Batch: 180; loss: 0.92; acc: 0.78
Batch: 200; loss: 1.04; acc: 0.72
Batch: 220; loss: 1.23; acc: 0.69
Batch: 240; loss: 1.2; acc: 0.7
Batch: 260; loss: 1.04; acc: 0.73
Batch: 280; loss: 1.11; acc: 0.61
Batch: 300; loss: 1.25; acc: 0.66
Batch: 320; loss: 1.37; acc: 0.59
Batch: 340; loss: 1.38; acc: 0.55
Batch: 360; loss: 1.16; acc: 0.67
Batch: 380; loss: 1.23; acc: 0.7
Batch: 400; loss: 0.97; acc: 0.7
Batch: 420; loss: 1.17; acc: 0.69
Batch: 440; loss: 1.06; acc: 0.69
Batch: 460; loss: 1.07; acc: 0.64
Batch: 480; loss: 1.3; acc: 0.64
Batch: 500; loss: 1.12; acc: 0.67
Batch: 520; loss: 1.28; acc: 0.64
Batch: 540; loss: 1.26; acc: 0.61
Batch: 560; loss: 1.14; acc: 0.75
Batch: 580; loss: 1.4; acc: 0.58
Batch: 600; loss: 1.13; acc: 0.66
Batch: 620; loss: 1.17; acc: 0.7
Batch: 640; loss: 0.99; acc: 0.7
Batch: 660; loss: 1.31; acc: 0.61
Batch: 680; loss: 1.22; acc: 0.66
Batch: 700; loss: 1.18; acc: 0.64
Batch: 720; loss: 1.19; acc: 0.66
Batch: 740; loss: 1.44; acc: 0.64
Batch: 760; loss: 1.17; acc: 0.75
Batch: 780; loss: 1.08; acc: 0.64
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448863618692; val_accuracy: 0.6875 

Epoch 36 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 0.92; acc: 0.83
Batch: 20; loss: 1.1; acc: 0.75
Batch: 40; loss: 1.03; acc: 0.69
Batch: 60; loss: 1.05; acc: 0.7
Batch: 80; loss: 1.08; acc: 0.73
Batch: 100; loss: 1.05; acc: 0.77
Batch: 120; loss: 1.19; acc: 0.64
Batch: 140; loss: 0.98; acc: 0.77
Batch: 160; loss: 1.18; acc: 0.69
Batch: 180; loss: 1.11; acc: 0.67
Batch: 200; loss: 1.19; acc: 0.66
Batch: 220; loss: 1.2; acc: 0.7
Batch: 240; loss: 1.2; acc: 0.62
Batch: 260; loss: 1.03; acc: 0.7
Batch: 280; loss: 1.19; acc: 0.64
Batch: 300; loss: 1.27; acc: 0.69
Batch: 320; loss: 1.17; acc: 0.66
Batch: 340; loss: 1.37; acc: 0.55
Batch: 360; loss: 1.36; acc: 0.59
Batch: 380; loss: 1.5; acc: 0.55
Batch: 400; loss: 1.13; acc: 0.7
Batch: 420; loss: 1.29; acc: 0.62
Batch: 440; loss: 1.13; acc: 0.78
Batch: 460; loss: 1.28; acc: 0.66
Batch: 480; loss: 1.24; acc: 0.59
Batch: 500; loss: 1.1; acc: 0.72
Batch: 520; loss: 1.3; acc: 0.56
Batch: 540; loss: 1.13; acc: 0.62
Batch: 560; loss: 1.11; acc: 0.67
Batch: 580; loss: 1.08; acc: 0.75
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 1.26; acc: 0.66
Batch: 640; loss: 1.08; acc: 0.69
Batch: 660; loss: 1.21; acc: 0.66
Batch: 680; loss: 1.23; acc: 0.62
Batch: 700; loss: 1.13; acc: 0.66
Batch: 720; loss: 1.33; acc: 0.62
Batch: 740; loss: 1.11; acc: 0.66
Batch: 760; loss: 1.14; acc: 0.73
Batch: 780; loss: 1.12; acc: 0.64
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448856025745; val_accuracy: 0.6875 

Epoch 37 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 1.11; acc: 0.69
Batch: 20; loss: 1.02; acc: 0.72
Batch: 40; loss: 1.13; acc: 0.62
Batch: 60; loss: 1.24; acc: 0.66
Batch: 80; loss: 1.2; acc: 0.67
Batch: 100; loss: 1.03; acc: 0.7
Batch: 120; loss: 1.05; acc: 0.73
Batch: 140; loss: 1.11; acc: 0.7
Batch: 160; loss: 1.15; acc: 0.72
Batch: 180; loss: 1.23; acc: 0.67
Batch: 200; loss: 1.2; acc: 0.69
Batch: 220; loss: 1.46; acc: 0.53
Batch: 240; loss: 1.16; acc: 0.67
Batch: 260; loss: 1.26; acc: 0.72
Batch: 280; loss: 1.02; acc: 0.75
Batch: 300; loss: 1.08; acc: 0.72
Batch: 320; loss: 1.04; acc: 0.75
Batch: 340; loss: 1.26; acc: 0.61
Batch: 360; loss: 1.03; acc: 0.72
Batch: 380; loss: 1.53; acc: 0.55
Batch: 400; loss: 1.06; acc: 0.75
Batch: 420; loss: 1.37; acc: 0.55
Batch: 440; loss: 1.12; acc: 0.62
Batch: 460; loss: 1.15; acc: 0.66
Batch: 480; loss: 1.25; acc: 0.7
Batch: 500; loss: 1.21; acc: 0.66
Batch: 520; loss: 1.0; acc: 0.75
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 1.29; acc: 0.59
Batch: 580; loss: 1.16; acc: 0.66
Batch: 600; loss: 1.16; acc: 0.61
Batch: 620; loss: 1.13; acc: 0.62
Batch: 640; loss: 1.31; acc: 0.64
Batch: 660; loss: 1.2; acc: 0.67
Batch: 680; loss: 1.12; acc: 0.7
Batch: 700; loss: 1.21; acc: 0.62
Batch: 720; loss: 1.28; acc: 0.59
Batch: 740; loss: 1.1; acc: 0.67
Batch: 760; loss: 1.36; acc: 0.64
Batch: 780; loss: 1.1; acc: 0.8
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448840839848; val_accuracy: 0.6875 

Epoch 38 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 1.2; acc: 0.69
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 1.12; acc: 0.69
Batch: 60; loss: 0.95; acc: 0.69
Batch: 80; loss: 1.14; acc: 0.64
Batch: 100; loss: 1.27; acc: 0.72
Batch: 120; loss: 1.4; acc: 0.62
Batch: 140; loss: 1.08; acc: 0.75
Batch: 160; loss: 1.11; acc: 0.61
Batch: 180; loss: 1.15; acc: 0.64
Batch: 200; loss: 1.33; acc: 0.59
Batch: 220; loss: 1.13; acc: 0.69
Batch: 240; loss: 1.22; acc: 0.72
Batch: 260; loss: 1.15; acc: 0.75
Batch: 280; loss: 1.26; acc: 0.66
Batch: 300; loss: 1.25; acc: 0.69
Batch: 320; loss: 1.11; acc: 0.72
Batch: 340; loss: 1.14; acc: 0.7
Batch: 360; loss: 1.27; acc: 0.62
Batch: 380; loss: 1.24; acc: 0.61
Batch: 400; loss: 1.56; acc: 0.59
Batch: 420; loss: 1.03; acc: 0.7
Batch: 440; loss: 1.17; acc: 0.7
Batch: 460; loss: 1.18; acc: 0.66
Batch: 480; loss: 0.89; acc: 0.81
Batch: 500; loss: 1.23; acc: 0.72
Batch: 520; loss: 1.13; acc: 0.64
Batch: 540; loss: 1.14; acc: 0.58
Batch: 560; loss: 1.21; acc: 0.59
Batch: 580; loss: 1.2; acc: 0.75
Batch: 600; loss: 1.1; acc: 0.7
Batch: 620; loss: 1.12; acc: 0.67
Batch: 640; loss: 1.17; acc: 0.7
Batch: 660; loss: 1.17; acc: 0.66
Batch: 680; loss: 1.28; acc: 0.59
Batch: 700; loss: 1.06; acc: 0.75
Batch: 720; loss: 1.45; acc: 0.53
Batch: 740; loss: 1.04; acc: 0.7
Batch: 760; loss: 1.21; acc: 0.64
Batch: 780; loss: 1.22; acc: 0.61
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448840839848; val_accuracy: 0.6875 

Epoch 39 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.07; acc: 0.67
Batch: 40; loss: 1.18; acc: 0.67
Batch: 60; loss: 1.09; acc: 0.67
Batch: 80; loss: 1.08; acc: 0.67
Batch: 100; loss: 0.95; acc: 0.8
Batch: 120; loss: 1.15; acc: 0.62
Batch: 140; loss: 1.34; acc: 0.61
Batch: 160; loss: 1.23; acc: 0.58
Batch: 180; loss: 1.36; acc: 0.69
Batch: 200; loss: 1.21; acc: 0.67
Batch: 220; loss: 1.42; acc: 0.58
Batch: 240; loss: 1.22; acc: 0.72
Batch: 260; loss: 1.11; acc: 0.73
Batch: 280; loss: 1.25; acc: 0.69
Batch: 300; loss: 1.21; acc: 0.64
Batch: 320; loss: 1.17; acc: 0.69
Batch: 340; loss: 1.09; acc: 0.62
Batch: 360; loss: 1.16; acc: 0.67
Batch: 380; loss: 1.2; acc: 0.7
Batch: 400; loss: 1.18; acc: 0.66
Batch: 420; loss: 1.17; acc: 0.64
Batch: 440; loss: 1.22; acc: 0.59
Batch: 460; loss: 1.09; acc: 0.7
Batch: 480; loss: 1.23; acc: 0.59
Batch: 500; loss: 1.18; acc: 0.69
Batch: 520; loss: 1.07; acc: 0.75
Batch: 540; loss: 1.21; acc: 0.64
Batch: 560; loss: 1.31; acc: 0.59
Batch: 580; loss: 1.16; acc: 0.72
Batch: 600; loss: 1.17; acc: 0.66
Batch: 620; loss: 1.29; acc: 0.58
Batch: 640; loss: 1.2; acc: 0.64
Batch: 660; loss: 1.34; acc: 0.66
Batch: 680; loss: 1.47; acc: 0.53
Batch: 700; loss: 1.24; acc: 0.62
Batch: 720; loss: 1.19; acc: 0.69
Batch: 740; loss: 1.21; acc: 0.69
Batch: 760; loss: 1.09; acc: 0.7
Batch: 780; loss: 1.22; acc: 0.58
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448818061003; val_accuracy: 0.6875 

Epoch 40 start
The current lr is: 1.0000000000000004e-10
Batch: 0; loss: 1.12; acc: 0.72
Batch: 20; loss: 1.19; acc: 0.67
Batch: 40; loss: 1.33; acc: 0.64
Batch: 60; loss: 1.24; acc: 0.62
Batch: 80; loss: 1.26; acc: 0.62
Batch: 100; loss: 1.17; acc: 0.69
Batch: 120; loss: 1.06; acc: 0.8
Batch: 140; loss: 1.19; acc: 0.64
Batch: 160; loss: 1.17; acc: 0.7
Batch: 180; loss: 1.27; acc: 0.66
Batch: 200; loss: 1.11; acc: 0.77
Batch: 220; loss: 1.3; acc: 0.59
Batch: 240; loss: 1.26; acc: 0.61
Batch: 260; loss: 1.26; acc: 0.69
Batch: 280; loss: 1.09; acc: 0.75
Batch: 300; loss: 1.11; acc: 0.72
Batch: 320; loss: 1.32; acc: 0.64
Batch: 340; loss: 1.15; acc: 0.73
Batch: 360; loss: 1.27; acc: 0.64
Batch: 380; loss: 1.13; acc: 0.64
Batch: 400; loss: 1.21; acc: 0.64
Batch: 420; loss: 1.04; acc: 0.73
Batch: 440; loss: 1.11; acc: 0.73
Batch: 460; loss: 1.33; acc: 0.64
Batch: 480; loss: 1.13; acc: 0.7
Batch: 500; loss: 1.31; acc: 0.62
Batch: 520; loss: 1.09; acc: 0.66
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 0.98; acc: 0.77
Batch: 580; loss: 1.33; acc: 0.66
Batch: 600; loss: 1.22; acc: 0.7
Batch: 620; loss: 1.04; acc: 0.7
Batch: 640; loss: 1.15; acc: 0.66
Batch: 660; loss: 1.19; acc: 0.61
Batch: 680; loss: 1.0; acc: 0.72
Batch: 700; loss: 1.17; acc: 0.62
Batch: 720; loss: 1.11; acc: 0.72
Batch: 740; loss: 1.36; acc: 0.53
Batch: 760; loss: 1.14; acc: 0.7
Batch: 780; loss: 1.21; acc: 0.59
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 41 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 1.21; acc: 0.64
Batch: 20; loss: 1.33; acc: 0.56
Batch: 40; loss: 1.11; acc: 0.75
Batch: 60; loss: 1.02; acc: 0.78
Batch: 80; loss: 1.34; acc: 0.61
Batch: 100; loss: 1.1; acc: 0.77
Batch: 120; loss: 1.28; acc: 0.61
Batch: 140; loss: 1.27; acc: 0.64
Batch: 160; loss: 1.0; acc: 0.78
Batch: 180; loss: 1.18; acc: 0.67
Batch: 200; loss: 1.19; acc: 0.69
Batch: 220; loss: 1.13; acc: 0.61
Batch: 240; loss: 1.16; acc: 0.72
Batch: 260; loss: 1.25; acc: 0.64
Batch: 280; loss: 1.09; acc: 0.67
Batch: 300; loss: 1.25; acc: 0.62
Batch: 320; loss: 1.17; acc: 0.64
Batch: 340; loss: 1.38; acc: 0.59
Batch: 360; loss: 1.07; acc: 0.77
Batch: 380; loss: 1.05; acc: 0.72
Batch: 400; loss: 1.36; acc: 0.58
Batch: 420; loss: 1.12; acc: 0.69
Batch: 440; loss: 1.29; acc: 0.66
Batch: 460; loss: 1.25; acc: 0.64
Batch: 480; loss: 1.11; acc: 0.7
Batch: 500; loss: 1.24; acc: 0.59
Batch: 520; loss: 1.01; acc: 0.73
Batch: 540; loss: 1.33; acc: 0.62
Batch: 560; loss: 1.33; acc: 0.59
Batch: 580; loss: 1.17; acc: 0.64
Batch: 600; loss: 1.19; acc: 0.67
Batch: 620; loss: 1.26; acc: 0.64
Batch: 640; loss: 1.31; acc: 0.56
Batch: 660; loss: 1.22; acc: 0.69
Batch: 680; loss: 1.34; acc: 0.64
Batch: 700; loss: 0.98; acc: 0.8
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.28; acc: 0.66
Batch: 760; loss: 1.2; acc: 0.67
Batch: 780; loss: 1.15; acc: 0.61
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 42 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 1.1; acc: 0.69
Batch: 20; loss: 1.12; acc: 0.78
Batch: 40; loss: 1.36; acc: 0.5
Batch: 60; loss: 1.21; acc: 0.62
Batch: 80; loss: 1.3; acc: 0.64
Batch: 100; loss: 1.06; acc: 0.72
Batch: 120; loss: 1.01; acc: 0.69
Batch: 140; loss: 1.21; acc: 0.75
Batch: 160; loss: 1.2; acc: 0.61
Batch: 180; loss: 1.13; acc: 0.67
Batch: 200; loss: 1.17; acc: 0.72
Batch: 220; loss: 1.18; acc: 0.66
Batch: 240; loss: 1.39; acc: 0.58
Batch: 260; loss: 1.3; acc: 0.61
Batch: 280; loss: 1.08; acc: 0.7
Batch: 300; loss: 1.16; acc: 0.69
Batch: 320; loss: 1.31; acc: 0.64
Batch: 340; loss: 1.23; acc: 0.61
Batch: 360; loss: 1.29; acc: 0.59
Batch: 380; loss: 1.08; acc: 0.67
Batch: 400; loss: 1.26; acc: 0.64
Batch: 420; loss: 1.16; acc: 0.7
Batch: 440; loss: 1.38; acc: 0.66
Batch: 460; loss: 1.28; acc: 0.61
Batch: 480; loss: 1.17; acc: 0.66
Batch: 500; loss: 1.14; acc: 0.64
Batch: 520; loss: 1.33; acc: 0.61
Batch: 540; loss: 1.16; acc: 0.77
Batch: 560; loss: 1.08; acc: 0.7
Batch: 580; loss: 1.14; acc: 0.67
Batch: 600; loss: 1.16; acc: 0.72
Batch: 620; loss: 1.12; acc: 0.62
Batch: 640; loss: 1.08; acc: 0.7
Batch: 660; loss: 0.99; acc: 0.67
Batch: 680; loss: 1.17; acc: 0.61
Batch: 700; loss: 1.2; acc: 0.66
Batch: 720; loss: 1.31; acc: 0.55
Batch: 740; loss: 1.24; acc: 0.61
Batch: 760; loss: 1.06; acc: 0.72
Batch: 780; loss: 1.19; acc: 0.67
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 43 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 0.98; acc: 0.73
Batch: 20; loss: 1.06; acc: 0.67
Batch: 40; loss: 1.05; acc: 0.75
Batch: 60; loss: 1.12; acc: 0.69
Batch: 80; loss: 1.29; acc: 0.69
Batch: 100; loss: 1.05; acc: 0.78
Batch: 120; loss: 1.39; acc: 0.61
Batch: 140; loss: 1.03; acc: 0.77
Batch: 160; loss: 1.13; acc: 0.67
Batch: 180; loss: 1.31; acc: 0.59
Batch: 200; loss: 0.96; acc: 0.72
Batch: 220; loss: 1.21; acc: 0.67
Batch: 240; loss: 1.29; acc: 0.52
Batch: 260; loss: 1.02; acc: 0.8
Batch: 280; loss: 1.25; acc: 0.64
Batch: 300; loss: 1.09; acc: 0.66
Batch: 320; loss: 1.26; acc: 0.55
Batch: 340; loss: 1.09; acc: 0.7
Batch: 360; loss: 1.18; acc: 0.67
Batch: 380; loss: 1.14; acc: 0.7
Batch: 400; loss: 1.11; acc: 0.72
Batch: 420; loss: 1.22; acc: 0.67
Batch: 440; loss: 1.11; acc: 0.69
Batch: 460; loss: 1.13; acc: 0.66
Batch: 480; loss: 1.15; acc: 0.67
Batch: 500; loss: 0.97; acc: 0.73
Batch: 520; loss: 1.15; acc: 0.69
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.05; acc: 0.73
Batch: 580; loss: 1.21; acc: 0.62
Batch: 600; loss: 1.28; acc: 0.56
Batch: 620; loss: 1.11; acc: 0.72
Batch: 640; loss: 1.08; acc: 0.67
Batch: 660; loss: 1.17; acc: 0.69
Batch: 680; loss: 1.22; acc: 0.69
Batch: 700; loss: 1.27; acc: 0.73
Batch: 720; loss: 1.22; acc: 0.59
Batch: 740; loss: 1.21; acc: 0.69
Batch: 760; loss: 1.23; acc: 0.67
Batch: 780; loss: 1.19; acc: 0.64
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 44 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 1.13; acc: 0.7
Batch: 20; loss: 1.13; acc: 0.69
Batch: 40; loss: 1.23; acc: 0.62
Batch: 60; loss: 1.16; acc: 0.69
Batch: 80; loss: 1.22; acc: 0.64
Batch: 100; loss: 0.93; acc: 0.77
Batch: 120; loss: 1.17; acc: 0.69
Batch: 140; loss: 1.11; acc: 0.69
Batch: 160; loss: 1.08; acc: 0.72
Batch: 180; loss: 0.95; acc: 0.77
Batch: 200; loss: 1.11; acc: 0.69
Batch: 220; loss: 1.13; acc: 0.66
Batch: 240; loss: 1.15; acc: 0.7
Batch: 260; loss: 1.15; acc: 0.69
Batch: 280; loss: 1.21; acc: 0.69
Batch: 300; loss: 1.18; acc: 0.58
Batch: 320; loss: 1.08; acc: 0.67
Batch: 340; loss: 1.15; acc: 0.66
Batch: 360; loss: 1.15; acc: 0.64
Batch: 380; loss: 1.17; acc: 0.7
Batch: 400; loss: 1.26; acc: 0.62
Batch: 420; loss: 1.17; acc: 0.67
Batch: 440; loss: 1.29; acc: 0.58
Batch: 460; loss: 1.06; acc: 0.73
Batch: 480; loss: 1.13; acc: 0.7
Batch: 500; loss: 1.13; acc: 0.67
Batch: 520; loss: 1.18; acc: 0.59
Batch: 540; loss: 0.98; acc: 0.69
Batch: 560; loss: 1.05; acc: 0.7
Batch: 580; loss: 0.95; acc: 0.72
Batch: 600; loss: 1.21; acc: 0.67
Batch: 620; loss: 1.17; acc: 0.72
Batch: 640; loss: 1.08; acc: 0.72
Batch: 660; loss: 1.05; acc: 0.7
Batch: 680; loss: 1.15; acc: 0.64
Batch: 700; loss: 1.02; acc: 0.73
Batch: 720; loss: 1.05; acc: 0.81
Batch: 740; loss: 1.11; acc: 0.67
Batch: 760; loss: 1.06; acc: 0.69
Batch: 780; loss: 0.91; acc: 0.7
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 45 start
The current lr is: 1.0000000000000006e-11
Batch: 0; loss: 1.25; acc: 0.62
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 1.3; acc: 0.61
Batch: 60; loss: 0.96; acc: 0.81
Batch: 80; loss: 0.94; acc: 0.77
Batch: 100; loss: 1.27; acc: 0.52
Batch: 120; loss: 1.13; acc: 0.7
Batch: 140; loss: 1.18; acc: 0.62
Batch: 160; loss: 1.18; acc: 0.67
Batch: 180; loss: 1.25; acc: 0.67
Batch: 200; loss: 0.95; acc: 0.73
Batch: 220; loss: 1.22; acc: 0.64
Batch: 240; loss: 1.16; acc: 0.62
Batch: 260; loss: 1.23; acc: 0.61
Batch: 280; loss: 1.04; acc: 0.69
Batch: 300; loss: 1.12; acc: 0.66
Batch: 320; loss: 0.96; acc: 0.77
Batch: 340; loss: 1.11; acc: 0.67
Batch: 360; loss: 1.03; acc: 0.7
Batch: 380; loss: 1.07; acc: 0.67
Batch: 400; loss: 1.25; acc: 0.69
Batch: 420; loss: 1.17; acc: 0.61
Batch: 440; loss: 1.2; acc: 0.7
Batch: 460; loss: 1.3; acc: 0.61
Batch: 480; loss: 1.11; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.73
Batch: 520; loss: 1.08; acc: 0.64
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 1.17; acc: 0.67
Batch: 580; loss: 1.05; acc: 0.81
Batch: 600; loss: 1.3; acc: 0.61
Batch: 620; loss: 1.08; acc: 0.75
Batch: 640; loss: 0.95; acc: 0.75
Batch: 660; loss: 0.99; acc: 0.73
Batch: 680; loss: 1.0; acc: 0.73
Batch: 700; loss: 1.23; acc: 0.66
Batch: 720; loss: 1.27; acc: 0.62
Batch: 740; loss: 1.08; acc: 0.69
Batch: 760; loss: 1.35; acc: 0.59
Batch: 780; loss: 1.01; acc: 0.72
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 46 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.12; acc: 0.7
Batch: 40; loss: 1.28; acc: 0.69
Batch: 60; loss: 1.23; acc: 0.69
Batch: 80; loss: 1.24; acc: 0.67
Batch: 100; loss: 1.32; acc: 0.62
Batch: 120; loss: 1.1; acc: 0.7
Batch: 140; loss: 1.15; acc: 0.7
Batch: 160; loss: 1.03; acc: 0.72
Batch: 180; loss: 1.0; acc: 0.78
Batch: 200; loss: 1.21; acc: 0.61
Batch: 220; loss: 1.28; acc: 0.59
Batch: 240; loss: 1.19; acc: 0.72
Batch: 260; loss: 1.15; acc: 0.66
Batch: 280; loss: 1.11; acc: 0.7
Batch: 300; loss: 1.24; acc: 0.62
Batch: 320; loss: 1.2; acc: 0.7
Batch: 340; loss: 1.21; acc: 0.66
Batch: 360; loss: 1.33; acc: 0.59
Batch: 380; loss: 1.18; acc: 0.64
Batch: 400; loss: 1.41; acc: 0.64
Batch: 420; loss: 1.2; acc: 0.61
Batch: 440; loss: 1.34; acc: 0.59
Batch: 460; loss: 1.2; acc: 0.66
Batch: 480; loss: 1.33; acc: 0.64
Batch: 500; loss: 1.33; acc: 0.56
Batch: 520; loss: 1.14; acc: 0.69
Batch: 540; loss: 1.45; acc: 0.55
Batch: 560; loss: 1.26; acc: 0.66
Batch: 580; loss: 1.32; acc: 0.64
Batch: 600; loss: 1.18; acc: 0.66
Batch: 620; loss: 1.25; acc: 0.67
Batch: 640; loss: 1.08; acc: 0.7
Batch: 660; loss: 1.11; acc: 0.72
Batch: 680; loss: 1.0; acc: 0.72
Batch: 700; loss: 1.14; acc: 0.7
Batch: 720; loss: 1.16; acc: 0.62
Batch: 740; loss: 1.11; acc: 0.7
Batch: 760; loss: 1.09; acc: 0.77
Batch: 780; loss: 1.23; acc: 0.7
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 47 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 1.09; acc: 0.7
Batch: 20; loss: 1.25; acc: 0.61
Batch: 40; loss: 1.31; acc: 0.61
Batch: 60; loss: 1.26; acc: 0.62
Batch: 80; loss: 1.17; acc: 0.69
Batch: 100; loss: 1.38; acc: 0.55
Batch: 120; loss: 1.23; acc: 0.61
Batch: 140; loss: 1.21; acc: 0.67
Batch: 160; loss: 1.28; acc: 0.61
Batch: 180; loss: 1.17; acc: 0.66
Batch: 200; loss: 1.12; acc: 0.75
Batch: 220; loss: 1.09; acc: 0.72
Batch: 240; loss: 1.24; acc: 0.67
Batch: 260; loss: 1.33; acc: 0.56
Batch: 280; loss: 1.12; acc: 0.69
Batch: 300; loss: 1.04; acc: 0.62
Batch: 320; loss: 1.07; acc: 0.73
Batch: 340; loss: 1.31; acc: 0.61
Batch: 360; loss: 1.19; acc: 0.66
Batch: 380; loss: 1.08; acc: 0.7
Batch: 400; loss: 1.01; acc: 0.73
Batch: 420; loss: 1.28; acc: 0.73
Batch: 440; loss: 1.2; acc: 0.66
Batch: 460; loss: 1.11; acc: 0.7
Batch: 480; loss: 1.2; acc: 0.64
Batch: 500; loss: 1.26; acc: 0.62
Batch: 520; loss: 1.16; acc: 0.66
Batch: 540; loss: 0.99; acc: 0.75
Batch: 560; loss: 1.26; acc: 0.67
Batch: 580; loss: 1.39; acc: 0.53
Batch: 600; loss: 1.23; acc: 0.61
Batch: 620; loss: 1.08; acc: 0.69
Batch: 640; loss: 1.09; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.72
Batch: 680; loss: 1.33; acc: 0.64
Batch: 700; loss: 1.18; acc: 0.64
Batch: 720; loss: 1.28; acc: 0.7
Batch: 740; loss: 1.22; acc: 0.61
Batch: 760; loss: 1.08; acc: 0.67
Batch: 780; loss: 1.23; acc: 0.61
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 48 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 1.23; acc: 0.66
Batch: 20; loss: 1.24; acc: 0.7
Batch: 40; loss: 1.1; acc: 0.7
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.38; acc: 0.55
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 1.13; acc: 0.62
Batch: 140; loss: 1.3; acc: 0.58
Batch: 160; loss: 1.19; acc: 0.69
Batch: 180; loss: 1.11; acc: 0.67
Batch: 200; loss: 1.12; acc: 0.72
Batch: 220; loss: 1.23; acc: 0.67
Batch: 240; loss: 1.2; acc: 0.66
Batch: 260; loss: 1.19; acc: 0.66
Batch: 280; loss: 1.03; acc: 0.72
Batch: 300; loss: 1.36; acc: 0.58
Batch: 320; loss: 1.22; acc: 0.61
Batch: 340; loss: 1.08; acc: 0.66
Batch: 360; loss: 1.18; acc: 0.67
Batch: 380; loss: 1.15; acc: 0.62
Batch: 400; loss: 1.26; acc: 0.64
Batch: 420; loss: 1.22; acc: 0.7
Batch: 440; loss: 1.04; acc: 0.75
Batch: 460; loss: 1.28; acc: 0.69
Batch: 480; loss: 1.22; acc: 0.58
Batch: 500; loss: 1.18; acc: 0.72
Batch: 520; loss: 1.11; acc: 0.72
Batch: 540; loss: 1.27; acc: 0.69
Batch: 560; loss: 1.25; acc: 0.72
Batch: 580; loss: 1.08; acc: 0.78
Batch: 600; loss: 1.08; acc: 0.67
Batch: 620; loss: 1.25; acc: 0.58
Batch: 640; loss: 1.21; acc: 0.61
Batch: 660; loss: 1.28; acc: 0.66
Batch: 680; loss: 1.13; acc: 0.7
Batch: 700; loss: 1.15; acc: 0.66
Batch: 720; loss: 1.26; acc: 0.7
Batch: 740; loss: 1.31; acc: 0.61
Batch: 760; loss: 1.01; acc: 0.72
Batch: 780; loss: 1.11; acc: 0.67
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 49 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 1.02; acc: 0.66
Batch: 20; loss: 1.13; acc: 0.72
Batch: 40; loss: 1.37; acc: 0.58
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 1.14; acc: 0.66
Batch: 100; loss: 1.15; acc: 0.7
Batch: 120; loss: 1.21; acc: 0.73
Batch: 140; loss: 1.14; acc: 0.61
Batch: 160; loss: 1.23; acc: 0.69
Batch: 180; loss: 1.05; acc: 0.73
Batch: 200; loss: 1.19; acc: 0.62
Batch: 220; loss: 1.12; acc: 0.7
Batch: 240; loss: 1.16; acc: 0.7
Batch: 260; loss: 1.21; acc: 0.69
Batch: 280; loss: 1.08; acc: 0.8
Batch: 300; loss: 1.21; acc: 0.67
Batch: 320; loss: 1.11; acc: 0.73
Batch: 340; loss: 1.24; acc: 0.62
Batch: 360; loss: 1.34; acc: 0.61
Batch: 380; loss: 1.14; acc: 0.69
Batch: 400; loss: 1.09; acc: 0.69
Batch: 420; loss: 1.36; acc: 0.55
Batch: 440; loss: 1.11; acc: 0.66
Batch: 460; loss: 1.2; acc: 0.61
Batch: 480; loss: 1.16; acc: 0.7
Batch: 500; loss: 1.09; acc: 0.75
Batch: 520; loss: 1.28; acc: 0.59
Batch: 540; loss: 1.22; acc: 0.64
Batch: 560; loss: 1.29; acc: 0.59
Batch: 580; loss: 1.17; acc: 0.7
Batch: 600; loss: 1.25; acc: 0.64
Batch: 620; loss: 1.1; acc: 0.7
Batch: 640; loss: 1.21; acc: 0.61
Batch: 660; loss: 1.13; acc: 0.69
Batch: 680; loss: 0.99; acc: 0.75
Batch: 700; loss: 1.14; acc: 0.7
Batch: 720; loss: 1.28; acc: 0.62
Batch: 740; loss: 1.21; acc: 0.64
Batch: 760; loss: 1.21; acc: 0.66
Batch: 780; loss: 1.2; acc: 0.66
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

Epoch 50 start
The current lr is: 1.0000000000000006e-12
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.18; acc: 0.67
Batch: 40; loss: 1.21; acc: 0.61
Batch: 60; loss: 1.27; acc: 0.64
Batch: 80; loss: 1.2; acc: 0.61
Batch: 100; loss: 1.07; acc: 0.7
Batch: 120; loss: 1.14; acc: 0.64
Batch: 140; loss: 1.27; acc: 0.62
Batch: 160; loss: 1.16; acc: 0.72
Batch: 180; loss: 1.15; acc: 0.69
Batch: 200; loss: 1.21; acc: 0.7
Batch: 220; loss: 1.32; acc: 0.52
Batch: 240; loss: 1.15; acc: 0.62
Batch: 260; loss: 1.09; acc: 0.73
Batch: 280; loss: 1.25; acc: 0.58
Batch: 300; loss: 1.22; acc: 0.61
Batch: 320; loss: 1.1; acc: 0.66
Batch: 340; loss: 1.09; acc: 0.72
Batch: 360; loss: 1.01; acc: 0.73
Batch: 380; loss: 1.08; acc: 0.67
Batch: 400; loss: 1.29; acc: 0.56
Batch: 420; loss: 1.16; acc: 0.7
Batch: 440; loss: 1.36; acc: 0.59
Batch: 460; loss: 1.2; acc: 0.73
Batch: 480; loss: 1.27; acc: 0.56
Batch: 500; loss: 1.33; acc: 0.59
Batch: 520; loss: 1.14; acc: 0.64
Batch: 540; loss: 1.26; acc: 0.66
Batch: 560; loss: 1.19; acc: 0.67
Batch: 580; loss: 1.09; acc: 0.73
Batch: 600; loss: 1.25; acc: 0.59
Batch: 620; loss: 1.05; acc: 0.69
Batch: 640; loss: 1.08; acc: 0.75
Batch: 660; loss: 1.07; acc: 0.75
Batch: 680; loss: 1.31; acc: 0.64
Batch: 700; loss: 1.21; acc: 0.66
Batch: 720; loss: 1.13; acc: 0.67
Batch: 740; loss: 1.19; acc: 0.67
Batch: 760; loss: 1.2; acc: 0.66
Batch: 780; loss: 0.89; acc: 0.75
Train Epoch over. train_loss: 1.18; train_accuracy: 0.67 

Batch: 0; loss: 1.15; acc: 0.73
Batch: 20; loss: 1.29; acc: 0.55
Batch: 40; loss: 0.96; acc: 0.73
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.09; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.22; acc: 0.69
Batch: 140; loss: 0.95; acc: 0.8
Val Epoch over. val_loss: 1.1512448810468054; val_accuracy: 0.6875 

plots/no_subspace_training/reg_lenet/2020-01-19 04:54:36/d_dim_1000_lr_0.001_gamma_0.1_sched_freq_5_seed_1_epochs_50_batchsize_64
