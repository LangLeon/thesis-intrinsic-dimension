model : lenet
optimizer : SGD
lr : 1.0
schedule : True
schedule_gamma : 0.4
schedule_freq : 10
seed : 1
n_epochs : 50
batch_size : 64
non_wrapped : False
chunked : False
dense : False
parameter_correction : False
print_freq : 20
print_prec : 2
device : cuda
subspace_training : True
ddim_vs_acc : True
timestamp : 2020-01-22 14:35:09
nonzero elements in E: 2054
elements in E: 444260
fraction nonzero: 0.004623418718768289
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.32; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.31; acc: 0.09
Batch: 260; loss: 2.31; acc: 0.12
Batch: 280; loss: 2.32; acc: 0.08
Batch: 300; loss: 2.29; acc: 0.19
Batch: 320; loss: 2.32; acc: 0.08
Batch: 340; loss: 2.3; acc: 0.19
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.12
Batch: 400; loss: 2.31; acc: 0.09
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.3; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.03
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.06
Batch: 520; loss: 2.32; acc: 0.05
Batch: 540; loss: 2.31; acc: 0.08
Batch: 560; loss: 2.3; acc: 0.05
Batch: 580; loss: 2.32; acc: 0.06
Batch: 600; loss: 2.31; acc: 0.05
Batch: 620; loss: 2.31; acc: 0.09
Batch: 640; loss: 2.31; acc: 0.06
Batch: 660; loss: 2.28; acc: 0.17
Batch: 680; loss: 2.31; acc: 0.08
Batch: 700; loss: 2.31; acc: 0.11
Batch: 720; loss: 2.3; acc: 0.08
Batch: 740; loss: 2.31; acc: 0.03
Batch: 760; loss: 2.3; acc: 0.05
Batch: 780; loss: 2.3; acc: 0.08
Train Epoch over. train_loss: 2.31; train_accuracy: 0.09 

Batch: 0; loss: 2.3; acc: 0.03
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.32; acc: 0.03
Batch: 120; loss: 2.31; acc: 0.03
Batch: 140; loss: 2.3; acc: 0.05
Val Epoch over. val_loss: 2.3027064208012478; val_accuracy: 0.07633359872611464 

The current subspace-distance is: 1.8942007500299951e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.11
Batch: 20; loss: 2.29; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.31; acc: 0.03
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.31; acc: 0.05
Batch: 160; loss: 2.31; acc: 0.06
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.05
Batch: 220; loss: 2.3; acc: 0.09
Batch: 240; loss: 2.3; acc: 0.06
Batch: 260; loss: 2.31; acc: 0.02
Batch: 280; loss: 2.3; acc: 0.06
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.32; acc: 0.02
Batch: 340; loss: 2.31; acc: 0.05
Batch: 360; loss: 2.31; acc: 0.06
Batch: 380; loss: 2.29; acc: 0.09
Batch: 400; loss: 2.3; acc: 0.08
Batch: 420; loss: 2.31; acc: 0.09
Batch: 440; loss: 2.29; acc: 0.16
Batch: 460; loss: 2.28; acc: 0.14
Batch: 480; loss: 2.31; acc: 0.05
Batch: 500; loss: 2.29; acc: 0.14
Batch: 520; loss: 2.3; acc: 0.16
Batch: 540; loss: 2.31; acc: 0.09
Batch: 560; loss: 2.31; acc: 0.14
Batch: 580; loss: 2.3; acc: 0.05
Batch: 600; loss: 2.3; acc: 0.16
Batch: 620; loss: 2.3; acc: 0.09
Batch: 640; loss: 2.32; acc: 0.03
Batch: 660; loss: 2.29; acc: 0.05
Batch: 680; loss: 2.32; acc: 0.09
Batch: 700; loss: 2.3; acc: 0.09
Batch: 720; loss: 2.29; acc: 0.12
Batch: 740; loss: 2.3; acc: 0.09
Batch: 760; loss: 2.29; acc: 0.05
Batch: 780; loss: 2.31; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.08 

Batch: 0; loss: 2.3; acc: 0.05
Batch: 20; loss: 2.31; acc: 0.09
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.32; acc: 0.08
Batch: 120; loss: 2.31; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.06
Val Epoch over. val_loss: 2.2986766851631697; val_accuracy: 0.08449442675159236 

The current subspace-distance is: 3.023860926987254e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.32; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.08
Batch: 40; loss: 2.29; acc: 0.06
Batch: 60; loss: 2.3; acc: 0.05
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.29; acc: 0.03
Batch: 160; loss: 2.3; acc: 0.06
Batch: 180; loss: 2.29; acc: 0.06
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.31; acc: 0.05
Batch: 240; loss: 2.32; acc: 0.08
Batch: 260; loss: 2.3; acc: 0.14
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.29; acc: 0.06
Batch: 320; loss: 2.29; acc: 0.12
Batch: 340; loss: 2.28; acc: 0.09
Batch: 360; loss: 2.28; acc: 0.12
Batch: 380; loss: 2.3; acc: 0.09
Batch: 400; loss: 2.3; acc: 0.08
Batch: 420; loss: 2.31; acc: 0.11
Batch: 440; loss: 2.29; acc: 0.02
Batch: 460; loss: 2.29; acc: 0.09
Batch: 480; loss: 2.3; acc: 0.02
Batch: 500; loss: 2.29; acc: 0.11
Batch: 520; loss: 2.31; acc: 0.08
Batch: 540; loss: 2.29; acc: 0.14
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.31; acc: 0.06
Batch: 600; loss: 2.28; acc: 0.12
Batch: 620; loss: 2.3; acc: 0.05
Batch: 640; loss: 2.3; acc: 0.08
Batch: 660; loss: 2.3; acc: 0.12
Batch: 680; loss: 2.29; acc: 0.09
Batch: 700; loss: 2.29; acc: 0.14
Batch: 720; loss: 2.3; acc: 0.06
Batch: 740; loss: 2.29; acc: 0.14
Batch: 760; loss: 2.28; acc: 0.19
Batch: 780; loss: 2.29; acc: 0.12
Train Epoch over. train_loss: 2.3; train_accuracy: 0.09 

Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.3; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.09
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.29; acc: 0.08
Val Epoch over. val_loss: 2.294399563673955; val_accuracy: 0.11265923566878981 

The current subspace-distance is: 4.1996822801593225e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.08
Batch: 20; loss: 2.29; acc: 0.08
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.28; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.29; acc: 0.08
Batch: 160; loss: 2.27; acc: 0.17
Batch: 180; loss: 2.28; acc: 0.16
Batch: 200; loss: 2.3; acc: 0.12
Batch: 220; loss: 2.28; acc: 0.19
Batch: 240; loss: 2.31; acc: 0.02
Batch: 260; loss: 2.3; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.09
Batch: 320; loss: 2.28; acc: 0.14
Batch: 340; loss: 2.28; acc: 0.17
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.28; acc: 0.09
Batch: 400; loss: 2.29; acc: 0.08
Batch: 420; loss: 2.3; acc: 0.03
Batch: 440; loss: 2.29; acc: 0.11
Batch: 460; loss: 2.29; acc: 0.12
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.29; acc: 0.12
Batch: 520; loss: 2.28; acc: 0.14
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.3; acc: 0.16
Batch: 580; loss: 2.31; acc: 0.06
Batch: 600; loss: 2.28; acc: 0.17
Batch: 620; loss: 2.3; acc: 0.19
Batch: 640; loss: 2.28; acc: 0.17
Batch: 660; loss: 2.29; acc: 0.09
Batch: 680; loss: 2.29; acc: 0.17
Batch: 700; loss: 2.28; acc: 0.25
Batch: 720; loss: 2.28; acc: 0.19
Batch: 740; loss: 2.28; acc: 0.19
Batch: 760; loss: 2.27; acc: 0.2
Batch: 780; loss: 2.28; acc: 0.16
Train Epoch over. train_loss: 2.29; train_accuracy: 0.13 

Batch: 0; loss: 2.29; acc: 0.16
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.28; acc: 0.11
Batch: 60; loss: 2.29; acc: 0.17
Batch: 80; loss: 2.29; acc: 0.14
Batch: 100; loss: 2.28; acc: 0.16
Batch: 120; loss: 2.29; acc: 0.22
Batch: 140; loss: 2.28; acc: 0.2
Val Epoch over. val_loss: 2.2856315063063506; val_accuracy: 0.1618232484076433 

The current subspace-distance is: 5.781753316114191e-06 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.19
Batch: 20; loss: 2.3; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.19
Batch: 60; loss: 2.28; acc: 0.17
Batch: 80; loss: 2.28; acc: 0.17
Batch: 100; loss: 2.29; acc: 0.22
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.29; acc: 0.17
Batch: 160; loss: 2.28; acc: 0.22
Batch: 180; loss: 2.28; acc: 0.08
Batch: 200; loss: 2.3; acc: 0.09
Batch: 220; loss: 2.28; acc: 0.19
Batch: 240; loss: 2.28; acc: 0.2
Batch: 260; loss: 2.27; acc: 0.14
Batch: 280; loss: 2.28; acc: 0.19
Batch: 300; loss: 2.28; acc: 0.14
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.29; acc: 0.17
Batch: 360; loss: 2.29; acc: 0.16
Batch: 380; loss: 2.3; acc: 0.09
Batch: 400; loss: 2.27; acc: 0.17
Batch: 420; loss: 2.27; acc: 0.19
Batch: 440; loss: 2.28; acc: 0.11
Batch: 460; loss: 2.25; acc: 0.3
Batch: 480; loss: 2.28; acc: 0.14
Batch: 500; loss: 2.28; acc: 0.12
Batch: 520; loss: 2.27; acc: 0.17
Batch: 540; loss: 2.26; acc: 0.19
Batch: 560; loss: 2.28; acc: 0.14
Batch: 580; loss: 2.28; acc: 0.14
Batch: 600; loss: 2.28; acc: 0.16
Batch: 620; loss: 2.29; acc: 0.16
Batch: 640; loss: 2.28; acc: 0.09
Batch: 660; loss: 2.29; acc: 0.09
Batch: 680; loss: 2.28; acc: 0.12
Batch: 700; loss: 2.28; acc: 0.19
Batch: 720; loss: 2.27; acc: 0.19
Batch: 740; loss: 2.29; acc: 0.14
Batch: 760; loss: 2.27; acc: 0.11
Batch: 780; loss: 2.29; acc: 0.11
Train Epoch over. train_loss: 2.28; train_accuracy: 0.14 

Batch: 0; loss: 2.28; acc: 0.16
Batch: 20; loss: 2.28; acc: 0.09
Batch: 40; loss: 2.27; acc: 0.16
Batch: 60; loss: 2.28; acc: 0.17
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.25; acc: 0.17
Batch: 120; loss: 2.26; acc: 0.17
Batch: 140; loss: 2.27; acc: 0.17
Val Epoch over. val_loss: 2.2774105011277896; val_accuracy: 0.13594745222929935 

The current subspace-distance is: 6.53851066090283e-06 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.12
Batch: 20; loss: 2.29; acc: 0.19
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.28; acc: 0.16
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.27; acc: 0.19
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.27; acc: 0.16
Batch: 160; loss: 2.28; acc: 0.12
Batch: 180; loss: 2.28; acc: 0.12
Batch: 200; loss: 2.28; acc: 0.06
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.14
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.26; acc: 0.08
Batch: 300; loss: 2.29; acc: 0.12
Batch: 320; loss: 2.28; acc: 0.12
Batch: 340; loss: 2.28; acc: 0.11
Batch: 360; loss: 2.26; acc: 0.14
Batch: 380; loss: 2.28; acc: 0.11
Batch: 400; loss: 2.28; acc: 0.14
Batch: 420; loss: 2.28; acc: 0.09
Batch: 440; loss: 2.29; acc: 0.05
Batch: 460; loss: 2.26; acc: 0.09
Batch: 480; loss: 2.28; acc: 0.16
Batch: 500; loss: 2.29; acc: 0.06
Batch: 520; loss: 2.27; acc: 0.19
Batch: 540; loss: 2.27; acc: 0.14
Batch: 560; loss: 2.27; acc: 0.09
Batch: 580; loss: 2.26; acc: 0.11
Batch: 600; loss: 2.27; acc: 0.14
Batch: 620; loss: 2.28; acc: 0.08
Batch: 640; loss: 2.26; acc: 0.16
Batch: 660; loss: 2.27; acc: 0.12
Batch: 680; loss: 2.26; acc: 0.16
Batch: 700; loss: 2.27; acc: 0.12
Batch: 720; loss: 2.27; acc: 0.12
Batch: 740; loss: 2.3; acc: 0.12
Batch: 760; loss: 2.28; acc: 0.11
Batch: 780; loss: 2.27; acc: 0.16
Train Epoch over. train_loss: 2.28; train_accuracy: 0.12 

Batch: 0; loss: 2.27; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.25; acc: 0.11
Batch: 60; loss: 2.28; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.23; acc: 0.19
Batch: 120; loss: 2.25; acc: 0.11
Batch: 140; loss: 2.26; acc: 0.14
Val Epoch over. val_loss: 2.27078980093549; val_accuracy: 0.12042197452229299 

The current subspace-distance is: 7.059016752464231e-06 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.26; acc: 0.11
Batch: 40; loss: 2.27; acc: 0.14
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.25; acc: 0.09
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.14
Batch: 140; loss: 2.24; acc: 0.16
Batch: 160; loss: 2.3; acc: 0.06
Batch: 180; loss: 2.29; acc: 0.06
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.26; acc: 0.19
Batch: 240; loss: 2.27; acc: 0.05
Batch: 260; loss: 2.27; acc: 0.16
Batch: 280; loss: 2.28; acc: 0.09
Batch: 300; loss: 2.31; acc: 0.11
Batch: 320; loss: 2.26; acc: 0.16
Batch: 340; loss: 2.27; acc: 0.12
Batch: 360; loss: 2.28; acc: 0.11
Batch: 380; loss: 2.27; acc: 0.14
Batch: 400; loss: 2.26; acc: 0.12
Batch: 420; loss: 2.24; acc: 0.14
Batch: 440; loss: 2.25; acc: 0.14
Batch: 460; loss: 2.25; acc: 0.14
Batch: 480; loss: 2.27; acc: 0.14
Batch: 500; loss: 2.25; acc: 0.16
Batch: 520; loss: 2.28; acc: 0.12
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.26; acc: 0.11
Batch: 580; loss: 2.28; acc: 0.14
Batch: 600; loss: 2.24; acc: 0.08
Batch: 620; loss: 2.28; acc: 0.05
Batch: 640; loss: 2.28; acc: 0.09
Batch: 660; loss: 2.28; acc: 0.12
Batch: 680; loss: 2.27; acc: 0.14
Batch: 700; loss: 2.28; acc: 0.16
Batch: 720; loss: 2.25; acc: 0.16
Batch: 740; loss: 2.25; acc: 0.12
Batch: 760; loss: 2.28; acc: 0.08
Batch: 780; loss: 2.27; acc: 0.12
Train Epoch over. train_loss: 2.27; train_accuracy: 0.11 

Batch: 0; loss: 2.25; acc: 0.2
Batch: 20; loss: 2.26; acc: 0.11
Batch: 40; loss: 2.24; acc: 0.12
Batch: 60; loss: 2.27; acc: 0.12
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.21; acc: 0.23
Batch: 120; loss: 2.23; acc: 0.14
Batch: 140; loss: 2.24; acc: 0.16
Val Epoch over. val_loss: 2.261957771459203; val_accuracy: 0.1259952229299363 

The current subspace-distance is: 9.637921721150633e-06 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.11
Batch: 20; loss: 2.26; acc: 0.12
Batch: 40; loss: 2.25; acc: 0.08
Batch: 60; loss: 2.27; acc: 0.09
Batch: 80; loss: 2.27; acc: 0.11
Batch: 100; loss: 2.27; acc: 0.11
Batch: 120; loss: 2.25; acc: 0.12
Batch: 140; loss: 2.26; acc: 0.17
Batch: 160; loss: 2.25; acc: 0.14
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.26; acc: 0.14
Batch: 220; loss: 2.26; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.14
Batch: 260; loss: 2.28; acc: 0.09
Batch: 280; loss: 2.27; acc: 0.06
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.26; acc: 0.12
Batch: 340; loss: 2.28; acc: 0.14
Batch: 360; loss: 2.31; acc: 0.09
Batch: 380; loss: 2.26; acc: 0.09
Batch: 400; loss: 2.25; acc: 0.09
Batch: 420; loss: 2.29; acc: 0.08
Batch: 440; loss: 2.25; acc: 0.05
Batch: 460; loss: 2.27; acc: 0.09
Batch: 480; loss: 2.28; acc: 0.17
Batch: 500; loss: 2.26; acc: 0.2
Batch: 520; loss: 2.28; acc: 0.16
Batch: 540; loss: 2.27; acc: 0.12
Batch: 560; loss: 2.29; acc: 0.17
Batch: 580; loss: 2.25; acc: 0.16
Batch: 600; loss: 2.27; acc: 0.09
Batch: 620; loss: 2.28; acc: 0.06
Batch: 640; loss: 2.26; acc: 0.22
Batch: 660; loss: 2.27; acc: 0.16
Batch: 680; loss: 2.26; acc: 0.14
Batch: 700; loss: 2.23; acc: 0.17
Batch: 720; loss: 2.28; acc: 0.14
Batch: 740; loss: 2.22; acc: 0.17
Batch: 760; loss: 2.25; acc: 0.05
Batch: 780; loss: 2.25; acc: 0.11
Train Epoch over. train_loss: 2.26; train_accuracy: 0.12 

Batch: 0; loss: 2.23; acc: 0.19
Batch: 20; loss: 2.25; acc: 0.09
Batch: 40; loss: 2.22; acc: 0.19
Batch: 60; loss: 2.25; acc: 0.17
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.19; acc: 0.17
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.11
Val Epoch over. val_loss: 2.2532957678387877; val_accuracy: 0.14122213375796178 

The current subspace-distance is: 1.0900162124016788e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.22; acc: 0.14
Batch: 20; loss: 2.26; acc: 0.08
Batch: 40; loss: 2.26; acc: 0.16
Batch: 60; loss: 2.2; acc: 0.28
Batch: 80; loss: 2.27; acc: 0.06
Batch: 100; loss: 2.27; acc: 0.11
Batch: 120; loss: 2.28; acc: 0.14
Batch: 140; loss: 2.23; acc: 0.16
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.25; acc: 0.16
Batch: 200; loss: 2.29; acc: 0.06
Batch: 220; loss: 2.22; acc: 0.09
Batch: 240; loss: 2.27; acc: 0.09
Batch: 260; loss: 2.26; acc: 0.12
Batch: 280; loss: 2.26; acc: 0.17
Batch: 300; loss: 2.24; acc: 0.12
Batch: 320; loss: 2.25; acc: 0.14
Batch: 340; loss: 2.26; acc: 0.17
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.22; acc: 0.23
Batch: 400; loss: 2.27; acc: 0.16
Batch: 420; loss: 2.29; acc: 0.06
Batch: 440; loss: 2.24; acc: 0.19
Batch: 460; loss: 2.25; acc: 0.11
Batch: 480; loss: 2.26; acc: 0.09
Batch: 500; loss: 2.26; acc: 0.09
Batch: 520; loss: 2.26; acc: 0.14
Batch: 540; loss: 2.27; acc: 0.2
Batch: 560; loss: 2.22; acc: 0.12
Batch: 580; loss: 2.23; acc: 0.17
Batch: 600; loss: 2.27; acc: 0.12
Batch: 620; loss: 2.21; acc: 0.2
Batch: 640; loss: 2.25; acc: 0.19
Batch: 660; loss: 2.27; acc: 0.11
Batch: 680; loss: 2.22; acc: 0.17
Batch: 700; loss: 2.29; acc: 0.09
Batch: 720; loss: 2.24; acc: 0.11
Batch: 740; loss: 2.23; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.12
Batch: 780; loss: 2.24; acc: 0.16
Train Epoch over. train_loss: 2.26; train_accuracy: 0.14 

Batch: 0; loss: 2.22; acc: 0.2
Batch: 20; loss: 2.25; acc: 0.11
Batch: 40; loss: 2.2; acc: 0.23
Batch: 60; loss: 2.23; acc: 0.19
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.2; acc: 0.16
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.22; acc: 0.08
Val Epoch over. val_loss: 2.24570795988581; val_accuracy: 0.140625 

The current subspace-distance is: 1.1574707968975417e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 2.31; acc: 0.06
Batch: 20; loss: 2.24; acc: 0.09
Batch: 40; loss: 2.24; acc: 0.09
Batch: 60; loss: 2.26; acc: 0.11
Batch: 80; loss: 2.25; acc: 0.17
Batch: 100; loss: 2.23; acc: 0.17
Batch: 120; loss: 2.26; acc: 0.12
Batch: 140; loss: 2.24; acc: 0.08
Batch: 160; loss: 2.26; acc: 0.11
Batch: 180; loss: 2.34; acc: 0.12
Batch: 200; loss: 2.19; acc: 0.17
Batch: 220; loss: 2.26; acc: 0.16
Batch: 240; loss: 2.18; acc: 0.16
Batch: 260; loss: 2.19; acc: 0.25
Batch: 280; loss: 2.25; acc: 0.12
Batch: 300; loss: 2.27; acc: 0.08
Batch: 320; loss: 2.25; acc: 0.19
Batch: 340; loss: 2.27; acc: 0.11
Batch: 360; loss: 2.25; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.05
Batch: 400; loss: 2.28; acc: 0.16
Batch: 420; loss: 2.25; acc: 0.17
Batch: 440; loss: 2.23; acc: 0.16
Batch: 460; loss: 2.27; acc: 0.12
Batch: 480; loss: 2.33; acc: 0.08
Batch: 500; loss: 2.24; acc: 0.14
Batch: 520; loss: 2.28; acc: 0.14
Batch: 540; loss: 2.24; acc: 0.12
Batch: 560; loss: 2.23; acc: 0.16
Batch: 580; loss: 2.28; acc: 0.12
Batch: 600; loss: 2.23; acc: 0.16
Batch: 620; loss: 2.22; acc: 0.16
Batch: 640; loss: 2.26; acc: 0.09
Batch: 660; loss: 2.31; acc: 0.08
Batch: 680; loss: 2.22; acc: 0.19
Batch: 700; loss: 2.24; acc: 0.17
Batch: 720; loss: 2.24; acc: 0.16
Batch: 740; loss: 2.24; acc: 0.11
Batch: 760; loss: 2.22; acc: 0.14
Batch: 780; loss: 2.32; acc: 0.14
Train Epoch over. train_loss: 2.25; train_accuracy: 0.14 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.26; acc: 0.08
Batch: 40; loss: 2.2; acc: 0.22
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.16
Batch: 100; loss: 2.21; acc: 0.14
Batch: 120; loss: 2.19; acc: 0.14
Batch: 140; loss: 2.22; acc: 0.05
Val Epoch over. val_loss: 2.2418841768981546; val_accuracy: 0.12679140127388536 

The current subspace-distance is: 1.2987939953745808e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 2.28; acc: 0.14
Batch: 20; loss: 2.26; acc: 0.12
Batch: 40; loss: 2.21; acc: 0.11
Batch: 60; loss: 2.19; acc: 0.12
Batch: 80; loss: 2.25; acc: 0.09
Batch: 100; loss: 2.28; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.25; acc: 0.19
Batch: 160; loss: 2.21; acc: 0.17
Batch: 180; loss: 2.19; acc: 0.08
Batch: 200; loss: 2.3; acc: 0.14
Batch: 220; loss: 2.22; acc: 0.2
Batch: 240; loss: 2.26; acc: 0.12
Batch: 260; loss: 2.19; acc: 0.12
Batch: 280; loss: 2.32; acc: 0.12
Batch: 300; loss: 2.26; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.06
Batch: 340; loss: 2.25; acc: 0.11
Batch: 360; loss: 2.24; acc: 0.14
Batch: 380; loss: 2.27; acc: 0.12
Batch: 400; loss: 2.27; acc: 0.09
Batch: 420; loss: 2.27; acc: 0.06
Batch: 440; loss: 2.24; acc: 0.11
Batch: 460; loss: 2.28; acc: 0.22
Batch: 480; loss: 2.24; acc: 0.17
Batch: 500; loss: 2.23; acc: 0.14
Batch: 520; loss: 2.27; acc: 0.08
Batch: 540; loss: 2.26; acc: 0.14
Batch: 560; loss: 2.27; acc: 0.05
Batch: 580; loss: 2.27; acc: 0.19
Batch: 600; loss: 2.28; acc: 0.11
Batch: 620; loss: 2.25; acc: 0.08
Batch: 640; loss: 2.23; acc: 0.17
Batch: 660; loss: 2.21; acc: 0.09
Batch: 680; loss: 2.27; acc: 0.09
Batch: 700; loss: 2.23; acc: 0.12
Batch: 720; loss: 2.3; acc: 0.09
Batch: 740; loss: 2.26; acc: 0.19
Batch: 760; loss: 2.3; acc: 0.12
Batch: 780; loss: 2.2; acc: 0.22
Train Epoch over. train_loss: 2.25; train_accuracy: 0.13 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.26; acc: 0.08
Batch: 40; loss: 2.2; acc: 0.22
Batch: 60; loss: 2.22; acc: 0.12
Batch: 80; loss: 2.3; acc: 0.16
Batch: 100; loss: 2.21; acc: 0.14
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413840354627865; val_accuracy: 0.12320859872611464 

The current subspace-distance is: 1.6775371477706358e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.21; acc: 0.14
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.25; acc: 0.2
Batch: 100; loss: 2.25; acc: 0.11
Batch: 120; loss: 2.25; acc: 0.14
Batch: 140; loss: 2.24; acc: 0.16
Batch: 160; loss: 2.22; acc: 0.16
Batch: 180; loss: 2.25; acc: 0.11
Batch: 200; loss: 2.28; acc: 0.12
Batch: 220; loss: 2.29; acc: 0.11
Batch: 240; loss: 2.25; acc: 0.08
Batch: 260; loss: 2.26; acc: 0.08
Batch: 280; loss: 2.22; acc: 0.28
Batch: 300; loss: 2.26; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.17
Batch: 340; loss: 2.26; acc: 0.05
Batch: 360; loss: 2.24; acc: 0.19
Batch: 380; loss: 2.26; acc: 0.19
Batch: 400; loss: 2.25; acc: 0.06
Batch: 420; loss: 2.27; acc: 0.09
Batch: 440; loss: 2.25; acc: 0.11
Batch: 460; loss: 2.28; acc: 0.09
Batch: 480; loss: 2.31; acc: 0.14
Batch: 500; loss: 2.26; acc: 0.08
Batch: 520; loss: 2.27; acc: 0.09
Batch: 540; loss: 2.24; acc: 0.14
Batch: 560; loss: 2.17; acc: 0.19
Batch: 580; loss: 2.19; acc: 0.23
Batch: 600; loss: 2.28; acc: 0.12
Batch: 620; loss: 2.24; acc: 0.16
Batch: 640; loss: 2.22; acc: 0.14
Batch: 660; loss: 2.23; acc: 0.09
Batch: 680; loss: 2.21; acc: 0.09
Batch: 700; loss: 2.26; acc: 0.17
Batch: 720; loss: 2.26; acc: 0.09
Batch: 740; loss: 2.3; acc: 0.08
Batch: 760; loss: 2.23; acc: 0.11
Batch: 780; loss: 2.25; acc: 0.12
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.22
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.21; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241214343696643; val_accuracy: 0.12151671974522293 

The current subspace-distance is: 1.646619421080686e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 2.3; acc: 0.14
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.32; acc: 0.16
Batch: 60; loss: 2.22; acc: 0.19
Batch: 80; loss: 2.24; acc: 0.11
Batch: 100; loss: 2.25; acc: 0.08
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.22; acc: 0.17
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.27; acc: 0.06
Batch: 200; loss: 2.26; acc: 0.09
Batch: 220; loss: 2.33; acc: 0.03
Batch: 240; loss: 2.27; acc: 0.08
Batch: 260; loss: 2.24; acc: 0.09
Batch: 280; loss: 2.28; acc: 0.11
Batch: 300; loss: 2.25; acc: 0.12
Batch: 320; loss: 2.23; acc: 0.12
Batch: 340; loss: 2.2; acc: 0.12
Batch: 360; loss: 2.29; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.11
Batch: 400; loss: 2.23; acc: 0.08
Batch: 420; loss: 2.3; acc: 0.19
Batch: 440; loss: 2.28; acc: 0.09
Batch: 460; loss: 2.25; acc: 0.16
Batch: 480; loss: 2.27; acc: 0.09
Batch: 500; loss: 2.25; acc: 0.12
Batch: 520; loss: 2.23; acc: 0.12
Batch: 540; loss: 2.26; acc: 0.19
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.31; acc: 0.08
Batch: 600; loss: 2.32; acc: 0.06
Batch: 620; loss: 2.25; acc: 0.09
Batch: 640; loss: 2.28; acc: 0.08
Batch: 660; loss: 2.22; acc: 0.06
Batch: 680; loss: 2.26; acc: 0.09
Batch: 700; loss: 2.22; acc: 0.12
Batch: 720; loss: 2.23; acc: 0.14
Batch: 740; loss: 2.23; acc: 0.17
Batch: 760; loss: 2.26; acc: 0.08
Batch: 780; loss: 2.32; acc: 0.11
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241244650190803; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 1.942523340403568e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 2.26; acc: 0.09
Batch: 20; loss: 2.25; acc: 0.09
Batch: 40; loss: 2.21; acc: 0.14
Batch: 60; loss: 2.22; acc: 0.17
Batch: 80; loss: 2.23; acc: 0.06
Batch: 100; loss: 2.22; acc: 0.08
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.25; acc: 0.16
Batch: 160; loss: 2.23; acc: 0.12
Batch: 180; loss: 2.2; acc: 0.16
Batch: 200; loss: 2.2; acc: 0.14
Batch: 220; loss: 2.23; acc: 0.12
Batch: 240; loss: 2.25; acc: 0.2
Batch: 260; loss: 2.28; acc: 0.2
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.27; acc: 0.11
Batch: 320; loss: 2.21; acc: 0.16
Batch: 340; loss: 2.3; acc: 0.09
Batch: 360; loss: 2.26; acc: 0.09
Batch: 380; loss: 2.2; acc: 0.16
Batch: 400; loss: 2.36; acc: 0.06
Batch: 420; loss: 2.21; acc: 0.12
Batch: 440; loss: 2.34; acc: 0.08
Batch: 460; loss: 2.26; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.08
Batch: 500; loss: 2.27; acc: 0.16
Batch: 520; loss: 2.2; acc: 0.11
Batch: 540; loss: 2.25; acc: 0.09
Batch: 560; loss: 2.24; acc: 0.11
Batch: 580; loss: 2.28; acc: 0.11
Batch: 600; loss: 2.23; acc: 0.12
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.23; acc: 0.09
Batch: 660; loss: 2.24; acc: 0.14
Batch: 680; loss: 2.29; acc: 0.09
Batch: 700; loss: 2.18; acc: 0.06
Batch: 720; loss: 2.27; acc: 0.06
Batch: 740; loss: 2.32; acc: 0.12
Batch: 760; loss: 2.28; acc: 0.19
Batch: 780; loss: 2.24; acc: 0.09
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.16
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2412631223156194; val_accuracy: 0.12191480891719746 

The current subspace-distance is: 2.084760672005359e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 2.26; acc: 0.12
Batch: 20; loss: 2.24; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.08
Batch: 60; loss: 2.26; acc: 0.14
Batch: 80; loss: 2.25; acc: 0.05
Batch: 100; loss: 2.21; acc: 0.09
Batch: 120; loss: 2.26; acc: 0.09
Batch: 140; loss: 2.27; acc: 0.11
Batch: 160; loss: 2.24; acc: 0.14
Batch: 180; loss: 2.14; acc: 0.2
Batch: 200; loss: 2.27; acc: 0.14
Batch: 220; loss: 2.23; acc: 0.08
Batch: 240; loss: 2.29; acc: 0.08
Batch: 260; loss: 2.26; acc: 0.09
Batch: 280; loss: 2.22; acc: 0.14
Batch: 300; loss: 2.24; acc: 0.09
Batch: 320; loss: 2.24; acc: 0.12
Batch: 340; loss: 2.22; acc: 0.19
Batch: 360; loss: 2.24; acc: 0.11
Batch: 380; loss: 2.22; acc: 0.12
Batch: 400; loss: 2.21; acc: 0.14
Batch: 420; loss: 2.32; acc: 0.14
Batch: 440; loss: 2.28; acc: 0.14
Batch: 460; loss: 2.3; acc: 0.12
Batch: 480; loss: 2.22; acc: 0.14
Batch: 500; loss: 2.22; acc: 0.19
Batch: 520; loss: 2.28; acc: 0.11
Batch: 540; loss: 2.24; acc: 0.11
Batch: 560; loss: 2.22; acc: 0.11
Batch: 580; loss: 2.21; acc: 0.08
Batch: 600; loss: 2.23; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.05
Batch: 640; loss: 2.29; acc: 0.11
Batch: 660; loss: 2.31; acc: 0.09
Batch: 680; loss: 2.21; acc: 0.11
Batch: 700; loss: 2.26; acc: 0.11
Batch: 720; loss: 2.25; acc: 0.08
Batch: 740; loss: 2.27; acc: 0.06
Batch: 760; loss: 2.3; acc: 0.11
Batch: 780; loss: 2.24; acc: 0.12
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.21; acc: 0.09
Batch: 120; loss: 2.2; acc: 0.14
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241263629524571; val_accuracy: 0.11972531847133758 

The current subspace-distance is: 2.249202771054115e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 2.25; acc: 0.08
Batch: 20; loss: 2.24; acc: 0.19
Batch: 40; loss: 2.29; acc: 0.12
Batch: 60; loss: 2.23; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.06
Batch: 100; loss: 2.25; acc: 0.11
Batch: 120; loss: 2.24; acc: 0.12
Batch: 140; loss: 2.25; acc: 0.16
Batch: 160; loss: 2.32; acc: 0.05
Batch: 180; loss: 2.16; acc: 0.16
Batch: 200; loss: 2.23; acc: 0.06
Batch: 220; loss: 2.21; acc: 0.17
Batch: 240; loss: 2.28; acc: 0.16
Batch: 260; loss: 2.21; acc: 0.09
Batch: 280; loss: 2.25; acc: 0.08
Batch: 300; loss: 2.27; acc: 0.11
Batch: 320; loss: 2.24; acc: 0.16
Batch: 340; loss: 2.27; acc: 0.09
Batch: 360; loss: 2.25; acc: 0.14
Batch: 380; loss: 2.26; acc: 0.16
Batch: 400; loss: 2.18; acc: 0.14
Batch: 420; loss: 2.22; acc: 0.12
Batch: 440; loss: 2.24; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.09
Batch: 480; loss: 2.24; acc: 0.08
Batch: 500; loss: 2.3; acc: 0.09
Batch: 520; loss: 2.3; acc: 0.12
Batch: 540; loss: 2.26; acc: 0.05
Batch: 560; loss: 2.19; acc: 0.11
Batch: 580; loss: 2.29; acc: 0.11
Batch: 600; loss: 2.24; acc: 0.17
Batch: 620; loss: 2.22; acc: 0.14
Batch: 640; loss: 2.26; acc: 0.14
Batch: 660; loss: 2.18; acc: 0.09
Batch: 680; loss: 2.23; acc: 0.14
Batch: 700; loss: 2.18; acc: 0.17
Batch: 720; loss: 2.26; acc: 0.06
Batch: 740; loss: 2.29; acc: 0.11
Batch: 760; loss: 2.2; acc: 0.11
Batch: 780; loss: 2.33; acc: 0.06
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413640341181664; val_accuracy: 0.11972531847133758 

The current subspace-distance is: 1.914368476718664e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 2.26; acc: 0.08
Batch: 20; loss: 2.25; acc: 0.09
Batch: 40; loss: 2.24; acc: 0.19
Batch: 60; loss: 2.27; acc: 0.09
Batch: 80; loss: 2.27; acc: 0.06
Batch: 100; loss: 2.24; acc: 0.12
Batch: 120; loss: 2.25; acc: 0.19
Batch: 140; loss: 2.17; acc: 0.12
Batch: 160; loss: 2.27; acc: 0.08
Batch: 180; loss: 2.25; acc: 0.17
Batch: 200; loss: 2.28; acc: 0.09
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.25; acc: 0.16
Batch: 260; loss: 2.27; acc: 0.14
Batch: 280; loss: 2.22; acc: 0.19
Batch: 300; loss: 2.29; acc: 0.14
Batch: 320; loss: 2.2; acc: 0.14
Batch: 340; loss: 2.28; acc: 0.11
Batch: 360; loss: 2.18; acc: 0.16
Batch: 380; loss: 2.28; acc: 0.16
Batch: 400; loss: 2.26; acc: 0.08
Batch: 420; loss: 2.27; acc: 0.11
Batch: 440; loss: 2.25; acc: 0.12
Batch: 460; loss: 2.26; acc: 0.09
Batch: 480; loss: 2.18; acc: 0.16
Batch: 500; loss: 2.22; acc: 0.16
Batch: 520; loss: 2.29; acc: 0.16
Batch: 540; loss: 2.23; acc: 0.14
Batch: 560; loss: 2.28; acc: 0.09
Batch: 580; loss: 2.3; acc: 0.11
Batch: 600; loss: 2.23; acc: 0.16
Batch: 620; loss: 2.24; acc: 0.17
Batch: 640; loss: 2.23; acc: 0.12
Batch: 660; loss: 2.21; acc: 0.08
Batch: 680; loss: 2.24; acc: 0.12
Batch: 700; loss: 2.21; acc: 0.11
Batch: 720; loss: 2.22; acc: 0.12
Batch: 740; loss: 2.2; acc: 0.23
Batch: 760; loss: 2.19; acc: 0.16
Batch: 780; loss: 2.25; acc: 0.12
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241345490619635; val_accuracy: 0.11922770700636942 

The current subspace-distance is: 2.063219108094927e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.19
Batch: 20; loss: 2.29; acc: 0.08
Batch: 40; loss: 2.24; acc: 0.22
Batch: 60; loss: 2.24; acc: 0.09
Batch: 80; loss: 2.32; acc: 0.11
Batch: 100; loss: 2.24; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.27; acc: 0.14
Batch: 160; loss: 2.25; acc: 0.06
Batch: 180; loss: 2.19; acc: 0.2
Batch: 200; loss: 2.23; acc: 0.06
Batch: 220; loss: 2.17; acc: 0.17
Batch: 240; loss: 2.26; acc: 0.11
Batch: 260; loss: 2.19; acc: 0.14
Batch: 280; loss: 2.24; acc: 0.14
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.16
Batch: 340; loss: 2.28; acc: 0.11
Batch: 360; loss: 2.31; acc: 0.02
Batch: 380; loss: 2.2; acc: 0.14
Batch: 400; loss: 2.26; acc: 0.05
Batch: 420; loss: 2.28; acc: 0.12
Batch: 440; loss: 2.3; acc: 0.16
Batch: 460; loss: 2.26; acc: 0.12
Batch: 480; loss: 2.31; acc: 0.11
Batch: 500; loss: 2.22; acc: 0.11
Batch: 520; loss: 2.16; acc: 0.2
Batch: 540; loss: 2.15; acc: 0.22
Batch: 560; loss: 2.23; acc: 0.16
Batch: 580; loss: 2.22; acc: 0.2
Batch: 600; loss: 2.28; acc: 0.12
Batch: 620; loss: 2.27; acc: 0.12
Batch: 640; loss: 2.24; acc: 0.17
Batch: 660; loss: 2.22; acc: 0.2
Batch: 680; loss: 2.26; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.12
Batch: 720; loss: 2.21; acc: 0.16
Batch: 740; loss: 2.21; acc: 0.11
Batch: 760; loss: 2.22; acc: 0.12
Batch: 780; loss: 2.23; acc: 0.06
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.16
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241293336175809; val_accuracy: 0.11992436305732485 

The current subspace-distance is: 2.0911757019348443e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 2.29; acc: 0.08
Batch: 20; loss: 2.25; acc: 0.12
Batch: 40; loss: 2.25; acc: 0.09
Batch: 60; loss: 2.27; acc: 0.08
Batch: 80; loss: 2.33; acc: 0.06
Batch: 100; loss: 2.24; acc: 0.08
Batch: 120; loss: 2.24; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.05
Batch: 160; loss: 2.28; acc: 0.11
Batch: 180; loss: 2.26; acc: 0.16
Batch: 200; loss: 2.23; acc: 0.17
Batch: 220; loss: 2.28; acc: 0.12
Batch: 240; loss: 2.24; acc: 0.19
Batch: 260; loss: 2.28; acc: 0.11
Batch: 280; loss: 2.27; acc: 0.09
Batch: 300; loss: 2.23; acc: 0.09
Batch: 320; loss: 2.21; acc: 0.11
Batch: 340; loss: 2.19; acc: 0.16
Batch: 360; loss: 2.31; acc: 0.09
Batch: 380; loss: 2.29; acc: 0.16
Batch: 400; loss: 2.21; acc: 0.12
Batch: 420; loss: 2.26; acc: 0.09
Batch: 440; loss: 2.27; acc: 0.09
Batch: 460; loss: 2.22; acc: 0.09
Batch: 480; loss: 2.21; acc: 0.12
Batch: 500; loss: 2.27; acc: 0.14
Batch: 520; loss: 2.31; acc: 0.12
Batch: 540; loss: 2.27; acc: 0.05
Batch: 560; loss: 2.2; acc: 0.16
Batch: 580; loss: 2.24; acc: 0.19
Batch: 600; loss: 2.27; acc: 0.05
Batch: 620; loss: 2.27; acc: 0.17
Batch: 640; loss: 2.25; acc: 0.08
Batch: 660; loss: 2.24; acc: 0.14
Batch: 680; loss: 2.27; acc: 0.12
Batch: 700; loss: 2.22; acc: 0.12
Batch: 720; loss: 2.23; acc: 0.14
Batch: 740; loss: 2.21; acc: 0.09
Batch: 760; loss: 2.22; acc: 0.05
Batch: 780; loss: 2.19; acc: 0.12
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.16
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2412712771421783; val_accuracy: 0.12131767515923567 

The current subspace-distance is: 2.0586441678460687e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 2.23; acc: 0.11
Batch: 20; loss: 2.25; acc: 0.08
Batch: 40; loss: 2.18; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.16
Batch: 80; loss: 2.16; acc: 0.19
Batch: 100; loss: 2.23; acc: 0.09
Batch: 120; loss: 2.23; acc: 0.16
Batch: 140; loss: 2.26; acc: 0.14
Batch: 160; loss: 2.24; acc: 0.14
Batch: 180; loss: 2.28; acc: 0.09
Batch: 200; loss: 2.21; acc: 0.23
Batch: 220; loss: 2.25; acc: 0.12
Batch: 240; loss: 2.26; acc: 0.14
Batch: 260; loss: 2.2; acc: 0.2
Batch: 280; loss: 2.2; acc: 0.16
Batch: 300; loss: 2.25; acc: 0.09
Batch: 320; loss: 2.27; acc: 0.2
Batch: 340; loss: 2.23; acc: 0.14
Batch: 360; loss: 2.17; acc: 0.12
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.16; acc: 0.17
Batch: 420; loss: 2.3; acc: 0.08
Batch: 440; loss: 2.22; acc: 0.2
Batch: 460; loss: 2.32; acc: 0.12
Batch: 480; loss: 2.26; acc: 0.12
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.23; acc: 0.11
Batch: 540; loss: 2.23; acc: 0.12
Batch: 560; loss: 2.22; acc: 0.08
Batch: 580; loss: 2.23; acc: 0.11
Batch: 600; loss: 2.24; acc: 0.14
Batch: 620; loss: 2.25; acc: 0.12
Batch: 640; loss: 2.25; acc: 0.12
Batch: 660; loss: 2.24; acc: 0.17
Batch: 680; loss: 2.18; acc: 0.16
Batch: 700; loss: 2.19; acc: 0.19
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.29; acc: 0.05
Batch: 760; loss: 2.26; acc: 0.12
Batch: 780; loss: 2.25; acc: 0.16
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.03
Val Epoch over. val_loss: 2.2413980824172874; val_accuracy: 0.11863057324840764 

The current subspace-distance is: 2.5252511477447115e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.25; acc: 0.12
Batch: 20; loss: 2.25; acc: 0.08
Batch: 40; loss: 2.26; acc: 0.16
Batch: 60; loss: 2.23; acc: 0.11
Batch: 80; loss: 2.27; acc: 0.06
Batch: 100; loss: 2.31; acc: 0.11
Batch: 120; loss: 2.21; acc: 0.14
Batch: 140; loss: 2.24; acc: 0.16
Batch: 160; loss: 2.3; acc: 0.14
Batch: 180; loss: 2.26; acc: 0.12
Batch: 200; loss: 2.29; acc: 0.08
Batch: 220; loss: 2.21; acc: 0.12
Batch: 240; loss: 2.26; acc: 0.11
Batch: 260; loss: 2.22; acc: 0.23
Batch: 280; loss: 2.29; acc: 0.08
Batch: 300; loss: 2.25; acc: 0.11
Batch: 320; loss: 2.24; acc: 0.05
Batch: 340; loss: 2.21; acc: 0.12
Batch: 360; loss: 2.2; acc: 0.2
Batch: 380; loss: 2.26; acc: 0.09
Batch: 400; loss: 2.28; acc: 0.14
Batch: 420; loss: 2.25; acc: 0.14
Batch: 440; loss: 2.24; acc: 0.11
Batch: 460; loss: 2.26; acc: 0.05
Batch: 480; loss: 2.21; acc: 0.14
Batch: 500; loss: 2.29; acc: 0.06
Batch: 520; loss: 2.22; acc: 0.17
Batch: 540; loss: 2.24; acc: 0.06
Batch: 560; loss: 2.22; acc: 0.17
Batch: 580; loss: 2.19; acc: 0.14
Batch: 600; loss: 2.26; acc: 0.14
Batch: 620; loss: 2.26; acc: 0.06
Batch: 640; loss: 2.28; acc: 0.11
Batch: 660; loss: 2.26; acc: 0.06
Batch: 680; loss: 2.24; acc: 0.06
Batch: 700; loss: 2.27; acc: 0.14
Batch: 720; loss: 2.2; acc: 0.08
Batch: 740; loss: 2.27; acc: 0.17
Batch: 760; loss: 2.25; acc: 0.14
Batch: 780; loss: 2.2; acc: 0.11
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413632125611516; val_accuracy: 0.11962579617834394 

The current subspace-distance is: 2.808951830957085e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.23; acc: 0.09
Batch: 20; loss: 2.25; acc: 0.09
Batch: 40; loss: 2.28; acc: 0.06
Batch: 60; loss: 2.22; acc: 0.09
Batch: 80; loss: 2.19; acc: 0.19
Batch: 100; loss: 2.19; acc: 0.2
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.29; acc: 0.08
Batch: 160; loss: 2.27; acc: 0.09
Batch: 180; loss: 2.21; acc: 0.09
Batch: 200; loss: 2.23; acc: 0.14
Batch: 220; loss: 2.23; acc: 0.09
Batch: 240; loss: 2.26; acc: 0.08
Batch: 260; loss: 2.22; acc: 0.12
Batch: 280; loss: 2.32; acc: 0.06
Batch: 300; loss: 2.18; acc: 0.19
Batch: 320; loss: 2.19; acc: 0.17
Batch: 340; loss: 2.21; acc: 0.12
Batch: 360; loss: 2.28; acc: 0.08
Batch: 380; loss: 2.24; acc: 0.14
Batch: 400; loss: 2.24; acc: 0.16
Batch: 420; loss: 2.22; acc: 0.11
Batch: 440; loss: 2.22; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.12
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.16; acc: 0.16
Batch: 520; loss: 2.29; acc: 0.09
Batch: 540; loss: 2.23; acc: 0.14
Batch: 560; loss: 2.22; acc: 0.08
Batch: 580; loss: 2.29; acc: 0.11
Batch: 600; loss: 2.25; acc: 0.09
Batch: 620; loss: 2.2; acc: 0.14
Batch: 640; loss: 2.28; acc: 0.14
Batch: 660; loss: 2.24; acc: 0.11
Batch: 680; loss: 2.26; acc: 0.08
Batch: 700; loss: 2.27; acc: 0.11
Batch: 720; loss: 2.24; acc: 0.14
Batch: 740; loss: 2.21; acc: 0.12
Batch: 760; loss: 2.26; acc: 0.09
Batch: 780; loss: 2.32; acc: 0.09
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241331729159993; val_accuracy: 0.11962579617834394 

The current subspace-distance is: 2.5592469683033414e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.27; acc: 0.08
Batch: 20; loss: 2.26; acc: 0.12
Batch: 40; loss: 2.23; acc: 0.14
Batch: 60; loss: 2.23; acc: 0.11
Batch: 80; loss: 2.28; acc: 0.12
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.22; acc: 0.05
Batch: 160; loss: 2.28; acc: 0.11
Batch: 180; loss: 2.23; acc: 0.08
Batch: 200; loss: 2.22; acc: 0.16
Batch: 220; loss: 2.27; acc: 0.08
Batch: 240; loss: 2.23; acc: 0.14
Batch: 260; loss: 2.24; acc: 0.11
Batch: 280; loss: 2.33; acc: 0.12
Batch: 300; loss: 2.22; acc: 0.09
Batch: 320; loss: 2.25; acc: 0.2
Batch: 340; loss: 2.22; acc: 0.11
Batch: 360; loss: 2.25; acc: 0.19
Batch: 380; loss: 2.31; acc: 0.03
Batch: 400; loss: 2.23; acc: 0.16
Batch: 420; loss: 2.19; acc: 0.16
Batch: 440; loss: 2.28; acc: 0.14
Batch: 460; loss: 2.25; acc: 0.16
Batch: 480; loss: 2.2; acc: 0.11
Batch: 500; loss: 2.23; acc: 0.17
Batch: 520; loss: 2.23; acc: 0.19
Batch: 540; loss: 2.29; acc: 0.06
Batch: 560; loss: 2.18; acc: 0.19
Batch: 580; loss: 2.26; acc: 0.08
Batch: 600; loss: 2.24; acc: 0.09
Batch: 620; loss: 2.32; acc: 0.14
Batch: 640; loss: 2.33; acc: 0.08
Batch: 660; loss: 2.2; acc: 0.17
Batch: 680; loss: 2.24; acc: 0.12
Batch: 700; loss: 2.21; acc: 0.23
Batch: 720; loss: 2.22; acc: 0.22
Batch: 740; loss: 2.28; acc: 0.08
Batch: 760; loss: 2.32; acc: 0.12
Batch: 780; loss: 2.28; acc: 0.09
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.22; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.21; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2412979921717553; val_accuracy: 0.1201234076433121 

The current subspace-distance is: 2.2530748537974432e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.22; acc: 0.11
Batch: 20; loss: 2.27; acc: 0.06
Batch: 40; loss: 2.27; acc: 0.09
Batch: 60; loss: 2.29; acc: 0.06
Batch: 80; loss: 2.23; acc: 0.12
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.2; acc: 0.14
Batch: 160; loss: 2.22; acc: 0.08
Batch: 180; loss: 2.28; acc: 0.08
Batch: 200; loss: 2.31; acc: 0.05
Batch: 220; loss: 2.28; acc: 0.11
Batch: 240; loss: 2.27; acc: 0.19
Batch: 260; loss: 2.28; acc: 0.14
Batch: 280; loss: 2.25; acc: 0.14
Batch: 300; loss: 2.24; acc: 0.12
Batch: 320; loss: 2.3; acc: 0.11
Batch: 340; loss: 2.26; acc: 0.16
Batch: 360; loss: 2.24; acc: 0.16
Batch: 380; loss: 2.19; acc: 0.19
Batch: 400; loss: 2.24; acc: 0.16
Batch: 420; loss: 2.2; acc: 0.17
Batch: 440; loss: 2.28; acc: 0.11
Batch: 460; loss: 2.29; acc: 0.09
Batch: 480; loss: 2.28; acc: 0.08
Batch: 500; loss: 2.26; acc: 0.17
Batch: 520; loss: 2.29; acc: 0.08
Batch: 540; loss: 2.26; acc: 0.08
Batch: 560; loss: 2.19; acc: 0.14
Batch: 580; loss: 2.17; acc: 0.22
Batch: 600; loss: 2.28; acc: 0.14
Batch: 620; loss: 2.2; acc: 0.16
Batch: 640; loss: 2.24; acc: 0.09
Batch: 660; loss: 2.23; acc: 0.09
Batch: 680; loss: 2.22; acc: 0.14
Batch: 700; loss: 2.21; acc: 0.22
Batch: 720; loss: 2.22; acc: 0.12
Batch: 740; loss: 2.3; acc: 0.16
Batch: 760; loss: 2.27; acc: 0.12
Batch: 780; loss: 2.27; acc: 0.09
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.21; acc: 0.09
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413105524269636; val_accuracy: 0.11992436305732485 

The current subspace-distance is: 2.1432819266919978e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.25; acc: 0.09
Batch: 20; loss: 2.2; acc: 0.11
Batch: 40; loss: 2.26; acc: 0.06
Batch: 60; loss: 2.24; acc: 0.11
Batch: 80; loss: 2.23; acc: 0.14
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.23; acc: 0.12
Batch: 140; loss: 2.22; acc: 0.16
Batch: 160; loss: 2.23; acc: 0.16
Batch: 180; loss: 2.23; acc: 0.19
Batch: 200; loss: 2.18; acc: 0.17
Batch: 220; loss: 2.3; acc: 0.08
Batch: 240; loss: 2.22; acc: 0.17
Batch: 260; loss: 2.26; acc: 0.14
Batch: 280; loss: 2.29; acc: 0.09
Batch: 300; loss: 2.22; acc: 0.09
Batch: 320; loss: 2.28; acc: 0.11
Batch: 340; loss: 2.23; acc: 0.16
Batch: 360; loss: 2.24; acc: 0.17
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.31; acc: 0.05
Batch: 420; loss: 2.25; acc: 0.05
Batch: 440; loss: 2.27; acc: 0.12
Batch: 460; loss: 2.26; acc: 0.14
Batch: 480; loss: 2.31; acc: 0.02
Batch: 500; loss: 2.23; acc: 0.14
Batch: 520; loss: 2.16; acc: 0.17
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.2; acc: 0.12
Batch: 580; loss: 2.22; acc: 0.14
Batch: 600; loss: 2.28; acc: 0.14
Batch: 620; loss: 2.22; acc: 0.14
Batch: 640; loss: 2.23; acc: 0.06
Batch: 660; loss: 2.23; acc: 0.19
Batch: 680; loss: 2.27; acc: 0.2
Batch: 700; loss: 2.25; acc: 0.09
Batch: 720; loss: 2.27; acc: 0.09
Batch: 740; loss: 2.24; acc: 0.22
Batch: 760; loss: 2.22; acc: 0.19
Batch: 780; loss: 2.25; acc: 0.14
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.24134551491707; val_accuracy: 0.12032245222929937 

The current subspace-distance is: 2.241109177703038e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.31; acc: 0.05
Batch: 20; loss: 2.23; acc: 0.12
Batch: 40; loss: 2.25; acc: 0.11
Batch: 60; loss: 2.26; acc: 0.12
Batch: 80; loss: 2.27; acc: 0.12
Batch: 100; loss: 2.26; acc: 0.17
Batch: 120; loss: 2.19; acc: 0.14
Batch: 140; loss: 2.25; acc: 0.14
Batch: 160; loss: 2.26; acc: 0.11
Batch: 180; loss: 2.23; acc: 0.05
Batch: 200; loss: 2.25; acc: 0.12
Batch: 220; loss: 2.23; acc: 0.16
Batch: 240; loss: 2.28; acc: 0.11
Batch: 260; loss: 2.24; acc: 0.14
Batch: 280; loss: 2.23; acc: 0.17
Batch: 300; loss: 2.27; acc: 0.11
Batch: 320; loss: 2.28; acc: 0.17
Batch: 340; loss: 2.25; acc: 0.14
Batch: 360; loss: 2.28; acc: 0.12
Batch: 380; loss: 2.22; acc: 0.03
Batch: 400; loss: 2.23; acc: 0.09
Batch: 420; loss: 2.27; acc: 0.16
Batch: 440; loss: 2.23; acc: 0.08
Batch: 460; loss: 2.24; acc: 0.08
Batch: 480; loss: 2.3; acc: 0.03
Batch: 500; loss: 2.24; acc: 0.09
Batch: 520; loss: 2.23; acc: 0.06
Batch: 540; loss: 2.31; acc: 0.05
Batch: 560; loss: 2.22; acc: 0.17
Batch: 580; loss: 2.2; acc: 0.11
Batch: 600; loss: 2.25; acc: 0.08
Batch: 620; loss: 2.33; acc: 0.09
Batch: 640; loss: 2.28; acc: 0.09
Batch: 660; loss: 2.24; acc: 0.08
Batch: 680; loss: 2.2; acc: 0.2
Batch: 700; loss: 2.3; acc: 0.11
Batch: 720; loss: 2.22; acc: 0.11
Batch: 740; loss: 2.27; acc: 0.16
Batch: 760; loss: 2.27; acc: 0.09
Batch: 780; loss: 2.31; acc: 0.09
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241325287302588; val_accuracy: 0.12002388535031847 

The current subspace-distance is: 2.2708625692757778e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.26; acc: 0.16
Batch: 20; loss: 2.19; acc: 0.2
Batch: 40; loss: 2.21; acc: 0.11
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.24; acc: 0.09
Batch: 100; loss: 2.22; acc: 0.17
Batch: 120; loss: 2.24; acc: 0.09
Batch: 140; loss: 2.26; acc: 0.11
Batch: 160; loss: 2.22; acc: 0.08
Batch: 180; loss: 2.25; acc: 0.12
Batch: 200; loss: 2.25; acc: 0.16
Batch: 220; loss: 2.28; acc: 0.06
Batch: 240; loss: 2.28; acc: 0.03
Batch: 260; loss: 2.31; acc: 0.08
Batch: 280; loss: 2.22; acc: 0.12
Batch: 300; loss: 2.22; acc: 0.14
Batch: 320; loss: 2.26; acc: 0.17
Batch: 340; loss: 2.27; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.12
Batch: 380; loss: 2.28; acc: 0.12
Batch: 400; loss: 2.28; acc: 0.09
Batch: 420; loss: 2.24; acc: 0.12
Batch: 440; loss: 2.19; acc: 0.11
Batch: 460; loss: 2.34; acc: 0.09
Batch: 480; loss: 2.22; acc: 0.25
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.27; acc: 0.12
Batch: 540; loss: 2.16; acc: 0.22
Batch: 560; loss: 2.3; acc: 0.12
Batch: 580; loss: 2.26; acc: 0.09
Batch: 600; loss: 2.25; acc: 0.05
Batch: 620; loss: 2.24; acc: 0.17
Batch: 640; loss: 2.18; acc: 0.22
Batch: 660; loss: 2.18; acc: 0.11
Batch: 680; loss: 2.25; acc: 0.12
Batch: 700; loss: 2.24; acc: 0.14
Batch: 720; loss: 2.29; acc: 0.09
Batch: 740; loss: 2.26; acc: 0.08
Batch: 760; loss: 2.24; acc: 0.19
Batch: 780; loss: 2.3; acc: 0.08
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241336693429643; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 2.4553250113967806e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.21; acc: 0.11
Batch: 20; loss: 2.24; acc: 0.08
Batch: 40; loss: 2.22; acc: 0.11
Batch: 60; loss: 2.32; acc: 0.08
Batch: 80; loss: 2.26; acc: 0.06
Batch: 100; loss: 2.23; acc: 0.11
Batch: 120; loss: 2.28; acc: 0.09
Batch: 140; loss: 2.22; acc: 0.16
Batch: 160; loss: 2.32; acc: 0.06
Batch: 180; loss: 2.25; acc: 0.19
Batch: 200; loss: 2.22; acc: 0.17
Batch: 220; loss: 2.21; acc: 0.2
Batch: 240; loss: 2.22; acc: 0.17
Batch: 260; loss: 2.32; acc: 0.08
Batch: 280; loss: 2.22; acc: 0.19
Batch: 300; loss: 2.18; acc: 0.12
Batch: 320; loss: 2.22; acc: 0.2
Batch: 340; loss: 2.29; acc: 0.09
Batch: 360; loss: 2.32; acc: 0.08
Batch: 380; loss: 2.27; acc: 0.09
Batch: 400; loss: 2.23; acc: 0.12
Batch: 420; loss: 2.24; acc: 0.16
Batch: 440; loss: 2.22; acc: 0.09
Batch: 460; loss: 2.26; acc: 0.14
Batch: 480; loss: 2.22; acc: 0.12
Batch: 500; loss: 2.16; acc: 0.2
Batch: 520; loss: 2.25; acc: 0.12
Batch: 540; loss: 2.2; acc: 0.14
Batch: 560; loss: 2.23; acc: 0.11
Batch: 580; loss: 2.27; acc: 0.08
Batch: 600; loss: 2.25; acc: 0.06
Batch: 620; loss: 2.2; acc: 0.14
Batch: 640; loss: 2.27; acc: 0.16
Batch: 660; loss: 2.23; acc: 0.17
Batch: 680; loss: 2.24; acc: 0.12
Batch: 700; loss: 2.19; acc: 0.12
Batch: 720; loss: 2.18; acc: 0.2
Batch: 740; loss: 2.26; acc: 0.14
Batch: 760; loss: 2.22; acc: 0.12
Batch: 780; loss: 2.28; acc: 0.16
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413299645587896; val_accuracy: 0.12072054140127389 

The current subspace-distance is: 2.666905493242666e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.24; acc: 0.14
Batch: 20; loss: 2.21; acc: 0.08
Batch: 40; loss: 2.19; acc: 0.12
Batch: 60; loss: 2.2; acc: 0.25
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.16
Batch: 120; loss: 2.24; acc: 0.12
Batch: 140; loss: 2.23; acc: 0.08
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.23; acc: 0.14
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.29; acc: 0.11
Batch: 240; loss: 2.26; acc: 0.09
Batch: 260; loss: 2.2; acc: 0.16
Batch: 280; loss: 2.21; acc: 0.14
Batch: 300; loss: 2.31; acc: 0.09
Batch: 320; loss: 2.3; acc: 0.12
Batch: 340; loss: 2.22; acc: 0.06
Batch: 360; loss: 2.23; acc: 0.11
Batch: 380; loss: 2.21; acc: 0.14
Batch: 400; loss: 2.18; acc: 0.19
Batch: 420; loss: 2.25; acc: 0.08
Batch: 440; loss: 2.26; acc: 0.16
Batch: 460; loss: 2.19; acc: 0.19
Batch: 480; loss: 2.23; acc: 0.23
Batch: 500; loss: 2.26; acc: 0.09
Batch: 520; loss: 2.28; acc: 0.12
Batch: 540; loss: 2.25; acc: 0.08
Batch: 560; loss: 2.24; acc: 0.22
Batch: 580; loss: 2.28; acc: 0.11
Batch: 600; loss: 2.23; acc: 0.16
Batch: 620; loss: 2.18; acc: 0.16
Batch: 640; loss: 2.32; acc: 0.11
Batch: 660; loss: 2.29; acc: 0.14
Batch: 680; loss: 2.27; acc: 0.14
Batch: 700; loss: 2.25; acc: 0.16
Batch: 720; loss: 2.2; acc: 0.16
Batch: 740; loss: 2.28; acc: 0.17
Batch: 760; loss: 2.3; acc: 0.11
Batch: 780; loss: 2.21; acc: 0.12
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.16
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241344544538267; val_accuracy: 0.12042197452229299 

The current subspace-distance is: 2.8438771551009268e-05 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.28; acc: 0.12
Batch: 20; loss: 2.21; acc: 0.22
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.25; acc: 0.11
Batch: 80; loss: 2.22; acc: 0.14
Batch: 100; loss: 2.25; acc: 0.11
Batch: 120; loss: 2.27; acc: 0.23
Batch: 140; loss: 2.19; acc: 0.05
Batch: 160; loss: 2.25; acc: 0.08
Batch: 180; loss: 2.2; acc: 0.17
Batch: 200; loss: 2.22; acc: 0.06
Batch: 220; loss: 2.22; acc: 0.16
Batch: 240; loss: 2.26; acc: 0.14
Batch: 260; loss: 2.2; acc: 0.16
Batch: 280; loss: 2.26; acc: 0.05
Batch: 300; loss: 2.22; acc: 0.16
Batch: 320; loss: 2.24; acc: 0.16
Batch: 340; loss: 2.24; acc: 0.09
Batch: 360; loss: 2.25; acc: 0.11
Batch: 380; loss: 2.24; acc: 0.19
Batch: 400; loss: 2.22; acc: 0.08
Batch: 420; loss: 2.2; acc: 0.14
Batch: 440; loss: 2.29; acc: 0.09
Batch: 460; loss: 2.31; acc: 0.06
Batch: 480; loss: 2.19; acc: 0.06
Batch: 500; loss: 2.27; acc: 0.17
Batch: 520; loss: 2.24; acc: 0.11
Batch: 540; loss: 2.21; acc: 0.17
Batch: 560; loss: 2.26; acc: 0.11
Batch: 580; loss: 2.2; acc: 0.22
Batch: 600; loss: 2.24; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.12
Batch: 640; loss: 2.29; acc: 0.09
Batch: 660; loss: 2.27; acc: 0.12
Batch: 680; loss: 2.27; acc: 0.09
Batch: 700; loss: 2.26; acc: 0.11
Batch: 720; loss: 2.26; acc: 0.05
Batch: 740; loss: 2.32; acc: 0.08
Batch: 760; loss: 2.28; acc: 0.06
Batch: 780; loss: 2.3; acc: 0.05
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413454602478415; val_accuracy: 0.12022292993630573 

The current subspace-distance is: 2.7616722945822403e-05 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.28; acc: 0.06
Batch: 20; loss: 2.26; acc: 0.09
Batch: 40; loss: 2.18; acc: 0.19
Batch: 60; loss: 2.23; acc: 0.12
Batch: 80; loss: 2.19; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.36; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.24; acc: 0.09
Batch: 180; loss: 2.23; acc: 0.2
Batch: 200; loss: 2.21; acc: 0.16
Batch: 220; loss: 2.26; acc: 0.12
Batch: 240; loss: 2.21; acc: 0.09
Batch: 260; loss: 2.27; acc: 0.16
Batch: 280; loss: 2.23; acc: 0.16
Batch: 300; loss: 2.24; acc: 0.16
Batch: 320; loss: 2.24; acc: 0.08
Batch: 340; loss: 2.2; acc: 0.09
Batch: 360; loss: 2.26; acc: 0.02
Batch: 380; loss: 2.25; acc: 0.19
Batch: 400; loss: 2.27; acc: 0.11
Batch: 420; loss: 2.31; acc: 0.06
Batch: 440; loss: 2.28; acc: 0.09
Batch: 460; loss: 2.26; acc: 0.11
Batch: 480; loss: 2.22; acc: 0.08
Batch: 500; loss: 2.28; acc: 0.14
Batch: 520; loss: 2.28; acc: 0.14
Batch: 540; loss: 2.24; acc: 0.17
Batch: 560; loss: 2.31; acc: 0.09
Batch: 580; loss: 2.24; acc: 0.05
Batch: 600; loss: 2.21; acc: 0.19
Batch: 620; loss: 2.22; acc: 0.09
Batch: 640; loss: 2.26; acc: 0.11
Batch: 660; loss: 2.26; acc: 0.08
Batch: 680; loss: 2.17; acc: 0.2
Batch: 700; loss: 2.3; acc: 0.12
Batch: 720; loss: 2.29; acc: 0.16
Batch: 740; loss: 2.28; acc: 0.11
Batch: 760; loss: 2.28; acc: 0.12
Batch: 780; loss: 2.23; acc: 0.11
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241352577877652; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 3.049908991670236e-05 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.22; acc: 0.16
Batch: 20; loss: 2.23; acc: 0.09
Batch: 40; loss: 2.24; acc: 0.08
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.21; acc: 0.09
Batch: 100; loss: 2.31; acc: 0.11
Batch: 120; loss: 2.22; acc: 0.16
Batch: 140; loss: 2.26; acc: 0.12
Batch: 160; loss: 2.27; acc: 0.08
Batch: 180; loss: 2.28; acc: 0.08
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.25; acc: 0.11
Batch: 240; loss: 2.19; acc: 0.17
Batch: 260; loss: 2.25; acc: 0.11
Batch: 280; loss: 2.32; acc: 0.09
Batch: 300; loss: 2.19; acc: 0.22
Batch: 320; loss: 2.22; acc: 0.2
Batch: 340; loss: 2.26; acc: 0.14
Batch: 360; loss: 2.2; acc: 0.16
Batch: 380; loss: 2.27; acc: 0.08
Batch: 400; loss: 2.3; acc: 0.06
Batch: 420; loss: 2.21; acc: 0.16
Batch: 440; loss: 2.29; acc: 0.14
Batch: 460; loss: 2.25; acc: 0.16
Batch: 480; loss: 2.25; acc: 0.12
Batch: 500; loss: 2.26; acc: 0.16
Batch: 520; loss: 2.21; acc: 0.16
Batch: 540; loss: 2.28; acc: 0.11
Batch: 560; loss: 2.26; acc: 0.11
Batch: 580; loss: 2.21; acc: 0.16
Batch: 600; loss: 2.24; acc: 0.11
Batch: 620; loss: 2.19; acc: 0.19
Batch: 640; loss: 2.2; acc: 0.2
Batch: 660; loss: 2.33; acc: 0.0
Batch: 680; loss: 2.22; acc: 0.09
Batch: 700; loss: 2.27; acc: 0.12
Batch: 720; loss: 2.23; acc: 0.09
Batch: 740; loss: 2.25; acc: 0.12
Batch: 760; loss: 2.25; acc: 0.19
Batch: 780; loss: 2.19; acc: 0.12
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413540007961785; val_accuracy: 0.12052149681528662 

The current subspace-distance is: 3.246520645916462e-05 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.3; acc: 0.22
Batch: 20; loss: 2.27; acc: 0.09
Batch: 40; loss: 2.26; acc: 0.11
Batch: 60; loss: 2.22; acc: 0.17
Batch: 80; loss: 2.2; acc: 0.19
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.32; acc: 0.08
Batch: 140; loss: 2.25; acc: 0.16
Batch: 160; loss: 2.22; acc: 0.08
Batch: 180; loss: 2.24; acc: 0.09
Batch: 200; loss: 2.29; acc: 0.16
Batch: 220; loss: 2.34; acc: 0.06
Batch: 240; loss: 2.22; acc: 0.16
Batch: 260; loss: 2.22; acc: 0.09
Batch: 280; loss: 2.26; acc: 0.12
Batch: 300; loss: 2.22; acc: 0.22
Batch: 320; loss: 2.29; acc: 0.08
Batch: 340; loss: 2.32; acc: 0.02
Batch: 360; loss: 2.29; acc: 0.11
Batch: 380; loss: 2.18; acc: 0.14
Batch: 400; loss: 2.27; acc: 0.12
Batch: 420; loss: 2.29; acc: 0.2
Batch: 440; loss: 2.25; acc: 0.12
Batch: 460; loss: 2.23; acc: 0.17
Batch: 480; loss: 2.34; acc: 0.06
Batch: 500; loss: 2.17; acc: 0.2
Batch: 520; loss: 2.3; acc: 0.08
Batch: 540; loss: 2.27; acc: 0.14
Batch: 560; loss: 2.31; acc: 0.12
Batch: 580; loss: 2.16; acc: 0.19
Batch: 600; loss: 2.16; acc: 0.2
Batch: 620; loss: 2.25; acc: 0.14
Batch: 640; loss: 2.25; acc: 0.12
Batch: 660; loss: 2.21; acc: 0.16
Batch: 680; loss: 2.24; acc: 0.17
Batch: 700; loss: 2.22; acc: 0.19
Batch: 720; loss: 2.25; acc: 0.17
Batch: 740; loss: 2.21; acc: 0.17
Batch: 760; loss: 2.27; acc: 0.14
Batch: 780; loss: 2.22; acc: 0.19
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241347930993244; val_accuracy: 0.12052149681528662 

The current subspace-distance is: 3.0293091185740195e-05 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.25; acc: 0.08
Batch: 20; loss: 2.27; acc: 0.09
Batch: 40; loss: 2.24; acc: 0.17
Batch: 60; loss: 2.2; acc: 0.17
Batch: 80; loss: 2.27; acc: 0.09
Batch: 100; loss: 2.25; acc: 0.12
Batch: 120; loss: 2.25; acc: 0.11
Batch: 140; loss: 2.25; acc: 0.08
Batch: 160; loss: 2.23; acc: 0.14
Batch: 180; loss: 2.22; acc: 0.11
Batch: 200; loss: 2.28; acc: 0.11
Batch: 220; loss: 2.19; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.08
Batch: 260; loss: 2.23; acc: 0.14
Batch: 280; loss: 2.24; acc: 0.27
Batch: 300; loss: 2.28; acc: 0.03
Batch: 320; loss: 2.25; acc: 0.16
Batch: 340; loss: 2.28; acc: 0.14
Batch: 360; loss: 2.27; acc: 0.14
Batch: 380; loss: 2.25; acc: 0.12
Batch: 400; loss: 2.25; acc: 0.14
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.29; acc: 0.05
Batch: 460; loss: 2.25; acc: 0.17
Batch: 480; loss: 2.31; acc: 0.09
Batch: 500; loss: 2.28; acc: 0.11
Batch: 520; loss: 2.26; acc: 0.14
Batch: 540; loss: 2.23; acc: 0.16
Batch: 560; loss: 2.26; acc: 0.16
Batch: 580; loss: 2.26; acc: 0.12
Batch: 600; loss: 2.34; acc: 0.12
Batch: 620; loss: 2.27; acc: 0.14
Batch: 640; loss: 2.31; acc: 0.12
Batch: 660; loss: 2.26; acc: 0.08
Batch: 680; loss: 2.23; acc: 0.16
Batch: 700; loss: 2.27; acc: 0.11
Batch: 720; loss: 2.24; acc: 0.12
Batch: 740; loss: 2.26; acc: 0.11
Batch: 760; loss: 2.31; acc: 0.09
Batch: 780; loss: 2.21; acc: 0.17
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241351454121292; val_accuracy: 0.12072054140127389 

The current subspace-distance is: 2.6835048629436642e-05 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.26; acc: 0.08
Batch: 20; loss: 2.21; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.03
Batch: 60; loss: 2.2; acc: 0.11
Batch: 80; loss: 2.26; acc: 0.11
Batch: 100; loss: 2.21; acc: 0.11
Batch: 120; loss: 2.29; acc: 0.11
Batch: 140; loss: 2.23; acc: 0.12
Batch: 160; loss: 2.24; acc: 0.09
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.24; acc: 0.09
Batch: 220; loss: 2.25; acc: 0.14
Batch: 240; loss: 2.21; acc: 0.14
Batch: 260; loss: 2.22; acc: 0.12
Batch: 280; loss: 2.23; acc: 0.17
Batch: 300; loss: 2.17; acc: 0.19
Batch: 320; loss: 2.25; acc: 0.06
Batch: 340; loss: 2.27; acc: 0.09
Batch: 360; loss: 2.26; acc: 0.14
Batch: 380; loss: 2.33; acc: 0.08
Batch: 400; loss: 2.18; acc: 0.19
Batch: 420; loss: 2.25; acc: 0.11
Batch: 440; loss: 2.23; acc: 0.12
Batch: 460; loss: 2.24; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.17
Batch: 500; loss: 2.21; acc: 0.19
Batch: 520; loss: 2.24; acc: 0.16
Batch: 540; loss: 2.23; acc: 0.09
Batch: 560; loss: 2.25; acc: 0.08
Batch: 580; loss: 2.25; acc: 0.14
Batch: 600; loss: 2.26; acc: 0.09
Batch: 620; loss: 2.29; acc: 0.12
Batch: 640; loss: 2.23; acc: 0.08
Batch: 660; loss: 2.23; acc: 0.16
Batch: 680; loss: 2.37; acc: 0.02
Batch: 700; loss: 2.28; acc: 0.08
Batch: 720; loss: 2.26; acc: 0.11
Batch: 740; loss: 2.2; acc: 0.19
Batch: 760; loss: 2.23; acc: 0.14
Batch: 780; loss: 2.24; acc: 0.09
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.25
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.11
Batch: 120; loss: 2.2; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413563409428687; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 2.881117325159721e-05 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.18; acc: 0.19
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.03
Batch: 60; loss: 2.18; acc: 0.16
Batch: 80; loss: 2.25; acc: 0.14
Batch: 100; loss: 2.18; acc: 0.16
Batch: 120; loss: 2.26; acc: 0.11
Batch: 140; loss: 2.15; acc: 0.19
Batch: 160; loss: 2.27; acc: 0.16
Batch: 180; loss: 2.23; acc: 0.17
Batch: 200; loss: 2.24; acc: 0.19
Batch: 220; loss: 2.29; acc: 0.11
Batch: 240; loss: 2.24; acc: 0.09
Batch: 260; loss: 2.21; acc: 0.09
Batch: 280; loss: 2.2; acc: 0.16
Batch: 300; loss: 2.17; acc: 0.19
Batch: 320; loss: 2.24; acc: 0.14
Batch: 340; loss: 2.31; acc: 0.08
Batch: 360; loss: 2.32; acc: 0.06
Batch: 380; loss: 2.23; acc: 0.08
Batch: 400; loss: 2.2; acc: 0.16
Batch: 420; loss: 2.28; acc: 0.12
Batch: 440; loss: 2.27; acc: 0.12
Batch: 460; loss: 2.31; acc: 0.11
Batch: 480; loss: 2.22; acc: 0.09
Batch: 500; loss: 2.3; acc: 0.05
Batch: 520; loss: 2.23; acc: 0.16
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.32; acc: 0.09
Batch: 580; loss: 2.24; acc: 0.11
Batch: 600; loss: 2.26; acc: 0.16
Batch: 620; loss: 2.3; acc: 0.08
Batch: 640; loss: 2.26; acc: 0.08
Batch: 660; loss: 2.26; acc: 0.16
Batch: 680; loss: 2.23; acc: 0.12
Batch: 700; loss: 2.18; acc: 0.2
Batch: 720; loss: 2.31; acc: 0.11
Batch: 740; loss: 2.26; acc: 0.08
Batch: 760; loss: 2.31; acc: 0.05
Batch: 780; loss: 2.18; acc: 0.14
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413551139224106; val_accuracy: 0.12052149681528662 

The current subspace-distance is: 3.389248013263568e-05 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.32; acc: 0.06
Batch: 20; loss: 2.16; acc: 0.19
Batch: 40; loss: 2.19; acc: 0.06
Batch: 60; loss: 2.28; acc: 0.09
Batch: 80; loss: 2.26; acc: 0.16
Batch: 100; loss: 2.26; acc: 0.14
Batch: 120; loss: 2.23; acc: 0.12
Batch: 140; loss: 2.25; acc: 0.12
Batch: 160; loss: 2.23; acc: 0.12
Batch: 180; loss: 2.21; acc: 0.14
Batch: 200; loss: 2.23; acc: 0.11
Batch: 220; loss: 2.27; acc: 0.09
Batch: 240; loss: 2.18; acc: 0.19
Batch: 260; loss: 2.27; acc: 0.14
Batch: 280; loss: 2.26; acc: 0.17
Batch: 300; loss: 2.2; acc: 0.17
Batch: 320; loss: 2.2; acc: 0.08
Batch: 340; loss: 2.32; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.24; acc: 0.08
Batch: 400; loss: 2.26; acc: 0.19
Batch: 420; loss: 2.3; acc: 0.06
Batch: 440; loss: 2.23; acc: 0.11
Batch: 460; loss: 2.24; acc: 0.12
Batch: 480; loss: 2.25; acc: 0.12
Batch: 500; loss: 2.24; acc: 0.19
Batch: 520; loss: 2.27; acc: 0.09
Batch: 540; loss: 2.24; acc: 0.12
Batch: 560; loss: 2.28; acc: 0.17
Batch: 580; loss: 2.28; acc: 0.06
Batch: 600; loss: 2.27; acc: 0.14
Batch: 620; loss: 2.2; acc: 0.16
Batch: 640; loss: 2.25; acc: 0.05
Batch: 660; loss: 2.25; acc: 0.17
Batch: 680; loss: 2.32; acc: 0.06
Batch: 700; loss: 2.3; acc: 0.09
Batch: 720; loss: 2.23; acc: 0.17
Batch: 740; loss: 2.27; acc: 0.08
Batch: 760; loss: 2.23; acc: 0.14
Batch: 780; loss: 2.27; acc: 0.08
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413545960833314; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 3.751772237592377e-05 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.24; acc: 0.08
Batch: 20; loss: 2.2; acc: 0.12
Batch: 40; loss: 2.31; acc: 0.06
Batch: 60; loss: 2.22; acc: 0.2
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.21; acc: 0.11
Batch: 120; loss: 2.27; acc: 0.2
Batch: 140; loss: 2.24; acc: 0.06
Batch: 160; loss: 2.17; acc: 0.23
Batch: 180; loss: 2.25; acc: 0.05
Batch: 200; loss: 2.2; acc: 0.19
Batch: 220; loss: 2.21; acc: 0.19
Batch: 240; loss: 2.21; acc: 0.16
Batch: 260; loss: 2.36; acc: 0.05
Batch: 280; loss: 2.24; acc: 0.17
Batch: 300; loss: 2.22; acc: 0.12
Batch: 320; loss: 2.29; acc: 0.12
Batch: 340; loss: 2.26; acc: 0.08
Batch: 360; loss: 2.26; acc: 0.09
Batch: 380; loss: 2.29; acc: 0.08
Batch: 400; loss: 2.25; acc: 0.12
Batch: 420; loss: 2.25; acc: 0.09
Batch: 440; loss: 2.3; acc: 0.11
Batch: 460; loss: 2.2; acc: 0.14
Batch: 480; loss: 2.26; acc: 0.08
Batch: 500; loss: 2.22; acc: 0.09
Batch: 520; loss: 2.26; acc: 0.08
Batch: 540; loss: 2.3; acc: 0.19
Batch: 560; loss: 2.28; acc: 0.06
Batch: 580; loss: 2.2; acc: 0.16
Batch: 600; loss: 2.26; acc: 0.11
Batch: 620; loss: 2.28; acc: 0.08
Batch: 640; loss: 2.23; acc: 0.17
Batch: 660; loss: 2.27; acc: 0.14
Batch: 680; loss: 2.26; acc: 0.12
Batch: 700; loss: 2.22; acc: 0.14
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.22; acc: 0.09
Batch: 760; loss: 2.25; acc: 0.11
Batch: 780; loss: 2.27; acc: 0.09
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413557760275094; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 3.6642170016421005e-05 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.3; acc: 0.06
Batch: 20; loss: 2.23; acc: 0.16
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.24; acc: 0.09
Batch: 80; loss: 2.21; acc: 0.17
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.23; acc: 0.09
Batch: 140; loss: 2.2; acc: 0.17
Batch: 160; loss: 2.31; acc: 0.05
Batch: 180; loss: 2.24; acc: 0.14
Batch: 200; loss: 2.25; acc: 0.16
Batch: 220; loss: 2.18; acc: 0.12
Batch: 240; loss: 2.28; acc: 0.14
Batch: 260; loss: 2.23; acc: 0.19
Batch: 280; loss: 2.23; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.05
Batch: 320; loss: 2.32; acc: 0.16
Batch: 340; loss: 2.29; acc: 0.09
Batch: 360; loss: 2.28; acc: 0.06
Batch: 380; loss: 2.32; acc: 0.08
Batch: 400; loss: 2.21; acc: 0.12
Batch: 420; loss: 2.19; acc: 0.14
Batch: 440; loss: 2.28; acc: 0.11
Batch: 460; loss: 2.18; acc: 0.12
Batch: 480; loss: 2.24; acc: 0.16
Batch: 500; loss: 2.28; acc: 0.17
Batch: 520; loss: 2.21; acc: 0.2
Batch: 540; loss: 2.22; acc: 0.14
Batch: 560; loss: 2.34; acc: 0.09
Batch: 580; loss: 2.18; acc: 0.16
Batch: 600; loss: 2.26; acc: 0.12
Batch: 620; loss: 2.24; acc: 0.16
Batch: 640; loss: 2.3; acc: 0.08
Batch: 660; loss: 2.25; acc: 0.09
Batch: 680; loss: 2.26; acc: 0.05
Batch: 700; loss: 2.22; acc: 0.16
Batch: 720; loss: 2.25; acc: 0.09
Batch: 740; loss: 2.25; acc: 0.12
Batch: 760; loss: 2.24; acc: 0.16
Batch: 780; loss: 2.29; acc: 0.06
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413540326865617; val_accuracy: 0.12072054140127389 

The current subspace-distance is: 3.484596891212277e-05 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 2.2; acc: 0.09
Batch: 20; loss: 2.29; acc: 0.12
Batch: 40; loss: 2.26; acc: 0.11
Batch: 60; loss: 2.27; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.11
Batch: 100; loss: 2.28; acc: 0.08
Batch: 120; loss: 2.23; acc: 0.16
Batch: 140; loss: 2.22; acc: 0.19
Batch: 160; loss: 2.26; acc: 0.09
Batch: 180; loss: 2.25; acc: 0.16
Batch: 200; loss: 2.24; acc: 0.14
Batch: 220; loss: 2.26; acc: 0.11
Batch: 240; loss: 2.22; acc: 0.14
Batch: 260; loss: 2.18; acc: 0.11
Batch: 280; loss: 2.17; acc: 0.14
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.27; acc: 0.14
Batch: 340; loss: 2.22; acc: 0.12
Batch: 360; loss: 2.2; acc: 0.17
Batch: 380; loss: 2.26; acc: 0.09
Batch: 400; loss: 2.33; acc: 0.17
Batch: 420; loss: 2.21; acc: 0.19
Batch: 440; loss: 2.26; acc: 0.11
Batch: 460; loss: 2.26; acc: 0.09
Batch: 480; loss: 2.21; acc: 0.17
Batch: 500; loss: 2.27; acc: 0.11
Batch: 520; loss: 2.2; acc: 0.14
Batch: 540; loss: 2.21; acc: 0.16
Batch: 560; loss: 2.22; acc: 0.12
Batch: 580; loss: 2.25; acc: 0.08
Batch: 600; loss: 2.25; acc: 0.09
Batch: 620; loss: 2.24; acc: 0.16
Batch: 640; loss: 2.22; acc: 0.12
Batch: 660; loss: 2.24; acc: 0.16
Batch: 680; loss: 2.21; acc: 0.14
Batch: 700; loss: 2.2; acc: 0.11
Batch: 720; loss: 2.27; acc: 0.08
Batch: 740; loss: 2.22; acc: 0.09
Batch: 760; loss: 2.28; acc: 0.05
Batch: 780; loss: 2.25; acc: 0.14
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.24135596129545; val_accuracy: 0.12072054140127389 

The current subspace-distance is: 3.423909583943896e-05 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.26; acc: 0.09
Batch: 20; loss: 2.26; acc: 0.16
Batch: 40; loss: 2.29; acc: 0.12
Batch: 60; loss: 2.24; acc: 0.14
Batch: 80; loss: 2.17; acc: 0.14
Batch: 100; loss: 2.26; acc: 0.11
Batch: 120; loss: 2.17; acc: 0.19
Batch: 140; loss: 2.23; acc: 0.16
Batch: 160; loss: 2.19; acc: 0.08
Batch: 180; loss: 2.26; acc: 0.12
Batch: 200; loss: 2.24; acc: 0.14
Batch: 220; loss: 2.23; acc: 0.14
Batch: 240; loss: 2.2; acc: 0.19
Batch: 260; loss: 2.2; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.06
Batch: 300; loss: 2.23; acc: 0.19
Batch: 320; loss: 2.28; acc: 0.06
Batch: 340; loss: 2.19; acc: 0.08
Batch: 360; loss: 2.23; acc: 0.12
Batch: 380; loss: 2.2; acc: 0.14
Batch: 400; loss: 2.27; acc: 0.11
Batch: 420; loss: 2.23; acc: 0.09
Batch: 440; loss: 2.25; acc: 0.16
Batch: 460; loss: 2.23; acc: 0.19
Batch: 480; loss: 2.26; acc: 0.17
Batch: 500; loss: 2.24; acc: 0.16
Batch: 520; loss: 2.2; acc: 0.14
Batch: 540; loss: 2.31; acc: 0.08
Batch: 560; loss: 2.28; acc: 0.05
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.23; acc: 0.16
Batch: 620; loss: 2.27; acc: 0.06
Batch: 640; loss: 2.21; acc: 0.19
Batch: 660; loss: 2.25; acc: 0.08
Batch: 680; loss: 2.21; acc: 0.11
Batch: 700; loss: 2.21; acc: 0.11
Batch: 720; loss: 2.25; acc: 0.11
Batch: 740; loss: 2.21; acc: 0.14
Batch: 760; loss: 2.25; acc: 0.06
Batch: 780; loss: 2.22; acc: 0.09
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241356611251831; val_accuracy: 0.12072054140127389 

The current subspace-distance is: 3.4889231756096706e-05 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.27; acc: 0.09
Batch: 20; loss: 2.18; acc: 0.17
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.21; acc: 0.25
Batch: 80; loss: 2.24; acc: 0.08
Batch: 100; loss: 2.27; acc: 0.08
Batch: 120; loss: 2.24; acc: 0.11
Batch: 140; loss: 2.28; acc: 0.06
Batch: 160; loss: 2.25; acc: 0.11
Batch: 180; loss: 2.22; acc: 0.16
Batch: 200; loss: 2.28; acc: 0.09
Batch: 220; loss: 2.23; acc: 0.12
Batch: 240; loss: 2.23; acc: 0.14
Batch: 260; loss: 2.24; acc: 0.06
Batch: 280; loss: 2.26; acc: 0.14
Batch: 300; loss: 2.27; acc: 0.14
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.24; acc: 0.09
Batch: 360; loss: 2.25; acc: 0.11
Batch: 380; loss: 2.23; acc: 0.17
Batch: 400; loss: 2.16; acc: 0.17
Batch: 420; loss: 2.22; acc: 0.09
Batch: 440; loss: 2.25; acc: 0.14
Batch: 460; loss: 2.26; acc: 0.09
Batch: 480; loss: 2.27; acc: 0.16
Batch: 500; loss: 2.25; acc: 0.09
Batch: 520; loss: 2.26; acc: 0.11
Batch: 540; loss: 2.23; acc: 0.17
Batch: 560; loss: 2.22; acc: 0.12
Batch: 580; loss: 2.2; acc: 0.16
Batch: 600; loss: 2.26; acc: 0.08
Batch: 620; loss: 2.17; acc: 0.22
Batch: 640; loss: 2.19; acc: 0.11
Batch: 660; loss: 2.27; acc: 0.12
Batch: 680; loss: 2.21; acc: 0.14
Batch: 700; loss: 2.21; acc: 0.14
Batch: 720; loss: 2.23; acc: 0.11
Batch: 740; loss: 2.23; acc: 0.16
Batch: 760; loss: 2.2; acc: 0.16
Batch: 780; loss: 2.24; acc: 0.17
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241356911932587; val_accuracy: 0.12072054140127389 

The current subspace-distance is: 3.5124128771713004e-05 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.25; acc: 0.16
Batch: 20; loss: 2.27; acc: 0.2
Batch: 40; loss: 2.27; acc: 0.12
Batch: 60; loss: 2.25; acc: 0.03
Batch: 80; loss: 2.32; acc: 0.06
Batch: 100; loss: 2.28; acc: 0.11
Batch: 120; loss: 2.23; acc: 0.16
Batch: 140; loss: 2.25; acc: 0.19
Batch: 160; loss: 2.26; acc: 0.06
Batch: 180; loss: 2.24; acc: 0.16
Batch: 200; loss: 2.31; acc: 0.14
Batch: 220; loss: 2.28; acc: 0.09
Batch: 240; loss: 2.22; acc: 0.14
Batch: 260; loss: 2.32; acc: 0.09
Batch: 280; loss: 2.24; acc: 0.16
Batch: 300; loss: 2.28; acc: 0.11
Batch: 320; loss: 2.28; acc: 0.14
Batch: 340; loss: 2.24; acc: 0.12
Batch: 360; loss: 2.25; acc: 0.11
Batch: 380; loss: 2.27; acc: 0.12
Batch: 400; loss: 2.21; acc: 0.14
Batch: 420; loss: 2.34; acc: 0.06
Batch: 440; loss: 2.23; acc: 0.06
Batch: 460; loss: 2.24; acc: 0.12
Batch: 480; loss: 2.2; acc: 0.17
Batch: 500; loss: 2.16; acc: 0.16
Batch: 520; loss: 2.24; acc: 0.12
Batch: 540; loss: 2.25; acc: 0.05
Batch: 560; loss: 2.28; acc: 0.09
Batch: 580; loss: 2.24; acc: 0.11
Batch: 600; loss: 2.26; acc: 0.19
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.27; acc: 0.14
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.28; acc: 0.11
Batch: 700; loss: 2.24; acc: 0.14
Batch: 720; loss: 2.29; acc: 0.11
Batch: 740; loss: 2.29; acc: 0.06
Batch: 760; loss: 2.24; acc: 0.14
Batch: 780; loss: 2.3; acc: 0.06
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.08
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413573629537207; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 3.8792055420344695e-05 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.21; acc: 0.17
Batch: 20; loss: 2.27; acc: 0.12
Batch: 40; loss: 2.25; acc: 0.14
Batch: 60; loss: 2.21; acc: 0.06
Batch: 80; loss: 2.23; acc: 0.12
Batch: 100; loss: 2.33; acc: 0.11
Batch: 120; loss: 2.25; acc: 0.06
Batch: 140; loss: 2.28; acc: 0.11
Batch: 160; loss: 2.24; acc: 0.11
Batch: 180; loss: 2.18; acc: 0.17
Batch: 200; loss: 2.23; acc: 0.06
Batch: 220; loss: 2.29; acc: 0.06
Batch: 240; loss: 2.29; acc: 0.06
Batch: 260; loss: 2.2; acc: 0.12
Batch: 280; loss: 2.3; acc: 0.12
Batch: 300; loss: 2.21; acc: 0.22
Batch: 320; loss: 2.28; acc: 0.12
Batch: 340; loss: 2.28; acc: 0.12
Batch: 360; loss: 2.22; acc: 0.14
Batch: 380; loss: 2.28; acc: 0.17
Batch: 400; loss: 2.27; acc: 0.12
Batch: 420; loss: 2.27; acc: 0.17
Batch: 440; loss: 2.27; acc: 0.12
Batch: 460; loss: 2.28; acc: 0.14
Batch: 480; loss: 2.27; acc: 0.11
Batch: 500; loss: 2.27; acc: 0.12
Batch: 520; loss: 2.23; acc: 0.16
Batch: 540; loss: 2.26; acc: 0.09
Batch: 560; loss: 2.25; acc: 0.06
Batch: 580; loss: 2.2; acc: 0.12
Batch: 600; loss: 2.21; acc: 0.16
Batch: 620; loss: 2.26; acc: 0.16
Batch: 640; loss: 2.24; acc: 0.2
Batch: 660; loss: 2.25; acc: 0.08
Batch: 680; loss: 2.3; acc: 0.06
Batch: 700; loss: 2.25; acc: 0.09
Batch: 720; loss: 2.33; acc: 0.14
Batch: 740; loss: 2.31; acc: 0.09
Batch: 760; loss: 2.24; acc: 0.09
Batch: 780; loss: 2.21; acc: 0.14
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.08
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241357871681262; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 4.010112752439454e-05 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.22; acc: 0.09
Batch: 20; loss: 2.18; acc: 0.16
Batch: 40; loss: 2.27; acc: 0.09
Batch: 60; loss: 2.28; acc: 0.06
Batch: 80; loss: 2.22; acc: 0.19
Batch: 100; loss: 2.32; acc: 0.03
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.24; acc: 0.09
Batch: 160; loss: 2.21; acc: 0.14
Batch: 180; loss: 2.25; acc: 0.09
Batch: 200; loss: 2.32; acc: 0.12
Batch: 220; loss: 2.27; acc: 0.09
Batch: 240; loss: 2.25; acc: 0.14
Batch: 260; loss: 2.21; acc: 0.12
Batch: 280; loss: 2.26; acc: 0.11
Batch: 300; loss: 2.25; acc: 0.22
Batch: 320; loss: 2.29; acc: 0.11
Batch: 340; loss: 2.23; acc: 0.12
Batch: 360; loss: 2.25; acc: 0.08
Batch: 380; loss: 2.17; acc: 0.14
Batch: 400; loss: 2.24; acc: 0.12
Batch: 420; loss: 2.22; acc: 0.12
Batch: 440; loss: 2.22; acc: 0.11
Batch: 460; loss: 2.23; acc: 0.14
Batch: 480; loss: 2.3; acc: 0.06
Batch: 500; loss: 2.26; acc: 0.12
Batch: 520; loss: 2.29; acc: 0.12
Batch: 540; loss: 2.26; acc: 0.14
Batch: 560; loss: 2.25; acc: 0.14
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.27; acc: 0.09
Batch: 620; loss: 2.26; acc: 0.14
Batch: 640; loss: 2.28; acc: 0.08
Batch: 660; loss: 2.24; acc: 0.09
Batch: 680; loss: 2.3; acc: 0.09
Batch: 700; loss: 2.29; acc: 0.19
Batch: 720; loss: 2.25; acc: 0.14
Batch: 740; loss: 2.19; acc: 0.2
Batch: 760; loss: 2.25; acc: 0.14
Batch: 780; loss: 2.29; acc: 0.03
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.08
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241357660597297; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 3.523933264659718e-05 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.23; acc: 0.12
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.24; acc: 0.16
Batch: 60; loss: 2.26; acc: 0.11
Batch: 80; loss: 2.25; acc: 0.09
Batch: 100; loss: 2.32; acc: 0.05
Batch: 120; loss: 2.21; acc: 0.09
Batch: 140; loss: 2.27; acc: 0.14
Batch: 160; loss: 2.22; acc: 0.14
Batch: 180; loss: 2.31; acc: 0.14
Batch: 200; loss: 2.22; acc: 0.12
Batch: 220; loss: 2.31; acc: 0.11
Batch: 240; loss: 2.25; acc: 0.12
Batch: 260; loss: 2.25; acc: 0.09
Batch: 280; loss: 2.28; acc: 0.14
Batch: 300; loss: 2.29; acc: 0.09
Batch: 320; loss: 2.35; acc: 0.05
Batch: 340; loss: 2.25; acc: 0.08
Batch: 360; loss: 2.25; acc: 0.16
Batch: 380; loss: 2.25; acc: 0.08
Batch: 400; loss: 2.27; acc: 0.06
Batch: 420; loss: 2.27; acc: 0.06
Batch: 440; loss: 2.2; acc: 0.19
Batch: 460; loss: 2.25; acc: 0.09
Batch: 480; loss: 2.21; acc: 0.14
Batch: 500; loss: 2.25; acc: 0.12
Batch: 520; loss: 2.22; acc: 0.12
Batch: 540; loss: 2.23; acc: 0.23
Batch: 560; loss: 2.27; acc: 0.14
Batch: 580; loss: 2.23; acc: 0.19
Batch: 600; loss: 2.27; acc: 0.11
Batch: 620; loss: 2.28; acc: 0.09
Batch: 640; loss: 2.24; acc: 0.11
Batch: 660; loss: 2.29; acc: 0.12
Batch: 680; loss: 2.23; acc: 0.16
Batch: 700; loss: 2.28; acc: 0.09
Batch: 720; loss: 2.28; acc: 0.16
Batch: 740; loss: 2.22; acc: 0.11
Batch: 760; loss: 2.3; acc: 0.06
Batch: 780; loss: 2.27; acc: 0.16
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.08
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241356015964678; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 3.4418204450048506e-05 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.18; acc: 0.16
Batch: 20; loss: 2.24; acc: 0.14
Batch: 40; loss: 2.22; acc: 0.11
Batch: 60; loss: 2.37; acc: 0.11
Batch: 80; loss: 2.18; acc: 0.12
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.27; acc: 0.09
Batch: 140; loss: 2.23; acc: 0.16
Batch: 160; loss: 2.26; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.03
Batch: 200; loss: 2.24; acc: 0.11
Batch: 220; loss: 2.22; acc: 0.11
Batch: 240; loss: 2.24; acc: 0.08
Batch: 260; loss: 2.28; acc: 0.11
Batch: 280; loss: 2.26; acc: 0.11
Batch: 300; loss: 2.31; acc: 0.09
Batch: 320; loss: 2.17; acc: 0.14
Batch: 340; loss: 2.29; acc: 0.09
Batch: 360; loss: 2.21; acc: 0.16
Batch: 380; loss: 2.27; acc: 0.08
Batch: 400; loss: 2.27; acc: 0.14
Batch: 420; loss: 2.23; acc: 0.12
Batch: 440; loss: 2.24; acc: 0.16
Batch: 460; loss: 2.29; acc: 0.05
Batch: 480; loss: 2.29; acc: 0.14
Batch: 500; loss: 2.23; acc: 0.09
Batch: 520; loss: 2.33; acc: 0.17
Batch: 540; loss: 2.23; acc: 0.16
Batch: 560; loss: 2.2; acc: 0.2
Batch: 580; loss: 2.26; acc: 0.12
Batch: 600; loss: 2.23; acc: 0.14
Batch: 620; loss: 2.27; acc: 0.05
Batch: 640; loss: 2.29; acc: 0.08
Batch: 660; loss: 2.29; acc: 0.11
Batch: 680; loss: 2.22; acc: 0.11
Batch: 700; loss: 2.25; acc: 0.11
Batch: 720; loss: 2.23; acc: 0.16
Batch: 740; loss: 2.22; acc: 0.16
Batch: 760; loss: 2.27; acc: 0.11
Batch: 780; loss: 2.27; acc: 0.05
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.08
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413564548370943; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 3.7150897696847096e-05 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.29; acc: 0.09
Batch: 20; loss: 2.24; acc: 0.09
Batch: 40; loss: 2.23; acc: 0.09
Batch: 60; loss: 2.31; acc: 0.12
Batch: 80; loss: 2.22; acc: 0.09
Batch: 100; loss: 2.27; acc: 0.11
Batch: 120; loss: 2.25; acc: 0.11
Batch: 140; loss: 2.32; acc: 0.12
Batch: 160; loss: 2.24; acc: 0.2
Batch: 180; loss: 2.31; acc: 0.08
Batch: 200; loss: 2.24; acc: 0.14
Batch: 220; loss: 2.33; acc: 0.06
Batch: 240; loss: 2.22; acc: 0.06
Batch: 260; loss: 2.18; acc: 0.14
Batch: 280; loss: 2.25; acc: 0.09
Batch: 300; loss: 2.28; acc: 0.19
Batch: 320; loss: 2.23; acc: 0.08
Batch: 340; loss: 2.23; acc: 0.08
Batch: 360; loss: 2.25; acc: 0.09
Batch: 380; loss: 2.28; acc: 0.09
Batch: 400; loss: 2.22; acc: 0.17
Batch: 420; loss: 2.3; acc: 0.16
Batch: 440; loss: 2.28; acc: 0.08
Batch: 460; loss: 2.31; acc: 0.16
Batch: 480; loss: 2.21; acc: 0.19
Batch: 500; loss: 2.26; acc: 0.08
Batch: 520; loss: 2.22; acc: 0.14
Batch: 540; loss: 2.29; acc: 0.08
Batch: 560; loss: 2.31; acc: 0.09
Batch: 580; loss: 2.24; acc: 0.12
Batch: 600; loss: 2.28; acc: 0.12
Batch: 620; loss: 2.21; acc: 0.2
Batch: 640; loss: 2.22; acc: 0.09
Batch: 660; loss: 2.27; acc: 0.14
Batch: 680; loss: 2.32; acc: 0.09
Batch: 700; loss: 2.2; acc: 0.09
Batch: 720; loss: 2.25; acc: 0.19
Batch: 740; loss: 2.21; acc: 0.12
Batch: 760; loss: 2.22; acc: 0.11
Batch: 780; loss: 2.27; acc: 0.12
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.08
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.24135675551785; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 4.045354580739513e-05 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.19; acc: 0.19
Batch: 20; loss: 2.19; acc: 0.11
Batch: 40; loss: 2.27; acc: 0.09
Batch: 60; loss: 2.25; acc: 0.16
Batch: 80; loss: 2.3; acc: 0.08
Batch: 100; loss: 2.28; acc: 0.09
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.31; acc: 0.12
Batch: 160; loss: 2.25; acc: 0.05
Batch: 180; loss: 2.31; acc: 0.08
Batch: 200; loss: 2.26; acc: 0.11
Batch: 220; loss: 2.24; acc: 0.12
Batch: 240; loss: 2.2; acc: 0.23
Batch: 260; loss: 2.25; acc: 0.14
Batch: 280; loss: 2.19; acc: 0.17
Batch: 300; loss: 2.23; acc: 0.19
Batch: 320; loss: 2.19; acc: 0.14
Batch: 340; loss: 2.24; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.09
Batch: 380; loss: 2.24; acc: 0.11
Batch: 400; loss: 2.23; acc: 0.17
Batch: 420; loss: 2.2; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.11
Batch: 460; loss: 2.26; acc: 0.11
Batch: 480; loss: 2.23; acc: 0.14
Batch: 500; loss: 2.25; acc: 0.14
Batch: 520; loss: 2.22; acc: 0.12
Batch: 540; loss: 2.31; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.12
Batch: 580; loss: 2.26; acc: 0.12
Batch: 600; loss: 2.31; acc: 0.05
Batch: 620; loss: 2.27; acc: 0.12
Batch: 640; loss: 2.28; acc: 0.12
Batch: 660; loss: 2.32; acc: 0.09
Batch: 680; loss: 2.25; acc: 0.12
Batch: 700; loss: 2.2; acc: 0.2
Batch: 720; loss: 2.27; acc: 0.16
Batch: 740; loss: 2.2; acc: 0.09
Batch: 760; loss: 2.25; acc: 0.11
Batch: 780; loss: 2.31; acc: 0.14
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.08
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.2413580751722786; val_accuracy: 0.12062101910828026 

The current subspace-distance is: 4.3993153667543083e-05 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.31; acc: 0.05
Batch: 20; loss: 2.21; acc: 0.16
Batch: 40; loss: 2.3; acc: 0.11
Batch: 60; loss: 2.27; acc: 0.14
Batch: 80; loss: 2.25; acc: 0.14
Batch: 100; loss: 2.27; acc: 0.08
Batch: 120; loss: 2.28; acc: 0.16
Batch: 140; loss: 2.21; acc: 0.09
Batch: 160; loss: 2.32; acc: 0.09
Batch: 180; loss: 2.24; acc: 0.08
Batch: 200; loss: 2.29; acc: 0.14
Batch: 220; loss: 2.27; acc: 0.09
Batch: 240; loss: 2.25; acc: 0.08
Batch: 260; loss: 2.2; acc: 0.14
Batch: 280; loss: 2.28; acc: 0.11
Batch: 300; loss: 2.25; acc: 0.17
Batch: 320; loss: 2.29; acc: 0.08
Batch: 340; loss: 2.25; acc: 0.11
Batch: 360; loss: 2.26; acc: 0.11
Batch: 380; loss: 2.3; acc: 0.09
Batch: 400; loss: 2.2; acc: 0.2
Batch: 420; loss: 2.16; acc: 0.17
Batch: 440; loss: 2.25; acc: 0.08
Batch: 460; loss: 2.27; acc: 0.14
Batch: 480; loss: 2.22; acc: 0.16
Batch: 500; loss: 2.27; acc: 0.09
Batch: 520; loss: 2.28; acc: 0.11
Batch: 540; loss: 2.26; acc: 0.17
Batch: 560; loss: 2.22; acc: 0.17
Batch: 580; loss: 2.3; acc: 0.16
Batch: 600; loss: 2.3; acc: 0.08
Batch: 620; loss: 2.2; acc: 0.08
Batch: 640; loss: 2.32; acc: 0.17
Batch: 660; loss: 2.23; acc: 0.09
Batch: 680; loss: 2.3; acc: 0.08
Batch: 700; loss: 2.28; acc: 0.14
Batch: 720; loss: 2.26; acc: 0.14
Batch: 740; loss: 2.21; acc: 0.08
Batch: 760; loss: 2.27; acc: 0.06
Batch: 780; loss: 2.26; acc: 0.11
Train Epoch over. train_loss: 2.25; train_accuracy: 0.12 

Batch: 0; loss: 2.21; acc: 0.23
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.21; acc: 0.2
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.3; acc: 0.14
Batch: 100; loss: 2.22; acc: 0.09
Batch: 120; loss: 2.19; acc: 0.17
Batch: 140; loss: 2.23; acc: 0.05
Val Epoch over. val_loss: 2.241357545184482; val_accuracy: 0.12072054140127389 

The current subspace-distance is: 4.1909213905455545e-05 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_10_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 5222
elements in E: 1110650
fraction nonzero: 0.004701751226759105
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.06
Batch: 140; loss: 2.3; acc: 0.12
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.11
Batch: 200; loss: 2.32; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.14
Batch: 240; loss: 2.31; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.14
Batch: 280; loss: 2.32; acc: 0.08
Batch: 300; loss: 2.29; acc: 0.2
Batch: 320; loss: 2.31; acc: 0.08
Batch: 340; loss: 2.3; acc: 0.16
Batch: 360; loss: 2.29; acc: 0.12
Batch: 380; loss: 2.3; acc: 0.14
Batch: 400; loss: 2.31; acc: 0.11
Batch: 420; loss: 2.3; acc: 0.16
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.31; acc: 0.03
Batch: 480; loss: 2.3; acc: 0.11
Batch: 500; loss: 2.3; acc: 0.06
Batch: 520; loss: 2.31; acc: 0.06
Batch: 540; loss: 2.32; acc: 0.06
Batch: 560; loss: 2.3; acc: 0.06
Batch: 580; loss: 2.32; acc: 0.08
Batch: 600; loss: 2.3; acc: 0.06
Batch: 620; loss: 2.3; acc: 0.12
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.28; acc: 0.17
Batch: 680; loss: 2.31; acc: 0.09
Batch: 700; loss: 2.3; acc: 0.12
Batch: 720; loss: 2.3; acc: 0.14
Batch: 740; loss: 2.31; acc: 0.08
Batch: 760; loss: 2.31; acc: 0.05
Batch: 780; loss: 2.29; acc: 0.11
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.3; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.12
Batch: 100; loss: 2.32; acc: 0.05
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.11
Val Epoch over. val_loss: 2.2998734963168004; val_accuracy: 0.10459792993630573 

The current subspace-distance is: 2.132833515133825e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.12
Batch: 20; loss: 2.3; acc: 0.09
Batch: 40; loss: 2.29; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.14
Batch: 80; loss: 2.31; acc: 0.05
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.3; acc: 0.06
Batch: 160; loss: 2.31; acc: 0.09
Batch: 180; loss: 2.3; acc: 0.12
Batch: 200; loss: 2.3; acc: 0.12
Batch: 220; loss: 2.29; acc: 0.14
Batch: 240; loss: 2.3; acc: 0.14
Batch: 260; loss: 2.31; acc: 0.08
Batch: 280; loss: 2.3; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.12
Batch: 320; loss: 2.32; acc: 0.03
Batch: 340; loss: 2.3; acc: 0.06
Batch: 360; loss: 2.31; acc: 0.11
Batch: 380; loss: 2.29; acc: 0.14
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.3; acc: 0.11
Batch: 440; loss: 2.27; acc: 0.25
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.31; acc: 0.09
Batch: 500; loss: 2.29; acc: 0.16
Batch: 520; loss: 2.3; acc: 0.16
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.14
Batch: 580; loss: 2.29; acc: 0.11
Batch: 600; loss: 2.27; acc: 0.23
Batch: 620; loss: 2.3; acc: 0.12
Batch: 640; loss: 2.31; acc: 0.08
Batch: 660; loss: 2.28; acc: 0.17
Batch: 680; loss: 2.29; acc: 0.11
Batch: 700; loss: 2.28; acc: 0.2
Batch: 720; loss: 2.28; acc: 0.11
Batch: 740; loss: 2.29; acc: 0.19
Batch: 760; loss: 2.27; acc: 0.12
Batch: 780; loss: 2.29; acc: 0.12
Train Epoch over. train_loss: 2.29; train_accuracy: 0.12 

Batch: 0; loss: 2.27; acc: 0.23
Batch: 20; loss: 2.3; acc: 0.11
Batch: 40; loss: 2.28; acc: 0.2
Batch: 60; loss: 2.28; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.16
Batch: 100; loss: 2.3; acc: 0.06
Batch: 120; loss: 2.28; acc: 0.11
Batch: 140; loss: 2.27; acc: 0.16
Val Epoch over. val_loss: 2.276750459792508; val_accuracy: 0.14759156050955413 

The current subspace-distance is: 4.392106802697526e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.12
Batch: 20; loss: 2.27; acc: 0.16
Batch: 40; loss: 2.27; acc: 0.2
Batch: 60; loss: 2.28; acc: 0.16
Batch: 80; loss: 2.27; acc: 0.14
Batch: 100; loss: 2.26; acc: 0.14
Batch: 120; loss: 2.24; acc: 0.23
Batch: 140; loss: 2.26; acc: 0.14
Batch: 160; loss: 2.26; acc: 0.16
Batch: 180; loss: 2.24; acc: 0.19
Batch: 200; loss: 2.27; acc: 0.16
Batch: 220; loss: 2.25; acc: 0.14
Batch: 240; loss: 2.29; acc: 0.09
Batch: 260; loss: 2.24; acc: 0.12
Batch: 280; loss: 2.26; acc: 0.08
Batch: 300; loss: 2.25; acc: 0.19
Batch: 320; loss: 2.24; acc: 0.22
Batch: 340; loss: 2.22; acc: 0.17
Batch: 360; loss: 2.23; acc: 0.17
Batch: 380; loss: 2.24; acc: 0.17
Batch: 400; loss: 2.21; acc: 0.28
Batch: 420; loss: 2.24; acc: 0.22
Batch: 440; loss: 2.22; acc: 0.2
Batch: 460; loss: 2.23; acc: 0.22
Batch: 480; loss: 2.21; acc: 0.28
Batch: 500; loss: 2.24; acc: 0.16
Batch: 520; loss: 2.18; acc: 0.2
Batch: 540; loss: 2.19; acc: 0.22
Batch: 560; loss: 2.23; acc: 0.19
Batch: 580; loss: 2.17; acc: 0.22
Batch: 600; loss: 2.19; acc: 0.23
Batch: 620; loss: 2.19; acc: 0.25
Batch: 640; loss: 2.2; acc: 0.22
Batch: 660; loss: 2.17; acc: 0.27
Batch: 680; loss: 2.15; acc: 0.3
Batch: 700; loss: 2.18; acc: 0.2
Batch: 720; loss: 2.15; acc: 0.27
Batch: 740; loss: 2.16; acc: 0.2
Batch: 760; loss: 2.13; acc: 0.28
Batch: 780; loss: 2.12; acc: 0.27
Train Epoch over. train_loss: 2.22; train_accuracy: 0.2 

Batch: 0; loss: 2.16; acc: 0.17
Batch: 20; loss: 2.2; acc: 0.19
Batch: 40; loss: 2.12; acc: 0.23
Batch: 60; loss: 2.1; acc: 0.33
Batch: 80; loss: 2.15; acc: 0.25
Batch: 100; loss: 2.17; acc: 0.19
Batch: 120; loss: 2.14; acc: 0.19
Batch: 140; loss: 2.09; acc: 0.3
Val Epoch over. val_loss: 2.1475583109886025; val_accuracy: 0.24721337579617833 

The current subspace-distance is: 7.888522304710932e-06 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.09; acc: 0.28
Batch: 20; loss: 2.19; acc: 0.23
Batch: 40; loss: 2.13; acc: 0.3
Batch: 60; loss: 2.11; acc: 0.23
Batch: 80; loss: 2.13; acc: 0.27
Batch: 100; loss: 2.15; acc: 0.3
Batch: 120; loss: 2.14; acc: 0.31
Batch: 140; loss: 2.12; acc: 0.3
Batch: 160; loss: 2.07; acc: 0.3
Batch: 180; loss: 2.06; acc: 0.3
Batch: 200; loss: 2.1; acc: 0.38
Batch: 220; loss: 2.12; acc: 0.28
Batch: 240; loss: 2.15; acc: 0.22
Batch: 260; loss: 2.06; acc: 0.27
Batch: 280; loss: 2.16; acc: 0.23
Batch: 300; loss: 2.05; acc: 0.33
Batch: 320; loss: 2.12; acc: 0.17
Batch: 340; loss: 2.14; acc: 0.17
Batch: 360; loss: 2.09; acc: 0.2
Batch: 380; loss: 2.07; acc: 0.3
Batch: 400; loss: 2.11; acc: 0.3
Batch: 420; loss: 2.0; acc: 0.36
Batch: 440; loss: 1.97; acc: 0.39
Batch: 460; loss: 2.13; acc: 0.28
Batch: 480; loss: 2.13; acc: 0.25
Batch: 500; loss: 2.02; acc: 0.28
Batch: 520; loss: 1.99; acc: 0.38
Batch: 540; loss: 1.91; acc: 0.47
Batch: 560; loss: 2.09; acc: 0.2
Batch: 580; loss: 1.97; acc: 0.34
Batch: 600; loss: 1.93; acc: 0.38
Batch: 620; loss: 1.88; acc: 0.41
Batch: 640; loss: 1.92; acc: 0.34
Batch: 660; loss: 1.89; acc: 0.34
Batch: 680; loss: 1.89; acc: 0.34
Batch: 700; loss: 2.01; acc: 0.28
Batch: 720; loss: 1.97; acc: 0.27
Batch: 740; loss: 1.94; acc: 0.34
Batch: 760; loss: 1.86; acc: 0.34
Batch: 780; loss: 1.92; acc: 0.31
Train Epoch over. train_loss: 2.05; train_accuracy: 0.29 

Batch: 0; loss: 1.84; acc: 0.3
Batch: 20; loss: 1.91; acc: 0.36
Batch: 40; loss: 1.63; acc: 0.42
Batch: 60; loss: 1.62; acc: 0.53
Batch: 80; loss: 1.74; acc: 0.42
Batch: 100; loss: 1.9; acc: 0.31
Batch: 120; loss: 1.97; acc: 0.34
Batch: 140; loss: 1.79; acc: 0.34
Val Epoch over. val_loss: 1.8257134332778349; val_accuracy: 0.35668789808917195 

The current subspace-distance is: 1.1695699868141674e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.84; acc: 0.34
Batch: 20; loss: 1.86; acc: 0.3
Batch: 40; loss: 1.68; acc: 0.34
Batch: 60; loss: 1.83; acc: 0.39
Batch: 80; loss: 1.9; acc: 0.34
Batch: 100; loss: 1.85; acc: 0.3
Batch: 120; loss: 1.75; acc: 0.42
Batch: 140; loss: 1.87; acc: 0.33
Batch: 160; loss: 1.94; acc: 0.31
Batch: 180; loss: 1.7; acc: 0.38
Batch: 200; loss: 1.8; acc: 0.38
Batch: 220; loss: 1.82; acc: 0.39
Batch: 240; loss: 1.81; acc: 0.33
Batch: 260; loss: 1.83; acc: 0.31
Batch: 280; loss: 1.83; acc: 0.33
Batch: 300; loss: 1.84; acc: 0.34
Batch: 320; loss: 1.81; acc: 0.41
Batch: 340; loss: 1.72; acc: 0.42
Batch: 360; loss: 1.83; acc: 0.31
Batch: 380; loss: 1.74; acc: 0.42
Batch: 400; loss: 1.55; acc: 0.53
Batch: 420; loss: 1.7; acc: 0.36
Batch: 440; loss: 1.74; acc: 0.33
Batch: 460; loss: 1.68; acc: 0.33
Batch: 480; loss: 1.67; acc: 0.36
Batch: 500; loss: 1.89; acc: 0.38
Batch: 520; loss: 1.64; acc: 0.42
Batch: 540; loss: 1.59; acc: 0.5
Batch: 560; loss: 1.81; acc: 0.38
Batch: 580; loss: 1.78; acc: 0.33
Batch: 600; loss: 1.85; acc: 0.3
Batch: 620; loss: 2.03; acc: 0.33
Batch: 640; loss: 1.74; acc: 0.34
Batch: 660; loss: 2.03; acc: 0.16
Batch: 680; loss: 2.04; acc: 0.23
Batch: 700; loss: 1.89; acc: 0.36
Batch: 720; loss: 1.87; acc: 0.33
Batch: 740; loss: 1.77; acc: 0.39
Batch: 760; loss: 1.72; acc: 0.47
Batch: 780; loss: 1.72; acc: 0.39
Train Epoch over. train_loss: 1.81; train_accuracy: 0.36 

Batch: 0; loss: 1.78; acc: 0.41
Batch: 20; loss: 1.98; acc: 0.27
Batch: 40; loss: 1.52; acc: 0.5
Batch: 60; loss: 1.56; acc: 0.55
Batch: 80; loss: 1.66; acc: 0.42
Batch: 100; loss: 1.96; acc: 0.33
Batch: 120; loss: 1.99; acc: 0.31
Batch: 140; loss: 1.64; acc: 0.39
Val Epoch over. val_loss: 1.7842679350239457; val_accuracy: 0.36046974522292996 

The current subspace-distance is: 1.5527952200500295e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.93; acc: 0.36
Batch: 20; loss: 1.94; acc: 0.31
Batch: 40; loss: 1.84; acc: 0.34
Batch: 60; loss: 1.83; acc: 0.3
Batch: 80; loss: 1.79; acc: 0.34
Batch: 100; loss: 1.7; acc: 0.44
Batch: 120; loss: 1.68; acc: 0.45
Batch: 140; loss: 1.56; acc: 0.42
Batch: 160; loss: 1.91; acc: 0.25
Batch: 180; loss: 1.9; acc: 0.41
Batch: 200; loss: 1.62; acc: 0.38
Batch: 220; loss: 1.92; acc: 0.28
Batch: 240; loss: 1.66; acc: 0.44
Batch: 260; loss: 1.75; acc: 0.39
Batch: 280; loss: 1.86; acc: 0.38
Batch: 300; loss: 1.92; acc: 0.33
Batch: 320; loss: 1.8; acc: 0.31
Batch: 340; loss: 1.74; acc: 0.47
Batch: 360; loss: 1.56; acc: 0.41
Batch: 380; loss: 1.79; acc: 0.28
Batch: 400; loss: 1.83; acc: 0.34
Batch: 420; loss: 1.6; acc: 0.5
Batch: 440; loss: 1.7; acc: 0.3
Batch: 460; loss: 1.9; acc: 0.34
Batch: 480; loss: 1.89; acc: 0.39
Batch: 500; loss: 1.74; acc: 0.41
Batch: 520; loss: 1.76; acc: 0.36
Batch: 540; loss: 1.9; acc: 0.34
Batch: 560; loss: 1.84; acc: 0.34
Batch: 580; loss: 1.86; acc: 0.38
Batch: 600; loss: 1.76; acc: 0.36
Batch: 620; loss: 1.83; acc: 0.3
Batch: 640; loss: 1.67; acc: 0.45
Batch: 660; loss: 1.77; acc: 0.31
Batch: 680; loss: 1.83; acc: 0.33
Batch: 700; loss: 1.87; acc: 0.33
Batch: 720; loss: 1.86; acc: 0.36
Batch: 740; loss: 1.94; acc: 0.28
Batch: 760; loss: 1.68; acc: 0.47
Batch: 780; loss: 1.63; acc: 0.45
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.77; acc: 0.39
Batch: 20; loss: 2.02; acc: 0.28
Batch: 40; loss: 1.53; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.69; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.31
Batch: 120; loss: 2.01; acc: 0.36
Batch: 140; loss: 1.68; acc: 0.38
Val Epoch over. val_loss: 1.7837352016169554; val_accuracy: 0.3600716560509554 

The current subspace-distance is: 1.8469976566848345e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.72; acc: 0.34
Batch: 20; loss: 1.88; acc: 0.3
Batch: 40; loss: 1.83; acc: 0.31
Batch: 60; loss: 1.77; acc: 0.36
Batch: 80; loss: 1.78; acc: 0.39
Batch: 100; loss: 1.72; acc: 0.47
Batch: 120; loss: 1.92; acc: 0.27
Batch: 140; loss: 1.54; acc: 0.5
Batch: 160; loss: 1.83; acc: 0.34
Batch: 180; loss: 1.94; acc: 0.34
Batch: 200; loss: 1.84; acc: 0.39
Batch: 220; loss: 1.85; acc: 0.38
Batch: 240; loss: 1.75; acc: 0.42
Batch: 260; loss: 1.79; acc: 0.48
Batch: 280; loss: 1.87; acc: 0.3
Batch: 300; loss: 1.74; acc: 0.41
Batch: 320; loss: 1.82; acc: 0.39
Batch: 340; loss: 1.82; acc: 0.38
Batch: 360; loss: 1.85; acc: 0.3
Batch: 380; loss: 1.79; acc: 0.36
Batch: 400; loss: 1.63; acc: 0.47
Batch: 420; loss: 1.73; acc: 0.39
Batch: 440; loss: 1.8; acc: 0.38
Batch: 460; loss: 1.76; acc: 0.41
Batch: 480; loss: 1.77; acc: 0.42
Batch: 500; loss: 1.79; acc: 0.39
Batch: 520; loss: 1.72; acc: 0.36
Batch: 540; loss: 1.84; acc: 0.33
Batch: 560; loss: 1.81; acc: 0.44
Batch: 580; loss: 1.83; acc: 0.34
Batch: 600; loss: 1.82; acc: 0.28
Batch: 620; loss: 1.86; acc: 0.3
Batch: 640; loss: 1.75; acc: 0.38
Batch: 660; loss: 2.01; acc: 0.28
Batch: 680; loss: 1.81; acc: 0.34
Batch: 700; loss: 1.72; acc: 0.44
Batch: 720; loss: 1.79; acc: 0.31
Batch: 740; loss: 1.94; acc: 0.23
Batch: 760; loss: 1.73; acc: 0.44
Batch: 780; loss: 1.87; acc: 0.38
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.03; acc: 0.27
Batch: 40; loss: 1.49; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.31
Batch: 120; loss: 1.98; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.34
Val Epoch over. val_loss: 1.7880220268941989; val_accuracy: 0.36027070063694266 

The current subspace-distance is: 1.8807129890774377e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 2.18; acc: 0.28
Batch: 20; loss: 1.89; acc: 0.39
Batch: 40; loss: 1.84; acc: 0.31
Batch: 60; loss: 1.8; acc: 0.33
Batch: 80; loss: 1.82; acc: 0.38
Batch: 100; loss: 1.8; acc: 0.36
Batch: 120; loss: 1.65; acc: 0.36
Batch: 140; loss: 1.91; acc: 0.3
Batch: 160; loss: 1.79; acc: 0.42
Batch: 180; loss: 1.85; acc: 0.36
Batch: 200; loss: 1.84; acc: 0.39
Batch: 220; loss: 1.73; acc: 0.41
Batch: 240; loss: 1.85; acc: 0.42
Batch: 260; loss: 1.9; acc: 0.3
Batch: 280; loss: 1.83; acc: 0.39
Batch: 300; loss: 1.81; acc: 0.33
Batch: 320; loss: 1.79; acc: 0.39
Batch: 340; loss: 1.83; acc: 0.28
Batch: 360; loss: 1.87; acc: 0.31
Batch: 380; loss: 1.98; acc: 0.33
Batch: 400; loss: 1.84; acc: 0.34
Batch: 420; loss: 1.91; acc: 0.27
Batch: 440; loss: 1.83; acc: 0.39
Batch: 460; loss: 1.83; acc: 0.33
Batch: 480; loss: 1.73; acc: 0.39
Batch: 500; loss: 1.82; acc: 0.42
Batch: 520; loss: 1.81; acc: 0.33
Batch: 540; loss: 1.83; acc: 0.31
Batch: 560; loss: 1.97; acc: 0.17
Batch: 580; loss: 1.66; acc: 0.47
Batch: 600; loss: 1.67; acc: 0.44
Batch: 620; loss: 1.68; acc: 0.42
Batch: 640; loss: 1.65; acc: 0.34
Batch: 660; loss: 1.92; acc: 0.34
Batch: 680; loss: 1.76; acc: 0.42
Batch: 700; loss: 1.72; acc: 0.42
Batch: 720; loss: 1.81; acc: 0.36
Batch: 740; loss: 1.92; acc: 0.27
Batch: 760; loss: 1.73; acc: 0.45
Batch: 780; loss: 1.77; acc: 0.38
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 2.05; acc: 0.23
Batch: 40; loss: 1.5; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.68; acc: 0.47
Batch: 100; loss: 1.95; acc: 0.3
Batch: 120; loss: 2.0; acc: 0.33
Batch: 140; loss: 1.7; acc: 0.38
Val Epoch over. val_loss: 1.7852399774417755; val_accuracy: 0.363953025477707 

The current subspace-distance is: 2.1114959963597357e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.39
Batch: 20; loss: 2.06; acc: 0.25
Batch: 40; loss: 1.85; acc: 0.34
Batch: 60; loss: 1.85; acc: 0.27
Batch: 80; loss: 1.8; acc: 0.38
Batch: 100; loss: 1.58; acc: 0.45
Batch: 120; loss: 1.78; acc: 0.34
Batch: 140; loss: 1.93; acc: 0.34
Batch: 160; loss: 1.71; acc: 0.45
Batch: 180; loss: 1.83; acc: 0.36
Batch: 200; loss: 1.95; acc: 0.34
Batch: 220; loss: 1.68; acc: 0.41
Batch: 240; loss: 1.85; acc: 0.33
Batch: 260; loss: 1.82; acc: 0.47
Batch: 280; loss: 1.83; acc: 0.3
Batch: 300; loss: 1.84; acc: 0.33
Batch: 320; loss: 1.84; acc: 0.38
Batch: 340; loss: 1.8; acc: 0.41
Batch: 360; loss: 1.79; acc: 0.41
Batch: 380; loss: 1.82; acc: 0.39
Batch: 400; loss: 1.92; acc: 0.34
Batch: 420; loss: 1.86; acc: 0.34
Batch: 440; loss: 1.7; acc: 0.41
Batch: 460; loss: 1.88; acc: 0.34
Batch: 480; loss: 1.83; acc: 0.39
Batch: 500; loss: 1.89; acc: 0.27
Batch: 520; loss: 1.78; acc: 0.38
Batch: 540; loss: 1.8; acc: 0.39
Batch: 560; loss: 1.92; acc: 0.31
Batch: 580; loss: 1.69; acc: 0.39
Batch: 600; loss: 1.79; acc: 0.36
Batch: 620; loss: 1.84; acc: 0.38
Batch: 640; loss: 1.68; acc: 0.41
Batch: 660; loss: 1.83; acc: 0.31
Batch: 680; loss: 1.74; acc: 0.42
Batch: 700; loss: 1.79; acc: 0.38
Batch: 720; loss: 1.93; acc: 0.3
Batch: 740; loss: 1.78; acc: 0.45
Batch: 760; loss: 1.7; acc: 0.38
Batch: 780; loss: 1.94; acc: 0.33
Train Epoch over. train_loss: 1.81; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.01; acc: 0.3
Batch: 40; loss: 1.5; acc: 0.53
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.66; acc: 0.42
Batch: 100; loss: 1.97; acc: 0.3
Batch: 120; loss: 1.98; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.39
Val Epoch over. val_loss: 1.7829264873152326; val_accuracy: 0.36315684713375795 

The current subspace-distance is: 2.3512377083534375e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.86; acc: 0.34
Batch: 20; loss: 1.85; acc: 0.33
Batch: 40; loss: 1.82; acc: 0.42
Batch: 60; loss: 1.9; acc: 0.3
Batch: 80; loss: 1.85; acc: 0.39
Batch: 100; loss: 1.69; acc: 0.39
Batch: 120; loss: 1.89; acc: 0.3
Batch: 140; loss: 1.84; acc: 0.25
Batch: 160; loss: 1.84; acc: 0.34
Batch: 180; loss: 1.92; acc: 0.3
Batch: 200; loss: 1.78; acc: 0.33
Batch: 220; loss: 1.57; acc: 0.42
Batch: 240; loss: 1.87; acc: 0.33
Batch: 260; loss: 1.87; acc: 0.27
Batch: 280; loss: 1.57; acc: 0.48
Batch: 300; loss: 1.73; acc: 0.39
Batch: 320; loss: 1.83; acc: 0.38
Batch: 340; loss: 2.04; acc: 0.28
Batch: 360; loss: 1.88; acc: 0.31
Batch: 380; loss: 1.74; acc: 0.41
Batch: 400; loss: 1.77; acc: 0.3
Batch: 420; loss: 1.71; acc: 0.39
Batch: 440; loss: 1.69; acc: 0.38
Batch: 460; loss: 1.76; acc: 0.33
Batch: 480; loss: 1.94; acc: 0.33
Batch: 500; loss: 1.76; acc: 0.33
Batch: 520; loss: 1.91; acc: 0.42
Batch: 540; loss: 1.78; acc: 0.33
Batch: 560; loss: 1.76; acc: 0.36
Batch: 580; loss: 1.87; acc: 0.27
Batch: 600; loss: 1.76; acc: 0.39
Batch: 620; loss: 1.77; acc: 0.31
Batch: 640; loss: 1.7; acc: 0.42
Batch: 660; loss: 1.79; acc: 0.33
Batch: 680; loss: 1.82; acc: 0.39
Batch: 700; loss: 1.95; acc: 0.3
Batch: 720; loss: 1.71; acc: 0.42
Batch: 740; loss: 1.76; acc: 0.36
Batch: 760; loss: 1.75; acc: 0.38
Batch: 780; loss: 1.67; acc: 0.38
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.75; acc: 0.39
Batch: 20; loss: 2.03; acc: 0.25
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.48
Batch: 80; loss: 1.66; acc: 0.45
Batch: 100; loss: 1.96; acc: 0.33
Batch: 120; loss: 1.98; acc: 0.31
Batch: 140; loss: 1.68; acc: 0.38
Val Epoch over. val_loss: 1.7832661677318014; val_accuracy: 0.368531050955414 

The current subspace-distance is: 2.4735387341934256e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 2.2; acc: 0.31
Batch: 20; loss: 1.69; acc: 0.41
Batch: 40; loss: 1.87; acc: 0.31
Batch: 60; loss: 1.74; acc: 0.33
Batch: 80; loss: 1.92; acc: 0.3
Batch: 100; loss: 1.89; acc: 0.3
Batch: 120; loss: 1.92; acc: 0.39
Batch: 140; loss: 1.82; acc: 0.34
Batch: 160; loss: 2.02; acc: 0.27
Batch: 180; loss: 1.83; acc: 0.38
Batch: 200; loss: 1.81; acc: 0.33
Batch: 220; loss: 1.69; acc: 0.44
Batch: 240; loss: 1.88; acc: 0.25
Batch: 260; loss: 1.97; acc: 0.3
Batch: 280; loss: 1.82; acc: 0.44
Batch: 300; loss: 1.7; acc: 0.31
Batch: 320; loss: 1.9; acc: 0.36
Batch: 340; loss: 1.86; acc: 0.33
Batch: 360; loss: 1.61; acc: 0.47
Batch: 380; loss: 1.78; acc: 0.36
Batch: 400; loss: 1.7; acc: 0.38
Batch: 420; loss: 2.05; acc: 0.27
Batch: 440; loss: 2.02; acc: 0.3
Batch: 460; loss: 1.88; acc: 0.34
Batch: 480; loss: 1.7; acc: 0.36
Batch: 500; loss: 1.73; acc: 0.33
Batch: 520; loss: 1.88; acc: 0.36
Batch: 540; loss: 1.65; acc: 0.41
Batch: 560; loss: 1.75; acc: 0.33
Batch: 580; loss: 1.79; acc: 0.34
Batch: 600; loss: 1.89; acc: 0.34
Batch: 620; loss: 1.91; acc: 0.3
Batch: 640; loss: 1.82; acc: 0.34
Batch: 660; loss: 1.95; acc: 0.28
Batch: 680; loss: 1.8; acc: 0.44
Batch: 700; loss: 1.88; acc: 0.36
Batch: 720; loss: 1.77; acc: 0.42
Batch: 740; loss: 1.78; acc: 0.3
Batch: 760; loss: 1.81; acc: 0.34
Batch: 780; loss: 1.91; acc: 0.36
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.75; acc: 0.38
Batch: 20; loss: 2.01; acc: 0.3
Batch: 40; loss: 1.52; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.56
Batch: 80; loss: 1.68; acc: 0.36
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.36
Batch: 140; loss: 1.68; acc: 0.38
Val Epoch over. val_loss: 1.7797398119215753; val_accuracy: 0.3632563694267516 

The current subspace-distance is: 2.7077130653196946e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.86; acc: 0.36
Batch: 20; loss: 2.02; acc: 0.3
Batch: 40; loss: 1.84; acc: 0.36
Batch: 60; loss: 1.59; acc: 0.45
Batch: 80; loss: 1.73; acc: 0.38
Batch: 100; loss: 1.79; acc: 0.44
Batch: 120; loss: 1.71; acc: 0.3
Batch: 140; loss: 1.78; acc: 0.41
Batch: 160; loss: 1.57; acc: 0.48
Batch: 180; loss: 1.86; acc: 0.34
Batch: 200; loss: 1.9; acc: 0.3
Batch: 220; loss: 1.79; acc: 0.39
Batch: 240; loss: 1.87; acc: 0.42
Batch: 260; loss: 1.72; acc: 0.38
Batch: 280; loss: 1.72; acc: 0.42
Batch: 300; loss: 1.64; acc: 0.39
Batch: 320; loss: 1.81; acc: 0.38
Batch: 340; loss: 1.72; acc: 0.42
Batch: 360; loss: 1.91; acc: 0.38
Batch: 380; loss: 1.71; acc: 0.44
Batch: 400; loss: 1.87; acc: 0.34
Batch: 420; loss: 1.88; acc: 0.33
Batch: 440; loss: 1.76; acc: 0.38
Batch: 460; loss: 2.02; acc: 0.33
Batch: 480; loss: 2.11; acc: 0.28
Batch: 500; loss: 1.94; acc: 0.34
Batch: 520; loss: 1.81; acc: 0.38
Batch: 540; loss: 1.71; acc: 0.41
Batch: 560; loss: 1.84; acc: 0.31
Batch: 580; loss: 1.9; acc: 0.3
Batch: 600; loss: 1.81; acc: 0.34
Batch: 620; loss: 1.97; acc: 0.3
Batch: 640; loss: 2.12; acc: 0.3
Batch: 660; loss: 1.75; acc: 0.41
Batch: 680; loss: 2.12; acc: 0.25
Batch: 700; loss: 1.81; acc: 0.36
Batch: 720; loss: 1.94; acc: 0.28
Batch: 740; loss: 1.76; acc: 0.38
Batch: 760; loss: 1.85; acc: 0.34
Batch: 780; loss: 1.89; acc: 0.28
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.77; acc: 0.38
Batch: 20; loss: 2.0; acc: 0.28
Batch: 40; loss: 1.52; acc: 0.5
Batch: 60; loss: 1.56; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.42
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.38
Batch: 140; loss: 1.66; acc: 0.38
Val Epoch over. val_loss: 1.7790453661778929; val_accuracy: 0.3638535031847134 

The current subspace-distance is: 3.058727816096507e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.74; acc: 0.39
Batch: 20; loss: 1.91; acc: 0.3
Batch: 40; loss: 1.96; acc: 0.25
Batch: 60; loss: 1.77; acc: 0.42
Batch: 80; loss: 1.79; acc: 0.36
Batch: 100; loss: 1.81; acc: 0.27
Batch: 120; loss: 1.82; acc: 0.33
Batch: 140; loss: 1.7; acc: 0.33
Batch: 160; loss: 1.74; acc: 0.41
Batch: 180; loss: 1.86; acc: 0.28
Batch: 200; loss: 1.72; acc: 0.41
Batch: 220; loss: 1.82; acc: 0.31
Batch: 240; loss: 1.85; acc: 0.33
Batch: 260; loss: 1.86; acc: 0.39
Batch: 280; loss: 1.92; acc: 0.3
Batch: 300; loss: 1.85; acc: 0.34
Batch: 320; loss: 1.67; acc: 0.47
Batch: 340; loss: 1.84; acc: 0.31
Batch: 360; loss: 1.81; acc: 0.38
Batch: 380; loss: 1.65; acc: 0.39
Batch: 400; loss: 1.75; acc: 0.39
Batch: 420; loss: 1.93; acc: 0.28
Batch: 440; loss: 1.83; acc: 0.3
Batch: 460; loss: 1.77; acc: 0.33
Batch: 480; loss: 1.85; acc: 0.38
Batch: 500; loss: 1.85; acc: 0.42
Batch: 520; loss: 1.95; acc: 0.3
Batch: 540; loss: 1.89; acc: 0.42
Batch: 560; loss: 1.86; acc: 0.33
Batch: 580; loss: 1.74; acc: 0.38
Batch: 600; loss: 1.74; acc: 0.31
Batch: 620; loss: 1.66; acc: 0.38
Batch: 640; loss: 1.86; acc: 0.23
Batch: 660; loss: 1.7; acc: 0.39
Batch: 680; loss: 1.84; acc: 0.38
Batch: 700; loss: 1.9; acc: 0.3
Batch: 720; loss: 1.96; acc: 0.27
Batch: 740; loss: 1.9; acc: 0.39
Batch: 760; loss: 1.85; acc: 0.34
Batch: 780; loss: 1.81; acc: 0.38
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.75; acc: 0.39
Batch: 20; loss: 2.02; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.53
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.68; acc: 0.41
Batch: 100; loss: 1.95; acc: 0.33
Batch: 120; loss: 2.0; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.779361952641967; val_accuracy: 0.36375398089171973 

The current subspace-distance is: 2.941649108834099e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.84; acc: 0.42
Batch: 20; loss: 2.09; acc: 0.23
Batch: 40; loss: 1.55; acc: 0.5
Batch: 60; loss: 1.81; acc: 0.36
Batch: 80; loss: 1.83; acc: 0.28
Batch: 100; loss: 1.71; acc: 0.36
Batch: 120; loss: 1.82; acc: 0.36
Batch: 140; loss: 1.8; acc: 0.31
Batch: 160; loss: 1.92; acc: 0.22
Batch: 180; loss: 1.79; acc: 0.38
Batch: 200; loss: 1.92; acc: 0.25
Batch: 220; loss: 1.73; acc: 0.42
Batch: 240; loss: 1.77; acc: 0.31
Batch: 260; loss: 1.66; acc: 0.52
Batch: 280; loss: 1.76; acc: 0.36
Batch: 300; loss: 1.75; acc: 0.36
Batch: 320; loss: 1.68; acc: 0.52
Batch: 340; loss: 2.04; acc: 0.28
Batch: 360; loss: 1.79; acc: 0.39
Batch: 380; loss: 1.73; acc: 0.3
Batch: 400; loss: 1.75; acc: 0.45
Batch: 420; loss: 1.96; acc: 0.31
Batch: 440; loss: 1.76; acc: 0.39
Batch: 460; loss: 1.95; acc: 0.36
Batch: 480; loss: 1.79; acc: 0.33
Batch: 500; loss: 1.7; acc: 0.38
Batch: 520; loss: 1.74; acc: 0.41
Batch: 540; loss: 1.53; acc: 0.5
Batch: 560; loss: 1.92; acc: 0.33
Batch: 580; loss: 1.5; acc: 0.41
Batch: 600; loss: 1.9; acc: 0.28
Batch: 620; loss: 1.93; acc: 0.31
Batch: 640; loss: 1.82; acc: 0.34
Batch: 660; loss: 1.66; acc: 0.34
Batch: 680; loss: 1.94; acc: 0.34
Batch: 700; loss: 1.87; acc: 0.34
Batch: 720; loss: 1.62; acc: 0.42
Batch: 740; loss: 1.67; acc: 0.34
Batch: 760; loss: 1.68; acc: 0.41
Batch: 780; loss: 1.95; acc: 0.34
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 2.02; acc: 0.27
Batch: 40; loss: 1.52; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.69; acc: 0.42
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7812886655710305; val_accuracy: 0.3613654458598726 

The current subspace-distance is: 3.1195020710583776e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.8; acc: 0.34
Batch: 20; loss: 1.58; acc: 0.44
Batch: 40; loss: 1.86; acc: 0.38
Batch: 60; loss: 1.63; acc: 0.41
Batch: 80; loss: 1.72; acc: 0.39
Batch: 100; loss: 1.76; acc: 0.42
Batch: 120; loss: 1.66; acc: 0.38
Batch: 140; loss: 1.89; acc: 0.3
Batch: 160; loss: 1.85; acc: 0.33
Batch: 180; loss: 1.72; acc: 0.36
Batch: 200; loss: 1.72; acc: 0.45
Batch: 220; loss: 1.78; acc: 0.36
Batch: 240; loss: 1.87; acc: 0.27
Batch: 260; loss: 1.93; acc: 0.28
Batch: 280; loss: 1.81; acc: 0.33
Batch: 300; loss: 1.83; acc: 0.33
Batch: 320; loss: 1.9; acc: 0.3
Batch: 340; loss: 1.64; acc: 0.44
Batch: 360; loss: 1.75; acc: 0.42
Batch: 380; loss: 1.79; acc: 0.38
Batch: 400; loss: 1.62; acc: 0.44
Batch: 420; loss: 1.89; acc: 0.38
Batch: 440; loss: 1.85; acc: 0.34
Batch: 460; loss: 1.93; acc: 0.36
Batch: 480; loss: 1.77; acc: 0.39
Batch: 500; loss: 1.83; acc: 0.39
Batch: 520; loss: 1.93; acc: 0.28
Batch: 540; loss: 1.84; acc: 0.39
Batch: 560; loss: 1.77; acc: 0.42
Batch: 580; loss: 1.75; acc: 0.39
Batch: 600; loss: 1.76; acc: 0.34
Batch: 620; loss: 1.76; acc: 0.44
Batch: 640; loss: 1.86; acc: 0.33
Batch: 660; loss: 1.55; acc: 0.45
Batch: 680; loss: 1.6; acc: 0.47
Batch: 700; loss: 2.0; acc: 0.23
Batch: 720; loss: 1.78; acc: 0.33
Batch: 740; loss: 1.73; acc: 0.38
Batch: 760; loss: 1.87; acc: 0.33
Batch: 780; loss: 1.76; acc: 0.41
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.28
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.56
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.96; acc: 0.31
Batch: 120; loss: 1.98; acc: 0.36
Batch: 140; loss: 1.65; acc: 0.39
Val Epoch over. val_loss: 1.7792363349039844; val_accuracy: 0.3613654458598726 

The current subspace-distance is: 3.406109317438677e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.66; acc: 0.39
Batch: 20; loss: 1.85; acc: 0.33
Batch: 40; loss: 1.75; acc: 0.38
Batch: 60; loss: 1.7; acc: 0.44
Batch: 80; loss: 1.8; acc: 0.3
Batch: 100; loss: 1.81; acc: 0.39
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.97; acc: 0.23
Batch: 160; loss: 1.92; acc: 0.3
Batch: 180; loss: 1.99; acc: 0.28
Batch: 200; loss: 1.76; acc: 0.36
Batch: 220; loss: 1.87; acc: 0.31
Batch: 240; loss: 1.93; acc: 0.3
Batch: 260; loss: 1.83; acc: 0.28
Batch: 280; loss: 1.85; acc: 0.36
Batch: 300; loss: 1.83; acc: 0.34
Batch: 320; loss: 1.88; acc: 0.36
Batch: 340; loss: 1.83; acc: 0.36
Batch: 360; loss: 1.78; acc: 0.33
Batch: 380; loss: 1.56; acc: 0.41
Batch: 400; loss: 1.79; acc: 0.33
Batch: 420; loss: 1.82; acc: 0.31
Batch: 440; loss: 1.9; acc: 0.3
Batch: 460; loss: 1.8; acc: 0.44
Batch: 480; loss: 1.74; acc: 0.34
Batch: 500; loss: 1.9; acc: 0.34
Batch: 520; loss: 1.79; acc: 0.36
Batch: 540; loss: 1.6; acc: 0.42
Batch: 560; loss: 1.84; acc: 0.28
Batch: 580; loss: 2.05; acc: 0.23
Batch: 600; loss: 1.66; acc: 0.39
Batch: 620; loss: 1.71; acc: 0.44
Batch: 640; loss: 1.72; acc: 0.41
Batch: 660; loss: 1.85; acc: 0.34
Batch: 680; loss: 1.75; acc: 0.41
Batch: 700; loss: 1.88; acc: 0.33
Batch: 720; loss: 1.84; acc: 0.33
Batch: 740; loss: 1.75; acc: 0.36
Batch: 760; loss: 1.84; acc: 0.33
Batch: 780; loss: 1.68; acc: 0.52
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.28
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.33
Batch: 120; loss: 1.98; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7802243088461032; val_accuracy: 0.3622611464968153 

The current subspace-distance is: 3.437041232245974e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.84; acc: 0.41
Batch: 20; loss: 1.88; acc: 0.38
Batch: 40; loss: 1.94; acc: 0.23
Batch: 60; loss: 1.78; acc: 0.38
Batch: 80; loss: 1.93; acc: 0.28
Batch: 100; loss: 2.0; acc: 0.27
Batch: 120; loss: 1.75; acc: 0.36
Batch: 140; loss: 2.08; acc: 0.28
Batch: 160; loss: 1.68; acc: 0.39
Batch: 180; loss: 1.8; acc: 0.38
Batch: 200; loss: 1.59; acc: 0.45
Batch: 220; loss: 1.87; acc: 0.42
Batch: 240; loss: 1.82; acc: 0.36
Batch: 260; loss: 1.88; acc: 0.38
Batch: 280; loss: 1.83; acc: 0.42
Batch: 300; loss: 1.78; acc: 0.25
Batch: 320; loss: 1.82; acc: 0.38
Batch: 340; loss: 1.78; acc: 0.33
Batch: 360; loss: 1.99; acc: 0.28
Batch: 380; loss: 1.85; acc: 0.34
Batch: 400; loss: 1.84; acc: 0.31
Batch: 420; loss: 1.72; acc: 0.39
Batch: 440; loss: 1.63; acc: 0.44
Batch: 460; loss: 1.84; acc: 0.31
Batch: 480; loss: 1.61; acc: 0.45
Batch: 500; loss: 1.81; acc: 0.39
Batch: 520; loss: 1.63; acc: 0.38
Batch: 540; loss: 1.74; acc: 0.27
Batch: 560; loss: 1.8; acc: 0.31
Batch: 580; loss: 1.76; acc: 0.39
Batch: 600; loss: 1.7; acc: 0.42
Batch: 620; loss: 1.76; acc: 0.31
Batch: 640; loss: 1.88; acc: 0.3
Batch: 660; loss: 1.84; acc: 0.31
Batch: 680; loss: 1.81; acc: 0.34
Batch: 700; loss: 1.71; acc: 0.39
Batch: 720; loss: 1.77; acc: 0.41
Batch: 740; loss: 1.87; acc: 0.36
Batch: 760; loss: 1.82; acc: 0.33
Batch: 780; loss: 1.92; acc: 0.27
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.02; acc: 0.28
Batch: 40; loss: 1.52; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.56
Batch: 80; loss: 1.68; acc: 0.42
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.68; acc: 0.38
Val Epoch over. val_loss: 1.7803330398668908; val_accuracy: 0.36375398089171973 

The current subspace-distance is: 3.905201447196305e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.78; acc: 0.36
Batch: 20; loss: 1.73; acc: 0.33
Batch: 40; loss: 1.88; acc: 0.33
Batch: 60; loss: 1.9; acc: 0.31
Batch: 80; loss: 1.97; acc: 0.28
Batch: 100; loss: 1.75; acc: 0.41
Batch: 120; loss: 1.75; acc: 0.3
Batch: 140; loss: 1.83; acc: 0.31
Batch: 160; loss: 1.74; acc: 0.31
Batch: 180; loss: 1.69; acc: 0.41
Batch: 200; loss: 1.76; acc: 0.39
Batch: 220; loss: 1.99; acc: 0.3
Batch: 240; loss: 1.71; acc: 0.42
Batch: 260; loss: 1.78; acc: 0.34
Batch: 280; loss: 1.91; acc: 0.38
Batch: 300; loss: 1.63; acc: 0.45
Batch: 320; loss: 1.86; acc: 0.36
Batch: 340; loss: 1.7; acc: 0.39
Batch: 360; loss: 1.79; acc: 0.38
Batch: 380; loss: 1.83; acc: 0.31
Batch: 400; loss: 1.69; acc: 0.34
Batch: 420; loss: 1.96; acc: 0.25
Batch: 440; loss: 1.97; acc: 0.27
Batch: 460; loss: 2.1; acc: 0.2
Batch: 480; loss: 1.64; acc: 0.55
Batch: 500; loss: 1.89; acc: 0.36
Batch: 520; loss: 1.76; acc: 0.48
Batch: 540; loss: 1.81; acc: 0.34
Batch: 560; loss: 1.89; acc: 0.42
Batch: 580; loss: 1.91; acc: 0.41
Batch: 600; loss: 1.77; acc: 0.42
Batch: 620; loss: 1.61; acc: 0.48
Batch: 640; loss: 1.71; acc: 0.39
Batch: 660; loss: 1.73; acc: 0.39
Batch: 680; loss: 1.76; acc: 0.38
Batch: 700; loss: 1.78; acc: 0.31
Batch: 720; loss: 1.86; acc: 0.3
Batch: 740; loss: 2.05; acc: 0.23
Batch: 760; loss: 1.85; acc: 0.33
Batch: 780; loss: 2.0; acc: 0.27
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.75; acc: 0.38
Batch: 20; loss: 2.01; acc: 0.3
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.55
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.96; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7793780922130416; val_accuracy: 0.36106687898089174 

The current subspace-distance is: 4.268361590220593e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.78; acc: 0.41
Batch: 20; loss: 1.75; acc: 0.36
Batch: 40; loss: 1.95; acc: 0.33
Batch: 60; loss: 1.79; acc: 0.33
Batch: 80; loss: 1.91; acc: 0.28
Batch: 100; loss: 1.74; acc: 0.39
Batch: 120; loss: 1.89; acc: 0.33
Batch: 140; loss: 1.86; acc: 0.31
Batch: 160; loss: 1.68; acc: 0.41
Batch: 180; loss: 1.83; acc: 0.3
Batch: 200; loss: 2.05; acc: 0.31
Batch: 220; loss: 1.77; acc: 0.39
Batch: 240; loss: 1.85; acc: 0.38
Batch: 260; loss: 1.6; acc: 0.53
Batch: 280; loss: 1.7; acc: 0.47
Batch: 300; loss: 1.76; acc: 0.36
Batch: 320; loss: 1.86; acc: 0.38
Batch: 340; loss: 1.66; acc: 0.42
Batch: 360; loss: 1.81; acc: 0.33
Batch: 380; loss: 1.86; acc: 0.38
Batch: 400; loss: 1.85; acc: 0.34
Batch: 420; loss: 1.87; acc: 0.31
Batch: 440; loss: 2.01; acc: 0.28
Batch: 460; loss: 1.63; acc: 0.45
Batch: 480; loss: 1.76; acc: 0.34
Batch: 500; loss: 1.62; acc: 0.44
Batch: 520; loss: 1.89; acc: 0.36
Batch: 540; loss: 1.67; acc: 0.42
Batch: 560; loss: 1.56; acc: 0.45
Batch: 580; loss: 1.94; acc: 0.3
Batch: 600; loss: 1.76; acc: 0.41
Batch: 620; loss: 1.78; acc: 0.45
Batch: 640; loss: 1.71; acc: 0.38
Batch: 660; loss: 1.75; acc: 0.36
Batch: 680; loss: 1.58; acc: 0.48
Batch: 700; loss: 1.93; acc: 0.28
Batch: 720; loss: 1.77; acc: 0.39
Batch: 740; loss: 1.79; acc: 0.42
Batch: 760; loss: 1.91; acc: 0.3
Batch: 780; loss: 1.65; acc: 0.41
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.48
Batch: 60; loss: 1.55; acc: 0.56
Batch: 80; loss: 1.68; acc: 0.41
Batch: 100; loss: 1.94; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.33
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7815229171400617; val_accuracy: 0.36156449044585987 

The current subspace-distance is: 4.024110967293382e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.75; acc: 0.36
Batch: 20; loss: 1.75; acc: 0.33
Batch: 40; loss: 1.84; acc: 0.42
Batch: 60; loss: 1.67; acc: 0.41
Batch: 80; loss: 1.88; acc: 0.42
Batch: 100; loss: 2.08; acc: 0.22
Batch: 120; loss: 1.77; acc: 0.34
Batch: 140; loss: 1.8; acc: 0.34
Batch: 160; loss: 1.71; acc: 0.39
Batch: 180; loss: 1.83; acc: 0.41
Batch: 200; loss: 1.68; acc: 0.34
Batch: 220; loss: 1.83; acc: 0.36
Batch: 240; loss: 1.83; acc: 0.36
Batch: 260; loss: 1.82; acc: 0.3
Batch: 280; loss: 1.72; acc: 0.34
Batch: 300; loss: 1.75; acc: 0.34
Batch: 320; loss: 1.89; acc: 0.38
Batch: 340; loss: 1.69; acc: 0.36
Batch: 360; loss: 1.64; acc: 0.47
Batch: 380; loss: 1.68; acc: 0.5
Batch: 400; loss: 1.58; acc: 0.47
Batch: 420; loss: 1.86; acc: 0.27
Batch: 440; loss: 1.84; acc: 0.34
Batch: 460; loss: 1.91; acc: 0.36
Batch: 480; loss: 2.02; acc: 0.28
Batch: 500; loss: 1.69; acc: 0.45
Batch: 520; loss: 1.69; acc: 0.33
Batch: 540; loss: 1.7; acc: 0.38
Batch: 560; loss: 1.67; acc: 0.36
Batch: 580; loss: 1.69; acc: 0.39
Batch: 600; loss: 1.73; acc: 0.36
Batch: 620; loss: 1.76; acc: 0.41
Batch: 640; loss: 1.74; acc: 0.34
Batch: 660; loss: 1.74; acc: 0.34
Batch: 680; loss: 1.69; acc: 0.34
Batch: 700; loss: 1.74; acc: 0.39
Batch: 720; loss: 1.72; acc: 0.41
Batch: 740; loss: 1.72; acc: 0.42
Batch: 760; loss: 1.69; acc: 0.42
Batch: 780; loss: 1.55; acc: 0.41
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.75; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.28
Batch: 40; loss: 1.5; acc: 0.5
Batch: 60; loss: 1.54; acc: 0.53
Batch: 80; loss: 1.66; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.31
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7789507390587194; val_accuracy: 0.36315684713375795 

The current subspace-distance is: 4.393215567688458e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.89; acc: 0.23
Batch: 20; loss: 1.74; acc: 0.47
Batch: 40; loss: 1.73; acc: 0.33
Batch: 60; loss: 1.89; acc: 0.36
Batch: 80; loss: 1.8; acc: 0.31
Batch: 100; loss: 1.8; acc: 0.33
Batch: 120; loss: 1.92; acc: 0.3
Batch: 140; loss: 1.79; acc: 0.31
Batch: 160; loss: 1.91; acc: 0.25
Batch: 180; loss: 1.85; acc: 0.3
Batch: 200; loss: 1.85; acc: 0.33
Batch: 220; loss: 1.93; acc: 0.31
Batch: 240; loss: 1.93; acc: 0.38
Batch: 260; loss: 1.8; acc: 0.36
Batch: 280; loss: 1.9; acc: 0.33
Batch: 300; loss: 1.76; acc: 0.34
Batch: 320; loss: 1.83; acc: 0.34
Batch: 340; loss: 1.72; acc: 0.34
Batch: 360; loss: 1.94; acc: 0.27
Batch: 380; loss: 1.78; acc: 0.39
Batch: 400; loss: 1.63; acc: 0.48
Batch: 420; loss: 1.79; acc: 0.31
Batch: 440; loss: 1.85; acc: 0.42
Batch: 460; loss: 1.92; acc: 0.33
Batch: 480; loss: 1.64; acc: 0.42
Batch: 500; loss: 1.71; acc: 0.44
Batch: 520; loss: 1.7; acc: 0.41
Batch: 540; loss: 1.61; acc: 0.41
Batch: 560; loss: 1.74; acc: 0.31
Batch: 580; loss: 1.72; acc: 0.42
Batch: 600; loss: 1.79; acc: 0.33
Batch: 620; loss: 1.81; acc: 0.33
Batch: 640; loss: 1.71; acc: 0.44
Batch: 660; loss: 1.87; acc: 0.27
Batch: 680; loss: 1.83; acc: 0.27
Batch: 700; loss: 1.65; acc: 0.41
Batch: 720; loss: 1.82; acc: 0.36
Batch: 740; loss: 1.82; acc: 0.38
Batch: 760; loss: 1.69; acc: 0.39
Batch: 780; loss: 1.85; acc: 0.31
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.52; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.55
Batch: 80; loss: 1.67; acc: 0.36
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.77901708700095; val_accuracy: 0.36106687898089174 

The current subspace-distance is: 4.508579877438024e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.91; acc: 0.28
Batch: 20; loss: 1.87; acc: 0.28
Batch: 40; loss: 1.91; acc: 0.3
Batch: 60; loss: 1.78; acc: 0.34
Batch: 80; loss: 1.69; acc: 0.39
Batch: 100; loss: 1.55; acc: 0.48
Batch: 120; loss: 1.69; acc: 0.41
Batch: 140; loss: 1.86; acc: 0.33
Batch: 160; loss: 1.87; acc: 0.33
Batch: 180; loss: 1.88; acc: 0.28
Batch: 200; loss: 1.87; acc: 0.34
Batch: 220; loss: 1.86; acc: 0.34
Batch: 240; loss: 2.06; acc: 0.31
Batch: 260; loss: 1.8; acc: 0.27
Batch: 280; loss: 1.77; acc: 0.3
Batch: 300; loss: 1.76; acc: 0.36
Batch: 320; loss: 1.89; acc: 0.28
Batch: 340; loss: 2.0; acc: 0.28
Batch: 360; loss: 1.85; acc: 0.39
Batch: 380; loss: 1.79; acc: 0.39
Batch: 400; loss: 1.91; acc: 0.33
Batch: 420; loss: 1.69; acc: 0.36
Batch: 440; loss: 1.87; acc: 0.33
Batch: 460; loss: 1.88; acc: 0.36
Batch: 480; loss: 1.84; acc: 0.33
Batch: 500; loss: 1.74; acc: 0.39
Batch: 520; loss: 1.69; acc: 0.45
Batch: 540; loss: 1.88; acc: 0.34
Batch: 560; loss: 1.83; acc: 0.3
Batch: 580; loss: 1.82; acc: 0.34
Batch: 600; loss: 1.82; acc: 0.39
Batch: 620; loss: 2.03; acc: 0.31
Batch: 640; loss: 1.91; acc: 0.3
Batch: 660; loss: 1.92; acc: 0.23
Batch: 680; loss: 1.79; acc: 0.36
Batch: 700; loss: 1.78; acc: 0.34
Batch: 720; loss: 1.6; acc: 0.41
Batch: 740; loss: 1.81; acc: 0.34
Batch: 760; loss: 2.01; acc: 0.34
Batch: 780; loss: 1.84; acc: 0.36
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.68; acc: 0.41
Batch: 100; loss: 1.95; acc: 0.33
Batch: 120; loss: 2.0; acc: 0.36
Batch: 140; loss: 1.68; acc: 0.38
Val Epoch over. val_loss: 1.7805236198340253; val_accuracy: 0.36246019108280253 

The current subspace-distance is: 4.42781783931423e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.89; acc: 0.38
Batch: 20; loss: 1.55; acc: 0.47
Batch: 40; loss: 1.91; acc: 0.23
Batch: 60; loss: 1.77; acc: 0.28
Batch: 80; loss: 1.68; acc: 0.45
Batch: 100; loss: 1.88; acc: 0.25
Batch: 120; loss: 1.73; acc: 0.39
Batch: 140; loss: 1.92; acc: 0.2
Batch: 160; loss: 1.67; acc: 0.39
Batch: 180; loss: 1.98; acc: 0.33
Batch: 200; loss: 1.88; acc: 0.31
Batch: 220; loss: 1.78; acc: 0.31
Batch: 240; loss: 1.87; acc: 0.39
Batch: 260; loss: 1.75; acc: 0.39
Batch: 280; loss: 1.73; acc: 0.42
Batch: 300; loss: 2.04; acc: 0.3
Batch: 320; loss: 1.86; acc: 0.33
Batch: 340; loss: 1.81; acc: 0.34
Batch: 360; loss: 1.83; acc: 0.36
Batch: 380; loss: 1.73; acc: 0.45
Batch: 400; loss: 1.69; acc: 0.38
Batch: 420; loss: 1.65; acc: 0.41
Batch: 440; loss: 1.82; acc: 0.38
Batch: 460; loss: 1.81; acc: 0.41
Batch: 480; loss: 1.73; acc: 0.33
Batch: 500; loss: 1.77; acc: 0.39
Batch: 520; loss: 1.72; acc: 0.41
Batch: 540; loss: 1.84; acc: 0.38
Batch: 560; loss: 1.63; acc: 0.45
Batch: 580; loss: 1.95; acc: 0.34
Batch: 600; loss: 1.69; acc: 0.38
Batch: 620; loss: 1.85; acc: 0.36
Batch: 640; loss: 1.7; acc: 0.47
Batch: 660; loss: 1.65; acc: 0.47
Batch: 680; loss: 1.94; acc: 0.3
Batch: 700; loss: 1.78; acc: 0.38
Batch: 720; loss: 2.13; acc: 0.22
Batch: 740; loss: 1.64; acc: 0.39
Batch: 760; loss: 1.84; acc: 0.34
Batch: 780; loss: 1.85; acc: 0.28
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.01; acc: 0.28
Batch: 40; loss: 1.51; acc: 0.53
Batch: 60; loss: 1.55; acc: 0.56
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7794054184749628; val_accuracy: 0.3622611464968153 

The current subspace-distance is: 4.665162123274058e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.69; acc: 0.39
Batch: 20; loss: 1.77; acc: 0.41
Batch: 40; loss: 1.73; acc: 0.42
Batch: 60; loss: 1.93; acc: 0.33
Batch: 80; loss: 1.89; acc: 0.31
Batch: 100; loss: 1.71; acc: 0.39
Batch: 120; loss: 1.82; acc: 0.31
Batch: 140; loss: 1.89; acc: 0.31
Batch: 160; loss: 1.85; acc: 0.33
Batch: 180; loss: 1.93; acc: 0.3
Batch: 200; loss: 1.89; acc: 0.39
Batch: 220; loss: 1.64; acc: 0.44
Batch: 240; loss: 1.88; acc: 0.27
Batch: 260; loss: 1.7; acc: 0.36
Batch: 280; loss: 1.78; acc: 0.34
Batch: 300; loss: 1.78; acc: 0.38
Batch: 320; loss: 1.81; acc: 0.28
Batch: 340; loss: 1.82; acc: 0.27
Batch: 360; loss: 1.78; acc: 0.39
Batch: 380; loss: 1.86; acc: 0.36
Batch: 400; loss: 1.74; acc: 0.41
Batch: 420; loss: 1.81; acc: 0.31
Batch: 440; loss: 1.79; acc: 0.39
Batch: 460; loss: 1.77; acc: 0.42
Batch: 480; loss: 1.78; acc: 0.38
Batch: 500; loss: 1.62; acc: 0.42
Batch: 520; loss: 1.72; acc: 0.39
Batch: 540; loss: 1.93; acc: 0.34
Batch: 560; loss: 1.72; acc: 0.38
Batch: 580; loss: 1.76; acc: 0.31
Batch: 600; loss: 1.78; acc: 0.31
Batch: 620; loss: 1.81; acc: 0.36
Batch: 640; loss: 1.73; acc: 0.36
Batch: 660; loss: 1.86; acc: 0.33
Batch: 680; loss: 1.93; acc: 0.33
Batch: 700; loss: 1.8; acc: 0.36
Batch: 720; loss: 1.83; acc: 0.33
Batch: 740; loss: 1.88; acc: 0.33
Batch: 760; loss: 1.58; acc: 0.41
Batch: 780; loss: 1.82; acc: 0.38
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.38
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7786882371659491; val_accuracy: 0.36186305732484075 

The current subspace-distance is: 4.811333565157838e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.78; acc: 0.36
Batch: 20; loss: 1.7; acc: 0.41
Batch: 40; loss: 1.87; acc: 0.31
Batch: 60; loss: 1.91; acc: 0.34
Batch: 80; loss: 1.87; acc: 0.39
Batch: 100; loss: 1.82; acc: 0.39
Batch: 120; loss: 1.78; acc: 0.38
Batch: 140; loss: 1.8; acc: 0.36
Batch: 160; loss: 1.7; acc: 0.38
Batch: 180; loss: 1.8; acc: 0.34
Batch: 200; loss: 1.93; acc: 0.31
Batch: 220; loss: 1.69; acc: 0.41
Batch: 240; loss: 2.04; acc: 0.22
Batch: 260; loss: 1.79; acc: 0.38
Batch: 280; loss: 1.93; acc: 0.33
Batch: 300; loss: 1.74; acc: 0.36
Batch: 320; loss: 1.77; acc: 0.34
Batch: 340; loss: 1.77; acc: 0.36
Batch: 360; loss: 1.76; acc: 0.41
Batch: 380; loss: 2.16; acc: 0.28
Batch: 400; loss: 1.98; acc: 0.19
Batch: 420; loss: 1.83; acc: 0.38
Batch: 440; loss: 1.71; acc: 0.41
Batch: 460; loss: 1.74; acc: 0.36
Batch: 480; loss: 1.66; acc: 0.38
Batch: 500; loss: 1.76; acc: 0.33
Batch: 520; loss: 1.92; acc: 0.33
Batch: 540; loss: 1.96; acc: 0.28
Batch: 560; loss: 1.73; acc: 0.44
Batch: 580; loss: 1.74; acc: 0.38
Batch: 600; loss: 1.82; acc: 0.31
Batch: 620; loss: 1.7; acc: 0.48
Batch: 640; loss: 1.69; acc: 0.41
Batch: 660; loss: 1.66; acc: 0.36
Batch: 680; loss: 1.98; acc: 0.34
Batch: 700; loss: 1.9; acc: 0.23
Batch: 720; loss: 2.16; acc: 0.3
Batch: 740; loss: 1.85; acc: 0.28
Batch: 760; loss: 1.83; acc: 0.3
Batch: 780; loss: 1.7; acc: 0.44
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.36
Batch: 20; loss: 2.01; acc: 0.25
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.95; acc: 0.33
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.68; acc: 0.38
Val Epoch over. val_loss: 1.7789640305148569; val_accuracy: 0.3619625796178344 

The current subspace-distance is: 4.7088244173210114e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.79; acc: 0.39
Batch: 20; loss: 1.86; acc: 0.36
Batch: 40; loss: 1.92; acc: 0.31
Batch: 60; loss: 1.78; acc: 0.42
Batch: 80; loss: 1.86; acc: 0.27
Batch: 100; loss: 1.76; acc: 0.39
Batch: 120; loss: 1.86; acc: 0.33
Batch: 140; loss: 1.74; acc: 0.44
Batch: 160; loss: 1.76; acc: 0.41
Batch: 180; loss: 1.76; acc: 0.36
Batch: 200; loss: 1.83; acc: 0.36
Batch: 220; loss: 1.78; acc: 0.3
Batch: 240; loss: 1.68; acc: 0.42
Batch: 260; loss: 1.87; acc: 0.33
Batch: 280; loss: 1.74; acc: 0.38
Batch: 300; loss: 1.7; acc: 0.33
Batch: 320; loss: 1.7; acc: 0.39
Batch: 340; loss: 1.81; acc: 0.39
Batch: 360; loss: 1.84; acc: 0.23
Batch: 380; loss: 1.91; acc: 0.23
Batch: 400; loss: 1.84; acc: 0.36
Batch: 420; loss: 1.73; acc: 0.38
Batch: 440; loss: 1.8; acc: 0.44
Batch: 460; loss: 1.72; acc: 0.41
Batch: 480; loss: 1.77; acc: 0.42
Batch: 500; loss: 1.84; acc: 0.38
Batch: 520; loss: 1.81; acc: 0.33
Batch: 540; loss: 1.79; acc: 0.25
Batch: 560; loss: 1.78; acc: 0.34
Batch: 580; loss: 1.77; acc: 0.36
Batch: 600; loss: 1.56; acc: 0.45
Batch: 620; loss: 1.7; acc: 0.41
Batch: 640; loss: 1.77; acc: 0.38
Batch: 660; loss: 1.99; acc: 0.34
Batch: 680; loss: 1.62; acc: 0.42
Batch: 700; loss: 1.84; acc: 0.39
Batch: 720; loss: 1.86; acc: 0.28
Batch: 740; loss: 1.86; acc: 0.31
Batch: 760; loss: 1.82; acc: 0.33
Batch: 780; loss: 1.76; acc: 0.36
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.75; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.28
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.95; acc: 0.33
Batch: 120; loss: 1.98; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7788332070514654; val_accuracy: 0.3629578025477707 

The current subspace-distance is: 4.714141323347576e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.78; acc: 0.25
Batch: 20; loss: 1.82; acc: 0.39
Batch: 40; loss: 1.86; acc: 0.33
Batch: 60; loss: 1.7; acc: 0.42
Batch: 80; loss: 1.73; acc: 0.42
Batch: 100; loss: 1.74; acc: 0.39
Batch: 120; loss: 1.72; acc: 0.34
Batch: 140; loss: 1.85; acc: 0.3
Batch: 160; loss: 1.86; acc: 0.31
Batch: 180; loss: 1.81; acc: 0.38
Batch: 200; loss: 1.91; acc: 0.36
Batch: 220; loss: 1.98; acc: 0.33
Batch: 240; loss: 1.91; acc: 0.3
Batch: 260; loss: 1.91; acc: 0.38
Batch: 280; loss: 1.82; acc: 0.34
Batch: 300; loss: 1.82; acc: 0.33
Batch: 320; loss: 1.84; acc: 0.28
Batch: 340; loss: 1.67; acc: 0.36
Batch: 360; loss: 1.92; acc: 0.3
Batch: 380; loss: 1.87; acc: 0.34
Batch: 400; loss: 2.08; acc: 0.2
Batch: 420; loss: 1.87; acc: 0.31
Batch: 440; loss: 1.67; acc: 0.39
Batch: 460; loss: 2.04; acc: 0.28
Batch: 480; loss: 1.73; acc: 0.41
Batch: 500; loss: 1.9; acc: 0.36
Batch: 520; loss: 1.75; acc: 0.33
Batch: 540; loss: 1.73; acc: 0.38
Batch: 560; loss: 1.74; acc: 0.39
Batch: 580; loss: 1.96; acc: 0.31
Batch: 600; loss: 1.85; acc: 0.31
Batch: 620; loss: 1.66; acc: 0.42
Batch: 640; loss: 1.83; acc: 0.39
Batch: 660; loss: 1.78; acc: 0.38
Batch: 680; loss: 1.77; acc: 0.39
Batch: 700; loss: 1.88; acc: 0.28
Batch: 720; loss: 1.97; acc: 0.25
Batch: 740; loss: 1.62; acc: 0.47
Batch: 760; loss: 1.86; acc: 0.3
Batch: 780; loss: 1.64; acc: 0.42
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.75; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.28
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.55
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.95; acc: 0.33
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.66; acc: 0.38
Val Epoch over. val_loss: 1.778936004183095; val_accuracy: 0.3625597133757962 

The current subspace-distance is: 4.798215377377346e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.87; acc: 0.31
Batch: 20; loss: 1.82; acc: 0.34
Batch: 40; loss: 1.79; acc: 0.31
Batch: 60; loss: 1.78; acc: 0.42
Batch: 80; loss: 1.82; acc: 0.39
Batch: 100; loss: 1.71; acc: 0.39
Batch: 120; loss: 1.74; acc: 0.36
Batch: 140; loss: 1.73; acc: 0.42
Batch: 160; loss: 1.84; acc: 0.33
Batch: 180; loss: 1.67; acc: 0.41
Batch: 200; loss: 1.88; acc: 0.31
Batch: 220; loss: 1.91; acc: 0.34
Batch: 240; loss: 1.78; acc: 0.39
Batch: 260; loss: 1.81; acc: 0.38
Batch: 280; loss: 1.78; acc: 0.36
Batch: 300; loss: 1.72; acc: 0.42
Batch: 320; loss: 2.03; acc: 0.31
Batch: 340; loss: 1.8; acc: 0.39
Batch: 360; loss: 1.69; acc: 0.39
Batch: 380; loss: 1.78; acc: 0.36
Batch: 400; loss: 1.76; acc: 0.45
Batch: 420; loss: 1.86; acc: 0.33
Batch: 440; loss: 1.72; acc: 0.42
Batch: 460; loss: 2.06; acc: 0.27
Batch: 480; loss: 1.76; acc: 0.41
Batch: 500; loss: 1.81; acc: 0.3
Batch: 520; loss: 1.87; acc: 0.23
Batch: 540; loss: 1.88; acc: 0.31
Batch: 560; loss: 2.02; acc: 0.23
Batch: 580; loss: 1.88; acc: 0.28
Batch: 600; loss: 1.87; acc: 0.31
Batch: 620; loss: 1.82; acc: 0.31
Batch: 640; loss: 1.63; acc: 0.44
Batch: 660; loss: 1.82; acc: 0.38
Batch: 680; loss: 1.73; acc: 0.33
Batch: 700; loss: 1.65; acc: 0.47
Batch: 720; loss: 1.81; acc: 0.34
Batch: 740; loss: 1.83; acc: 0.39
Batch: 760; loss: 1.77; acc: 0.38
Batch: 780; loss: 1.89; acc: 0.31
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 2.02; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.53
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7788759492764807; val_accuracy: 0.3629578025477707 

The current subspace-distance is: 4.852148413192481e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.85; acc: 0.38
Batch: 20; loss: 2.03; acc: 0.19
Batch: 40; loss: 1.64; acc: 0.45
Batch: 60; loss: 1.76; acc: 0.39
Batch: 80; loss: 2.02; acc: 0.31
Batch: 100; loss: 1.58; acc: 0.42
Batch: 120; loss: 1.88; acc: 0.25
Batch: 140; loss: 1.74; acc: 0.41
Batch: 160; loss: 1.93; acc: 0.31
Batch: 180; loss: 1.96; acc: 0.36
Batch: 200; loss: 1.79; acc: 0.36
Batch: 220; loss: 1.82; acc: 0.36
Batch: 240; loss: 1.8; acc: 0.33
Batch: 260; loss: 1.64; acc: 0.45
Batch: 280; loss: 1.83; acc: 0.33
Batch: 300; loss: 1.75; acc: 0.44
Batch: 320; loss: 1.68; acc: 0.41
Batch: 340; loss: 1.85; acc: 0.33
Batch: 360; loss: 1.7; acc: 0.39
Batch: 380; loss: 1.72; acc: 0.44
Batch: 400; loss: 1.72; acc: 0.34
Batch: 420; loss: 1.93; acc: 0.25
Batch: 440; loss: 1.77; acc: 0.33
Batch: 460; loss: 1.77; acc: 0.42
Batch: 480; loss: 1.8; acc: 0.36
Batch: 500; loss: 1.86; acc: 0.28
Batch: 520; loss: 1.61; acc: 0.42
Batch: 540; loss: 1.77; acc: 0.34
Batch: 560; loss: 1.98; acc: 0.33
Batch: 580; loss: 1.95; acc: 0.3
Batch: 600; loss: 1.84; acc: 0.36
Batch: 620; loss: 1.97; acc: 0.31
Batch: 640; loss: 1.88; acc: 0.33
Batch: 660; loss: 1.71; acc: 0.42
Batch: 680; loss: 1.91; acc: 0.34
Batch: 700; loss: 1.88; acc: 0.31
Batch: 720; loss: 1.84; acc: 0.31
Batch: 740; loss: 1.61; acc: 0.34
Batch: 760; loss: 1.94; acc: 0.38
Batch: 780; loss: 1.74; acc: 0.39
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 2.02; acc: 0.28
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.66; acc: 0.42
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.36
Batch: 140; loss: 1.66; acc: 0.38
Val Epoch over. val_loss: 1.778225911650688; val_accuracy: 0.3640525477707006 

The current subspace-distance is: 4.585046190186404e-05 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 2.03; acc: 0.27
Batch: 20; loss: 1.86; acc: 0.33
Batch: 40; loss: 1.67; acc: 0.41
Batch: 60; loss: 1.83; acc: 0.38
Batch: 80; loss: 1.84; acc: 0.25
Batch: 100; loss: 1.83; acc: 0.36
Batch: 120; loss: 1.88; acc: 0.31
Batch: 140; loss: 1.76; acc: 0.36
Batch: 160; loss: 1.75; acc: 0.39
Batch: 180; loss: 1.93; acc: 0.27
Batch: 200; loss: 1.75; acc: 0.38
Batch: 220; loss: 1.7; acc: 0.45
Batch: 240; loss: 1.88; acc: 0.31
Batch: 260; loss: 1.79; acc: 0.42
Batch: 280; loss: 1.77; acc: 0.31
Batch: 300; loss: 1.76; acc: 0.38
Batch: 320; loss: 1.81; acc: 0.41
Batch: 340; loss: 1.85; acc: 0.38
Batch: 360; loss: 1.77; acc: 0.38
Batch: 380; loss: 1.91; acc: 0.27
Batch: 400; loss: 1.85; acc: 0.34
Batch: 420; loss: 1.84; acc: 0.27
Batch: 440; loss: 1.74; acc: 0.42
Batch: 460; loss: 1.91; acc: 0.33
Batch: 480; loss: 1.71; acc: 0.38
Batch: 500; loss: 1.77; acc: 0.39
Batch: 520; loss: 1.71; acc: 0.41
Batch: 540; loss: 1.87; acc: 0.28
Batch: 560; loss: 2.0; acc: 0.27
Batch: 580; loss: 1.62; acc: 0.45
Batch: 600; loss: 1.71; acc: 0.39
Batch: 620; loss: 1.71; acc: 0.44
Batch: 640; loss: 1.83; acc: 0.3
Batch: 660; loss: 1.84; acc: 0.33
Batch: 680; loss: 1.83; acc: 0.36
Batch: 700; loss: 1.7; acc: 0.44
Batch: 720; loss: 1.61; acc: 0.41
Batch: 740; loss: 1.72; acc: 0.42
Batch: 760; loss: 1.91; acc: 0.23
Batch: 780; loss: 1.66; acc: 0.36
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.36
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7787867693384742; val_accuracy: 0.36285828025477707 

The current subspace-distance is: 4.80315247841645e-05 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.79; acc: 0.41
Batch: 20; loss: 1.68; acc: 0.42
Batch: 40; loss: 1.93; acc: 0.25
Batch: 60; loss: 1.92; acc: 0.34
Batch: 80; loss: 1.7; acc: 0.44
Batch: 100; loss: 1.81; acc: 0.41
Batch: 120; loss: 1.88; acc: 0.34
Batch: 140; loss: 1.64; acc: 0.41
Batch: 160; loss: 1.7; acc: 0.41
Batch: 180; loss: 1.79; acc: 0.38
Batch: 200; loss: 1.77; acc: 0.33
Batch: 220; loss: 1.8; acc: 0.36
Batch: 240; loss: 1.57; acc: 0.48
Batch: 260; loss: 2.03; acc: 0.23
Batch: 280; loss: 1.85; acc: 0.38
Batch: 300; loss: 1.85; acc: 0.31
Batch: 320; loss: 1.78; acc: 0.31
Batch: 340; loss: 1.77; acc: 0.41
Batch: 360; loss: 1.87; acc: 0.31
Batch: 380; loss: 1.88; acc: 0.31
Batch: 400; loss: 1.8; acc: 0.36
Batch: 420; loss: 2.07; acc: 0.28
Batch: 440; loss: 1.76; acc: 0.31
Batch: 460; loss: 1.74; acc: 0.36
Batch: 480; loss: 1.9; acc: 0.34
Batch: 500; loss: 1.75; acc: 0.42
Batch: 520; loss: 1.89; acc: 0.33
Batch: 540; loss: 1.67; acc: 0.42
Batch: 560; loss: 1.69; acc: 0.44
Batch: 580; loss: 1.79; acc: 0.33
Batch: 600; loss: 1.87; acc: 0.23
Batch: 620; loss: 1.82; acc: 0.31
Batch: 640; loss: 1.82; acc: 0.36
Batch: 660; loss: 1.81; acc: 0.39
Batch: 680; loss: 1.64; acc: 0.39
Batch: 700; loss: 1.84; acc: 0.34
Batch: 720; loss: 1.81; acc: 0.36
Batch: 740; loss: 1.73; acc: 0.45
Batch: 760; loss: 1.63; acc: 0.41
Batch: 780; loss: 1.76; acc: 0.33
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7784235067428298; val_accuracy: 0.36285828025477707 

The current subspace-distance is: 5.064591459813528e-05 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.77; acc: 0.39
Batch: 20; loss: 1.69; acc: 0.34
Batch: 40; loss: 1.78; acc: 0.33
Batch: 60; loss: 1.75; acc: 0.41
Batch: 80; loss: 2.04; acc: 0.3
Batch: 100; loss: 1.77; acc: 0.44
Batch: 120; loss: 1.74; acc: 0.42
Batch: 140; loss: 1.83; acc: 0.41
Batch: 160; loss: 1.83; acc: 0.39
Batch: 180; loss: 1.77; acc: 0.42
Batch: 200; loss: 1.77; acc: 0.34
Batch: 220; loss: 1.83; acc: 0.31
Batch: 240; loss: 1.88; acc: 0.33
Batch: 260; loss: 1.82; acc: 0.36
Batch: 280; loss: 1.62; acc: 0.39
Batch: 300; loss: 1.8; acc: 0.36
Batch: 320; loss: 1.66; acc: 0.39
Batch: 340; loss: 1.75; acc: 0.39
Batch: 360; loss: 1.57; acc: 0.47
Batch: 380; loss: 2.08; acc: 0.3
Batch: 400; loss: 1.74; acc: 0.39
Batch: 420; loss: 1.76; acc: 0.34
Batch: 440; loss: 1.75; acc: 0.47
Batch: 460; loss: 1.79; acc: 0.31
Batch: 480; loss: 1.65; acc: 0.45
Batch: 500; loss: 1.75; acc: 0.44
Batch: 520; loss: 1.81; acc: 0.36
Batch: 540; loss: 1.67; acc: 0.45
Batch: 560; loss: 1.88; acc: 0.28
Batch: 580; loss: 1.95; acc: 0.34
Batch: 600; loss: 1.72; acc: 0.36
Batch: 620; loss: 1.99; acc: 0.23
Batch: 640; loss: 1.71; acc: 0.39
Batch: 660; loss: 1.93; acc: 0.28
Batch: 680; loss: 1.9; acc: 0.31
Batch: 700; loss: 1.99; acc: 0.23
Batch: 720; loss: 1.75; acc: 0.42
Batch: 740; loss: 1.59; acc: 0.41
Batch: 760; loss: 1.81; acc: 0.28
Batch: 780; loss: 1.84; acc: 0.31
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.96; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.36
Batch: 140; loss: 1.66; acc: 0.38
Val Epoch over. val_loss: 1.7783353450192008; val_accuracy: 0.36285828025477707 

The current subspace-distance is: 5.360350041883066e-05 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.83; acc: 0.41
Batch: 20; loss: 1.8; acc: 0.3
Batch: 40; loss: 2.01; acc: 0.3
Batch: 60; loss: 2.01; acc: 0.3
Batch: 80; loss: 1.82; acc: 0.36
Batch: 100; loss: 1.89; acc: 0.3
Batch: 120; loss: 1.6; acc: 0.42
Batch: 140; loss: 1.95; acc: 0.36
Batch: 160; loss: 1.73; acc: 0.42
Batch: 180; loss: 1.62; acc: 0.44
Batch: 200; loss: 1.87; acc: 0.27
Batch: 220; loss: 1.91; acc: 0.27
Batch: 240; loss: 1.59; acc: 0.41
Batch: 260; loss: 1.79; acc: 0.42
Batch: 280; loss: 1.65; acc: 0.5
Batch: 300; loss: 1.58; acc: 0.5
Batch: 320; loss: 1.63; acc: 0.47
Batch: 340; loss: 1.99; acc: 0.28
Batch: 360; loss: 1.7; acc: 0.44
Batch: 380; loss: 1.92; acc: 0.27
Batch: 400; loss: 1.87; acc: 0.36
Batch: 420; loss: 1.66; acc: 0.34
Batch: 440; loss: 1.98; acc: 0.33
Batch: 460; loss: 1.9; acc: 0.3
Batch: 480; loss: 1.53; acc: 0.52
Batch: 500; loss: 1.84; acc: 0.31
Batch: 520; loss: 1.91; acc: 0.28
Batch: 540; loss: 1.77; acc: 0.47
Batch: 560; loss: 1.86; acc: 0.34
Batch: 580; loss: 1.61; acc: 0.47
Batch: 600; loss: 1.93; acc: 0.33
Batch: 620; loss: 1.8; acc: 0.36
Batch: 640; loss: 1.63; acc: 0.45
Batch: 660; loss: 1.75; acc: 0.33
Batch: 680; loss: 1.61; acc: 0.48
Batch: 700; loss: 1.67; acc: 0.38
Batch: 720; loss: 2.0; acc: 0.3
Batch: 740; loss: 1.75; acc: 0.36
Batch: 760; loss: 1.69; acc: 0.42
Batch: 780; loss: 1.79; acc: 0.39
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.778502803699226; val_accuracy: 0.3614649681528662 

The current subspace-distance is: 5.6223248975584283e-05 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.86; acc: 0.3
Batch: 20; loss: 1.89; acc: 0.28
Batch: 40; loss: 1.53; acc: 0.48
Batch: 60; loss: 1.94; acc: 0.36
Batch: 80; loss: 2.01; acc: 0.23
Batch: 100; loss: 1.91; acc: 0.36
Batch: 120; loss: 1.79; acc: 0.36
Batch: 140; loss: 1.71; acc: 0.41
Batch: 160; loss: 2.0; acc: 0.25
Batch: 180; loss: 1.71; acc: 0.39
Batch: 200; loss: 2.02; acc: 0.33
Batch: 220; loss: 1.99; acc: 0.3
Batch: 240; loss: 1.8; acc: 0.33
Batch: 260; loss: 1.68; acc: 0.42
Batch: 280; loss: 1.81; acc: 0.36
Batch: 300; loss: 1.79; acc: 0.39
Batch: 320; loss: 1.77; acc: 0.38
Batch: 340; loss: 1.82; acc: 0.31
Batch: 360; loss: 1.82; acc: 0.36
Batch: 380; loss: 1.99; acc: 0.3
Batch: 400; loss: 1.55; acc: 0.42
Batch: 420; loss: 1.78; acc: 0.3
Batch: 440; loss: 1.94; acc: 0.31
Batch: 460; loss: 1.73; acc: 0.42
Batch: 480; loss: 2.02; acc: 0.31
Batch: 500; loss: 1.85; acc: 0.38
Batch: 520; loss: 1.73; acc: 0.36
Batch: 540; loss: 1.84; acc: 0.39
Batch: 560; loss: 1.85; acc: 0.36
Batch: 580; loss: 1.74; acc: 0.41
Batch: 600; loss: 1.68; acc: 0.38
Batch: 620; loss: 1.73; acc: 0.48
Batch: 640; loss: 1.7; acc: 0.39
Batch: 660; loss: 1.78; acc: 0.42
Batch: 680; loss: 1.9; acc: 0.31
Batch: 700; loss: 1.73; acc: 0.38
Batch: 720; loss: 1.9; acc: 0.31
Batch: 740; loss: 1.82; acc: 0.42
Batch: 760; loss: 1.63; acc: 0.45
Batch: 780; loss: 1.77; acc: 0.34
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.36
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.36
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7789748557813607; val_accuracy: 0.36285828025477707 

The current subspace-distance is: 5.627113569062203e-05 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.58; acc: 0.52
Batch: 20; loss: 2.05; acc: 0.2
Batch: 40; loss: 1.84; acc: 0.27
Batch: 60; loss: 2.05; acc: 0.3
Batch: 80; loss: 1.82; acc: 0.38
Batch: 100; loss: 1.9; acc: 0.36
Batch: 120; loss: 2.0; acc: 0.22
Batch: 140; loss: 1.74; acc: 0.33
Batch: 160; loss: 1.77; acc: 0.3
Batch: 180; loss: 1.91; acc: 0.33
Batch: 200; loss: 1.7; acc: 0.36
Batch: 220; loss: 1.95; acc: 0.28
Batch: 240; loss: 1.75; acc: 0.31
Batch: 260; loss: 1.78; acc: 0.41
Batch: 280; loss: 1.7; acc: 0.39
Batch: 300; loss: 1.67; acc: 0.47
Batch: 320; loss: 1.76; acc: 0.42
Batch: 340; loss: 1.88; acc: 0.28
Batch: 360; loss: 1.75; acc: 0.36
Batch: 380; loss: 1.56; acc: 0.47
Batch: 400; loss: 1.95; acc: 0.3
Batch: 420; loss: 1.64; acc: 0.45
Batch: 440; loss: 1.76; acc: 0.36
Batch: 460; loss: 1.69; acc: 0.44
Batch: 480; loss: 1.78; acc: 0.41
Batch: 500; loss: 1.79; acc: 0.42
Batch: 520; loss: 1.73; acc: 0.42
Batch: 540; loss: 2.04; acc: 0.27
Batch: 560; loss: 1.78; acc: 0.38
Batch: 580; loss: 1.6; acc: 0.5
Batch: 600; loss: 1.72; acc: 0.44
Batch: 620; loss: 1.81; acc: 0.39
Batch: 640; loss: 1.9; acc: 0.34
Batch: 660; loss: 1.91; acc: 0.33
Batch: 680; loss: 1.94; acc: 0.22
Batch: 700; loss: 1.91; acc: 0.31
Batch: 720; loss: 1.94; acc: 0.33
Batch: 740; loss: 1.67; acc: 0.41
Batch: 760; loss: 1.63; acc: 0.48
Batch: 780; loss: 1.85; acc: 0.31
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.41
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.77858267468252; val_accuracy: 0.36236066878980894 

The current subspace-distance is: 5.9231064369669184e-05 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.81; acc: 0.34
Batch: 20; loss: 1.87; acc: 0.27
Batch: 40; loss: 1.78; acc: 0.34
Batch: 60; loss: 1.8; acc: 0.34
Batch: 80; loss: 1.7; acc: 0.33
Batch: 100; loss: 1.83; acc: 0.34
Batch: 120; loss: 1.97; acc: 0.33
Batch: 140; loss: 1.91; acc: 0.27
Batch: 160; loss: 1.89; acc: 0.33
Batch: 180; loss: 1.84; acc: 0.28
Batch: 200; loss: 1.76; acc: 0.34
Batch: 220; loss: 1.8; acc: 0.44
Batch: 240; loss: 1.65; acc: 0.48
Batch: 260; loss: 1.95; acc: 0.3
Batch: 280; loss: 1.87; acc: 0.34
Batch: 300; loss: 1.81; acc: 0.38
Batch: 320; loss: 1.65; acc: 0.42
Batch: 340; loss: 1.93; acc: 0.36
Batch: 360; loss: 1.66; acc: 0.39
Batch: 380; loss: 1.91; acc: 0.34
Batch: 400; loss: 1.73; acc: 0.39
Batch: 420; loss: 1.86; acc: 0.33
Batch: 440; loss: 1.83; acc: 0.34
Batch: 460; loss: 1.96; acc: 0.33
Batch: 480; loss: 1.72; acc: 0.38
Batch: 500; loss: 1.86; acc: 0.33
Batch: 520; loss: 1.89; acc: 0.28
Batch: 540; loss: 1.77; acc: 0.39
Batch: 560; loss: 1.92; acc: 0.25
Batch: 580; loss: 1.96; acc: 0.31
Batch: 600; loss: 1.79; acc: 0.41
Batch: 620; loss: 1.73; acc: 0.36
Batch: 640; loss: 1.86; acc: 0.36
Batch: 660; loss: 1.83; acc: 0.34
Batch: 680; loss: 1.79; acc: 0.41
Batch: 700; loss: 2.05; acc: 0.33
Batch: 720; loss: 1.76; acc: 0.38
Batch: 740; loss: 1.73; acc: 0.38
Batch: 760; loss: 1.82; acc: 0.44
Batch: 780; loss: 1.72; acc: 0.39
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.778613950796188; val_accuracy: 0.3632563694267516 

The current subspace-distance is: 5.818208956043236e-05 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.85; acc: 0.33
Batch: 20; loss: 1.59; acc: 0.41
Batch: 40; loss: 1.58; acc: 0.44
Batch: 60; loss: 1.76; acc: 0.33
Batch: 80; loss: 1.63; acc: 0.42
Batch: 100; loss: 1.83; acc: 0.3
Batch: 120; loss: 1.8; acc: 0.41
Batch: 140; loss: 1.8; acc: 0.31
Batch: 160; loss: 1.74; acc: 0.42
Batch: 180; loss: 1.65; acc: 0.36
Batch: 200; loss: 1.76; acc: 0.34
Batch: 220; loss: 1.82; acc: 0.34
Batch: 240; loss: 1.65; acc: 0.47
Batch: 260; loss: 1.86; acc: 0.39
Batch: 280; loss: 1.78; acc: 0.39
Batch: 300; loss: 1.83; acc: 0.41
Batch: 320; loss: 1.71; acc: 0.45
Batch: 340; loss: 1.82; acc: 0.25
Batch: 360; loss: 1.78; acc: 0.3
Batch: 380; loss: 1.58; acc: 0.44
Batch: 400; loss: 1.77; acc: 0.33
Batch: 420; loss: 1.75; acc: 0.42
Batch: 440; loss: 1.7; acc: 0.38
Batch: 460; loss: 2.02; acc: 0.31
Batch: 480; loss: 1.73; acc: 0.38
Batch: 500; loss: 1.83; acc: 0.34
Batch: 520; loss: 1.99; acc: 0.28
Batch: 540; loss: 1.81; acc: 0.36
Batch: 560; loss: 1.74; acc: 0.41
Batch: 580; loss: 1.84; acc: 0.36
Batch: 600; loss: 1.71; acc: 0.45
Batch: 620; loss: 2.0; acc: 0.3
Batch: 640; loss: 1.52; acc: 0.52
Batch: 660; loss: 1.9; acc: 0.28
Batch: 680; loss: 1.76; acc: 0.39
Batch: 700; loss: 1.67; acc: 0.45
Batch: 720; loss: 1.72; acc: 0.34
Batch: 740; loss: 1.96; acc: 0.27
Batch: 760; loss: 1.88; acc: 0.36
Batch: 780; loss: 1.76; acc: 0.41
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.75; acc: 0.38
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.5; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.38
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7787879037249619; val_accuracy: 0.36335589171974525 

The current subspace-distance is: 6.255377229535952e-05 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.87; acc: 0.31
Batch: 20; loss: 1.84; acc: 0.25
Batch: 40; loss: 1.7; acc: 0.45
Batch: 60; loss: 1.88; acc: 0.42
Batch: 80; loss: 1.69; acc: 0.39
Batch: 100; loss: 1.9; acc: 0.31
Batch: 120; loss: 1.76; acc: 0.41
Batch: 140; loss: 1.7; acc: 0.47
Batch: 160; loss: 1.82; acc: 0.33
Batch: 180; loss: 1.8; acc: 0.38
Batch: 200; loss: 1.77; acc: 0.36
Batch: 220; loss: 1.84; acc: 0.36
Batch: 240; loss: 1.8; acc: 0.34
Batch: 260; loss: 1.66; acc: 0.39
Batch: 280; loss: 1.81; acc: 0.38
Batch: 300; loss: 1.78; acc: 0.44
Batch: 320; loss: 1.96; acc: 0.22
Batch: 340; loss: 1.89; acc: 0.34
Batch: 360; loss: 1.67; acc: 0.41
Batch: 380; loss: 1.76; acc: 0.38
Batch: 400; loss: 1.75; acc: 0.34
Batch: 420; loss: 1.8; acc: 0.36
Batch: 440; loss: 1.88; acc: 0.34
Batch: 460; loss: 1.82; acc: 0.39
Batch: 480; loss: 1.84; acc: 0.31
Batch: 500; loss: 1.86; acc: 0.34
Batch: 520; loss: 1.73; acc: 0.36
Batch: 540; loss: 1.9; acc: 0.3
Batch: 560; loss: 1.85; acc: 0.34
Batch: 580; loss: 1.94; acc: 0.28
Batch: 600; loss: 1.78; acc: 0.41
Batch: 620; loss: 1.75; acc: 0.42
Batch: 640; loss: 2.06; acc: 0.2
Batch: 660; loss: 1.72; acc: 0.33
Batch: 680; loss: 1.76; acc: 0.3
Batch: 700; loss: 1.7; acc: 0.38
Batch: 720; loss: 1.65; acc: 0.5
Batch: 740; loss: 1.78; acc: 0.34
Batch: 760; loss: 1.95; acc: 0.31
Batch: 780; loss: 1.67; acc: 0.41
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.42
Batch: 100; loss: 1.95; acc: 0.33
Batch: 120; loss: 1.99; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.778803435860166; val_accuracy: 0.3625597133757962 

The current subspace-distance is: 6.670750735793263e-05 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.78; acc: 0.41
Batch: 20; loss: 1.64; acc: 0.47
Batch: 40; loss: 1.68; acc: 0.44
Batch: 60; loss: 1.85; acc: 0.41
Batch: 80; loss: 1.9; acc: 0.34
Batch: 100; loss: 1.89; acc: 0.36
Batch: 120; loss: 1.8; acc: 0.33
Batch: 140; loss: 1.7; acc: 0.41
Batch: 160; loss: 1.8; acc: 0.42
Batch: 180; loss: 1.91; acc: 0.36
Batch: 200; loss: 1.62; acc: 0.39
Batch: 220; loss: 1.87; acc: 0.3
Batch: 240; loss: 1.74; acc: 0.42
Batch: 260; loss: 1.55; acc: 0.48
Batch: 280; loss: 1.59; acc: 0.42
Batch: 300; loss: 1.86; acc: 0.38
Batch: 320; loss: 1.83; acc: 0.34
Batch: 340; loss: 2.02; acc: 0.28
Batch: 360; loss: 1.87; acc: 0.33
Batch: 380; loss: 1.82; acc: 0.38
Batch: 400; loss: 1.8; acc: 0.34
Batch: 420; loss: 2.0; acc: 0.27
Batch: 440; loss: 1.91; acc: 0.27
Batch: 460; loss: 1.75; acc: 0.42
Batch: 480; loss: 1.87; acc: 0.33
Batch: 500; loss: 1.73; acc: 0.33
Batch: 520; loss: 1.57; acc: 0.52
Batch: 540; loss: 1.62; acc: 0.39
Batch: 560; loss: 1.9; acc: 0.28
Batch: 580; loss: 1.6; acc: 0.5
Batch: 600; loss: 1.77; acc: 0.36
Batch: 620; loss: 1.79; acc: 0.34
Batch: 640; loss: 1.69; acc: 0.39
Batch: 660; loss: 1.69; acc: 0.52
Batch: 680; loss: 1.63; acc: 0.39
Batch: 700; loss: 1.8; acc: 0.33
Batch: 720; loss: 1.64; acc: 0.48
Batch: 740; loss: 1.63; acc: 0.44
Batch: 760; loss: 1.66; acc: 0.44
Batch: 780; loss: 1.98; acc: 0.27
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.52; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.68; acc: 0.38
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7791498694450232; val_accuracy: 0.3638535031847134 

The current subspace-distance is: 7.31807595002465e-05 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.98; acc: 0.36
Batch: 20; loss: 1.94; acc: 0.3
Batch: 40; loss: 1.67; acc: 0.48
Batch: 60; loss: 1.74; acc: 0.33
Batch: 80; loss: 1.86; acc: 0.3
Batch: 100; loss: 1.76; acc: 0.34
Batch: 120; loss: 1.63; acc: 0.39
Batch: 140; loss: 1.63; acc: 0.36
Batch: 160; loss: 1.92; acc: 0.3
Batch: 180; loss: 1.77; acc: 0.34
Batch: 200; loss: 1.79; acc: 0.36
Batch: 220; loss: 1.7; acc: 0.33
Batch: 240; loss: 1.88; acc: 0.33
Batch: 260; loss: 1.83; acc: 0.3
Batch: 280; loss: 1.85; acc: 0.31
Batch: 300; loss: 1.8; acc: 0.44
Batch: 320; loss: 1.85; acc: 0.3
Batch: 340; loss: 1.81; acc: 0.33
Batch: 360; loss: 1.79; acc: 0.33
Batch: 380; loss: 1.68; acc: 0.41
Batch: 400; loss: 2.04; acc: 0.3
Batch: 420; loss: 1.97; acc: 0.28
Batch: 440; loss: 1.95; acc: 0.39
Batch: 460; loss: 1.77; acc: 0.41
Batch: 480; loss: 1.58; acc: 0.36
Batch: 500; loss: 1.68; acc: 0.38
Batch: 520; loss: 1.64; acc: 0.48
Batch: 540; loss: 1.82; acc: 0.42
Batch: 560; loss: 1.79; acc: 0.38
Batch: 580; loss: 1.89; acc: 0.31
Batch: 600; loss: 1.71; acc: 0.42
Batch: 620; loss: 1.7; acc: 0.44
Batch: 640; loss: 1.62; acc: 0.42
Batch: 660; loss: 1.8; acc: 0.38
Batch: 680; loss: 1.85; acc: 0.31
Batch: 700; loss: 1.74; acc: 0.41
Batch: 720; loss: 1.63; acc: 0.44
Batch: 740; loss: 1.97; acc: 0.33
Batch: 760; loss: 2.01; acc: 0.34
Batch: 780; loss: 1.8; acc: 0.31
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7787366718243642; val_accuracy: 0.3629578025477707 

The current subspace-distance is: 7.328855281230062e-05 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.68; acc: 0.39
Batch: 20; loss: 1.88; acc: 0.28
Batch: 40; loss: 2.0; acc: 0.28
Batch: 60; loss: 1.8; acc: 0.34
Batch: 80; loss: 2.06; acc: 0.28
Batch: 100; loss: 1.65; acc: 0.41
Batch: 120; loss: 1.81; acc: 0.33
Batch: 140; loss: 1.8; acc: 0.31
Batch: 160; loss: 1.9; acc: 0.31
Batch: 180; loss: 1.54; acc: 0.47
Batch: 200; loss: 1.83; acc: 0.41
Batch: 220; loss: 1.95; acc: 0.27
Batch: 240; loss: 1.95; acc: 0.31
Batch: 260; loss: 1.7; acc: 0.38
Batch: 280; loss: 1.89; acc: 0.34
Batch: 300; loss: 1.73; acc: 0.31
Batch: 320; loss: 1.71; acc: 0.38
Batch: 340; loss: 1.85; acc: 0.27
Batch: 360; loss: 1.73; acc: 0.31
Batch: 380; loss: 1.64; acc: 0.39
Batch: 400; loss: 1.81; acc: 0.41
Batch: 420; loss: 1.59; acc: 0.42
Batch: 440; loss: 1.99; acc: 0.31
Batch: 460; loss: 1.7; acc: 0.41
Batch: 480; loss: 1.88; acc: 0.36
Batch: 500; loss: 1.83; acc: 0.34
Batch: 520; loss: 1.97; acc: 0.3
Batch: 540; loss: 1.84; acc: 0.28
Batch: 560; loss: 1.72; acc: 0.42
Batch: 580; loss: 1.76; acc: 0.41
Batch: 600; loss: 1.83; acc: 0.38
Batch: 620; loss: 1.78; acc: 0.34
Batch: 640; loss: 1.89; acc: 0.3
Batch: 660; loss: 1.91; acc: 0.3
Batch: 680; loss: 1.78; acc: 0.42
Batch: 700; loss: 1.52; acc: 0.48
Batch: 720; loss: 1.8; acc: 0.36
Batch: 740; loss: 1.69; acc: 0.42
Batch: 760; loss: 1.85; acc: 0.3
Batch: 780; loss: 1.92; acc: 0.3
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7786483673533058; val_accuracy: 0.36206210191082805 

The current subspace-distance is: 7.656195521121845e-05 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.0; acc: 0.31
Batch: 20; loss: 1.7; acc: 0.38
Batch: 40; loss: 1.75; acc: 0.41
Batch: 60; loss: 1.72; acc: 0.36
Batch: 80; loss: 1.77; acc: 0.31
Batch: 100; loss: 1.81; acc: 0.31
Batch: 120; loss: 1.87; acc: 0.33
Batch: 140; loss: 1.7; acc: 0.39
Batch: 160; loss: 1.76; acc: 0.38
Batch: 180; loss: 1.73; acc: 0.36
Batch: 200; loss: 1.79; acc: 0.42
Batch: 220; loss: 1.77; acc: 0.45
Batch: 240; loss: 1.61; acc: 0.47
Batch: 260; loss: 1.73; acc: 0.41
Batch: 280; loss: 1.59; acc: 0.41
Batch: 300; loss: 1.74; acc: 0.39
Batch: 320; loss: 1.93; acc: 0.28
Batch: 340; loss: 1.93; acc: 0.31
Batch: 360; loss: 1.78; acc: 0.36
Batch: 380; loss: 1.85; acc: 0.34
Batch: 400; loss: 1.7; acc: 0.44
Batch: 420; loss: 1.77; acc: 0.38
Batch: 440; loss: 1.79; acc: 0.33
Batch: 460; loss: 1.62; acc: 0.45
Batch: 480; loss: 1.82; acc: 0.31
Batch: 500; loss: 1.9; acc: 0.3
Batch: 520; loss: 1.83; acc: 0.38
Batch: 540; loss: 1.82; acc: 0.34
Batch: 560; loss: 1.82; acc: 0.33
Batch: 580; loss: 1.7; acc: 0.34
Batch: 600; loss: 1.9; acc: 0.25
Batch: 620; loss: 1.8; acc: 0.39
Batch: 640; loss: 1.9; acc: 0.34
Batch: 660; loss: 1.85; acc: 0.41
Batch: 680; loss: 1.74; acc: 0.41
Batch: 700; loss: 1.79; acc: 0.41
Batch: 720; loss: 1.63; acc: 0.45
Batch: 740; loss: 1.74; acc: 0.36
Batch: 760; loss: 1.65; acc: 0.41
Batch: 780; loss: 1.81; acc: 0.31
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.55
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7785497516583486; val_accuracy: 0.361265923566879 

The current subspace-distance is: 7.666050805710256e-05 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.81; acc: 0.44
Batch: 20; loss: 1.83; acc: 0.27
Batch: 40; loss: 1.68; acc: 0.41
Batch: 60; loss: 1.82; acc: 0.28
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.65; acc: 0.39
Batch: 120; loss: 1.83; acc: 0.39
Batch: 140; loss: 1.74; acc: 0.38
Batch: 160; loss: 1.97; acc: 0.27
Batch: 180; loss: 1.91; acc: 0.3
Batch: 200; loss: 1.83; acc: 0.36
Batch: 220; loss: 1.99; acc: 0.28
Batch: 240; loss: 1.71; acc: 0.38
Batch: 260; loss: 1.74; acc: 0.36
Batch: 280; loss: 1.83; acc: 0.34
Batch: 300; loss: 1.86; acc: 0.33
Batch: 320; loss: 1.9; acc: 0.33
Batch: 340; loss: 1.67; acc: 0.42
Batch: 360; loss: 1.85; acc: 0.27
Batch: 380; loss: 1.86; acc: 0.34
Batch: 400; loss: 1.72; acc: 0.44
Batch: 420; loss: 1.97; acc: 0.2
Batch: 440; loss: 1.83; acc: 0.36
Batch: 460; loss: 2.0; acc: 0.2
Batch: 480; loss: 1.77; acc: 0.31
Batch: 500; loss: 1.81; acc: 0.31
Batch: 520; loss: 1.86; acc: 0.34
Batch: 540; loss: 1.74; acc: 0.33
Batch: 560; loss: 1.74; acc: 0.41
Batch: 580; loss: 1.84; acc: 0.41
Batch: 600; loss: 1.77; acc: 0.36
Batch: 620; loss: 1.93; acc: 0.31
Batch: 640; loss: 1.72; acc: 0.38
Batch: 660; loss: 1.54; acc: 0.52
Batch: 680; loss: 1.81; acc: 0.41
Batch: 700; loss: 1.9; acc: 0.31
Batch: 720; loss: 1.81; acc: 0.39
Batch: 740; loss: 1.8; acc: 0.33
Batch: 760; loss: 1.86; acc: 0.25
Batch: 780; loss: 1.71; acc: 0.42
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7785557432539145; val_accuracy: 0.36236066878980894 

The current subspace-distance is: 7.543984247604385e-05 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.0; acc: 0.3
Batch: 20; loss: 1.78; acc: 0.38
Batch: 40; loss: 1.87; acc: 0.39
Batch: 60; loss: 1.68; acc: 0.41
Batch: 80; loss: 1.83; acc: 0.38
Batch: 100; loss: 1.84; acc: 0.34
Batch: 120; loss: 1.9; acc: 0.3
Batch: 140; loss: 1.82; acc: 0.31
Batch: 160; loss: 1.61; acc: 0.39
Batch: 180; loss: 1.68; acc: 0.39
Batch: 200; loss: 1.72; acc: 0.39
Batch: 220; loss: 1.75; acc: 0.42
Batch: 240; loss: 1.86; acc: 0.36
Batch: 260; loss: 1.89; acc: 0.31
Batch: 280; loss: 1.83; acc: 0.3
Batch: 300; loss: 1.74; acc: 0.34
Batch: 320; loss: 1.79; acc: 0.33
Batch: 340; loss: 1.69; acc: 0.42
Batch: 360; loss: 1.68; acc: 0.39
Batch: 380; loss: 1.76; acc: 0.38
Batch: 400; loss: 1.82; acc: 0.34
Batch: 420; loss: 1.8; acc: 0.41
Batch: 440; loss: 1.84; acc: 0.3
Batch: 460; loss: 1.96; acc: 0.23
Batch: 480; loss: 1.81; acc: 0.3
Batch: 500; loss: 1.81; acc: 0.33
Batch: 520; loss: 1.75; acc: 0.42
Batch: 540; loss: 1.9; acc: 0.3
Batch: 560; loss: 1.71; acc: 0.42
Batch: 580; loss: 1.78; acc: 0.31
Batch: 600; loss: 1.97; acc: 0.41
Batch: 620; loss: 1.74; acc: 0.3
Batch: 640; loss: 1.78; acc: 0.34
Batch: 660; loss: 1.68; acc: 0.39
Batch: 680; loss: 1.94; acc: 0.28
Batch: 700; loss: 1.83; acc: 0.33
Batch: 720; loss: 1.82; acc: 0.39
Batch: 740; loss: 1.85; acc: 0.36
Batch: 760; loss: 1.98; acc: 0.3
Batch: 780; loss: 1.8; acc: 0.38
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.38
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.38
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7786118088254503; val_accuracy: 0.3614649681528662 

The current subspace-distance is: 7.50240869820118e-05 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.84; acc: 0.31
Batch: 20; loss: 1.56; acc: 0.44
Batch: 40; loss: 1.85; acc: 0.33
Batch: 60; loss: 1.88; acc: 0.38
Batch: 80; loss: 1.87; acc: 0.39
Batch: 100; loss: 1.65; acc: 0.33
Batch: 120; loss: 1.59; acc: 0.44
Batch: 140; loss: 1.89; acc: 0.3
Batch: 160; loss: 1.8; acc: 0.36
Batch: 180; loss: 1.79; acc: 0.34
Batch: 200; loss: 1.72; acc: 0.47
Batch: 220; loss: 1.59; acc: 0.52
Batch: 240; loss: 1.64; acc: 0.39
Batch: 260; loss: 1.8; acc: 0.36
Batch: 280; loss: 1.78; acc: 0.36
Batch: 300; loss: 1.97; acc: 0.34
Batch: 320; loss: 1.72; acc: 0.36
Batch: 340; loss: 1.75; acc: 0.39
Batch: 360; loss: 1.6; acc: 0.48
Batch: 380; loss: 1.71; acc: 0.44
Batch: 400; loss: 1.78; acc: 0.34
Batch: 420; loss: 1.79; acc: 0.38
Batch: 440; loss: 2.11; acc: 0.25
Batch: 460; loss: 1.93; acc: 0.33
Batch: 480; loss: 1.74; acc: 0.36
Batch: 500; loss: 1.84; acc: 0.39
Batch: 520; loss: 1.69; acc: 0.47
Batch: 540; loss: 1.56; acc: 0.53
Batch: 560; loss: 1.89; acc: 0.31
Batch: 580; loss: 1.74; acc: 0.44
Batch: 600; loss: 2.02; acc: 0.22
Batch: 620; loss: 1.77; acc: 0.28
Batch: 640; loss: 1.89; acc: 0.34
Batch: 660; loss: 1.59; acc: 0.45
Batch: 680; loss: 1.84; acc: 0.34
Batch: 700; loss: 1.82; acc: 0.38
Batch: 720; loss: 1.86; acc: 0.33
Batch: 740; loss: 1.59; acc: 0.45
Batch: 760; loss: 1.69; acc: 0.36
Batch: 780; loss: 1.75; acc: 0.41
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7784911227074398; val_accuracy: 0.36315684713375795 

The current subspace-distance is: 7.873309368733317e-05 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 2.09; acc: 0.25
Batch: 20; loss: 1.74; acc: 0.33
Batch: 40; loss: 1.64; acc: 0.41
Batch: 60; loss: 1.89; acc: 0.31
Batch: 80; loss: 2.12; acc: 0.23
Batch: 100; loss: 1.9; acc: 0.28
Batch: 120; loss: 1.71; acc: 0.45
Batch: 140; loss: 1.67; acc: 0.42
Batch: 160; loss: 1.92; acc: 0.31
Batch: 180; loss: 1.7; acc: 0.41
Batch: 200; loss: 1.68; acc: 0.38
Batch: 220; loss: 1.68; acc: 0.44
Batch: 240; loss: 1.91; acc: 0.3
Batch: 260; loss: 1.73; acc: 0.36
Batch: 280; loss: 1.91; acc: 0.36
Batch: 300; loss: 1.86; acc: 0.36
Batch: 320; loss: 1.87; acc: 0.34
Batch: 340; loss: 1.55; acc: 0.42
Batch: 360; loss: 1.9; acc: 0.33
Batch: 380; loss: 1.68; acc: 0.36
Batch: 400; loss: 1.65; acc: 0.38
Batch: 420; loss: 1.94; acc: 0.33
Batch: 440; loss: 1.85; acc: 0.39
Batch: 460; loss: 1.93; acc: 0.25
Batch: 480; loss: 1.92; acc: 0.33
Batch: 500; loss: 1.78; acc: 0.34
Batch: 520; loss: 1.9; acc: 0.25
Batch: 540; loss: 1.82; acc: 0.3
Batch: 560; loss: 1.91; acc: 0.33
Batch: 580; loss: 1.9; acc: 0.33
Batch: 600; loss: 1.61; acc: 0.55
Batch: 620; loss: 1.94; acc: 0.3
Batch: 640; loss: 1.84; acc: 0.31
Batch: 660; loss: 1.82; acc: 0.28
Batch: 680; loss: 1.7; acc: 0.36
Batch: 700; loss: 1.67; acc: 0.39
Batch: 720; loss: 1.9; acc: 0.33
Batch: 740; loss: 1.82; acc: 0.34
Batch: 760; loss: 1.88; acc: 0.34
Batch: 780; loss: 1.9; acc: 0.33
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7785787787407068; val_accuracy: 0.36186305732484075 

The current subspace-distance is: 7.883700891397893e-05 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.8; acc: 0.3
Batch: 20; loss: 1.71; acc: 0.38
Batch: 40; loss: 1.9; acc: 0.31
Batch: 60; loss: 1.56; acc: 0.48
Batch: 80; loss: 1.72; acc: 0.47
Batch: 100; loss: 1.97; acc: 0.27
Batch: 120; loss: 1.83; acc: 0.38
Batch: 140; loss: 1.85; acc: 0.3
Batch: 160; loss: 2.02; acc: 0.3
Batch: 180; loss: 1.82; acc: 0.33
Batch: 200; loss: 1.93; acc: 0.34
Batch: 220; loss: 1.81; acc: 0.34
Batch: 240; loss: 1.82; acc: 0.3
Batch: 260; loss: 1.93; acc: 0.3
Batch: 280; loss: 1.82; acc: 0.3
Batch: 300; loss: 1.63; acc: 0.42
Batch: 320; loss: 1.84; acc: 0.36
Batch: 340; loss: 1.82; acc: 0.36
Batch: 360; loss: 1.88; acc: 0.42
Batch: 380; loss: 1.81; acc: 0.41
Batch: 400; loss: 1.76; acc: 0.38
Batch: 420; loss: 1.81; acc: 0.42
Batch: 440; loss: 1.86; acc: 0.33
Batch: 460; loss: 1.76; acc: 0.41
Batch: 480; loss: 1.89; acc: 0.36
Batch: 500; loss: 1.78; acc: 0.38
Batch: 520; loss: 1.81; acc: 0.41
Batch: 540; loss: 1.79; acc: 0.34
Batch: 560; loss: 1.99; acc: 0.33
Batch: 580; loss: 1.82; acc: 0.42
Batch: 600; loss: 1.75; acc: 0.34
Batch: 620; loss: 1.75; acc: 0.34
Batch: 640; loss: 1.67; acc: 0.41
Batch: 660; loss: 1.68; acc: 0.39
Batch: 680; loss: 1.86; acc: 0.34
Batch: 700; loss: 1.79; acc: 0.33
Batch: 720; loss: 1.76; acc: 0.34
Batch: 740; loss: 1.67; acc: 0.45
Batch: 760; loss: 2.02; acc: 0.17
Batch: 780; loss: 1.86; acc: 0.34
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.98; acc: 0.36
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.778544892171386; val_accuracy: 0.3622611464968153 

The current subspace-distance is: 8.201676973840222e-05 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.84; acc: 0.38
Batch: 20; loss: 1.9; acc: 0.36
Batch: 40; loss: 1.46; acc: 0.48
Batch: 60; loss: 1.94; acc: 0.31
Batch: 80; loss: 1.92; acc: 0.27
Batch: 100; loss: 1.76; acc: 0.41
Batch: 120; loss: 1.9; acc: 0.3
Batch: 140; loss: 1.78; acc: 0.36
Batch: 160; loss: 1.89; acc: 0.39
Batch: 180; loss: 1.84; acc: 0.31
Batch: 200; loss: 1.79; acc: 0.34
Batch: 220; loss: 1.82; acc: 0.34
Batch: 240; loss: 1.83; acc: 0.34
Batch: 260; loss: 1.62; acc: 0.42
Batch: 280; loss: 1.85; acc: 0.31
Batch: 300; loss: 1.8; acc: 0.45
Batch: 320; loss: 1.75; acc: 0.34
Batch: 340; loss: 1.85; acc: 0.38
Batch: 360; loss: 1.95; acc: 0.3
Batch: 380; loss: 1.85; acc: 0.33
Batch: 400; loss: 1.79; acc: 0.38
Batch: 420; loss: 1.78; acc: 0.42
Batch: 440; loss: 1.83; acc: 0.39
Batch: 460; loss: 1.85; acc: 0.31
Batch: 480; loss: 1.63; acc: 0.42
Batch: 500; loss: 1.7; acc: 0.41
Batch: 520; loss: 1.72; acc: 0.42
Batch: 540; loss: 1.84; acc: 0.3
Batch: 560; loss: 1.91; acc: 0.3
Batch: 580; loss: 1.74; acc: 0.42
Batch: 600; loss: 1.69; acc: 0.44
Batch: 620; loss: 2.05; acc: 0.22
Batch: 640; loss: 1.76; acc: 0.38
Batch: 660; loss: 1.77; acc: 0.34
Batch: 680; loss: 1.83; acc: 0.38
Batch: 700; loss: 1.77; acc: 0.31
Batch: 720; loss: 1.84; acc: 0.36
Batch: 740; loss: 1.94; acc: 0.27
Batch: 760; loss: 1.88; acc: 0.38
Batch: 780; loss: 1.76; acc: 0.38
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.38
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.7786430765868753; val_accuracy: 0.3613654458598726 

The current subspace-distance is: 8.364867971977219e-05 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.89; acc: 0.34
Batch: 20; loss: 1.84; acc: 0.3
Batch: 40; loss: 2.0; acc: 0.27
Batch: 60; loss: 1.61; acc: 0.48
Batch: 80; loss: 1.81; acc: 0.33
Batch: 100; loss: 1.81; acc: 0.36
Batch: 120; loss: 1.6; acc: 0.42
Batch: 140; loss: 1.75; acc: 0.45
Batch: 160; loss: 1.69; acc: 0.47
Batch: 180; loss: 2.01; acc: 0.3
Batch: 200; loss: 1.72; acc: 0.38
Batch: 220; loss: 2.16; acc: 0.19
Batch: 240; loss: 1.9; acc: 0.28
Batch: 260; loss: 1.73; acc: 0.42
Batch: 280; loss: 1.81; acc: 0.38
Batch: 300; loss: 1.84; acc: 0.41
Batch: 320; loss: 1.95; acc: 0.36
Batch: 340; loss: 1.91; acc: 0.31
Batch: 360; loss: 1.8; acc: 0.33
Batch: 380; loss: 1.81; acc: 0.33
Batch: 400; loss: 1.82; acc: 0.34
Batch: 420; loss: 1.89; acc: 0.34
Batch: 440; loss: 1.67; acc: 0.52
Batch: 460; loss: 1.86; acc: 0.33
Batch: 480; loss: 1.87; acc: 0.33
Batch: 500; loss: 1.73; acc: 0.31
Batch: 520; loss: 1.83; acc: 0.34
Batch: 540; loss: 1.89; acc: 0.3
Batch: 560; loss: 1.69; acc: 0.38
Batch: 580; loss: 1.85; acc: 0.36
Batch: 600; loss: 1.87; acc: 0.33
Batch: 620; loss: 1.84; acc: 0.34
Batch: 640; loss: 1.95; acc: 0.28
Batch: 660; loss: 1.75; acc: 0.41
Batch: 680; loss: 1.67; acc: 0.53
Batch: 700; loss: 1.65; acc: 0.42
Batch: 720; loss: 1.77; acc: 0.3
Batch: 740; loss: 1.85; acc: 0.34
Batch: 760; loss: 1.69; acc: 0.38
Batch: 780; loss: 1.79; acc: 0.45
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.39
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.52
Batch: 60; loss: 1.55; acc: 0.53
Batch: 80; loss: 1.67; acc: 0.39
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.77871828322198; val_accuracy: 0.36216162420382164 

The current subspace-distance is: 9.041213343152776e-05 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.74; acc: 0.42
Batch: 20; loss: 1.62; acc: 0.39
Batch: 40; loss: 1.69; acc: 0.34
Batch: 60; loss: 1.79; acc: 0.42
Batch: 80; loss: 1.94; acc: 0.34
Batch: 100; loss: 1.84; acc: 0.33
Batch: 120; loss: 1.86; acc: 0.23
Batch: 140; loss: 1.82; acc: 0.28
Batch: 160; loss: 1.9; acc: 0.28
Batch: 180; loss: 1.88; acc: 0.3
Batch: 200; loss: 1.8; acc: 0.34
Batch: 220; loss: 1.69; acc: 0.42
Batch: 240; loss: 1.84; acc: 0.25
Batch: 260; loss: 1.89; acc: 0.3
Batch: 280; loss: 1.73; acc: 0.38
Batch: 300; loss: 1.8; acc: 0.39
Batch: 320; loss: 1.75; acc: 0.36
Batch: 340; loss: 1.93; acc: 0.38
Batch: 360; loss: 1.65; acc: 0.42
Batch: 380; loss: 1.81; acc: 0.38
Batch: 400; loss: 1.75; acc: 0.36
Batch: 420; loss: 1.73; acc: 0.39
Batch: 440; loss: 1.64; acc: 0.38
Batch: 460; loss: 1.73; acc: 0.44
Batch: 480; loss: 1.94; acc: 0.27
Batch: 500; loss: 1.78; acc: 0.33
Batch: 520; loss: 1.87; acc: 0.38
Batch: 540; loss: 1.74; acc: 0.31
Batch: 560; loss: 1.82; acc: 0.33
Batch: 580; loss: 1.97; acc: 0.27
Batch: 600; loss: 1.69; acc: 0.39
Batch: 620; loss: 1.84; acc: 0.3
Batch: 640; loss: 1.82; acc: 0.41
Batch: 660; loss: 1.9; acc: 0.3
Batch: 680; loss: 1.82; acc: 0.33
Batch: 700; loss: 1.75; acc: 0.41
Batch: 720; loss: 2.11; acc: 0.34
Batch: 740; loss: 1.82; acc: 0.31
Batch: 760; loss: 1.85; acc: 0.36
Batch: 780; loss: 1.82; acc: 0.36
Train Epoch over. train_loss: 1.8; train_accuracy: 0.36 

Batch: 0; loss: 1.76; acc: 0.41
Batch: 20; loss: 2.01; acc: 0.27
Batch: 40; loss: 1.51; acc: 0.5
Batch: 60; loss: 1.55; acc: 0.52
Batch: 80; loss: 1.67; acc: 0.38
Batch: 100; loss: 1.95; acc: 0.34
Batch: 120; loss: 1.99; acc: 0.34
Batch: 140; loss: 1.67; acc: 0.38
Val Epoch over. val_loss: 1.778611847549487; val_accuracy: 0.36176353503184716 

The current subspace-distance is: 9.299478551838547e-05 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_25_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 10631
elements in E: 2221300
fraction nonzero: 0.004785936163507856
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.08
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.12
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.32; acc: 0.05
Batch: 140; loss: 2.3; acc: 0.14
Batch: 160; loss: 2.3; acc: 0.09
Batch: 180; loss: 2.32; acc: 0.09
Batch: 200; loss: 2.31; acc: 0.06
Batch: 220; loss: 2.3; acc: 0.12
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.28; acc: 0.17
Batch: 320; loss: 2.31; acc: 0.09
Batch: 340; loss: 2.29; acc: 0.17
Batch: 360; loss: 2.28; acc: 0.08
Batch: 380; loss: 2.29; acc: 0.12
Batch: 400; loss: 2.3; acc: 0.12
Batch: 420; loss: 2.28; acc: 0.19
Batch: 440; loss: 2.29; acc: 0.12
Batch: 460; loss: 2.29; acc: 0.03
Batch: 480; loss: 2.29; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.06
Batch: 520; loss: 2.31; acc: 0.06
Batch: 540; loss: 2.29; acc: 0.09
Batch: 560; loss: 2.29; acc: 0.05
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.08
Batch: 620; loss: 2.29; acc: 0.11
Batch: 640; loss: 2.29; acc: 0.11
Batch: 660; loss: 2.26; acc: 0.22
Batch: 680; loss: 2.29; acc: 0.06
Batch: 700; loss: 2.29; acc: 0.14
Batch: 720; loss: 2.28; acc: 0.08
Batch: 740; loss: 2.3; acc: 0.06
Batch: 760; loss: 2.28; acc: 0.08
Batch: 780; loss: 2.28; acc: 0.08
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.28; acc: 0.06
Batch: 20; loss: 2.27; acc: 0.06
Batch: 40; loss: 2.28; acc: 0.08
Batch: 60; loss: 2.27; acc: 0.14
Batch: 80; loss: 2.27; acc: 0.11
Batch: 100; loss: 2.3; acc: 0.03
Batch: 120; loss: 2.28; acc: 0.08
Batch: 140; loss: 2.27; acc: 0.09
Val Epoch over. val_loss: 2.278381419029965; val_accuracy: 0.09106289808917198 

The current subspace-distance is: 3.3837554838100914e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.27; acc: 0.14
Batch: 20; loss: 2.27; acc: 0.08
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.12
Batch: 80; loss: 2.28; acc: 0.03
Batch: 100; loss: 2.28; acc: 0.09
Batch: 120; loss: 2.27; acc: 0.12
Batch: 140; loss: 2.28; acc: 0.09
Batch: 160; loss: 2.26; acc: 0.14
Batch: 180; loss: 2.27; acc: 0.06
Batch: 200; loss: 2.25; acc: 0.16
Batch: 220; loss: 2.26; acc: 0.16
Batch: 240; loss: 2.27; acc: 0.09
Batch: 260; loss: 2.26; acc: 0.08
Batch: 280; loss: 2.27; acc: 0.09
Batch: 300; loss: 2.26; acc: 0.09
Batch: 320; loss: 2.28; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.12
Batch: 360; loss: 2.27; acc: 0.14
Batch: 380; loss: 2.24; acc: 0.19
Batch: 400; loss: 2.26; acc: 0.17
Batch: 420; loss: 2.26; acc: 0.11
Batch: 440; loss: 2.23; acc: 0.22
Batch: 460; loss: 2.23; acc: 0.19
Batch: 480; loss: 2.26; acc: 0.14
Batch: 500; loss: 2.22; acc: 0.27
Batch: 520; loss: 2.19; acc: 0.28
Batch: 540; loss: 2.25; acc: 0.23
Batch: 560; loss: 2.23; acc: 0.23
Batch: 580; loss: 2.21; acc: 0.25
Batch: 600; loss: 2.23; acc: 0.23
Batch: 620; loss: 2.21; acc: 0.25
Batch: 640; loss: 2.23; acc: 0.19
Batch: 660; loss: 2.21; acc: 0.2
Batch: 680; loss: 2.26; acc: 0.14
Batch: 700; loss: 2.18; acc: 0.3
Batch: 720; loss: 2.17; acc: 0.23
Batch: 740; loss: 2.18; acc: 0.28
Batch: 760; loss: 2.14; acc: 0.27
Batch: 780; loss: 2.23; acc: 0.16
Train Epoch over. train_loss: 2.24; train_accuracy: 0.15 

Batch: 0; loss: 2.14; acc: 0.3
Batch: 20; loss: 2.1; acc: 0.41
Batch: 40; loss: 2.09; acc: 0.36
Batch: 60; loss: 2.1; acc: 0.38
Batch: 80; loss: 2.13; acc: 0.3
Batch: 100; loss: 2.16; acc: 0.3
Batch: 120; loss: 2.15; acc: 0.36
Batch: 140; loss: 2.15; acc: 0.3
Val Epoch over. val_loss: 2.1572714444178684; val_accuracy: 0.25865843949044587 

The current subspace-distance is: 6.865755040053045e-06 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.16; acc: 0.2
Batch: 20; loss: 2.15; acc: 0.25
Batch: 40; loss: 2.11; acc: 0.27
Batch: 60; loss: 2.09; acc: 0.31
Batch: 80; loss: 2.1; acc: 0.31
Batch: 100; loss: 2.12; acc: 0.23
Batch: 120; loss: 2.15; acc: 0.25
Batch: 140; loss: 2.08; acc: 0.27
Batch: 160; loss: 2.1; acc: 0.23
Batch: 180; loss: 2.01; acc: 0.33
Batch: 200; loss: 1.94; acc: 0.28
Batch: 220; loss: 2.07; acc: 0.25
Batch: 240; loss: 2.01; acc: 0.36
Batch: 260; loss: 1.96; acc: 0.36
Batch: 280; loss: 1.85; acc: 0.47
Batch: 300; loss: 1.79; acc: 0.41
Batch: 320; loss: 1.93; acc: 0.38
Batch: 340; loss: 1.9; acc: 0.33
Batch: 360; loss: 1.86; acc: 0.33
Batch: 380; loss: 1.84; acc: 0.44
Batch: 400; loss: 1.66; acc: 0.44
Batch: 420; loss: 1.71; acc: 0.47
Batch: 440; loss: 1.78; acc: 0.42
Batch: 460; loss: 1.83; acc: 0.3
Batch: 480; loss: 1.87; acc: 0.33
Batch: 500; loss: 1.71; acc: 0.52
Batch: 520; loss: 1.62; acc: 0.47
Batch: 540; loss: 1.59; acc: 0.44
Batch: 560; loss: 1.65; acc: 0.44
Batch: 580; loss: 1.67; acc: 0.45
Batch: 600; loss: 1.73; acc: 0.41
Batch: 620; loss: 1.83; acc: 0.36
Batch: 640; loss: 1.85; acc: 0.44
Batch: 660; loss: 1.81; acc: 0.42
Batch: 680; loss: 1.57; acc: 0.48
Batch: 700; loss: 1.61; acc: 0.5
Batch: 720; loss: 1.45; acc: 0.48
Batch: 740; loss: 1.54; acc: 0.41
Batch: 760; loss: 1.71; acc: 0.34
Batch: 780; loss: 1.58; acc: 0.44
Train Epoch over. train_loss: 1.84; train_accuracy: 0.38 

Batch: 0; loss: 1.64; acc: 0.41
Batch: 20; loss: 1.51; acc: 0.39
Batch: 40; loss: 1.1; acc: 0.66
Batch: 60; loss: 1.49; acc: 0.55
Batch: 80; loss: 1.39; acc: 0.62
Batch: 100; loss: 1.59; acc: 0.45
Batch: 120; loss: 1.64; acc: 0.45
Batch: 140; loss: 1.36; acc: 0.56
Val Epoch over. val_loss: 1.5983811237250165; val_accuracy: 0.46875 

The current subspace-distance is: 1.4072016710997559e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.58
Batch: 20; loss: 1.91; acc: 0.36
Batch: 40; loss: 1.54; acc: 0.53
Batch: 60; loss: 1.47; acc: 0.48
Batch: 80; loss: 1.67; acc: 0.5
Batch: 100; loss: 1.36; acc: 0.44
Batch: 120; loss: 1.57; acc: 0.38
Batch: 140; loss: 1.42; acc: 0.56
Batch: 160; loss: 1.69; acc: 0.48
Batch: 180; loss: 1.44; acc: 0.53
Batch: 200; loss: 1.62; acc: 0.47
Batch: 220; loss: 1.45; acc: 0.58
Batch: 240; loss: 1.59; acc: 0.42
Batch: 260; loss: 1.33; acc: 0.53
Batch: 280; loss: 1.57; acc: 0.47
Batch: 300; loss: 1.41; acc: 0.5
Batch: 320; loss: 1.44; acc: 0.55
Batch: 340; loss: 1.77; acc: 0.41
Batch: 360; loss: 1.37; acc: 0.5
Batch: 380; loss: 1.68; acc: 0.38
Batch: 400; loss: 1.83; acc: 0.44
Batch: 420; loss: 1.84; acc: 0.33
Batch: 440; loss: 1.35; acc: 0.47
Batch: 460; loss: 1.65; acc: 0.53
Batch: 480; loss: 1.39; acc: 0.52
Batch: 500; loss: 1.43; acc: 0.52
Batch: 520; loss: 1.35; acc: 0.58
Batch: 540; loss: 1.5; acc: 0.52
Batch: 560; loss: 1.87; acc: 0.39
Batch: 580; loss: 1.6; acc: 0.42
Batch: 600; loss: 1.53; acc: 0.52
Batch: 620; loss: 1.52; acc: 0.58
Batch: 640; loss: 1.52; acc: 0.44
Batch: 660; loss: 1.82; acc: 0.34
Batch: 680; loss: 1.65; acc: 0.41
Batch: 700; loss: 1.4; acc: 0.5
Batch: 720; loss: 1.48; acc: 0.53
Batch: 740; loss: 1.48; acc: 0.5
Batch: 760; loss: 1.6; acc: 0.5
Batch: 780; loss: 1.48; acc: 0.5
Train Epoch over. train_loss: 1.56; train_accuracy: 0.47 

Batch: 0; loss: 1.73; acc: 0.42
Batch: 20; loss: 1.59; acc: 0.41
Batch: 40; loss: 1.03; acc: 0.64
Batch: 60; loss: 1.37; acc: 0.59
Batch: 80; loss: 1.25; acc: 0.58
Batch: 100; loss: 1.68; acc: 0.44
Batch: 120; loss: 1.68; acc: 0.48
Batch: 140; loss: 1.23; acc: 0.53
Val Epoch over. val_loss: 1.5388339735140466; val_accuracy: 0.45790207006369427 

The current subspace-distance is: 1.9363158571650274e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.67; acc: 0.44
Batch: 20; loss: 1.58; acc: 0.53
Batch: 40; loss: 1.44; acc: 0.48
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.76; acc: 0.39
Batch: 100; loss: 1.49; acc: 0.52
Batch: 120; loss: 1.76; acc: 0.36
Batch: 140; loss: 1.38; acc: 0.5
Batch: 160; loss: 1.3; acc: 0.52
Batch: 180; loss: 1.37; acc: 0.56
Batch: 200; loss: 1.42; acc: 0.44
Batch: 220; loss: 1.65; acc: 0.38
Batch: 240; loss: 1.39; acc: 0.52
Batch: 260; loss: 1.64; acc: 0.47
Batch: 280; loss: 1.51; acc: 0.53
Batch: 300; loss: 1.44; acc: 0.47
Batch: 320; loss: 1.5; acc: 0.45
Batch: 340; loss: 1.52; acc: 0.5
Batch: 360; loss: 1.7; acc: 0.42
Batch: 380; loss: 1.38; acc: 0.52
Batch: 400; loss: 1.36; acc: 0.62
Batch: 420; loss: 1.47; acc: 0.56
Batch: 440; loss: 1.61; acc: 0.47
Batch: 460; loss: 1.47; acc: 0.5
Batch: 480; loss: 1.55; acc: 0.53
Batch: 500; loss: 1.44; acc: 0.52
Batch: 520; loss: 1.33; acc: 0.61
Batch: 540; loss: 1.41; acc: 0.61
Batch: 560; loss: 1.54; acc: 0.47
Batch: 580; loss: 1.61; acc: 0.56
Batch: 600; loss: 1.42; acc: 0.52
Batch: 620; loss: 1.46; acc: 0.41
Batch: 640; loss: 1.58; acc: 0.56
Batch: 660; loss: 1.87; acc: 0.41
Batch: 680; loss: 1.41; acc: 0.55
Batch: 700; loss: 1.66; acc: 0.48
Batch: 720; loss: 1.7; acc: 0.45
Batch: 740; loss: 1.86; acc: 0.36
Batch: 760; loss: 1.29; acc: 0.58
Batch: 780; loss: 1.76; acc: 0.41
Train Epoch over. train_loss: 1.52; train_accuracy: 0.48 

Batch: 0; loss: 1.69; acc: 0.41
Batch: 20; loss: 1.62; acc: 0.41
Batch: 40; loss: 0.97; acc: 0.67
Batch: 60; loss: 1.33; acc: 0.56
Batch: 80; loss: 1.19; acc: 0.58
Batch: 100; loss: 1.64; acc: 0.5
Batch: 120; loss: 1.62; acc: 0.52
Batch: 140; loss: 1.12; acc: 0.72
Val Epoch over. val_loss: 1.5059748329934042; val_accuracy: 0.4864649681528662 

The current subspace-distance is: 2.414924165350385e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.52; acc: 0.47
Batch: 20; loss: 1.56; acc: 0.38
Batch: 40; loss: 1.72; acc: 0.36
Batch: 60; loss: 1.29; acc: 0.56
Batch: 80; loss: 1.81; acc: 0.34
Batch: 100; loss: 1.5; acc: 0.45
Batch: 120; loss: 1.74; acc: 0.39
Batch: 140; loss: 1.47; acc: 0.58
Batch: 160; loss: 1.46; acc: 0.53
Batch: 180; loss: 1.54; acc: 0.47
Batch: 200; loss: 1.47; acc: 0.5
Batch: 220; loss: 1.67; acc: 0.45
Batch: 240; loss: 1.59; acc: 0.45
Batch: 260; loss: 1.48; acc: 0.52
Batch: 280; loss: 1.49; acc: 0.52
Batch: 300; loss: 1.65; acc: 0.48
Batch: 320; loss: 1.79; acc: 0.38
Batch: 340; loss: 1.51; acc: 0.47
Batch: 360; loss: 1.39; acc: 0.5
Batch: 380; loss: 1.47; acc: 0.58
Batch: 400; loss: 1.61; acc: 0.41
Batch: 420; loss: 1.44; acc: 0.61
Batch: 440; loss: 1.55; acc: 0.45
Batch: 460; loss: 1.32; acc: 0.58
Batch: 480; loss: 1.62; acc: 0.47
Batch: 500; loss: 1.44; acc: 0.56
Batch: 520; loss: 1.4; acc: 0.52
Batch: 540; loss: 1.69; acc: 0.48
Batch: 560; loss: 1.56; acc: 0.47
Batch: 580; loss: 1.37; acc: 0.5
Batch: 600; loss: 1.52; acc: 0.48
Batch: 620; loss: 1.67; acc: 0.48
Batch: 640; loss: 1.39; acc: 0.55
Batch: 660; loss: 1.38; acc: 0.52
Batch: 680; loss: 1.34; acc: 0.56
Batch: 700; loss: 1.71; acc: 0.44
Batch: 720; loss: 1.34; acc: 0.52
Batch: 740; loss: 1.45; acc: 0.5
Batch: 760; loss: 1.39; acc: 0.56
Batch: 780; loss: 1.4; acc: 0.48
Train Epoch over. train_loss: 1.52; train_accuracy: 0.49 

Batch: 0; loss: 1.7; acc: 0.42
Batch: 20; loss: 1.52; acc: 0.48
Batch: 40; loss: 0.99; acc: 0.67
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.21; acc: 0.58
Batch: 100; loss: 1.58; acc: 0.53
Batch: 120; loss: 1.66; acc: 0.48
Batch: 140; loss: 1.13; acc: 0.62
Val Epoch over. val_loss: 1.5049243478258705; val_accuracy: 0.4853702229299363 

The current subspace-distance is: 2.6710858946898952e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.55
Batch: 20; loss: 1.48; acc: 0.47
Batch: 40; loss: 1.66; acc: 0.38
Batch: 60; loss: 1.38; acc: 0.55
Batch: 80; loss: 1.45; acc: 0.53
Batch: 100; loss: 1.53; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.45
Batch: 140; loss: 1.4; acc: 0.59
Batch: 160; loss: 1.34; acc: 0.53
Batch: 180; loss: 1.36; acc: 0.56
Batch: 200; loss: 1.62; acc: 0.44
Batch: 220; loss: 1.72; acc: 0.36
Batch: 240; loss: 1.46; acc: 0.39
Batch: 260; loss: 1.51; acc: 0.45
Batch: 280; loss: 1.37; acc: 0.52
Batch: 300; loss: 1.46; acc: 0.52
Batch: 320; loss: 1.56; acc: 0.48
Batch: 340; loss: 1.5; acc: 0.42
Batch: 360; loss: 1.49; acc: 0.48
Batch: 380; loss: 1.46; acc: 0.53
Batch: 400; loss: 1.46; acc: 0.52
Batch: 420; loss: 1.4; acc: 0.53
Batch: 440; loss: 1.2; acc: 0.67
Batch: 460; loss: 1.64; acc: 0.48
Batch: 480; loss: 1.61; acc: 0.48
Batch: 500; loss: 1.9; acc: 0.42
Batch: 520; loss: 1.36; acc: 0.52
Batch: 540; loss: 1.58; acc: 0.44
Batch: 560; loss: 1.6; acc: 0.42
Batch: 580; loss: 1.65; acc: 0.42
Batch: 600; loss: 1.58; acc: 0.48
Batch: 620; loss: 1.6; acc: 0.45
Batch: 640; loss: 1.52; acc: 0.5
Batch: 660; loss: 1.54; acc: 0.47
Batch: 680; loss: 1.48; acc: 0.52
Batch: 700; loss: 1.4; acc: 0.55
Batch: 720; loss: 1.47; acc: 0.5
Batch: 740; loss: 1.65; acc: 0.48
Batch: 760; loss: 1.34; acc: 0.53
Batch: 780; loss: 1.48; acc: 0.48
Train Epoch over. train_loss: 1.51; train_accuracy: 0.48 

Batch: 0; loss: 1.64; acc: 0.39
Batch: 20; loss: 1.5; acc: 0.48
Batch: 40; loss: 0.94; acc: 0.67
Batch: 60; loss: 1.38; acc: 0.56
Batch: 80; loss: 1.2; acc: 0.61
Batch: 100; loss: 1.42; acc: 0.5
Batch: 120; loss: 1.66; acc: 0.48
Batch: 140; loss: 1.07; acc: 0.67
Val Epoch over. val_loss: 1.4920064822124068; val_accuracy: 0.5035828025477707 

The current subspace-distance is: 3.206663313903846e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.48
Batch: 20; loss: 1.46; acc: 0.61
Batch: 40; loss: 1.54; acc: 0.48
Batch: 60; loss: 1.4; acc: 0.5
Batch: 80; loss: 1.48; acc: 0.44
Batch: 100; loss: 1.38; acc: 0.52
Batch: 120; loss: 1.49; acc: 0.45
Batch: 140; loss: 1.41; acc: 0.5
Batch: 160; loss: 1.41; acc: 0.47
Batch: 180; loss: 1.7; acc: 0.44
Batch: 200; loss: 1.35; acc: 0.52
Batch: 220; loss: 1.35; acc: 0.53
Batch: 240; loss: 1.64; acc: 0.39
Batch: 260; loss: 1.47; acc: 0.48
Batch: 280; loss: 1.43; acc: 0.52
Batch: 300; loss: 1.48; acc: 0.5
Batch: 320; loss: 1.52; acc: 0.39
Batch: 340; loss: 1.51; acc: 0.48
Batch: 360; loss: 1.33; acc: 0.58
Batch: 380; loss: 1.51; acc: 0.48
Batch: 400; loss: 1.34; acc: 0.55
Batch: 420; loss: 1.32; acc: 0.59
Batch: 440; loss: 1.48; acc: 0.5
Batch: 460; loss: 1.47; acc: 0.45
Batch: 480; loss: 1.36; acc: 0.52
Batch: 500; loss: 1.96; acc: 0.36
Batch: 520; loss: 1.51; acc: 0.5
Batch: 540; loss: 1.48; acc: 0.52
Batch: 560; loss: 1.42; acc: 0.53
Batch: 580; loss: 1.39; acc: 0.53
Batch: 600; loss: 1.19; acc: 0.62
Batch: 620; loss: 1.15; acc: 0.56
Batch: 640; loss: 1.41; acc: 0.59
Batch: 660; loss: 1.6; acc: 0.56
Batch: 680; loss: 1.46; acc: 0.34
Batch: 700; loss: 1.6; acc: 0.53
Batch: 720; loss: 1.39; acc: 0.48
Batch: 740; loss: 1.42; acc: 0.5
Batch: 760; loss: 1.43; acc: 0.55
Batch: 780; loss: 1.26; acc: 0.56
Train Epoch over. train_loss: 1.45; train_accuracy: 0.51 

Batch: 0; loss: 1.52; acc: 0.42
Batch: 20; loss: 1.49; acc: 0.5
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 1.37; acc: 0.56
Batch: 80; loss: 1.05; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.53
Batch: 120; loss: 1.61; acc: 0.52
Batch: 140; loss: 0.99; acc: 0.66
Val Epoch over. val_loss: 1.3925149881156387; val_accuracy: 0.5338375796178344 

The current subspace-distance is: 3.4850367228500545e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.38; acc: 0.53
Batch: 20; loss: 1.38; acc: 0.59
Batch: 40; loss: 1.64; acc: 0.41
Batch: 60; loss: 1.66; acc: 0.41
Batch: 80; loss: 1.58; acc: 0.56
Batch: 100; loss: 1.49; acc: 0.47
Batch: 120; loss: 1.39; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.53
Batch: 160; loss: 1.45; acc: 0.52
Batch: 180; loss: 1.52; acc: 0.47
Batch: 200; loss: 1.7; acc: 0.41
Batch: 220; loss: 1.37; acc: 0.56
Batch: 240; loss: 1.33; acc: 0.48
Batch: 260; loss: 1.52; acc: 0.58
Batch: 280; loss: 1.5; acc: 0.48
Batch: 300; loss: 1.16; acc: 0.59
Batch: 320; loss: 1.59; acc: 0.5
Batch: 340; loss: 1.5; acc: 0.52
Batch: 360; loss: 1.71; acc: 0.44
Batch: 380; loss: 1.16; acc: 0.66
Batch: 400; loss: 1.42; acc: 0.5
Batch: 420; loss: 1.36; acc: 0.53
Batch: 440; loss: 1.48; acc: 0.55
Batch: 460; loss: 1.24; acc: 0.66
Batch: 480; loss: 1.35; acc: 0.56
Batch: 500; loss: 1.43; acc: 0.53
Batch: 520; loss: 1.33; acc: 0.56
Batch: 540; loss: 1.43; acc: 0.53
Batch: 560; loss: 1.5; acc: 0.44
Batch: 580; loss: 1.29; acc: 0.55
Batch: 600; loss: 1.49; acc: 0.52
Batch: 620; loss: 1.27; acc: 0.55
Batch: 640; loss: 1.37; acc: 0.55
Batch: 660; loss: 1.25; acc: 0.5
Batch: 680; loss: 1.68; acc: 0.39
Batch: 700; loss: 1.29; acc: 0.53
Batch: 720; loss: 1.34; acc: 0.58
Batch: 740; loss: 1.39; acc: 0.53
Batch: 760; loss: 1.28; acc: 0.62
Batch: 780; loss: 1.32; acc: 0.59
Train Epoch over. train_loss: 1.41; train_accuracy: 0.52 

Batch: 0; loss: 1.4; acc: 0.48
Batch: 20; loss: 1.5; acc: 0.47
Batch: 40; loss: 0.98; acc: 0.72
Batch: 60; loss: 1.39; acc: 0.55
Batch: 80; loss: 1.06; acc: 0.61
Batch: 100; loss: 1.41; acc: 0.55
Batch: 120; loss: 1.5; acc: 0.58
Batch: 140; loss: 0.98; acc: 0.66
Val Epoch over. val_loss: 1.3772160342544506; val_accuracy: 0.5402070063694268 

The current subspace-distance is: 3.727290823007934e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.47; acc: 0.52
Batch: 20; loss: 1.36; acc: 0.48
Batch: 40; loss: 1.43; acc: 0.55
Batch: 60; loss: 1.31; acc: 0.52
Batch: 80; loss: 1.58; acc: 0.34
Batch: 100; loss: 1.19; acc: 0.61
Batch: 120; loss: 1.55; acc: 0.58
Batch: 140; loss: 1.32; acc: 0.5
Batch: 160; loss: 1.53; acc: 0.53
Batch: 180; loss: 1.54; acc: 0.5
Batch: 200; loss: 1.27; acc: 0.56
Batch: 220; loss: 1.38; acc: 0.55
Batch: 240; loss: 1.6; acc: 0.55
Batch: 260; loss: 1.43; acc: 0.52
Batch: 280; loss: 1.37; acc: 0.58
Batch: 300; loss: 1.21; acc: 0.5
Batch: 320; loss: 1.7; acc: 0.41
Batch: 340; loss: 1.49; acc: 0.58
Batch: 360; loss: 1.36; acc: 0.52
Batch: 380; loss: 1.27; acc: 0.52
Batch: 400; loss: 1.38; acc: 0.52
Batch: 420; loss: 1.33; acc: 0.53
Batch: 440; loss: 1.45; acc: 0.5
Batch: 460; loss: 1.43; acc: 0.52
Batch: 480; loss: 1.41; acc: 0.55
Batch: 500; loss: 1.4; acc: 0.52
Batch: 520; loss: 1.52; acc: 0.48
Batch: 540; loss: 1.33; acc: 0.53
Batch: 560; loss: 1.37; acc: 0.58
Batch: 580; loss: 1.75; acc: 0.39
Batch: 600; loss: 1.39; acc: 0.56
Batch: 620; loss: 1.38; acc: 0.52
Batch: 640; loss: 1.38; acc: 0.52
Batch: 660; loss: 1.43; acc: 0.5
Batch: 680; loss: 1.35; acc: 0.48
Batch: 700; loss: 1.36; acc: 0.45
Batch: 720; loss: 1.32; acc: 0.58
Batch: 740; loss: 1.31; acc: 0.5
Batch: 760; loss: 1.36; acc: 0.5
Batch: 780; loss: 1.47; acc: 0.52
Train Epoch over. train_loss: 1.39; train_accuracy: 0.53 

Batch: 0; loss: 1.26; acc: 0.5
Batch: 20; loss: 1.43; acc: 0.45
Batch: 40; loss: 0.91; acc: 0.72
Batch: 60; loss: 1.36; acc: 0.53
Batch: 80; loss: 1.09; acc: 0.62
Batch: 100; loss: 1.39; acc: 0.5
Batch: 120; loss: 1.46; acc: 0.55
Batch: 140; loss: 1.0; acc: 0.64
Val Epoch over. val_loss: 1.3578014309239235; val_accuracy: 0.5429936305732485 

The current subspace-distance is: 4.220215487293899e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.57; acc: 0.48
Batch: 20; loss: 1.31; acc: 0.56
Batch: 40; loss: 1.22; acc: 0.59
Batch: 60; loss: 1.24; acc: 0.55
Batch: 80; loss: 1.39; acc: 0.55
Batch: 100; loss: 1.46; acc: 0.48
Batch: 120; loss: 1.34; acc: 0.53
Batch: 140; loss: 1.31; acc: 0.55
Batch: 160; loss: 1.53; acc: 0.41
Batch: 180; loss: 1.5; acc: 0.55
Batch: 200; loss: 1.24; acc: 0.52
Batch: 220; loss: 1.33; acc: 0.53
Batch: 240; loss: 1.17; acc: 0.56
Batch: 260; loss: 1.26; acc: 0.66
Batch: 280; loss: 1.49; acc: 0.59
Batch: 300; loss: 1.67; acc: 0.44
Batch: 320; loss: 1.46; acc: 0.44
Batch: 340; loss: 1.37; acc: 0.58
Batch: 360; loss: 1.45; acc: 0.53
Batch: 380; loss: 1.21; acc: 0.56
Batch: 400; loss: 1.32; acc: 0.58
Batch: 420; loss: 1.65; acc: 0.48
Batch: 440; loss: 1.37; acc: 0.58
Batch: 460; loss: 1.57; acc: 0.42
Batch: 480; loss: 1.41; acc: 0.52
Batch: 500; loss: 1.45; acc: 0.42
Batch: 520; loss: 1.52; acc: 0.48
Batch: 540; loss: 1.24; acc: 0.58
Batch: 560; loss: 1.42; acc: 0.56
Batch: 580; loss: 1.15; acc: 0.56
Batch: 600; loss: 1.26; acc: 0.56
Batch: 620; loss: 1.38; acc: 0.45
Batch: 640; loss: 1.2; acc: 0.59
Batch: 660; loss: 1.52; acc: 0.5
Batch: 680; loss: 1.3; acc: 0.53
Batch: 700; loss: 1.31; acc: 0.64
Batch: 720; loss: 1.38; acc: 0.56
Batch: 740; loss: 1.55; acc: 0.45
Batch: 760; loss: 1.33; acc: 0.58
Batch: 780; loss: 1.58; acc: 0.52
Train Epoch over. train_loss: 1.38; train_accuracy: 0.53 

Batch: 0; loss: 1.27; acc: 0.48
Batch: 20; loss: 1.44; acc: 0.5
Batch: 40; loss: 0.92; acc: 0.7
Batch: 60; loss: 1.33; acc: 0.56
Batch: 80; loss: 1.06; acc: 0.62
Batch: 100; loss: 1.39; acc: 0.5
Batch: 120; loss: 1.44; acc: 0.45
Batch: 140; loss: 1.02; acc: 0.64
Val Epoch over. val_loss: 1.3443884750840012; val_accuracy: 0.5476711783439491 

The current subspace-distance is: 4.3592339352471754e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.38; acc: 0.48
Batch: 20; loss: 1.46; acc: 0.55
Batch: 40; loss: 1.69; acc: 0.39
Batch: 60; loss: 1.43; acc: 0.48
Batch: 80; loss: 1.34; acc: 0.48
Batch: 100; loss: 1.32; acc: 0.58
Batch: 120; loss: 1.4; acc: 0.5
Batch: 140; loss: 1.33; acc: 0.5
Batch: 160; loss: 1.48; acc: 0.55
Batch: 180; loss: 1.19; acc: 0.61
Batch: 200; loss: 1.49; acc: 0.42
Batch: 220; loss: 1.35; acc: 0.53
Batch: 240; loss: 1.46; acc: 0.55
Batch: 260; loss: 1.45; acc: 0.48
Batch: 280; loss: 1.34; acc: 0.53
Batch: 300; loss: 1.25; acc: 0.52
Batch: 320; loss: 1.44; acc: 0.52
Batch: 340; loss: 1.58; acc: 0.47
Batch: 360; loss: 1.56; acc: 0.48
Batch: 380; loss: 1.6; acc: 0.47
Batch: 400; loss: 1.43; acc: 0.5
Batch: 420; loss: 1.75; acc: 0.41
Batch: 440; loss: 1.19; acc: 0.66
Batch: 460; loss: 1.46; acc: 0.44
Batch: 480; loss: 1.3; acc: 0.52
Batch: 500; loss: 1.28; acc: 0.53
Batch: 520; loss: 1.63; acc: 0.44
Batch: 540; loss: 1.37; acc: 0.48
Batch: 560; loss: 1.45; acc: 0.48
Batch: 580; loss: 1.24; acc: 0.58
Batch: 600; loss: 1.3; acc: 0.64
Batch: 620; loss: 1.31; acc: 0.55
Batch: 640; loss: 1.37; acc: 0.52
Batch: 660; loss: 1.22; acc: 0.61
Batch: 680; loss: 1.4; acc: 0.58
Batch: 700; loss: 1.15; acc: 0.62
Batch: 720; loss: 1.26; acc: 0.53
Batch: 740; loss: 1.65; acc: 0.41
Batch: 760; loss: 1.42; acc: 0.47
Batch: 780; loss: 1.2; acc: 0.61
Train Epoch over. train_loss: 1.37; train_accuracy: 0.53 

Batch: 0; loss: 1.29; acc: 0.5
Batch: 20; loss: 1.42; acc: 0.53
Batch: 40; loss: 0.9; acc: 0.72
Batch: 60; loss: 1.34; acc: 0.61
Batch: 80; loss: 1.03; acc: 0.61
Batch: 100; loss: 1.42; acc: 0.53
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.0; acc: 0.64
Val Epoch over. val_loss: 1.333998659234138; val_accuracy: 0.5519506369426752 

The current subspace-distance is: 4.7189267206704244e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.45; acc: 0.48
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 1.25; acc: 0.67
Batch: 60; loss: 1.33; acc: 0.55
Batch: 80; loss: 1.32; acc: 0.48
Batch: 100; loss: 1.06; acc: 0.66
Batch: 120; loss: 1.37; acc: 0.55
Batch: 140; loss: 1.12; acc: 0.56
Batch: 160; loss: 1.52; acc: 0.5
Batch: 180; loss: 1.58; acc: 0.48
Batch: 200; loss: 0.94; acc: 0.7
Batch: 220; loss: 1.48; acc: 0.45
Batch: 240; loss: 1.38; acc: 0.52
Batch: 260; loss: 1.49; acc: 0.53
Batch: 280; loss: 1.5; acc: 0.53
Batch: 300; loss: 1.41; acc: 0.48
Batch: 320; loss: 1.17; acc: 0.61
Batch: 340; loss: 1.44; acc: 0.52
Batch: 360; loss: 1.23; acc: 0.56
Batch: 380; loss: 1.29; acc: 0.61
Batch: 400; loss: 1.42; acc: 0.55
Batch: 420; loss: 1.31; acc: 0.61
Batch: 440; loss: 1.34; acc: 0.52
Batch: 460; loss: 1.26; acc: 0.62
Batch: 480; loss: 1.62; acc: 0.45
Batch: 500; loss: 1.54; acc: 0.5
Batch: 520; loss: 1.48; acc: 0.55
Batch: 540; loss: 1.4; acc: 0.52
Batch: 560; loss: 1.34; acc: 0.61
Batch: 580; loss: 1.04; acc: 0.67
Batch: 600; loss: 1.43; acc: 0.53
Batch: 620; loss: 1.36; acc: 0.52
Batch: 640; loss: 1.3; acc: 0.55
Batch: 660; loss: 1.52; acc: 0.53
Batch: 680; loss: 1.35; acc: 0.52
Batch: 700; loss: 1.34; acc: 0.5
Batch: 720; loss: 1.52; acc: 0.47
Batch: 740; loss: 1.55; acc: 0.52
Batch: 760; loss: 1.51; acc: 0.53
Batch: 780; loss: 1.5; acc: 0.45
Train Epoch over. train_loss: 1.37; train_accuracy: 0.53 

Batch: 0; loss: 1.27; acc: 0.55
Batch: 20; loss: 1.42; acc: 0.5
Batch: 40; loss: 0.91; acc: 0.75
Batch: 60; loss: 1.33; acc: 0.59
Batch: 80; loss: 1.03; acc: 0.58
Batch: 100; loss: 1.39; acc: 0.53
Batch: 120; loss: 1.42; acc: 0.48
Batch: 140; loss: 1.01; acc: 0.66
Val Epoch over. val_loss: 1.3328502390794694; val_accuracy: 0.5519506369426752 

The current subspace-distance is: 4.8357793275499716e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.33; acc: 0.55
Batch: 20; loss: 1.24; acc: 0.53
Batch: 40; loss: 1.41; acc: 0.44
Batch: 60; loss: 1.31; acc: 0.55
Batch: 80; loss: 1.41; acc: 0.48
Batch: 100; loss: 1.08; acc: 0.64
Batch: 120; loss: 1.36; acc: 0.56
Batch: 140; loss: 1.6; acc: 0.45
Batch: 160; loss: 1.58; acc: 0.55
Batch: 180; loss: 1.3; acc: 0.58
Batch: 200; loss: 1.35; acc: 0.58
Batch: 220; loss: 1.1; acc: 0.62
Batch: 240; loss: 1.26; acc: 0.56
Batch: 260; loss: 1.41; acc: 0.53
Batch: 280; loss: 1.24; acc: 0.61
Batch: 300; loss: 1.33; acc: 0.53
Batch: 320; loss: 1.08; acc: 0.67
Batch: 340; loss: 1.31; acc: 0.5
Batch: 360; loss: 1.26; acc: 0.58
Batch: 380; loss: 1.23; acc: 0.55
Batch: 400; loss: 1.53; acc: 0.52
Batch: 420; loss: 1.29; acc: 0.52
Batch: 440; loss: 1.5; acc: 0.55
Batch: 460; loss: 1.43; acc: 0.55
Batch: 480; loss: 1.28; acc: 0.59
Batch: 500; loss: 1.2; acc: 0.52
Batch: 520; loss: 1.35; acc: 0.56
Batch: 540; loss: 1.06; acc: 0.66
Batch: 560; loss: 1.31; acc: 0.55
Batch: 580; loss: 1.38; acc: 0.55
Batch: 600; loss: 1.36; acc: 0.52
Batch: 620; loss: 1.25; acc: 0.55
Batch: 640; loss: 1.06; acc: 0.67
Batch: 660; loss: 1.01; acc: 0.67
Batch: 680; loss: 1.73; acc: 0.44
Batch: 700; loss: 1.5; acc: 0.47
Batch: 720; loss: 1.17; acc: 0.62
Batch: 740; loss: 1.3; acc: 0.5
Batch: 760; loss: 1.46; acc: 0.52
Batch: 780; loss: 1.51; acc: 0.53
Train Epoch over. train_loss: 1.37; train_accuracy: 0.53 

Batch: 0; loss: 1.29; acc: 0.5
Batch: 20; loss: 1.44; acc: 0.5
Batch: 40; loss: 0.92; acc: 0.75
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.05; acc: 0.61
Batch: 100; loss: 1.39; acc: 0.53
Batch: 120; loss: 1.41; acc: 0.47
Batch: 140; loss: 1.0; acc: 0.67
Val Epoch over. val_loss: 1.336351885537433; val_accuracy: 0.5529458598726115 

The current subspace-distance is: 4.9681122618494555e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.2; acc: 0.56
Batch: 20; loss: 1.24; acc: 0.58
Batch: 40; loss: 1.54; acc: 0.47
Batch: 60; loss: 1.48; acc: 0.52
Batch: 80; loss: 1.43; acc: 0.41
Batch: 100; loss: 1.42; acc: 0.58
Batch: 120; loss: 1.09; acc: 0.66
Batch: 140; loss: 1.42; acc: 0.53
Batch: 160; loss: 1.37; acc: 0.59
Batch: 180; loss: 1.42; acc: 0.53
Batch: 200; loss: 1.31; acc: 0.58
Batch: 220; loss: 1.6; acc: 0.47
Batch: 240; loss: 1.25; acc: 0.42
Batch: 260; loss: 1.14; acc: 0.59
Batch: 280; loss: 1.51; acc: 0.5
Batch: 300; loss: 1.18; acc: 0.59
Batch: 320; loss: 1.33; acc: 0.58
Batch: 340; loss: 1.29; acc: 0.58
Batch: 360; loss: 1.34; acc: 0.52
Batch: 380; loss: 1.17; acc: 0.59
Batch: 400; loss: 1.31; acc: 0.59
Batch: 420; loss: 1.35; acc: 0.53
Batch: 440; loss: 1.5; acc: 0.48
Batch: 460; loss: 1.31; acc: 0.55
Batch: 480; loss: 1.56; acc: 0.44
Batch: 500; loss: 1.18; acc: 0.66
Batch: 520; loss: 1.57; acc: 0.48
Batch: 540; loss: 1.35; acc: 0.5
Batch: 560; loss: 1.16; acc: 0.59
Batch: 580; loss: 1.57; acc: 0.39
Batch: 600; loss: 1.39; acc: 0.5
Batch: 620; loss: 1.28; acc: 0.58
Batch: 640; loss: 1.32; acc: 0.55
Batch: 660; loss: 1.12; acc: 0.58
Batch: 680; loss: 1.57; acc: 0.52
Batch: 700; loss: 1.48; acc: 0.48
Batch: 720; loss: 1.3; acc: 0.55
Batch: 740; loss: 1.33; acc: 0.61
Batch: 760; loss: 1.37; acc: 0.58
Batch: 780; loss: 1.22; acc: 0.58
Train Epoch over. train_loss: 1.37; train_accuracy: 0.53 

Batch: 0; loss: 1.25; acc: 0.5
Batch: 20; loss: 1.43; acc: 0.48
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 1.33; acc: 0.59
Batch: 80; loss: 1.02; acc: 0.58
Batch: 100; loss: 1.36; acc: 0.59
Batch: 120; loss: 1.39; acc: 0.52
Batch: 140; loss: 0.98; acc: 0.67
Val Epoch over. val_loss: 1.3296747841652792; val_accuracy: 0.555234872611465 

The current subspace-distance is: 5.2836094255326316e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.19; acc: 0.55
Batch: 20; loss: 1.5; acc: 0.52
Batch: 40; loss: 1.27; acc: 0.58
Batch: 60; loss: 1.38; acc: 0.55
Batch: 80; loss: 1.27; acc: 0.56
Batch: 100; loss: 1.15; acc: 0.66
Batch: 120; loss: 1.47; acc: 0.52
Batch: 140; loss: 1.5; acc: 0.45
Batch: 160; loss: 1.43; acc: 0.41
Batch: 180; loss: 1.31; acc: 0.55
Batch: 200; loss: 1.28; acc: 0.56
Batch: 220; loss: 1.28; acc: 0.64
Batch: 240; loss: 1.23; acc: 0.58
Batch: 260; loss: 1.32; acc: 0.53
Batch: 280; loss: 1.44; acc: 0.53
Batch: 300; loss: 1.48; acc: 0.52
Batch: 320; loss: 1.52; acc: 0.42
Batch: 340; loss: 1.56; acc: 0.45
Batch: 360; loss: 1.2; acc: 0.58
Batch: 380; loss: 1.44; acc: 0.53
Batch: 400; loss: 1.18; acc: 0.61
Batch: 420; loss: 1.07; acc: 0.69
Batch: 440; loss: 1.59; acc: 0.52
Batch: 460; loss: 1.57; acc: 0.41
Batch: 480; loss: 1.37; acc: 0.59
Batch: 500; loss: 1.42; acc: 0.56
Batch: 520; loss: 1.48; acc: 0.5
Batch: 540; loss: 1.03; acc: 0.66
Batch: 560; loss: 1.26; acc: 0.56
Batch: 580; loss: 1.53; acc: 0.52
Batch: 600; loss: 1.51; acc: 0.52
Batch: 620; loss: 1.29; acc: 0.56
Batch: 640; loss: 1.29; acc: 0.55
Batch: 660; loss: 1.33; acc: 0.58
Batch: 680; loss: 1.38; acc: 0.59
Batch: 700; loss: 1.56; acc: 0.44
Batch: 720; loss: 1.61; acc: 0.48
Batch: 740; loss: 1.24; acc: 0.67
Batch: 760; loss: 1.36; acc: 0.52
Batch: 780; loss: 1.41; acc: 0.44
Train Epoch over. train_loss: 1.37; train_accuracy: 0.53 

Batch: 0; loss: 1.27; acc: 0.5
Batch: 20; loss: 1.43; acc: 0.52
Batch: 40; loss: 0.89; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.53
Batch: 80; loss: 1.02; acc: 0.56
Batch: 100; loss: 1.39; acc: 0.58
Batch: 120; loss: 1.38; acc: 0.55
Batch: 140; loss: 0.99; acc: 0.66
Val Epoch over. val_loss: 1.3286067217018953; val_accuracy: 0.5572253184713376 

The current subspace-distance is: 5.392295497586019e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.15; acc: 0.62
Batch: 20; loss: 1.34; acc: 0.53
Batch: 40; loss: 1.35; acc: 0.59
Batch: 60; loss: 1.38; acc: 0.47
Batch: 80; loss: 1.37; acc: 0.48
Batch: 100; loss: 1.74; acc: 0.28
Batch: 120; loss: 1.24; acc: 0.53
Batch: 140; loss: 1.45; acc: 0.62
Batch: 160; loss: 1.24; acc: 0.53
Batch: 180; loss: 1.59; acc: 0.5
Batch: 200; loss: 1.13; acc: 0.62
Batch: 220; loss: 1.6; acc: 0.45
Batch: 240; loss: 1.29; acc: 0.55
Batch: 260; loss: 1.32; acc: 0.59
Batch: 280; loss: 1.36; acc: 0.56
Batch: 300; loss: 1.29; acc: 0.56
Batch: 320; loss: 1.21; acc: 0.58
Batch: 340; loss: 1.46; acc: 0.52
Batch: 360; loss: 1.59; acc: 0.45
Batch: 380; loss: 1.47; acc: 0.55
Batch: 400; loss: 1.29; acc: 0.5
Batch: 420; loss: 1.16; acc: 0.64
Batch: 440; loss: 1.16; acc: 0.59
Batch: 460; loss: 1.43; acc: 0.45
Batch: 480; loss: 1.19; acc: 0.59
Batch: 500; loss: 1.24; acc: 0.61
Batch: 520; loss: 1.41; acc: 0.48
Batch: 540; loss: 1.32; acc: 0.61
Batch: 560; loss: 1.23; acc: 0.61
Batch: 580; loss: 1.29; acc: 0.53
Batch: 600; loss: 1.28; acc: 0.59
Batch: 620; loss: 1.34; acc: 0.55
Batch: 640; loss: 1.53; acc: 0.48
Batch: 660; loss: 1.27; acc: 0.58
Batch: 680; loss: 1.51; acc: 0.47
Batch: 700; loss: 1.4; acc: 0.52
Batch: 720; loss: 1.38; acc: 0.47
Batch: 740; loss: 1.28; acc: 0.55
Batch: 760; loss: 1.39; acc: 0.59
Batch: 780; loss: 1.35; acc: 0.61
Train Epoch over. train_loss: 1.37; train_accuracy: 0.53 

Batch: 0; loss: 1.26; acc: 0.55
Batch: 20; loss: 1.41; acc: 0.53
Batch: 40; loss: 0.9; acc: 0.75
Batch: 60; loss: 1.35; acc: 0.59
Batch: 80; loss: 1.04; acc: 0.61
Batch: 100; loss: 1.38; acc: 0.56
Batch: 120; loss: 1.39; acc: 0.5
Batch: 140; loss: 0.98; acc: 0.67
Val Epoch over. val_loss: 1.3301391905280435; val_accuracy: 0.5571257961783439 

The current subspace-distance is: 5.500421320903115e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.64; acc: 0.47
Batch: 20; loss: 1.31; acc: 0.52
Batch: 40; loss: 1.68; acc: 0.42
Batch: 60; loss: 1.5; acc: 0.45
Batch: 80; loss: 1.26; acc: 0.59
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.26; acc: 0.61
Batch: 140; loss: 1.39; acc: 0.5
Batch: 160; loss: 1.39; acc: 0.52
Batch: 180; loss: 1.26; acc: 0.48
Batch: 200; loss: 1.44; acc: 0.55
Batch: 220; loss: 1.13; acc: 0.62
Batch: 240; loss: 1.49; acc: 0.48
Batch: 260; loss: 1.34; acc: 0.56
Batch: 280; loss: 1.46; acc: 0.45
Batch: 300; loss: 1.26; acc: 0.58
Batch: 320; loss: 1.5; acc: 0.38
Batch: 340; loss: 1.35; acc: 0.53
Batch: 360; loss: 1.15; acc: 0.55
Batch: 380; loss: 1.52; acc: 0.45
Batch: 400; loss: 1.31; acc: 0.59
Batch: 420; loss: 1.61; acc: 0.47
Batch: 440; loss: 1.19; acc: 0.66
Batch: 460; loss: 1.55; acc: 0.47
Batch: 480; loss: 1.32; acc: 0.42
Batch: 500; loss: 1.35; acc: 0.55
Batch: 520; loss: 1.23; acc: 0.62
Batch: 540; loss: 1.49; acc: 0.42
Batch: 560; loss: 1.67; acc: 0.55
Batch: 580; loss: 1.64; acc: 0.44
Batch: 600; loss: 1.34; acc: 0.52
Batch: 620; loss: 1.4; acc: 0.58
Batch: 640; loss: 1.19; acc: 0.55
Batch: 660; loss: 1.17; acc: 0.62
Batch: 680; loss: 1.29; acc: 0.56
Batch: 700; loss: 1.23; acc: 0.59
Batch: 720; loss: 1.36; acc: 0.53
Batch: 740; loss: 1.4; acc: 0.53
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.24; acc: 0.62
Train Epoch over. train_loss: 1.37; train_accuracy: 0.53 

Batch: 0; loss: 1.26; acc: 0.5
Batch: 20; loss: 1.41; acc: 0.52
Batch: 40; loss: 0.88; acc: 0.75
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.02; acc: 0.58
Batch: 100; loss: 1.39; acc: 0.58
Batch: 120; loss: 1.37; acc: 0.56
Batch: 140; loss: 0.97; acc: 0.66
Val Epoch over. val_loss: 1.329921986646713; val_accuracy: 0.5567277070063694 

The current subspace-distance is: 5.545884414459579e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.47; acc: 0.52
Batch: 20; loss: 1.02; acc: 0.66
Batch: 40; loss: 1.54; acc: 0.45
Batch: 60; loss: 1.39; acc: 0.52
Batch: 80; loss: 1.29; acc: 0.45
Batch: 100; loss: 1.33; acc: 0.56
Batch: 120; loss: 1.27; acc: 0.59
Batch: 140; loss: 1.44; acc: 0.53
Batch: 160; loss: 1.17; acc: 0.61
Batch: 180; loss: 1.34; acc: 0.55
Batch: 200; loss: 1.49; acc: 0.5
Batch: 220; loss: 1.55; acc: 0.5
Batch: 240; loss: 1.42; acc: 0.45
Batch: 260; loss: 1.34; acc: 0.58
Batch: 280; loss: 1.31; acc: 0.59
Batch: 300; loss: 1.31; acc: 0.52
Batch: 320; loss: 1.43; acc: 0.5
Batch: 340; loss: 1.32; acc: 0.52
Batch: 360; loss: 1.27; acc: 0.59
Batch: 380; loss: 1.52; acc: 0.52
Batch: 400; loss: 1.34; acc: 0.55
Batch: 420; loss: 1.21; acc: 0.61
Batch: 440; loss: 1.32; acc: 0.55
Batch: 460; loss: 1.28; acc: 0.53
Batch: 480; loss: 1.12; acc: 0.62
Batch: 500; loss: 1.28; acc: 0.52
Batch: 520; loss: 1.48; acc: 0.48
Batch: 540; loss: 1.29; acc: 0.52
Batch: 560; loss: 1.28; acc: 0.53
Batch: 580; loss: 1.5; acc: 0.47
Batch: 600; loss: 1.33; acc: 0.52
Batch: 620; loss: 1.35; acc: 0.53
Batch: 640; loss: 1.37; acc: 0.53
Batch: 660; loss: 1.43; acc: 0.56
Batch: 680; loss: 1.26; acc: 0.55
Batch: 700; loss: 1.28; acc: 0.53
Batch: 720; loss: 1.36; acc: 0.56
Batch: 740; loss: 1.48; acc: 0.53
Batch: 760; loss: 1.34; acc: 0.56
Batch: 780; loss: 1.4; acc: 0.58
Train Epoch over. train_loss: 1.37; train_accuracy: 0.53 

Batch: 0; loss: 1.24; acc: 0.53
Batch: 20; loss: 1.41; acc: 0.5
Batch: 40; loss: 0.87; acc: 0.77
Batch: 60; loss: 1.36; acc: 0.56
Batch: 80; loss: 1.04; acc: 0.53
Batch: 100; loss: 1.36; acc: 0.58
Batch: 120; loss: 1.36; acc: 0.55
Batch: 140; loss: 0.97; acc: 0.67
Val Epoch over. val_loss: 1.3353891429627778; val_accuracy: 0.5558320063694268 

The current subspace-distance is: 5.812079689349048e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.28; acc: 0.55
Batch: 20; loss: 1.12; acc: 0.69
Batch: 40; loss: 1.21; acc: 0.58
Batch: 60; loss: 1.56; acc: 0.45
Batch: 80; loss: 1.36; acc: 0.64
Batch: 100; loss: 1.63; acc: 0.39
Batch: 120; loss: 1.22; acc: 0.56
Batch: 140; loss: 1.39; acc: 0.55
Batch: 160; loss: 1.54; acc: 0.44
Batch: 180; loss: 1.18; acc: 0.61
Batch: 200; loss: 1.33; acc: 0.53
Batch: 220; loss: 1.41; acc: 0.5
Batch: 240; loss: 1.48; acc: 0.5
Batch: 260; loss: 1.3; acc: 0.66
Batch: 280; loss: 1.22; acc: 0.62
Batch: 300; loss: 1.35; acc: 0.52
Batch: 320; loss: 1.37; acc: 0.47
Batch: 340; loss: 1.41; acc: 0.5
Batch: 360; loss: 1.58; acc: 0.42
Batch: 380; loss: 1.27; acc: 0.61
Batch: 400; loss: 1.53; acc: 0.47
Batch: 420; loss: 1.13; acc: 0.66
Batch: 440; loss: 1.35; acc: 0.53
Batch: 460; loss: 1.64; acc: 0.41
Batch: 480; loss: 1.55; acc: 0.48
Batch: 500; loss: 1.46; acc: 0.53
Batch: 520; loss: 1.26; acc: 0.66
Batch: 540; loss: 1.16; acc: 0.56
Batch: 560; loss: 1.28; acc: 0.62
Batch: 580; loss: 1.3; acc: 0.48
Batch: 600; loss: 1.27; acc: 0.59
Batch: 620; loss: 1.27; acc: 0.58
Batch: 640; loss: 1.56; acc: 0.45
Batch: 660; loss: 1.43; acc: 0.45
Batch: 680; loss: 1.37; acc: 0.42
Batch: 700; loss: 1.41; acc: 0.52
Batch: 720; loss: 1.51; acc: 0.58
Batch: 740; loss: 1.52; acc: 0.47
Batch: 760; loss: 1.29; acc: 0.56
Batch: 780; loss: 1.51; acc: 0.45
Train Epoch over. train_loss: 1.37; train_accuracy: 0.53 

Batch: 0; loss: 1.27; acc: 0.48
Batch: 20; loss: 1.41; acc: 0.5
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.02; acc: 0.56
Batch: 100; loss: 1.38; acc: 0.58
Batch: 120; loss: 1.38; acc: 0.53
Batch: 140; loss: 0.96; acc: 0.66
Val Epoch over. val_loss: 1.325374251338327; val_accuracy: 0.5618033439490446 

The current subspace-distance is: 6.195394962560385e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.41; acc: 0.56
Batch: 20; loss: 1.47; acc: 0.56
Batch: 40; loss: 1.45; acc: 0.48
Batch: 60; loss: 1.33; acc: 0.52
Batch: 80; loss: 1.29; acc: 0.58
Batch: 100; loss: 1.37; acc: 0.5
Batch: 120; loss: 1.43; acc: 0.44
Batch: 140; loss: 1.35; acc: 0.48
Batch: 160; loss: 1.22; acc: 0.58
Batch: 180; loss: 1.51; acc: 0.5
Batch: 200; loss: 1.27; acc: 0.56
Batch: 220; loss: 1.33; acc: 0.62
Batch: 240; loss: 1.4; acc: 0.53
Batch: 260; loss: 1.41; acc: 0.52
Batch: 280; loss: 1.36; acc: 0.56
Batch: 300; loss: 1.36; acc: 0.55
Batch: 320; loss: 1.43; acc: 0.59
Batch: 340; loss: 1.24; acc: 0.58
Batch: 360; loss: 1.31; acc: 0.55
Batch: 380; loss: 1.48; acc: 0.45
Batch: 400; loss: 1.57; acc: 0.42
Batch: 420; loss: 1.36; acc: 0.5
Batch: 440; loss: 1.33; acc: 0.55
Batch: 460; loss: 1.34; acc: 0.53
Batch: 480; loss: 1.21; acc: 0.5
Batch: 500; loss: 1.47; acc: 0.44
Batch: 520; loss: 1.39; acc: 0.48
Batch: 540; loss: 1.35; acc: 0.53
Batch: 560; loss: 1.4; acc: 0.55
Batch: 580; loss: 1.44; acc: 0.53
Batch: 600; loss: 1.4; acc: 0.61
Batch: 620; loss: 1.27; acc: 0.55
Batch: 640; loss: 1.37; acc: 0.52
Batch: 660; loss: 1.35; acc: 0.45
Batch: 680; loss: 1.5; acc: 0.55
Batch: 700; loss: 1.22; acc: 0.59
Batch: 720; loss: 1.06; acc: 0.73
Batch: 740; loss: 1.35; acc: 0.52
Batch: 760; loss: 1.22; acc: 0.62
Batch: 780; loss: 1.46; acc: 0.5
Train Epoch over. train_loss: 1.37; train_accuracy: 0.54 

Batch: 0; loss: 1.26; acc: 0.53
Batch: 20; loss: 1.42; acc: 0.5
Batch: 40; loss: 0.9; acc: 0.73
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.03; acc: 0.58
Batch: 100; loss: 1.38; acc: 0.58
Batch: 120; loss: 1.37; acc: 0.52
Batch: 140; loss: 0.96; acc: 0.64
Val Epoch over. val_loss: 1.3246408773076004; val_accuracy: 0.5616042993630573 

The current subspace-distance is: 6.045895497663878e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.32; acc: 0.53
Batch: 20; loss: 1.44; acc: 0.45
Batch: 40; loss: 1.17; acc: 0.62
Batch: 60; loss: 1.29; acc: 0.56
Batch: 80; loss: 1.39; acc: 0.52
Batch: 100; loss: 1.45; acc: 0.53
Batch: 120; loss: 1.29; acc: 0.59
Batch: 140; loss: 1.68; acc: 0.47
Batch: 160; loss: 1.37; acc: 0.53
Batch: 180; loss: 1.25; acc: 0.61
Batch: 200; loss: 1.4; acc: 0.56
Batch: 220; loss: 1.32; acc: 0.52
Batch: 240; loss: 1.31; acc: 0.56
Batch: 260; loss: 1.28; acc: 0.55
Batch: 280; loss: 1.64; acc: 0.47
Batch: 300; loss: 1.25; acc: 0.56
Batch: 320; loss: 1.33; acc: 0.56
Batch: 340; loss: 1.23; acc: 0.64
Batch: 360; loss: 1.21; acc: 0.59
Batch: 380; loss: 1.3; acc: 0.56
Batch: 400; loss: 1.13; acc: 0.64
Batch: 420; loss: 1.4; acc: 0.53
Batch: 440; loss: 1.05; acc: 0.67
Batch: 460; loss: 1.39; acc: 0.48
Batch: 480; loss: 1.43; acc: 0.5
Batch: 500; loss: 1.23; acc: 0.53
Batch: 520; loss: 1.5; acc: 0.52
Batch: 540; loss: 1.55; acc: 0.44
Batch: 560; loss: 1.08; acc: 0.66
Batch: 580; loss: 1.38; acc: 0.5
Batch: 600; loss: 1.3; acc: 0.52
Batch: 620; loss: 1.33; acc: 0.56
Batch: 640; loss: 1.81; acc: 0.36
Batch: 660; loss: 1.47; acc: 0.42
Batch: 680; loss: 1.21; acc: 0.61
Batch: 700; loss: 1.37; acc: 0.52
Batch: 720; loss: 1.23; acc: 0.55
Batch: 740; loss: 1.31; acc: 0.53
Batch: 760; loss: 1.52; acc: 0.48
Batch: 780; loss: 1.4; acc: 0.53
Train Epoch over. train_loss: 1.37; train_accuracy: 0.54 

Batch: 0; loss: 1.26; acc: 0.52
Batch: 20; loss: 1.39; acc: 0.52
Batch: 40; loss: 0.9; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.59
Batch: 80; loss: 1.02; acc: 0.58
Batch: 100; loss: 1.37; acc: 0.56
Batch: 120; loss: 1.36; acc: 0.55
Batch: 140; loss: 0.94; acc: 0.64
Val Epoch over. val_loss: 1.3246435876105243; val_accuracy: 0.5638933121019108 

The current subspace-distance is: 6.168046820675954e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.47; acc: 0.48
Batch: 20; loss: 1.31; acc: 0.59
Batch: 40; loss: 1.62; acc: 0.48
Batch: 60; loss: 1.31; acc: 0.53
Batch: 80; loss: 1.42; acc: 0.53
Batch: 100; loss: 1.3; acc: 0.58
Batch: 120; loss: 1.53; acc: 0.5
Batch: 140; loss: 1.37; acc: 0.62
Batch: 160; loss: 1.35; acc: 0.5
Batch: 180; loss: 1.52; acc: 0.55
Batch: 200; loss: 1.51; acc: 0.55
Batch: 220; loss: 1.6; acc: 0.52
Batch: 240; loss: 1.19; acc: 0.61
Batch: 260; loss: 1.31; acc: 0.55
Batch: 280; loss: 1.42; acc: 0.5
Batch: 300; loss: 1.54; acc: 0.44
Batch: 320; loss: 1.3; acc: 0.61
Batch: 340; loss: 1.49; acc: 0.53
Batch: 360; loss: 1.41; acc: 0.52
Batch: 380; loss: 1.34; acc: 0.5
Batch: 400; loss: 1.19; acc: 0.56
Batch: 420; loss: 1.3; acc: 0.53
Batch: 440; loss: 1.39; acc: 0.55
Batch: 460; loss: 1.33; acc: 0.52
Batch: 480; loss: 1.35; acc: 0.53
Batch: 500; loss: 1.44; acc: 0.53
Batch: 520; loss: 1.43; acc: 0.53
Batch: 540; loss: 1.61; acc: 0.45
Batch: 560; loss: 1.39; acc: 0.52
Batch: 580; loss: 1.05; acc: 0.66
Batch: 600; loss: 1.23; acc: 0.53
Batch: 620; loss: 1.22; acc: 0.64
Batch: 640; loss: 1.49; acc: 0.59
Batch: 660; loss: 1.31; acc: 0.53
Batch: 680; loss: 1.56; acc: 0.45
Batch: 700; loss: 1.21; acc: 0.48
Batch: 720; loss: 1.42; acc: 0.52
Batch: 740; loss: 1.39; acc: 0.53
Batch: 760; loss: 1.33; acc: 0.58
Batch: 780; loss: 1.56; acc: 0.42
Train Epoch over. train_loss: 1.37; train_accuracy: 0.54 

Batch: 0; loss: 1.25; acc: 0.5
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.9; acc: 0.73
Batch: 60; loss: 1.34; acc: 0.59
Batch: 80; loss: 1.01; acc: 0.59
Batch: 100; loss: 1.35; acc: 0.58
Batch: 120; loss: 1.35; acc: 0.53
Batch: 140; loss: 0.94; acc: 0.66
Val Epoch over. val_loss: 1.3226193462967113; val_accuracy: 0.5646894904458599 

The current subspace-distance is: 6.350217154249549e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.3; acc: 0.56
Batch: 20; loss: 1.45; acc: 0.47
Batch: 40; loss: 1.36; acc: 0.47
Batch: 60; loss: 1.44; acc: 0.48
Batch: 80; loss: 1.06; acc: 0.59
Batch: 100; loss: 1.33; acc: 0.5
Batch: 120; loss: 1.43; acc: 0.5
Batch: 140; loss: 1.41; acc: 0.56
Batch: 160; loss: 1.37; acc: 0.52
Batch: 180; loss: 1.66; acc: 0.45
Batch: 200; loss: 1.47; acc: 0.52
Batch: 220; loss: 1.53; acc: 0.45
Batch: 240; loss: 1.45; acc: 0.58
Batch: 260; loss: 1.37; acc: 0.55
Batch: 280; loss: 1.61; acc: 0.42
Batch: 300; loss: 1.42; acc: 0.5
Batch: 320; loss: 1.24; acc: 0.58
Batch: 340; loss: 1.49; acc: 0.5
Batch: 360; loss: 1.28; acc: 0.52
Batch: 380; loss: 1.37; acc: 0.52
Batch: 400; loss: 1.31; acc: 0.55
Batch: 420; loss: 1.45; acc: 0.48
Batch: 440; loss: 1.5; acc: 0.47
Batch: 460; loss: 1.31; acc: 0.61
Batch: 480; loss: 1.2; acc: 0.59
Batch: 500; loss: 1.28; acc: 0.56
Batch: 520; loss: 1.39; acc: 0.47
Batch: 540; loss: 1.55; acc: 0.47
Batch: 560; loss: 1.33; acc: 0.52
Batch: 580; loss: 1.34; acc: 0.53
Batch: 600; loss: 1.41; acc: 0.53
Batch: 620; loss: 1.3; acc: 0.58
Batch: 640; loss: 1.4; acc: 0.58
Batch: 660; loss: 1.2; acc: 0.61
Batch: 680; loss: 1.49; acc: 0.48
Batch: 700; loss: 1.35; acc: 0.53
Batch: 720; loss: 1.43; acc: 0.61
Batch: 740; loss: 1.18; acc: 0.55
Batch: 760; loss: 1.42; acc: 0.55
Batch: 780; loss: 1.32; acc: 0.55
Train Epoch over. train_loss: 1.37; train_accuracy: 0.54 

Batch: 0; loss: 1.26; acc: 0.47
Batch: 20; loss: 1.41; acc: 0.53
Batch: 40; loss: 0.9; acc: 0.75
Batch: 60; loss: 1.35; acc: 0.58
Batch: 80; loss: 1.01; acc: 0.61
Batch: 100; loss: 1.35; acc: 0.58
Batch: 120; loss: 1.35; acc: 0.52
Batch: 140; loss: 0.93; acc: 0.67
Val Epoch over. val_loss: 1.3225417015658822; val_accuracy: 0.5620023885350318 

The current subspace-distance is: 6.397338438546285e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.48; acc: 0.44
Batch: 20; loss: 1.14; acc: 0.66
Batch: 40; loss: 1.33; acc: 0.5
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.44; acc: 0.47
Batch: 100; loss: 1.51; acc: 0.45
Batch: 120; loss: 1.46; acc: 0.55
Batch: 140; loss: 1.5; acc: 0.45
Batch: 160; loss: 1.29; acc: 0.52
Batch: 180; loss: 1.39; acc: 0.5
Batch: 200; loss: 1.54; acc: 0.41
Batch: 220; loss: 1.64; acc: 0.39
Batch: 240; loss: 1.51; acc: 0.42
Batch: 260; loss: 1.39; acc: 0.53
Batch: 280; loss: 1.32; acc: 0.56
Batch: 300; loss: 1.46; acc: 0.47
Batch: 320; loss: 1.46; acc: 0.55
Batch: 340; loss: 1.25; acc: 0.52
Batch: 360; loss: 1.42; acc: 0.59
Batch: 380; loss: 1.95; acc: 0.47
Batch: 400; loss: 1.51; acc: 0.52
Batch: 420; loss: 1.41; acc: 0.53
Batch: 440; loss: 1.42; acc: 0.55
Batch: 460; loss: 1.4; acc: 0.55
Batch: 480; loss: 1.19; acc: 0.62
Batch: 500; loss: 1.48; acc: 0.48
Batch: 520; loss: 1.18; acc: 0.64
Batch: 540; loss: 1.54; acc: 0.44
Batch: 560; loss: 1.34; acc: 0.59
Batch: 580; loss: 1.12; acc: 0.64
Batch: 600; loss: 1.11; acc: 0.58
Batch: 620; loss: 1.36; acc: 0.45
Batch: 640; loss: 1.29; acc: 0.56
Batch: 660; loss: 1.32; acc: 0.61
Batch: 680; loss: 1.41; acc: 0.58
Batch: 700; loss: 1.4; acc: 0.55
Batch: 720; loss: 1.29; acc: 0.61
Batch: 740; loss: 1.33; acc: 0.55
Batch: 760; loss: 1.26; acc: 0.5
Batch: 780; loss: 1.32; acc: 0.55
Train Epoch over. train_loss: 1.37; train_accuracy: 0.54 

Batch: 0; loss: 1.26; acc: 0.5
Batch: 20; loss: 1.39; acc: 0.53
Batch: 40; loss: 0.9; acc: 0.77
Batch: 60; loss: 1.37; acc: 0.56
Batch: 80; loss: 1.0; acc: 0.59
Batch: 100; loss: 1.37; acc: 0.58
Batch: 120; loss: 1.35; acc: 0.53
Batch: 140; loss: 0.92; acc: 0.67
Val Epoch over. val_loss: 1.3215372790196898; val_accuracy: 0.5630971337579618 

The current subspace-distance is: 6.507508805952966e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.65; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.53
Batch: 40; loss: 1.21; acc: 0.58
Batch: 60; loss: 1.32; acc: 0.62
Batch: 80; loss: 1.65; acc: 0.41
Batch: 100; loss: 1.36; acc: 0.5
Batch: 120; loss: 1.64; acc: 0.47
Batch: 140; loss: 1.33; acc: 0.53
Batch: 160; loss: 1.41; acc: 0.53
Batch: 180; loss: 1.2; acc: 0.56
Batch: 200; loss: 1.33; acc: 0.45
Batch: 220; loss: 1.37; acc: 0.47
Batch: 240; loss: 1.46; acc: 0.52
Batch: 260; loss: 1.51; acc: 0.48
Batch: 280; loss: 1.33; acc: 0.61
Batch: 300; loss: 1.53; acc: 0.58
Batch: 320; loss: 1.37; acc: 0.52
Batch: 340; loss: 1.35; acc: 0.55
Batch: 360; loss: 1.38; acc: 0.55
Batch: 380; loss: 1.43; acc: 0.55
Batch: 400; loss: 1.2; acc: 0.58
Batch: 420; loss: 1.34; acc: 0.55
Batch: 440; loss: 1.29; acc: 0.53
Batch: 460; loss: 1.53; acc: 0.52
Batch: 480; loss: 1.49; acc: 0.53
Batch: 500; loss: 1.4; acc: 0.5
Batch: 520; loss: 1.56; acc: 0.42
Batch: 540; loss: 1.4; acc: 0.53
Batch: 560; loss: 1.39; acc: 0.53
Batch: 580; loss: 1.21; acc: 0.56
Batch: 600; loss: 1.28; acc: 0.62
Batch: 620; loss: 1.29; acc: 0.64
Batch: 640; loss: 1.33; acc: 0.56
Batch: 660; loss: 1.32; acc: 0.58
Batch: 680; loss: 1.2; acc: 0.66
Batch: 700; loss: 1.36; acc: 0.5
Batch: 720; loss: 1.21; acc: 0.58
Batch: 740; loss: 1.57; acc: 0.42
Batch: 760; loss: 1.24; acc: 0.58
Batch: 780; loss: 1.27; acc: 0.58
Train Epoch over. train_loss: 1.37; train_accuracy: 0.54 

Batch: 0; loss: 1.25; acc: 0.47
Batch: 20; loss: 1.38; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.72
Batch: 60; loss: 1.36; acc: 0.58
Batch: 80; loss: 1.0; acc: 0.61
Batch: 100; loss: 1.33; acc: 0.55
Batch: 120; loss: 1.33; acc: 0.55
Batch: 140; loss: 0.9; acc: 0.67
Val Epoch over. val_loss: 1.3190662431868778; val_accuracy: 0.5659832802547771 

The current subspace-distance is: 6.926064088474959e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.31; acc: 0.66
Batch: 20; loss: 1.59; acc: 0.41
Batch: 40; loss: 1.38; acc: 0.52
Batch: 60; loss: 1.33; acc: 0.56
Batch: 80; loss: 1.24; acc: 0.66
Batch: 100; loss: 1.46; acc: 0.5
Batch: 120; loss: 1.46; acc: 0.48
Batch: 140; loss: 1.44; acc: 0.52
Batch: 160; loss: 1.3; acc: 0.59
Batch: 180; loss: 1.2; acc: 0.58
Batch: 200; loss: 1.56; acc: 0.39
Batch: 220; loss: 1.29; acc: 0.56
Batch: 240; loss: 1.34; acc: 0.52
Batch: 260; loss: 1.12; acc: 0.58
Batch: 280; loss: 1.49; acc: 0.48
Batch: 300; loss: 1.52; acc: 0.48
Batch: 320; loss: 1.64; acc: 0.44
Batch: 340; loss: 1.37; acc: 0.55
Batch: 360; loss: 1.51; acc: 0.52
Batch: 380; loss: 1.43; acc: 0.52
Batch: 400; loss: 1.46; acc: 0.56
Batch: 420; loss: 1.44; acc: 0.53
Batch: 440; loss: 1.29; acc: 0.62
Batch: 460; loss: 1.85; acc: 0.34
Batch: 480; loss: 1.46; acc: 0.5
Batch: 500; loss: 1.2; acc: 0.59
Batch: 520; loss: 1.75; acc: 0.44
Batch: 540; loss: 1.23; acc: 0.56
Batch: 560; loss: 1.43; acc: 0.5
Batch: 580; loss: 1.36; acc: 0.5
Batch: 600; loss: 1.26; acc: 0.58
Batch: 620; loss: 1.36; acc: 0.55
Batch: 640; loss: 1.68; acc: 0.42
Batch: 660; loss: 1.47; acc: 0.47
Batch: 680; loss: 1.37; acc: 0.55
Batch: 700; loss: 1.29; acc: 0.64
Batch: 720; loss: 1.6; acc: 0.48
Batch: 740; loss: 1.22; acc: 0.55
Batch: 760; loss: 1.35; acc: 0.61
Batch: 780; loss: 1.39; acc: 0.48
Train Epoch over. train_loss: 1.36; train_accuracy: 0.54 

Batch: 0; loss: 1.26; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.87; acc: 0.72
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 0.99; acc: 0.62
Batch: 100; loss: 1.34; acc: 0.58
Batch: 120; loss: 1.33; acc: 0.55
Batch: 140; loss: 0.89; acc: 0.67
Val Epoch over. val_loss: 1.316929183568165; val_accuracy: 0.5677746815286624 

The current subspace-distance is: 6.932638643775135e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.3; acc: 0.64
Batch: 20; loss: 1.57; acc: 0.56
Batch: 40; loss: 1.34; acc: 0.53
Batch: 60; loss: 1.38; acc: 0.5
Batch: 80; loss: 1.15; acc: 0.59
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.47; acc: 0.39
Batch: 140; loss: 1.4; acc: 0.56
Batch: 160; loss: 1.44; acc: 0.47
Batch: 180; loss: 1.32; acc: 0.55
Batch: 200; loss: 1.13; acc: 0.61
Batch: 220; loss: 1.44; acc: 0.56
Batch: 240; loss: 1.01; acc: 0.64
Batch: 260; loss: 1.35; acc: 0.5
Batch: 280; loss: 1.06; acc: 0.67
Batch: 300; loss: 1.31; acc: 0.56
Batch: 320; loss: 1.67; acc: 0.45
Batch: 340; loss: 1.3; acc: 0.52
Batch: 360; loss: 1.46; acc: 0.53
Batch: 380; loss: 1.36; acc: 0.53
Batch: 400; loss: 1.47; acc: 0.47
Batch: 420; loss: 1.52; acc: 0.53
Batch: 440; loss: 1.25; acc: 0.61
Batch: 460; loss: 1.53; acc: 0.47
Batch: 480; loss: 1.24; acc: 0.61
Batch: 500; loss: 1.29; acc: 0.56
Batch: 520; loss: 1.34; acc: 0.62
Batch: 540; loss: 1.19; acc: 0.58
Batch: 560; loss: 1.5; acc: 0.52
Batch: 580; loss: 1.52; acc: 0.55
Batch: 600; loss: 1.17; acc: 0.67
Batch: 620; loss: 1.27; acc: 0.62
Batch: 640; loss: 1.29; acc: 0.67
Batch: 660; loss: 1.07; acc: 0.7
Batch: 680; loss: 1.11; acc: 0.62
Batch: 700; loss: 1.09; acc: 0.66
Batch: 720; loss: 1.27; acc: 0.64
Batch: 740; loss: 1.41; acc: 0.53
Batch: 760; loss: 1.21; acc: 0.59
Batch: 780; loss: 1.29; acc: 0.56
Train Epoch over. train_loss: 1.36; train_accuracy: 0.54 

Batch: 0; loss: 1.26; acc: 0.47
Batch: 20; loss: 1.41; acc: 0.52
Batch: 40; loss: 0.88; acc: 0.72
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 0.99; acc: 0.64
Batch: 100; loss: 1.34; acc: 0.59
Batch: 120; loss: 1.32; acc: 0.53
Batch: 140; loss: 0.88; acc: 0.7
Val Epoch over. val_loss: 1.3156251246762123; val_accuracy: 0.5665804140127388 

The current subspace-distance is: 7.025958620943129e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.22; acc: 0.62
Batch: 20; loss: 1.36; acc: 0.52
Batch: 40; loss: 1.58; acc: 0.48
Batch: 60; loss: 1.57; acc: 0.53
Batch: 80; loss: 1.5; acc: 0.47
Batch: 100; loss: 1.24; acc: 0.59
Batch: 120; loss: 1.41; acc: 0.48
Batch: 140; loss: 1.17; acc: 0.69
Batch: 160; loss: 1.51; acc: 0.48
Batch: 180; loss: 1.34; acc: 0.52
Batch: 200; loss: 1.49; acc: 0.47
Batch: 220; loss: 1.33; acc: 0.55
Batch: 240; loss: 1.2; acc: 0.58
Batch: 260; loss: 1.31; acc: 0.58
Batch: 280; loss: 1.32; acc: 0.5
Batch: 300; loss: 1.46; acc: 0.47
Batch: 320; loss: 1.22; acc: 0.55
Batch: 340; loss: 1.38; acc: 0.56
Batch: 360; loss: 1.53; acc: 0.53
Batch: 380; loss: 1.36; acc: 0.53
Batch: 400; loss: 1.16; acc: 0.61
Batch: 420; loss: 1.4; acc: 0.5
Batch: 440; loss: 1.49; acc: 0.44
Batch: 460; loss: 1.16; acc: 0.62
Batch: 480; loss: 1.57; acc: 0.58
Batch: 500; loss: 1.26; acc: 0.56
Batch: 520; loss: 1.26; acc: 0.59
Batch: 540; loss: 1.0; acc: 0.66
Batch: 560; loss: 1.49; acc: 0.47
Batch: 580; loss: 1.38; acc: 0.53
Batch: 600; loss: 1.47; acc: 0.45
Batch: 620; loss: 1.25; acc: 0.61
Batch: 640; loss: 1.25; acc: 0.59
Batch: 660; loss: 1.31; acc: 0.53
Batch: 680; loss: 1.21; acc: 0.5
Batch: 700; loss: 1.5; acc: 0.48
Batch: 720; loss: 1.42; acc: 0.5
Batch: 740; loss: 1.25; acc: 0.59
Batch: 760; loss: 1.54; acc: 0.41
Batch: 780; loss: 1.34; acc: 0.61
Train Epoch over. train_loss: 1.36; train_accuracy: 0.54 

Batch: 0; loss: 1.27; acc: 0.5
Batch: 20; loss: 1.43; acc: 0.53
Batch: 40; loss: 0.88; acc: 0.72
Batch: 60; loss: 1.35; acc: 0.62
Batch: 80; loss: 0.98; acc: 0.62
Batch: 100; loss: 1.34; acc: 0.52
Batch: 120; loss: 1.31; acc: 0.55
Batch: 140; loss: 0.88; acc: 0.69
Val Epoch over. val_loss: 1.315239703199666; val_accuracy: 0.5652866242038217 

The current subspace-distance is: 7.171286415541545e-05 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.64; acc: 0.5
Batch: 20; loss: 1.44; acc: 0.53
Batch: 40; loss: 1.49; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.32; acc: 0.55
Batch: 100; loss: 1.35; acc: 0.53
Batch: 120; loss: 1.32; acc: 0.56
Batch: 140; loss: 1.29; acc: 0.58
Batch: 160; loss: 1.54; acc: 0.45
Batch: 180; loss: 1.26; acc: 0.59
Batch: 200; loss: 1.23; acc: 0.53
Batch: 220; loss: 1.39; acc: 0.55
Batch: 240; loss: 1.39; acc: 0.52
Batch: 260; loss: 1.27; acc: 0.61
Batch: 280; loss: 1.27; acc: 0.55
Batch: 300; loss: 1.7; acc: 0.44
Batch: 320; loss: 1.26; acc: 0.61
Batch: 340; loss: 1.38; acc: 0.58
Batch: 360; loss: 1.32; acc: 0.58
Batch: 380; loss: 1.38; acc: 0.58
Batch: 400; loss: 1.21; acc: 0.61
Batch: 420; loss: 1.26; acc: 0.55
Batch: 440; loss: 1.23; acc: 0.58
Batch: 460; loss: 1.23; acc: 0.59
Batch: 480; loss: 1.18; acc: 0.61
Batch: 500; loss: 1.16; acc: 0.58
Batch: 520; loss: 1.39; acc: 0.56
Batch: 540; loss: 1.35; acc: 0.52
Batch: 560; loss: 1.44; acc: 0.44
Batch: 580; loss: 1.29; acc: 0.58
Batch: 600; loss: 1.09; acc: 0.59
Batch: 620; loss: 1.23; acc: 0.56
Batch: 640; loss: 1.44; acc: 0.5
Batch: 660; loss: 1.33; acc: 0.55
Batch: 680; loss: 1.52; acc: 0.52
Batch: 700; loss: 1.36; acc: 0.55
Batch: 720; loss: 1.49; acc: 0.52
Batch: 740; loss: 1.12; acc: 0.67
Batch: 760; loss: 1.47; acc: 0.53
Batch: 780; loss: 1.4; acc: 0.45
Train Epoch over. train_loss: 1.36; train_accuracy: 0.54 

Batch: 0; loss: 1.27; acc: 0.48
Batch: 20; loss: 1.39; acc: 0.52
Batch: 40; loss: 0.85; acc: 0.72
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 0.99; acc: 0.67
Batch: 100; loss: 1.34; acc: 0.52
Batch: 120; loss: 1.31; acc: 0.55
Batch: 140; loss: 0.85; acc: 0.7
Val Epoch over. val_loss: 1.31465960460104; val_accuracy: 0.5683718152866242 

The current subspace-distance is: 7.181335968198255e-05 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.6; acc: 0.48
Batch: 20; loss: 1.29; acc: 0.59
Batch: 40; loss: 1.4; acc: 0.55
Batch: 60; loss: 1.59; acc: 0.45
Batch: 80; loss: 1.19; acc: 0.56
Batch: 100; loss: 1.46; acc: 0.52
Batch: 120; loss: 1.61; acc: 0.53
Batch: 140; loss: 1.37; acc: 0.55
Batch: 160; loss: 1.42; acc: 0.44
Batch: 180; loss: 1.3; acc: 0.61
Batch: 200; loss: 1.3; acc: 0.59
Batch: 220; loss: 1.5; acc: 0.42
Batch: 240; loss: 1.01; acc: 0.69
Batch: 260; loss: 1.4; acc: 0.45
Batch: 280; loss: 1.29; acc: 0.59
Batch: 300; loss: 1.51; acc: 0.55
Batch: 320; loss: 1.24; acc: 0.64
Batch: 340; loss: 1.35; acc: 0.56
Batch: 360; loss: 1.3; acc: 0.53
Batch: 380; loss: 1.28; acc: 0.52
Batch: 400; loss: 1.31; acc: 0.55
Batch: 420; loss: 1.43; acc: 0.45
Batch: 440; loss: 1.28; acc: 0.55
Batch: 460; loss: 1.29; acc: 0.55
Batch: 480; loss: 1.24; acc: 0.61
Batch: 500; loss: 1.43; acc: 0.48
Batch: 520; loss: 1.62; acc: 0.52
Batch: 540; loss: 1.09; acc: 0.69
Batch: 560; loss: 1.07; acc: 0.69
Batch: 580; loss: 1.32; acc: 0.53
Batch: 600; loss: 1.28; acc: 0.56
Batch: 620; loss: 1.47; acc: 0.45
Batch: 640; loss: 1.26; acc: 0.56
Batch: 660; loss: 1.37; acc: 0.52
Batch: 680; loss: 1.61; acc: 0.47
Batch: 700; loss: 1.25; acc: 0.59
Batch: 720; loss: 1.32; acc: 0.56
Batch: 740; loss: 1.29; acc: 0.59
Batch: 760; loss: 1.37; acc: 0.58
Batch: 780; loss: 1.34; acc: 0.56
Train Epoch over. train_loss: 1.36; train_accuracy: 0.54 

Batch: 0; loss: 1.27; acc: 0.47
Batch: 20; loss: 1.41; acc: 0.52
Batch: 40; loss: 0.87; acc: 0.7
Batch: 60; loss: 1.37; acc: 0.61
Batch: 80; loss: 0.98; acc: 0.69
Batch: 100; loss: 1.34; acc: 0.5
Batch: 120; loss: 1.31; acc: 0.55
Batch: 140; loss: 0.86; acc: 0.72
Val Epoch over. val_loss: 1.311293710568908; val_accuracy: 0.5698646496815286 

The current subspace-distance is: 7.160771201597527e-05 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.4; acc: 0.62
Batch: 20; loss: 1.55; acc: 0.56
Batch: 40; loss: 1.4; acc: 0.52
Batch: 60; loss: 1.23; acc: 0.56
Batch: 80; loss: 1.29; acc: 0.62
Batch: 100; loss: 1.55; acc: 0.5
Batch: 120; loss: 1.43; acc: 0.48
Batch: 140; loss: 1.31; acc: 0.52
Batch: 160; loss: 1.49; acc: 0.5
Batch: 180; loss: 1.36; acc: 0.5
Batch: 200; loss: 1.39; acc: 0.47
Batch: 220; loss: 1.24; acc: 0.58
Batch: 240; loss: 1.51; acc: 0.47
Batch: 260; loss: 1.4; acc: 0.53
Batch: 280; loss: 1.17; acc: 0.59
Batch: 300; loss: 1.3; acc: 0.52
Batch: 320; loss: 1.32; acc: 0.61
Batch: 340; loss: 1.37; acc: 0.58
Batch: 360; loss: 1.21; acc: 0.55
Batch: 380; loss: 1.51; acc: 0.55
Batch: 400; loss: 1.4; acc: 0.52
Batch: 420; loss: 1.67; acc: 0.5
Batch: 440; loss: 1.31; acc: 0.62
Batch: 460; loss: 1.54; acc: 0.52
Batch: 480; loss: 1.27; acc: 0.52
Batch: 500; loss: 1.33; acc: 0.48
Batch: 520; loss: 1.21; acc: 0.58
Batch: 540; loss: 1.25; acc: 0.56
Batch: 560; loss: 1.6; acc: 0.42
Batch: 580; loss: 1.56; acc: 0.39
Batch: 600; loss: 1.45; acc: 0.52
Batch: 620; loss: 1.24; acc: 0.58
Batch: 640; loss: 1.5; acc: 0.55
Batch: 660; loss: 1.34; acc: 0.45
Batch: 680; loss: 1.56; acc: 0.47
Batch: 700; loss: 1.47; acc: 0.61
Batch: 720; loss: 1.21; acc: 0.61
Batch: 740; loss: 1.21; acc: 0.56
Batch: 760; loss: 1.19; acc: 0.56
Batch: 780; loss: 1.36; acc: 0.56
Train Epoch over. train_loss: 1.36; train_accuracy: 0.54 

Batch: 0; loss: 1.26; acc: 0.47
Batch: 20; loss: 1.39; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 1.36; acc: 0.62
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.5
Batch: 120; loss: 1.31; acc: 0.55
Batch: 140; loss: 0.86; acc: 0.7
Val Epoch over. val_loss: 1.3103412925058109; val_accuracy: 0.5731488853503185 

The current subspace-distance is: 7.360117160715163e-05 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.52; acc: 0.45
Batch: 20; loss: 1.2; acc: 0.61
Batch: 40; loss: 1.4; acc: 0.52
Batch: 60; loss: 1.23; acc: 0.52
Batch: 80; loss: 1.35; acc: 0.56
Batch: 100; loss: 1.24; acc: 0.64
Batch: 120; loss: 1.16; acc: 0.61
Batch: 140; loss: 1.44; acc: 0.53
Batch: 160; loss: 1.23; acc: 0.64
Batch: 180; loss: 1.36; acc: 0.52
Batch: 200; loss: 1.59; acc: 0.47
Batch: 220; loss: 1.31; acc: 0.55
Batch: 240; loss: 1.28; acc: 0.56
Batch: 260; loss: 1.4; acc: 0.55
Batch: 280; loss: 1.42; acc: 0.55
Batch: 300; loss: 1.36; acc: 0.59
Batch: 320; loss: 0.94; acc: 0.69
Batch: 340; loss: 1.41; acc: 0.58
Batch: 360; loss: 1.44; acc: 0.56
Batch: 380; loss: 1.25; acc: 0.58
Batch: 400; loss: 1.42; acc: 0.48
Batch: 420; loss: 1.27; acc: 0.55
Batch: 440; loss: 1.53; acc: 0.47
Batch: 460; loss: 1.37; acc: 0.61
Batch: 480; loss: 1.23; acc: 0.58
Batch: 500; loss: 1.4; acc: 0.5
Batch: 520; loss: 1.19; acc: 0.59
Batch: 540; loss: 1.39; acc: 0.58
Batch: 560; loss: 1.34; acc: 0.56
Batch: 580; loss: 1.23; acc: 0.62
Batch: 600; loss: 1.33; acc: 0.53
Batch: 620; loss: 1.49; acc: 0.48
Batch: 640; loss: 1.25; acc: 0.56
Batch: 660; loss: 1.26; acc: 0.53
Batch: 680; loss: 1.4; acc: 0.5
Batch: 700; loss: 1.31; acc: 0.5
Batch: 720; loss: 1.61; acc: 0.44
Batch: 740; loss: 1.28; acc: 0.66
Batch: 760; loss: 1.59; acc: 0.52
Batch: 780; loss: 1.39; acc: 0.5
Train Epoch over. train_loss: 1.36; train_accuracy: 0.54 

Batch: 0; loss: 1.27; acc: 0.47
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 0.98; acc: 0.66
Batch: 100; loss: 1.34; acc: 0.48
Batch: 120; loss: 1.31; acc: 0.53
Batch: 140; loss: 0.85; acc: 0.7
Val Epoch over. val_loss: 1.3109118084239353; val_accuracy: 0.5696656050955414 

The current subspace-distance is: 7.544810068793595e-05 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.53; acc: 0.44
Batch: 20; loss: 1.43; acc: 0.58
Batch: 40; loss: 1.38; acc: 0.56
Batch: 60; loss: 1.53; acc: 0.44
Batch: 80; loss: 1.65; acc: 0.45
Batch: 100; loss: 1.54; acc: 0.5
Batch: 120; loss: 1.44; acc: 0.52
Batch: 140; loss: 1.6; acc: 0.44
Batch: 160; loss: 1.7; acc: 0.52
Batch: 180; loss: 1.21; acc: 0.62
Batch: 200; loss: 1.46; acc: 0.56
Batch: 220; loss: 1.38; acc: 0.53
Batch: 240; loss: 1.23; acc: 0.58
Batch: 260; loss: 1.35; acc: 0.58
Batch: 280; loss: 1.46; acc: 0.41
Batch: 300; loss: 1.19; acc: 0.61
Batch: 320; loss: 1.3; acc: 0.62
Batch: 340; loss: 1.21; acc: 0.55
Batch: 360; loss: 1.15; acc: 0.62
Batch: 380; loss: 1.48; acc: 0.48
Batch: 400; loss: 1.1; acc: 0.77
Batch: 420; loss: 1.41; acc: 0.48
Batch: 440; loss: 1.42; acc: 0.48
Batch: 460; loss: 1.6; acc: 0.41
Batch: 480; loss: 1.37; acc: 0.53
Batch: 500; loss: 1.3; acc: 0.5
Batch: 520; loss: 1.43; acc: 0.53
Batch: 540; loss: 1.32; acc: 0.58
Batch: 560; loss: 1.52; acc: 0.45
Batch: 580; loss: 1.4; acc: 0.59
Batch: 600; loss: 1.23; acc: 0.59
Batch: 620; loss: 1.11; acc: 0.56
Batch: 640; loss: 1.25; acc: 0.59
Batch: 660; loss: 1.32; acc: 0.53
Batch: 680; loss: 1.33; acc: 0.5
Batch: 700; loss: 1.17; acc: 0.58
Batch: 720; loss: 1.47; acc: 0.48
Batch: 740; loss: 1.18; acc: 0.62
Batch: 760; loss: 1.45; acc: 0.47
Batch: 780; loss: 1.41; acc: 0.45
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.27; acc: 0.47
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.72
Batch: 60; loss: 1.37; acc: 0.62
Batch: 80; loss: 0.99; acc: 0.69
Batch: 100; loss: 1.34; acc: 0.52
Batch: 120; loss: 1.32; acc: 0.55
Batch: 140; loss: 0.85; acc: 0.72
Val Epoch over. val_loss: 1.311516959955738; val_accuracy: 0.5711584394904459 

The current subspace-distance is: 7.576421194244176e-05 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.46; acc: 0.5
Batch: 40; loss: 1.18; acc: 0.61
Batch: 60; loss: 1.33; acc: 0.53
Batch: 80; loss: 1.47; acc: 0.56
Batch: 100; loss: 1.29; acc: 0.61
Batch: 120; loss: 1.59; acc: 0.44
Batch: 140; loss: 1.15; acc: 0.56
Batch: 160; loss: 1.22; acc: 0.58
Batch: 180; loss: 1.31; acc: 0.61
Batch: 200; loss: 1.31; acc: 0.62
Batch: 220; loss: 1.46; acc: 0.53
Batch: 240; loss: 1.43; acc: 0.59
Batch: 260; loss: 1.37; acc: 0.48
Batch: 280; loss: 1.28; acc: 0.53
Batch: 300; loss: 1.37; acc: 0.47
Batch: 320; loss: 1.17; acc: 0.59
Batch: 340; loss: 1.33; acc: 0.58
Batch: 360; loss: 1.3; acc: 0.59
Batch: 380; loss: 1.13; acc: 0.66
Batch: 400; loss: 1.37; acc: 0.55
Batch: 420; loss: 1.43; acc: 0.42
Batch: 440; loss: 1.22; acc: 0.62
Batch: 460; loss: 1.43; acc: 0.5
Batch: 480; loss: 1.46; acc: 0.59
Batch: 500; loss: 1.41; acc: 0.56
Batch: 520; loss: 1.63; acc: 0.31
Batch: 540; loss: 1.51; acc: 0.48
Batch: 560; loss: 1.36; acc: 0.55
Batch: 580; loss: 1.37; acc: 0.5
Batch: 600; loss: 1.57; acc: 0.42
Batch: 620; loss: 1.25; acc: 0.56
Batch: 640; loss: 1.33; acc: 0.53
Batch: 660; loss: 1.46; acc: 0.56
Batch: 680; loss: 1.56; acc: 0.47
Batch: 700; loss: 1.44; acc: 0.53
Batch: 720; loss: 1.49; acc: 0.5
Batch: 740; loss: 1.59; acc: 0.45
Batch: 760; loss: 1.24; acc: 0.61
Batch: 780; loss: 1.35; acc: 0.53
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.27; acc: 0.48
Batch: 20; loss: 1.41; acc: 0.48
Batch: 40; loss: 0.87; acc: 0.72
Batch: 60; loss: 1.37; acc: 0.59
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.34; acc: 0.5
Batch: 120; loss: 1.32; acc: 0.56
Batch: 140; loss: 0.86; acc: 0.72
Val Epoch over. val_loss: 1.3090737575937987; val_accuracy: 0.5723527070063694 

The current subspace-distance is: 7.770309457555413e-05 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.27; acc: 0.5
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 1.3; acc: 0.55
Batch: 60; loss: 1.45; acc: 0.48
Batch: 80; loss: 1.25; acc: 0.59
Batch: 100; loss: 1.29; acc: 0.56
Batch: 120; loss: 1.66; acc: 0.44
Batch: 140; loss: 1.31; acc: 0.55
Batch: 160; loss: 1.3; acc: 0.62
Batch: 180; loss: 1.47; acc: 0.47
Batch: 200; loss: 1.35; acc: 0.58
Batch: 220; loss: 1.45; acc: 0.5
Batch: 240; loss: 1.27; acc: 0.55
Batch: 260; loss: 1.33; acc: 0.59
Batch: 280; loss: 1.45; acc: 0.41
Batch: 300; loss: 1.51; acc: 0.5
Batch: 320; loss: 1.07; acc: 0.67
Batch: 340; loss: 1.63; acc: 0.48
Batch: 360; loss: 1.44; acc: 0.53
Batch: 380; loss: 1.41; acc: 0.47
Batch: 400; loss: 1.1; acc: 0.64
Batch: 420; loss: 1.38; acc: 0.58
Batch: 440; loss: 1.29; acc: 0.47
Batch: 460; loss: 1.42; acc: 0.61
Batch: 480; loss: 1.25; acc: 0.55
Batch: 500; loss: 1.28; acc: 0.55
Batch: 520; loss: 1.53; acc: 0.53
Batch: 540; loss: 1.47; acc: 0.55
Batch: 560; loss: 1.32; acc: 0.58
Batch: 580; loss: 1.25; acc: 0.56
Batch: 600; loss: 1.32; acc: 0.55
Batch: 620; loss: 1.21; acc: 0.59
Batch: 640; loss: 1.25; acc: 0.62
Batch: 660; loss: 1.39; acc: 0.59
Batch: 680; loss: 1.49; acc: 0.47
Batch: 700; loss: 1.51; acc: 0.53
Batch: 720; loss: 1.24; acc: 0.61
Batch: 740; loss: 1.43; acc: 0.61
Batch: 760; loss: 1.38; acc: 0.53
Batch: 780; loss: 1.28; acc: 0.61
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.27; acc: 0.5
Batch: 20; loss: 1.4; acc: 0.48
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 0.97; acc: 0.69
Batch: 100; loss: 1.33; acc: 0.5
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 0.85; acc: 0.72
Val Epoch over. val_loss: 1.3077594288595162; val_accuracy: 0.5728503184713376 

The current subspace-distance is: 8.0828984209802e-05 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.55; acc: 0.52
Batch: 20; loss: 1.1; acc: 0.59
Batch: 40; loss: 0.92; acc: 0.78
Batch: 60; loss: 1.53; acc: 0.53
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.24; acc: 0.55
Batch: 120; loss: 1.53; acc: 0.53
Batch: 140; loss: 1.29; acc: 0.55
Batch: 160; loss: 1.33; acc: 0.55
Batch: 180; loss: 1.26; acc: 0.58
Batch: 200; loss: 1.52; acc: 0.52
Batch: 220; loss: 1.54; acc: 0.52
Batch: 240; loss: 1.5; acc: 0.52
Batch: 260; loss: 1.32; acc: 0.55
Batch: 280; loss: 1.39; acc: 0.52
Batch: 300; loss: 1.62; acc: 0.44
Batch: 320; loss: 1.07; acc: 0.66
Batch: 340; loss: 1.34; acc: 0.56
Batch: 360; loss: 1.38; acc: 0.52
Batch: 380; loss: 1.32; acc: 0.53
Batch: 400; loss: 1.13; acc: 0.62
Batch: 420; loss: 1.52; acc: 0.42
Batch: 440; loss: 1.23; acc: 0.59
Batch: 460; loss: 1.31; acc: 0.59
Batch: 480; loss: 1.43; acc: 0.52
Batch: 500; loss: 1.1; acc: 0.67
Batch: 520; loss: 1.43; acc: 0.5
Batch: 540; loss: 1.46; acc: 0.56
Batch: 560; loss: 1.58; acc: 0.53
Batch: 580; loss: 1.4; acc: 0.55
Batch: 600; loss: 1.15; acc: 0.66
Batch: 620; loss: 1.52; acc: 0.48
Batch: 640; loss: 1.06; acc: 0.67
Batch: 660; loss: 1.42; acc: 0.52
Batch: 680; loss: 1.31; acc: 0.59
Batch: 700; loss: 1.51; acc: 0.52
Batch: 720; loss: 1.45; acc: 0.56
Batch: 740; loss: 1.4; acc: 0.52
Batch: 760; loss: 1.27; acc: 0.61
Batch: 780; loss: 1.37; acc: 0.56
Train Epoch over. train_loss: 1.36; train_accuracy: 0.54 

Batch: 0; loss: 1.27; acc: 0.45
Batch: 20; loss: 1.4; acc: 0.48
Batch: 40; loss: 0.85; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 0.98; acc: 0.69
Batch: 100; loss: 1.32; acc: 0.52
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 0.84; acc: 0.73
Val Epoch over. val_loss: 1.3090270586833832; val_accuracy: 0.5722531847133758 

The current subspace-distance is: 8.140093996189535e-05 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.68; acc: 0.48
Batch: 20; loss: 1.33; acc: 0.55
Batch: 40; loss: 1.03; acc: 0.72
Batch: 60; loss: 1.63; acc: 0.47
Batch: 80; loss: 1.28; acc: 0.52
Batch: 100; loss: 1.35; acc: 0.53
Batch: 120; loss: 1.45; acc: 0.52
Batch: 140; loss: 1.43; acc: 0.55
Batch: 160; loss: 1.13; acc: 0.64
Batch: 180; loss: 1.29; acc: 0.59
Batch: 200; loss: 1.44; acc: 0.56
Batch: 220; loss: 1.27; acc: 0.56
Batch: 240; loss: 1.29; acc: 0.52
Batch: 260; loss: 1.36; acc: 0.55
Batch: 280; loss: 1.27; acc: 0.58
Batch: 300; loss: 1.53; acc: 0.45
Batch: 320; loss: 1.48; acc: 0.53
Batch: 340; loss: 1.13; acc: 0.64
Batch: 360; loss: 1.28; acc: 0.59
Batch: 380; loss: 1.43; acc: 0.52
Batch: 400; loss: 1.29; acc: 0.5
Batch: 420; loss: 1.56; acc: 0.5
Batch: 440; loss: 1.17; acc: 0.53
Batch: 460; loss: 1.56; acc: 0.48
Batch: 480; loss: 1.18; acc: 0.67
Batch: 500; loss: 1.55; acc: 0.48
Batch: 520; loss: 1.43; acc: 0.45
Batch: 540; loss: 1.39; acc: 0.53
Batch: 560; loss: 1.56; acc: 0.55
Batch: 580; loss: 1.16; acc: 0.66
Batch: 600; loss: 1.57; acc: 0.55
Batch: 620; loss: 1.29; acc: 0.59
Batch: 640; loss: 1.39; acc: 0.55
Batch: 660; loss: 1.47; acc: 0.53
Batch: 680; loss: 1.25; acc: 0.61
Batch: 700; loss: 1.47; acc: 0.47
Batch: 720; loss: 1.18; acc: 0.61
Batch: 740; loss: 1.28; acc: 0.58
Batch: 760; loss: 1.47; acc: 0.45
Batch: 780; loss: 1.14; acc: 0.61
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.28; acc: 0.5
Batch: 20; loss: 1.4; acc: 0.52
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 1.37; acc: 0.61
Batch: 80; loss: 0.98; acc: 0.69
Batch: 100; loss: 1.33; acc: 0.53
Batch: 120; loss: 1.32; acc: 0.53
Batch: 140; loss: 0.85; acc: 0.73
Val Epoch over. val_loss: 1.308797299861908; val_accuracy: 0.5730493630573248 

The current subspace-distance is: 8.350985444849357e-05 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.3; acc: 0.5
Batch: 20; loss: 1.34; acc: 0.56
Batch: 40; loss: 1.3; acc: 0.5
Batch: 60; loss: 1.38; acc: 0.55
Batch: 80; loss: 1.35; acc: 0.64
Batch: 100; loss: 1.51; acc: 0.5
Batch: 120; loss: 1.22; acc: 0.58
Batch: 140; loss: 1.18; acc: 0.62
Batch: 160; loss: 1.54; acc: 0.47
Batch: 180; loss: 1.56; acc: 0.47
Batch: 200; loss: 1.23; acc: 0.56
Batch: 220; loss: 1.5; acc: 0.5
Batch: 240; loss: 1.35; acc: 0.5
Batch: 260; loss: 1.18; acc: 0.69
Batch: 280; loss: 1.25; acc: 0.58
Batch: 300; loss: 1.47; acc: 0.48
Batch: 320; loss: 1.25; acc: 0.56
Batch: 340; loss: 1.49; acc: 0.53
Batch: 360; loss: 1.24; acc: 0.59
Batch: 380; loss: 1.27; acc: 0.53
Batch: 400; loss: 1.1; acc: 0.61
Batch: 420; loss: 1.56; acc: 0.45
Batch: 440; loss: 1.47; acc: 0.59
Batch: 460; loss: 1.23; acc: 0.61
Batch: 480; loss: 1.52; acc: 0.52
Batch: 500; loss: 1.09; acc: 0.67
Batch: 520; loss: 1.29; acc: 0.61
Batch: 540; loss: 1.24; acc: 0.69
Batch: 560; loss: 1.5; acc: 0.48
Batch: 580; loss: 1.31; acc: 0.56
Batch: 600; loss: 1.31; acc: 0.58
Batch: 620; loss: 1.32; acc: 0.66
Batch: 640; loss: 1.33; acc: 0.58
Batch: 660; loss: 1.28; acc: 0.55
Batch: 680; loss: 1.34; acc: 0.53
Batch: 700; loss: 1.35; acc: 0.52
Batch: 720; loss: 1.37; acc: 0.61
Batch: 740; loss: 1.34; acc: 0.58
Batch: 760; loss: 1.17; acc: 0.61
Batch: 780; loss: 1.22; acc: 0.58
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.28; acc: 0.47
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 0.98; acc: 0.69
Batch: 100; loss: 1.33; acc: 0.52
Batch: 120; loss: 1.32; acc: 0.55
Batch: 140; loss: 0.85; acc: 0.75
Val Epoch over. val_loss: 1.3069641946986983; val_accuracy: 0.5710589171974523 

The current subspace-distance is: 8.252502448158339e-05 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.08; acc: 0.62
Batch: 20; loss: 1.42; acc: 0.53
Batch: 40; loss: 1.28; acc: 0.56
Batch: 60; loss: 1.24; acc: 0.62
Batch: 80; loss: 1.44; acc: 0.45
Batch: 100; loss: 1.22; acc: 0.59
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 1.29; acc: 0.55
Batch: 160; loss: 1.61; acc: 0.39
Batch: 180; loss: 1.5; acc: 0.48
Batch: 200; loss: 1.5; acc: 0.52
Batch: 220; loss: 1.03; acc: 0.69
Batch: 240; loss: 1.37; acc: 0.58
Batch: 260; loss: 1.38; acc: 0.5
Batch: 280; loss: 1.72; acc: 0.41
Batch: 300; loss: 1.41; acc: 0.53
Batch: 320; loss: 1.56; acc: 0.41
Batch: 340; loss: 1.22; acc: 0.61
Batch: 360; loss: 1.33; acc: 0.53
Batch: 380; loss: 1.25; acc: 0.55
Batch: 400; loss: 1.64; acc: 0.48
Batch: 420; loss: 1.53; acc: 0.53
Batch: 440; loss: 1.42; acc: 0.52
Batch: 460; loss: 1.17; acc: 0.59
Batch: 480; loss: 1.41; acc: 0.59
Batch: 500; loss: 1.43; acc: 0.5
Batch: 520; loss: 1.19; acc: 0.59
Batch: 540; loss: 1.39; acc: 0.61
Batch: 560; loss: 1.37; acc: 0.62
Batch: 580; loss: 1.28; acc: 0.53
Batch: 600; loss: 1.23; acc: 0.59
Batch: 620; loss: 1.31; acc: 0.55
Batch: 640; loss: 1.26; acc: 0.52
Batch: 660; loss: 1.39; acc: 0.52
Batch: 680; loss: 1.19; acc: 0.67
Batch: 700; loss: 1.23; acc: 0.58
Batch: 720; loss: 1.14; acc: 0.64
Batch: 740; loss: 1.33; acc: 0.53
Batch: 760; loss: 1.42; acc: 0.48
Batch: 780; loss: 1.34; acc: 0.58
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.28; acc: 0.5
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.37; acc: 0.59
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.52
Batch: 120; loss: 1.33; acc: 0.58
Batch: 140; loss: 0.84; acc: 0.75
Val Epoch over. val_loss: 1.306492666909649; val_accuracy: 0.5728503184713376 

The current subspace-distance is: 8.297299063997343e-05 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.47; acc: 0.5
Batch: 20; loss: 1.54; acc: 0.56
Batch: 40; loss: 1.41; acc: 0.48
Batch: 60; loss: 1.31; acc: 0.55
Batch: 80; loss: 1.53; acc: 0.5
Batch: 100; loss: 1.05; acc: 0.64
Batch: 120; loss: 1.11; acc: 0.61
Batch: 140; loss: 1.23; acc: 0.61
Batch: 160; loss: 1.56; acc: 0.52
Batch: 180; loss: 1.35; acc: 0.52
Batch: 200; loss: 1.39; acc: 0.5
Batch: 220; loss: 1.41; acc: 0.48
Batch: 240; loss: 1.38; acc: 0.53
Batch: 260; loss: 1.38; acc: 0.47
Batch: 280; loss: 1.36; acc: 0.52
Batch: 300; loss: 1.57; acc: 0.56
Batch: 320; loss: 1.73; acc: 0.45
Batch: 340; loss: 1.46; acc: 0.5
Batch: 360; loss: 1.4; acc: 0.53
Batch: 380; loss: 1.17; acc: 0.59
Batch: 400; loss: 1.41; acc: 0.52
Batch: 420; loss: 1.24; acc: 0.58
Batch: 440; loss: 1.38; acc: 0.58
Batch: 460; loss: 1.52; acc: 0.53
Batch: 480; loss: 1.32; acc: 0.5
Batch: 500; loss: 1.01; acc: 0.66
Batch: 520; loss: 1.64; acc: 0.47
Batch: 540; loss: 1.08; acc: 0.66
Batch: 560; loss: 1.14; acc: 0.67
Batch: 580; loss: 1.21; acc: 0.66
Batch: 600; loss: 1.61; acc: 0.52
Batch: 620; loss: 1.11; acc: 0.62
Batch: 640; loss: 1.15; acc: 0.61
Batch: 660; loss: 1.39; acc: 0.52
Batch: 680; loss: 1.35; acc: 0.55
Batch: 700; loss: 1.21; acc: 0.58
Batch: 720; loss: 1.19; acc: 0.58
Batch: 740; loss: 1.41; acc: 0.55
Batch: 760; loss: 1.31; acc: 0.59
Batch: 780; loss: 1.3; acc: 0.56
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.28; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.52
Batch: 120; loss: 1.33; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 1.3060663308307623; val_accuracy: 0.5721536624203821 

The current subspace-distance is: 8.55094549478963e-05 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.22; acc: 0.58
Batch: 20; loss: 1.36; acc: 0.56
Batch: 40; loss: 1.22; acc: 0.52
Batch: 60; loss: 1.27; acc: 0.59
Batch: 80; loss: 1.35; acc: 0.58
Batch: 100; loss: 1.48; acc: 0.5
Batch: 120; loss: 1.26; acc: 0.59
Batch: 140; loss: 1.29; acc: 0.56
Batch: 160; loss: 1.14; acc: 0.67
Batch: 180; loss: 1.27; acc: 0.56
Batch: 200; loss: 1.18; acc: 0.59
Batch: 220; loss: 1.25; acc: 0.59
Batch: 240; loss: 1.52; acc: 0.42
Batch: 260; loss: 1.25; acc: 0.59
Batch: 280; loss: 1.35; acc: 0.5
Batch: 300; loss: 1.22; acc: 0.59
Batch: 320; loss: 1.31; acc: 0.52
Batch: 340; loss: 1.71; acc: 0.41
Batch: 360; loss: 1.28; acc: 0.61
Batch: 380; loss: 1.17; acc: 0.66
Batch: 400; loss: 1.43; acc: 0.58
Batch: 420; loss: 1.34; acc: 0.59
Batch: 440; loss: 1.3; acc: 0.59
Batch: 460; loss: 1.18; acc: 0.64
Batch: 480; loss: 1.32; acc: 0.52
Batch: 500; loss: 1.52; acc: 0.53
Batch: 520; loss: 1.01; acc: 0.69
Batch: 540; loss: 1.24; acc: 0.58
Batch: 560; loss: 1.64; acc: 0.42
Batch: 580; loss: 1.42; acc: 0.5
Batch: 600; loss: 1.22; acc: 0.59
Batch: 620; loss: 1.43; acc: 0.55
Batch: 640; loss: 1.4; acc: 0.5
Batch: 660; loss: 1.4; acc: 0.58
Batch: 680; loss: 1.19; acc: 0.58
Batch: 700; loss: 1.36; acc: 0.5
Batch: 720; loss: 1.16; acc: 0.61
Batch: 740; loss: 1.32; acc: 0.5
Batch: 760; loss: 1.44; acc: 0.52
Batch: 780; loss: 1.26; acc: 0.53
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.28; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 0.97; acc: 0.69
Batch: 100; loss: 1.33; acc: 0.5
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.75
Val Epoch over. val_loss: 1.305613442970689; val_accuracy: 0.5732484076433121 

The current subspace-distance is: 8.467448060400784e-05 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.44; acc: 0.55
Batch: 20; loss: 1.38; acc: 0.56
Batch: 40; loss: 1.19; acc: 0.7
Batch: 60; loss: 1.32; acc: 0.59
Batch: 80; loss: 1.5; acc: 0.55
Batch: 100; loss: 1.27; acc: 0.56
Batch: 120; loss: 1.38; acc: 0.53
Batch: 140; loss: 1.35; acc: 0.59
Batch: 160; loss: 1.09; acc: 0.67
Batch: 180; loss: 1.24; acc: 0.52
Batch: 200; loss: 1.74; acc: 0.42
Batch: 220; loss: 1.45; acc: 0.47
Batch: 240; loss: 1.27; acc: 0.48
Batch: 260; loss: 1.59; acc: 0.48
Batch: 280; loss: 1.27; acc: 0.56
Batch: 300; loss: 1.3; acc: 0.62
Batch: 320; loss: 1.24; acc: 0.5
Batch: 340; loss: 1.27; acc: 0.62
Batch: 360; loss: 1.41; acc: 0.5
Batch: 380; loss: 1.5; acc: 0.5
Batch: 400; loss: 1.24; acc: 0.53
Batch: 420; loss: 1.33; acc: 0.52
Batch: 440; loss: 1.32; acc: 0.56
Batch: 460; loss: 1.52; acc: 0.5
Batch: 480; loss: 1.28; acc: 0.56
Batch: 500; loss: 1.21; acc: 0.53
Batch: 520; loss: 1.4; acc: 0.53
Batch: 540; loss: 1.2; acc: 0.56
Batch: 560; loss: 1.56; acc: 0.47
Batch: 580; loss: 1.36; acc: 0.53
Batch: 600; loss: 1.32; acc: 0.62
Batch: 620; loss: 1.42; acc: 0.45
Batch: 640; loss: 1.52; acc: 0.5
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 1.45; acc: 0.55
Batch: 700; loss: 1.49; acc: 0.5
Batch: 720; loss: 1.21; acc: 0.53
Batch: 740; loss: 1.57; acc: 0.44
Batch: 760; loss: 1.5; acc: 0.55
Batch: 780; loss: 1.01; acc: 0.69
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.28; acc: 0.47
Batch: 20; loss: 1.41; acc: 0.48
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.5
Batch: 120; loss: 1.32; acc: 0.58
Batch: 140; loss: 0.84; acc: 0.75
Val Epoch over. val_loss: 1.305937248050787; val_accuracy: 0.5715565286624203 

The current subspace-distance is: 8.51254299050197e-05 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.28; acc: 0.62
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 1.49; acc: 0.52
Batch: 60; loss: 1.48; acc: 0.45
Batch: 80; loss: 1.26; acc: 0.59
Batch: 100; loss: 1.31; acc: 0.56
Batch: 120; loss: 1.37; acc: 0.56
Batch: 140; loss: 1.28; acc: 0.58
Batch: 160; loss: 1.2; acc: 0.64
Batch: 180; loss: 1.24; acc: 0.67
Batch: 200; loss: 1.64; acc: 0.52
Batch: 220; loss: 1.38; acc: 0.59
Batch: 240; loss: 1.26; acc: 0.59
Batch: 260; loss: 1.44; acc: 0.58
Batch: 280; loss: 1.48; acc: 0.48
Batch: 300; loss: 1.42; acc: 0.48
Batch: 320; loss: 1.52; acc: 0.47
Batch: 340; loss: 1.25; acc: 0.64
Batch: 360; loss: 1.34; acc: 0.55
Batch: 380; loss: 1.1; acc: 0.69
Batch: 400; loss: 1.58; acc: 0.42
Batch: 420; loss: 1.24; acc: 0.55
Batch: 440; loss: 1.22; acc: 0.59
Batch: 460; loss: 1.41; acc: 0.56
Batch: 480; loss: 1.23; acc: 0.55
Batch: 500; loss: 1.2; acc: 0.58
Batch: 520; loss: 1.1; acc: 0.64
Batch: 540; loss: 1.41; acc: 0.52
Batch: 560; loss: 1.31; acc: 0.56
Batch: 580; loss: 1.43; acc: 0.53
Batch: 600; loss: 1.41; acc: 0.55
Batch: 620; loss: 1.41; acc: 0.53
Batch: 640; loss: 1.55; acc: 0.5
Batch: 660; loss: 1.6; acc: 0.48
Batch: 680; loss: 1.49; acc: 0.5
Batch: 700; loss: 1.6; acc: 0.52
Batch: 720; loss: 1.42; acc: 0.56
Batch: 740; loss: 1.43; acc: 0.44
Batch: 760; loss: 1.27; acc: 0.56
Batch: 780; loss: 1.2; acc: 0.61
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.28; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.48
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.52
Batch: 120; loss: 1.33; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.75
Val Epoch over. val_loss: 1.3056602428673179; val_accuracy: 0.5727507961783439 

The current subspace-distance is: 8.733371214475483e-05 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.38; acc: 0.58
Batch: 20; loss: 1.11; acc: 0.64
Batch: 40; loss: 1.13; acc: 0.64
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 1.38; acc: 0.53
Batch: 100; loss: 1.53; acc: 0.45
Batch: 120; loss: 1.36; acc: 0.58
Batch: 140; loss: 1.07; acc: 0.7
Batch: 160; loss: 1.28; acc: 0.58
Batch: 180; loss: 1.32; acc: 0.61
Batch: 200; loss: 1.17; acc: 0.59
Batch: 220; loss: 1.46; acc: 0.44
Batch: 240; loss: 1.13; acc: 0.67
Batch: 260; loss: 1.65; acc: 0.55
Batch: 280; loss: 1.16; acc: 0.59
Batch: 300; loss: 1.6; acc: 0.45
Batch: 320; loss: 1.27; acc: 0.58
Batch: 340; loss: 1.46; acc: 0.52
Batch: 360; loss: 1.15; acc: 0.64
Batch: 380; loss: 1.4; acc: 0.53
Batch: 400; loss: 1.44; acc: 0.52
Batch: 420; loss: 1.39; acc: 0.55
Batch: 440; loss: 1.33; acc: 0.5
Batch: 460; loss: 1.4; acc: 0.56
Batch: 480; loss: 1.25; acc: 0.58
Batch: 500; loss: 1.57; acc: 0.47
Batch: 520; loss: 1.45; acc: 0.45
Batch: 540; loss: 1.31; acc: 0.69
Batch: 560; loss: 1.27; acc: 0.59
Batch: 580; loss: 1.36; acc: 0.48
Batch: 600; loss: 1.54; acc: 0.45
Batch: 620; loss: 1.33; acc: 0.53
Batch: 640; loss: 1.39; acc: 0.61
Batch: 660; loss: 1.38; acc: 0.59
Batch: 680; loss: 1.42; acc: 0.53
Batch: 700; loss: 1.41; acc: 0.56
Batch: 720; loss: 1.32; acc: 0.55
Batch: 740; loss: 1.33; acc: 0.56
Batch: 760; loss: 1.41; acc: 0.42
Batch: 780; loss: 1.54; acc: 0.44
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.28; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.48
Batch: 40; loss: 0.85; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.61
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 1.33; acc: 0.5
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 1.3050861480129752; val_accuracy: 0.5740445859872612 

The current subspace-distance is: 8.675998833496124e-05 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.56; acc: 0.5
Batch: 20; loss: 1.28; acc: 0.52
Batch: 40; loss: 1.14; acc: 0.67
Batch: 60; loss: 1.35; acc: 0.56
Batch: 80; loss: 1.24; acc: 0.56
Batch: 100; loss: 1.57; acc: 0.48
Batch: 120; loss: 1.11; acc: 0.62
Batch: 140; loss: 1.42; acc: 0.48
Batch: 160; loss: 1.29; acc: 0.59
Batch: 180; loss: 1.31; acc: 0.48
Batch: 200; loss: 1.22; acc: 0.59
Batch: 220; loss: 1.46; acc: 0.58
Batch: 240; loss: 1.38; acc: 0.56
Batch: 260; loss: 1.44; acc: 0.55
Batch: 280; loss: 1.52; acc: 0.48
Batch: 300; loss: 1.37; acc: 0.55
Batch: 320; loss: 1.29; acc: 0.53
Batch: 340; loss: 1.34; acc: 0.56
Batch: 360; loss: 1.46; acc: 0.56
Batch: 380; loss: 1.16; acc: 0.61
Batch: 400; loss: 1.39; acc: 0.56
Batch: 420; loss: 1.69; acc: 0.44
Batch: 440; loss: 1.18; acc: 0.64
Batch: 460; loss: 1.34; acc: 0.55
Batch: 480; loss: 1.29; acc: 0.56
Batch: 500; loss: 1.42; acc: 0.5
Batch: 520; loss: 1.49; acc: 0.53
Batch: 540; loss: 1.13; acc: 0.61
Batch: 560; loss: 1.65; acc: 0.5
Batch: 580; loss: 1.48; acc: 0.53
Batch: 600; loss: 1.21; acc: 0.58
Batch: 620; loss: 1.22; acc: 0.59
Batch: 640; loss: 1.53; acc: 0.36
Batch: 660; loss: 1.26; acc: 0.66
Batch: 680; loss: 1.39; acc: 0.52
Batch: 700; loss: 1.23; acc: 0.59
Batch: 720; loss: 1.54; acc: 0.58
Batch: 740; loss: 1.28; acc: 0.61
Batch: 760; loss: 1.47; acc: 0.53
Batch: 780; loss: 1.41; acc: 0.56
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.28; acc: 0.5
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 0.85; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 1.32; acc: 0.5
Batch: 120; loss: 1.32; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 1.3051615566205068; val_accuracy: 0.574343152866242 

The current subspace-distance is: 8.823531970847398e-05 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.67; acc: 0.52
Batch: 20; loss: 1.3; acc: 0.58
Batch: 40; loss: 1.52; acc: 0.45
Batch: 60; loss: 1.36; acc: 0.5
Batch: 80; loss: 1.36; acc: 0.53
Batch: 100; loss: 1.38; acc: 0.48
Batch: 120; loss: 1.34; acc: 0.52
Batch: 140; loss: 1.43; acc: 0.5
Batch: 160; loss: 1.63; acc: 0.45
Batch: 180; loss: 1.22; acc: 0.55
Batch: 200; loss: 1.36; acc: 0.48
Batch: 220; loss: 1.16; acc: 0.61
Batch: 240; loss: 1.33; acc: 0.58
Batch: 260; loss: 1.39; acc: 0.59
Batch: 280; loss: 1.36; acc: 0.44
Batch: 300; loss: 1.56; acc: 0.47
Batch: 320; loss: 1.31; acc: 0.59
Batch: 340; loss: 1.2; acc: 0.55
Batch: 360; loss: 1.41; acc: 0.56
Batch: 380; loss: 1.32; acc: 0.61
Batch: 400; loss: 1.46; acc: 0.52
Batch: 420; loss: 1.4; acc: 0.52
Batch: 440; loss: 1.07; acc: 0.66
Batch: 460; loss: 1.49; acc: 0.55
Batch: 480; loss: 1.2; acc: 0.55
Batch: 500; loss: 1.53; acc: 0.48
Batch: 520; loss: 1.39; acc: 0.59
Batch: 540; loss: 1.32; acc: 0.61
Batch: 560; loss: 1.62; acc: 0.59
Batch: 580; loss: 1.35; acc: 0.55
Batch: 600; loss: 1.02; acc: 0.64
Batch: 620; loss: 1.35; acc: 0.56
Batch: 640; loss: 1.22; acc: 0.58
Batch: 660; loss: 1.17; acc: 0.55
Batch: 680; loss: 1.37; acc: 0.52
Batch: 700; loss: 1.2; acc: 0.58
Batch: 720; loss: 1.46; acc: 0.53
Batch: 740; loss: 1.33; acc: 0.62
Batch: 760; loss: 1.23; acc: 0.59
Batch: 780; loss: 1.37; acc: 0.58
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.29; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.5
Batch: 120; loss: 1.33; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.75
Val Epoch over. val_loss: 1.3054044557984468; val_accuracy: 0.5736464968152867 

The current subspace-distance is: 9.039113501785323e-05 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.4; acc: 0.53
Batch: 20; loss: 1.57; acc: 0.45
Batch: 40; loss: 1.15; acc: 0.67
Batch: 60; loss: 1.45; acc: 0.58
Batch: 80; loss: 1.3; acc: 0.59
Batch: 100; loss: 1.2; acc: 0.67
Batch: 120; loss: 1.39; acc: 0.58
Batch: 140; loss: 1.34; acc: 0.55
Batch: 160; loss: 1.4; acc: 0.53
Batch: 180; loss: 1.42; acc: 0.47
Batch: 200; loss: 1.51; acc: 0.44
Batch: 220; loss: 1.35; acc: 0.5
Batch: 240; loss: 1.4; acc: 0.48
Batch: 260; loss: 1.12; acc: 0.62
Batch: 280; loss: 1.47; acc: 0.45
Batch: 300; loss: 1.43; acc: 0.5
Batch: 320; loss: 1.52; acc: 0.42
Batch: 340; loss: 1.46; acc: 0.45
Batch: 360; loss: 1.15; acc: 0.61
Batch: 380; loss: 1.13; acc: 0.67
Batch: 400; loss: 1.22; acc: 0.69
Batch: 420; loss: 1.45; acc: 0.41
Batch: 440; loss: 1.4; acc: 0.58
Batch: 460; loss: 1.43; acc: 0.53
Batch: 480; loss: 1.08; acc: 0.66
Batch: 500; loss: 1.23; acc: 0.56
Batch: 520; loss: 1.35; acc: 0.53
Batch: 540; loss: 1.39; acc: 0.5
Batch: 560; loss: 1.56; acc: 0.48
Batch: 580; loss: 1.35; acc: 0.58
Batch: 600; loss: 1.45; acc: 0.52
Batch: 620; loss: 1.51; acc: 0.53
Batch: 640; loss: 1.4; acc: 0.52
Batch: 660; loss: 1.26; acc: 0.55
Batch: 680; loss: 1.54; acc: 0.5
Batch: 700; loss: 1.26; acc: 0.56
Batch: 720; loss: 1.25; acc: 0.56
Batch: 740; loss: 1.66; acc: 0.42
Batch: 760; loss: 1.21; acc: 0.56
Batch: 780; loss: 1.16; acc: 0.61
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.29; acc: 0.5
Batch: 20; loss: 1.4; acc: 0.48
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.48
Batch: 120; loss: 1.33; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 1.3049711812833311; val_accuracy: 0.5734474522292994 

The current subspace-distance is: 9.207291441271082e-05 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.43; acc: 0.5
Batch: 20; loss: 1.63; acc: 0.48
Batch: 40; loss: 1.21; acc: 0.56
Batch: 60; loss: 1.39; acc: 0.59
Batch: 80; loss: 1.36; acc: 0.53
Batch: 100; loss: 1.28; acc: 0.53
Batch: 120; loss: 1.43; acc: 0.44
Batch: 140; loss: 1.52; acc: 0.44
Batch: 160; loss: 0.98; acc: 0.7
Batch: 180; loss: 1.59; acc: 0.44
Batch: 200; loss: 1.45; acc: 0.47
Batch: 220; loss: 1.68; acc: 0.5
Batch: 240; loss: 1.48; acc: 0.5
Batch: 260; loss: 1.54; acc: 0.48
Batch: 280; loss: 1.47; acc: 0.52
Batch: 300; loss: 1.49; acc: 0.53
Batch: 320; loss: 1.27; acc: 0.61
Batch: 340; loss: 1.37; acc: 0.52
Batch: 360; loss: 1.15; acc: 0.58
Batch: 380; loss: 1.43; acc: 0.56
Batch: 400; loss: 1.41; acc: 0.55
Batch: 420; loss: 1.26; acc: 0.56
Batch: 440; loss: 1.25; acc: 0.5
Batch: 460; loss: 1.17; acc: 0.59
Batch: 480; loss: 1.4; acc: 0.48
Batch: 500; loss: 1.37; acc: 0.62
Batch: 520; loss: 1.43; acc: 0.56
Batch: 540; loss: 1.47; acc: 0.45
Batch: 560; loss: 0.94; acc: 0.8
Batch: 580; loss: 1.43; acc: 0.61
Batch: 600; loss: 1.42; acc: 0.48
Batch: 620; loss: 1.32; acc: 0.58
Batch: 640; loss: 1.49; acc: 0.59
Batch: 660; loss: 1.56; acc: 0.47
Batch: 680; loss: 1.51; acc: 0.52
Batch: 700; loss: 1.31; acc: 0.64
Batch: 720; loss: 1.7; acc: 0.38
Batch: 740; loss: 1.42; acc: 0.55
Batch: 760; loss: 1.41; acc: 0.5
Batch: 780; loss: 1.26; acc: 0.64
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.29; acc: 0.48
Batch: 20; loss: 1.4; acc: 0.5
Batch: 40; loss: 0.86; acc: 0.73
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 0.97; acc: 0.7
Batch: 100; loss: 1.33; acc: 0.48
Batch: 120; loss: 1.33; acc: 0.59
Batch: 140; loss: 0.84; acc: 0.75
Val Epoch over. val_loss: 1.3055254182997782; val_accuracy: 0.5739450636942676 

The current subspace-distance is: 9.315904753748327e-05 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.29; acc: 0.56
Batch: 20; loss: 1.07; acc: 0.66
Batch: 40; loss: 1.3; acc: 0.53
Batch: 60; loss: 1.37; acc: 0.59
Batch: 80; loss: 1.57; acc: 0.42
Batch: 100; loss: 1.3; acc: 0.55
Batch: 120; loss: 1.44; acc: 0.5
Batch: 140; loss: 1.57; acc: 0.47
Batch: 160; loss: 1.35; acc: 0.55
Batch: 180; loss: 1.37; acc: 0.62
Batch: 200; loss: 1.23; acc: 0.56
Batch: 220; loss: 1.57; acc: 0.39
Batch: 240; loss: 1.5; acc: 0.48
Batch: 260; loss: 1.42; acc: 0.5
Batch: 280; loss: 1.23; acc: 0.52
Batch: 300; loss: 1.47; acc: 0.52
Batch: 320; loss: 1.32; acc: 0.48
Batch: 340; loss: 1.56; acc: 0.45
Batch: 360; loss: 1.36; acc: 0.56
Batch: 380; loss: 1.63; acc: 0.45
Batch: 400; loss: 1.43; acc: 0.55
Batch: 420; loss: 1.59; acc: 0.48
Batch: 440; loss: 1.17; acc: 0.59
Batch: 460; loss: 1.41; acc: 0.56
Batch: 480; loss: 1.37; acc: 0.56
Batch: 500; loss: 1.59; acc: 0.45
Batch: 520; loss: 1.47; acc: 0.55
Batch: 540; loss: 1.5; acc: 0.53
Batch: 560; loss: 1.24; acc: 0.61
Batch: 580; loss: 1.56; acc: 0.42
Batch: 600; loss: 1.4; acc: 0.48
Batch: 620; loss: 1.12; acc: 0.58
Batch: 640; loss: 1.57; acc: 0.45
Batch: 660; loss: 1.3; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.59
Batch: 700; loss: 1.35; acc: 0.53
Batch: 720; loss: 1.34; acc: 0.53
Batch: 740; loss: 1.33; acc: 0.61
Batch: 760; loss: 1.36; acc: 0.52
Batch: 780; loss: 1.27; acc: 0.53
Train Epoch over. train_loss: 1.36; train_accuracy: 0.55 

Batch: 0; loss: 1.29; acc: 0.5
Batch: 20; loss: 1.39; acc: 0.5
Batch: 40; loss: 0.85; acc: 0.75
Batch: 60; loss: 1.36; acc: 0.59
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 1.32; acc: 0.5
Batch: 120; loss: 1.32; acc: 0.61
Batch: 140; loss: 0.84; acc: 0.77
Val Epoch over. val_loss: 1.3047272870495061; val_accuracy: 0.5746417197452229 

The current subspace-distance is: 9.19308586162515e-05 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 15953
elements in E: 3331950
fraction nonzero: 0.004787886973093834
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.08
Batch: 40; loss: 2.32; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.31; acc: 0.12
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.05
Batch: 140; loss: 2.29; acc: 0.14
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.06
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.29; acc: 0.08
Batch: 300; loss: 2.27; acc: 0.17
Batch: 320; loss: 2.29; acc: 0.09
Batch: 340; loss: 2.27; acc: 0.17
Batch: 360; loss: 2.27; acc: 0.08
Batch: 380; loss: 2.28; acc: 0.12
Batch: 400; loss: 2.28; acc: 0.16
Batch: 420; loss: 2.26; acc: 0.23
Batch: 440; loss: 2.27; acc: 0.23
Batch: 460; loss: 2.28; acc: 0.12
Batch: 480; loss: 2.27; acc: 0.27
Batch: 500; loss: 2.28; acc: 0.17
Batch: 520; loss: 2.26; acc: 0.22
Batch: 540; loss: 2.25; acc: 0.28
Batch: 560; loss: 2.25; acc: 0.23
Batch: 580; loss: 2.24; acc: 0.34
Batch: 600; loss: 2.24; acc: 0.33
Batch: 620; loss: 2.23; acc: 0.34
Batch: 640; loss: 2.23; acc: 0.28
Batch: 660; loss: 2.19; acc: 0.45
Batch: 680; loss: 2.23; acc: 0.17
Batch: 700; loss: 2.21; acc: 0.19
Batch: 720; loss: 2.19; acc: 0.39
Batch: 740; loss: 2.21; acc: 0.22
Batch: 760; loss: 2.13; acc: 0.3
Batch: 780; loss: 2.14; acc: 0.28
Train Epoch over. train_loss: 2.26; train_accuracy: 0.18 

Batch: 0; loss: 2.12; acc: 0.36
Batch: 20; loss: 2.06; acc: 0.31
Batch: 40; loss: 1.98; acc: 0.56
Batch: 60; loss: 2.05; acc: 0.39
Batch: 80; loss: 2.06; acc: 0.39
Batch: 100; loss: 2.07; acc: 0.39
Batch: 120; loss: 2.11; acc: 0.3
Batch: 140; loss: 2.07; acc: 0.31
Val Epoch over. val_loss: 2.103599402555235; val_accuracy: 0.3315087579617834 

The current subspace-distance is: 5.1494753279257566e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.18; acc: 0.22
Batch: 20; loss: 2.09; acc: 0.41
Batch: 40; loss: 2.11; acc: 0.3
Batch: 60; loss: 2.05; acc: 0.33
Batch: 80; loss: 1.89; acc: 0.44
Batch: 100; loss: 1.9; acc: 0.41
Batch: 120; loss: 1.88; acc: 0.42
Batch: 140; loss: 1.82; acc: 0.39
Batch: 160; loss: 1.7; acc: 0.44
Batch: 180; loss: 1.59; acc: 0.47
Batch: 200; loss: 1.42; acc: 0.5
Batch: 220; loss: 1.43; acc: 0.55
Batch: 240; loss: 1.62; acc: 0.41
Batch: 260; loss: 1.4; acc: 0.52
Batch: 280; loss: 1.19; acc: 0.67
Batch: 300; loss: 1.24; acc: 0.64
Batch: 320; loss: 1.17; acc: 0.59
Batch: 340; loss: 1.33; acc: 0.58
Batch: 360; loss: 1.3; acc: 0.52
Batch: 380; loss: 1.11; acc: 0.64
Batch: 400; loss: 1.27; acc: 0.55
Batch: 420; loss: 1.12; acc: 0.59
Batch: 440; loss: 1.32; acc: 0.61
Batch: 460; loss: 1.36; acc: 0.64
Batch: 480; loss: 1.37; acc: 0.56
Batch: 500; loss: 1.16; acc: 0.64
Batch: 520; loss: 1.01; acc: 0.67
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 1.05; acc: 0.73
Batch: 580; loss: 1.23; acc: 0.59
Batch: 600; loss: 1.07; acc: 0.66
Batch: 620; loss: 1.19; acc: 0.62
Batch: 640; loss: 1.34; acc: 0.58
Batch: 660; loss: 1.25; acc: 0.61
Batch: 680; loss: 1.12; acc: 0.67
Batch: 700; loss: 1.05; acc: 0.64
Batch: 720; loss: 1.25; acc: 0.55
Batch: 740; loss: 1.08; acc: 0.58
Batch: 760; loss: 1.13; acc: 0.61
Batch: 780; loss: 1.05; acc: 0.59
Train Epoch over. train_loss: 1.37; train_accuracy: 0.55 

Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.62
Batch: 40; loss: 0.66; acc: 0.72
Batch: 60; loss: 0.99; acc: 0.66
Batch: 80; loss: 1.07; acc: 0.64
Batch: 100; loss: 1.15; acc: 0.64
Batch: 120; loss: 1.27; acc: 0.61
Batch: 140; loss: 1.19; acc: 0.59
Val Epoch over. val_loss: 1.1310428536621628; val_accuracy: 0.6215167197452229 

The current subspace-distance is: 1.6563062672503293e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.38; acc: 0.53
Batch: 20; loss: 1.12; acc: 0.66
Batch: 40; loss: 1.05; acc: 0.62
Batch: 60; loss: 1.16; acc: 0.58
Batch: 80; loss: 1.22; acc: 0.61
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 1.17; acc: 0.61
Batch: 160; loss: 1.08; acc: 0.64
Batch: 180; loss: 1.17; acc: 0.61
Batch: 200; loss: 1.03; acc: 0.7
Batch: 220; loss: 0.97; acc: 0.62
Batch: 240; loss: 0.97; acc: 0.72
Batch: 260; loss: 1.14; acc: 0.7
Batch: 280; loss: 1.06; acc: 0.7
Batch: 300; loss: 1.02; acc: 0.67
Batch: 320; loss: 1.08; acc: 0.64
Batch: 340; loss: 1.13; acc: 0.66
Batch: 360; loss: 1.23; acc: 0.67
Batch: 380; loss: 1.34; acc: 0.5
Batch: 400; loss: 0.89; acc: 0.75
Batch: 420; loss: 0.9; acc: 0.67
Batch: 440; loss: 0.93; acc: 0.7
Batch: 460; loss: 1.1; acc: 0.64
Batch: 480; loss: 1.17; acc: 0.61
Batch: 500; loss: 1.0; acc: 0.62
Batch: 520; loss: 1.04; acc: 0.62
Batch: 540; loss: 1.02; acc: 0.64
Batch: 560; loss: 0.95; acc: 0.69
Batch: 580; loss: 0.98; acc: 0.64
Batch: 600; loss: 1.12; acc: 0.64
Batch: 620; loss: 1.15; acc: 0.64
Batch: 640; loss: 1.42; acc: 0.58
Batch: 660; loss: 1.21; acc: 0.59
Batch: 680; loss: 1.11; acc: 0.59
Batch: 700; loss: 0.89; acc: 0.7
Batch: 720; loss: 1.13; acc: 0.62
Batch: 740; loss: 1.05; acc: 0.72
Batch: 760; loss: 0.92; acc: 0.72
Batch: 780; loss: 1.23; acc: 0.66
Train Epoch over. train_loss: 1.1; train_accuracy: 0.64 

Batch: 0; loss: 1.06; acc: 0.62
Batch: 20; loss: 1.38; acc: 0.56
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.89; acc: 0.67
Batch: 80; loss: 0.97; acc: 0.72
Batch: 100; loss: 1.02; acc: 0.75
Batch: 120; loss: 1.33; acc: 0.56
Batch: 140; loss: 1.12; acc: 0.56
Val Epoch over. val_loss: 1.0701174542402765; val_accuracy: 0.6522691082802548 

The current subspace-distance is: 2.3219630747917108e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.65; acc: 0.84
Batch: 20; loss: 0.95; acc: 0.72
Batch: 40; loss: 0.95; acc: 0.64
Batch: 60; loss: 1.26; acc: 0.53
Batch: 80; loss: 0.98; acc: 0.67
Batch: 100; loss: 0.99; acc: 0.62
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 0.91; acc: 0.66
Batch: 160; loss: 0.94; acc: 0.75
Batch: 180; loss: 1.02; acc: 0.62
Batch: 200; loss: 1.19; acc: 0.61
Batch: 220; loss: 1.17; acc: 0.67
Batch: 240; loss: 1.17; acc: 0.61
Batch: 260; loss: 0.99; acc: 0.64
Batch: 280; loss: 1.23; acc: 0.62
Batch: 300; loss: 1.03; acc: 0.62
Batch: 320; loss: 0.89; acc: 0.73
Batch: 340; loss: 0.99; acc: 0.7
Batch: 360; loss: 0.93; acc: 0.64
Batch: 380; loss: 1.05; acc: 0.69
Batch: 400; loss: 1.05; acc: 0.66
Batch: 420; loss: 1.37; acc: 0.53
Batch: 440; loss: 0.92; acc: 0.7
Batch: 460; loss: 1.16; acc: 0.58
Batch: 480; loss: 0.84; acc: 0.73
Batch: 500; loss: 1.05; acc: 0.67
Batch: 520; loss: 0.82; acc: 0.69
Batch: 540; loss: 0.96; acc: 0.66
Batch: 560; loss: 1.28; acc: 0.64
Batch: 580; loss: 1.01; acc: 0.69
Batch: 600; loss: 1.16; acc: 0.67
Batch: 620; loss: 0.85; acc: 0.8
Batch: 640; loss: 1.21; acc: 0.59
Batch: 660; loss: 1.47; acc: 0.61
Batch: 680; loss: 1.27; acc: 0.67
Batch: 700; loss: 0.91; acc: 0.7
Batch: 720; loss: 1.14; acc: 0.59
Batch: 740; loss: 1.21; acc: 0.67
Batch: 760; loss: 0.98; acc: 0.7
Batch: 780; loss: 1.06; acc: 0.66
Train Epoch over. train_loss: 1.08; train_accuracy: 0.65 

Batch: 0; loss: 1.15; acc: 0.56
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 0.69; acc: 0.8
Batch: 60; loss: 1.05; acc: 0.7
Batch: 80; loss: 1.08; acc: 0.62
Batch: 100; loss: 1.14; acc: 0.66
Batch: 120; loss: 1.31; acc: 0.52
Batch: 140; loss: 1.27; acc: 0.53
Val Epoch over. val_loss: 1.1213395147566583; val_accuracy: 0.6271894904458599 

The current subspace-distance is: 2.7091053198091686e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.64
Batch: 20; loss: 1.11; acc: 0.58
Batch: 40; loss: 0.99; acc: 0.69
Batch: 60; loss: 1.13; acc: 0.66
Batch: 80; loss: 1.05; acc: 0.67
Batch: 100; loss: 1.37; acc: 0.55
Batch: 120; loss: 1.15; acc: 0.59
Batch: 140; loss: 0.93; acc: 0.69
Batch: 160; loss: 1.07; acc: 0.67
Batch: 180; loss: 0.77; acc: 0.7
Batch: 200; loss: 1.02; acc: 0.62
Batch: 220; loss: 1.31; acc: 0.62
Batch: 240; loss: 1.04; acc: 0.67
Batch: 260; loss: 1.26; acc: 0.59
Batch: 280; loss: 1.01; acc: 0.64
Batch: 300; loss: 1.15; acc: 0.66
Batch: 320; loss: 1.15; acc: 0.64
Batch: 340; loss: 0.9; acc: 0.69
Batch: 360; loss: 1.15; acc: 0.61
Batch: 380; loss: 1.02; acc: 0.72
Batch: 400; loss: 0.82; acc: 0.73
Batch: 420; loss: 1.06; acc: 0.73
Batch: 440; loss: 0.99; acc: 0.69
Batch: 460; loss: 0.9; acc: 0.75
Batch: 480; loss: 1.18; acc: 0.64
Batch: 500; loss: 1.21; acc: 0.64
Batch: 520; loss: 0.83; acc: 0.75
Batch: 540; loss: 1.16; acc: 0.66
Batch: 560; loss: 1.34; acc: 0.64
Batch: 580; loss: 1.38; acc: 0.62
Batch: 600; loss: 1.08; acc: 0.61
Batch: 620; loss: 0.98; acc: 0.67
Batch: 640; loss: 1.26; acc: 0.59
Batch: 660; loss: 1.05; acc: 0.64
Batch: 680; loss: 0.95; acc: 0.64
Batch: 700; loss: 1.14; acc: 0.62
Batch: 720; loss: 1.22; acc: 0.69
Batch: 740; loss: 1.08; acc: 0.64
Batch: 760; loss: 0.93; acc: 0.72
Batch: 780; loss: 1.15; acc: 0.58
Train Epoch over. train_loss: 1.08; train_accuracy: 0.65 

Batch: 0; loss: 1.15; acc: 0.67
Batch: 20; loss: 1.46; acc: 0.55
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.94; acc: 0.7
Batch: 80; loss: 0.94; acc: 0.7
Batch: 100; loss: 1.12; acc: 0.72
Batch: 120; loss: 1.31; acc: 0.58
Batch: 140; loss: 1.03; acc: 0.61
Val Epoch over. val_loss: 1.058838701551887; val_accuracy: 0.6611265923566879 

The current subspace-distance is: 3.100826870650053e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.26; acc: 0.64
Batch: 20; loss: 1.01; acc: 0.66
Batch: 40; loss: 1.34; acc: 0.55
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 1.12; acc: 0.58
Batch: 100; loss: 1.18; acc: 0.58
Batch: 120; loss: 0.84; acc: 0.7
Batch: 140; loss: 0.67; acc: 0.83
Batch: 160; loss: 1.23; acc: 0.62
Batch: 180; loss: 1.1; acc: 0.64
Batch: 200; loss: 1.07; acc: 0.67
Batch: 220; loss: 1.0; acc: 0.62
Batch: 240; loss: 1.03; acc: 0.67
Batch: 260; loss: 0.93; acc: 0.7
Batch: 280; loss: 0.98; acc: 0.7
Batch: 300; loss: 1.2; acc: 0.64
Batch: 320; loss: 1.06; acc: 0.59
Batch: 340; loss: 0.94; acc: 0.72
Batch: 360; loss: 1.13; acc: 0.64
Batch: 380; loss: 1.15; acc: 0.62
Batch: 400; loss: 1.03; acc: 0.64
Batch: 420; loss: 1.04; acc: 0.62
Batch: 440; loss: 1.13; acc: 0.67
Batch: 460; loss: 0.87; acc: 0.72
Batch: 480; loss: 1.21; acc: 0.67
Batch: 500; loss: 0.99; acc: 0.64
Batch: 520; loss: 0.85; acc: 0.73
Batch: 540; loss: 1.3; acc: 0.59
Batch: 560; loss: 1.26; acc: 0.67
Batch: 580; loss: 1.1; acc: 0.72
Batch: 600; loss: 0.86; acc: 0.75
Batch: 620; loss: 1.39; acc: 0.64
Batch: 640; loss: 0.87; acc: 0.73
Batch: 660; loss: 0.88; acc: 0.69
Batch: 680; loss: 1.07; acc: 0.59
Batch: 700; loss: 1.32; acc: 0.59
Batch: 720; loss: 0.99; acc: 0.66
Batch: 740; loss: 1.15; acc: 0.62
Batch: 760; loss: 0.96; acc: 0.7
Batch: 780; loss: 1.12; acc: 0.66
Train Epoch over. train_loss: 1.06; train_accuracy: 0.66 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.32; acc: 0.59
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.95; acc: 0.72
Batch: 80; loss: 1.07; acc: 0.69
Batch: 100; loss: 1.2; acc: 0.64
Batch: 120; loss: 1.2; acc: 0.52
Batch: 140; loss: 1.12; acc: 0.61
Val Epoch over. val_loss: 1.0602993122331656; val_accuracy: 0.648984872611465 

The current subspace-distance is: 3.5527140425983816e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.53
Batch: 20; loss: 0.94; acc: 0.66
Batch: 40; loss: 1.23; acc: 0.64
Batch: 60; loss: 0.9; acc: 0.7
Batch: 80; loss: 1.26; acc: 0.56
Batch: 100; loss: 1.03; acc: 0.62
Batch: 120; loss: 1.12; acc: 0.59
Batch: 140; loss: 1.0; acc: 0.72
Batch: 160; loss: 1.33; acc: 0.64
Batch: 180; loss: 1.2; acc: 0.58
Batch: 200; loss: 1.0; acc: 0.61
Batch: 220; loss: 1.02; acc: 0.72
Batch: 240; loss: 1.1; acc: 0.59
Batch: 260; loss: 1.05; acc: 0.61
Batch: 280; loss: 0.98; acc: 0.64
Batch: 300; loss: 1.14; acc: 0.72
Batch: 320; loss: 1.23; acc: 0.62
Batch: 340; loss: 1.03; acc: 0.58
Batch: 360; loss: 1.19; acc: 0.56
Batch: 380; loss: 1.34; acc: 0.58
Batch: 400; loss: 1.3; acc: 0.62
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.79; acc: 0.73
Batch: 460; loss: 1.23; acc: 0.56
Batch: 480; loss: 1.14; acc: 0.67
Batch: 500; loss: 1.17; acc: 0.66
Batch: 520; loss: 0.84; acc: 0.75
Batch: 540; loss: 1.23; acc: 0.56
Batch: 560; loss: 1.14; acc: 0.62
Batch: 580; loss: 1.12; acc: 0.55
Batch: 600; loss: 0.95; acc: 0.72
Batch: 620; loss: 1.29; acc: 0.59
Batch: 640; loss: 1.05; acc: 0.66
Batch: 660; loss: 1.14; acc: 0.61
Batch: 680; loss: 1.04; acc: 0.67
Batch: 700; loss: 0.88; acc: 0.72
Batch: 720; loss: 0.98; acc: 0.66
Batch: 740; loss: 1.03; acc: 0.66
Batch: 760; loss: 0.72; acc: 0.72
Batch: 780; loss: 0.95; acc: 0.69
Train Epoch over. train_loss: 1.03; train_accuracy: 0.66 

Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 1.25; acc: 0.55
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.82; acc: 0.75
Batch: 100; loss: 1.02; acc: 0.64
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 0.76; acc: 0.69
Val Epoch over. val_loss: 0.946446981969153; val_accuracy: 0.6884952229299363 

The current subspace-distance is: 3.988695243606344e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.14; acc: 0.69
Batch: 40; loss: 1.06; acc: 0.69
Batch: 60; loss: 1.05; acc: 0.67
Batch: 80; loss: 1.06; acc: 0.62
Batch: 100; loss: 0.85; acc: 0.75
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.9; acc: 0.67
Batch: 160; loss: 0.74; acc: 0.75
Batch: 180; loss: 1.3; acc: 0.59
Batch: 200; loss: 0.84; acc: 0.81
Batch: 220; loss: 1.1; acc: 0.56
Batch: 240; loss: 0.82; acc: 0.72
Batch: 260; loss: 1.08; acc: 0.64
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 1.04; acc: 0.66
Batch: 320; loss: 1.08; acc: 0.72
Batch: 340; loss: 1.05; acc: 0.72
Batch: 360; loss: 1.04; acc: 0.67
Batch: 380; loss: 0.93; acc: 0.69
Batch: 400; loss: 1.07; acc: 0.66
Batch: 420; loss: 0.95; acc: 0.67
Batch: 440; loss: 0.98; acc: 0.78
Batch: 460; loss: 1.3; acc: 0.62
Batch: 480; loss: 0.94; acc: 0.64
Batch: 500; loss: 1.51; acc: 0.5
Batch: 520; loss: 0.88; acc: 0.7
Batch: 540; loss: 0.95; acc: 0.7
Batch: 560; loss: 0.85; acc: 0.75
Batch: 580; loss: 1.14; acc: 0.69
Batch: 600; loss: 1.1; acc: 0.66
Batch: 620; loss: 0.86; acc: 0.69
Batch: 640; loss: 1.11; acc: 0.59
Batch: 660; loss: 0.93; acc: 0.62
Batch: 680; loss: 0.76; acc: 0.73
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.85; acc: 0.7
Batch: 740; loss: 1.08; acc: 0.61
Batch: 760; loss: 1.08; acc: 0.66
Batch: 780; loss: 1.04; acc: 0.64
Train Epoch over. train_loss: 1.0; train_accuracy: 0.67 

Batch: 0; loss: 0.98; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.5
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.79; acc: 0.7
Batch: 80; loss: 0.77; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.77
Batch: 120; loss: 1.22; acc: 0.61
Batch: 140; loss: 0.72; acc: 0.75
Val Epoch over. val_loss: 0.9341912899807001; val_accuracy: 0.689390923566879 

The current subspace-distance is: 4.311104567022994e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.69
Batch: 20; loss: 0.88; acc: 0.7
Batch: 40; loss: 1.17; acc: 0.59
Batch: 60; loss: 1.19; acc: 0.58
Batch: 80; loss: 1.09; acc: 0.64
Batch: 100; loss: 1.04; acc: 0.64
Batch: 120; loss: 1.16; acc: 0.66
Batch: 140; loss: 1.03; acc: 0.75
Batch: 160; loss: 1.03; acc: 0.69
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.91; acc: 0.7
Batch: 220; loss: 0.78; acc: 0.7
Batch: 240; loss: 1.1; acc: 0.69
Batch: 260; loss: 1.08; acc: 0.75
Batch: 280; loss: 0.75; acc: 0.72
Batch: 300; loss: 0.84; acc: 0.67
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 0.86; acc: 0.72
Batch: 360; loss: 1.16; acc: 0.59
Batch: 380; loss: 1.08; acc: 0.66
Batch: 400; loss: 1.04; acc: 0.59
Batch: 420; loss: 0.81; acc: 0.75
Batch: 440; loss: 1.26; acc: 0.61
Batch: 460; loss: 0.96; acc: 0.59
Batch: 480; loss: 0.85; acc: 0.69
Batch: 500; loss: 0.96; acc: 0.73
Batch: 520; loss: 1.13; acc: 0.62
Batch: 540; loss: 1.29; acc: 0.58
Batch: 560; loss: 1.19; acc: 0.61
Batch: 580; loss: 1.11; acc: 0.66
Batch: 600; loss: 1.04; acc: 0.69
Batch: 620; loss: 0.96; acc: 0.64
Batch: 640; loss: 1.04; acc: 0.7
Batch: 660; loss: 0.84; acc: 0.73
Batch: 680; loss: 1.01; acc: 0.69
Batch: 700; loss: 0.76; acc: 0.75
Batch: 720; loss: 1.1; acc: 0.56
Batch: 740; loss: 1.15; acc: 0.56
Batch: 760; loss: 0.79; acc: 0.75
Batch: 780; loss: 1.03; acc: 0.69
Train Epoch over. train_loss: 1.0; train_accuracy: 0.67 

Batch: 0; loss: 0.93; acc: 0.64
Batch: 20; loss: 1.16; acc: 0.55
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.84; acc: 0.75
Batch: 100; loss: 0.97; acc: 0.7
Batch: 120; loss: 1.08; acc: 0.58
Batch: 140; loss: 0.67; acc: 0.77
Val Epoch over. val_loss: 0.9295842089470784; val_accuracy: 0.68640525477707 

The current subspace-distance is: 4.5667606173083186e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.91; acc: 0.67
Batch: 20; loss: 1.03; acc: 0.62
Batch: 40; loss: 1.11; acc: 0.64
Batch: 60; loss: 1.0; acc: 0.67
Batch: 80; loss: 0.98; acc: 0.61
Batch: 100; loss: 0.88; acc: 0.69
Batch: 120; loss: 1.03; acc: 0.72
Batch: 140; loss: 1.19; acc: 0.62
Batch: 160; loss: 0.93; acc: 0.64
Batch: 180; loss: 1.07; acc: 0.75
Batch: 200; loss: 1.06; acc: 0.64
Batch: 220; loss: 0.97; acc: 0.69
Batch: 240; loss: 1.15; acc: 0.59
Batch: 260; loss: 0.96; acc: 0.62
Batch: 280; loss: 0.69; acc: 0.73
Batch: 300; loss: 0.83; acc: 0.75
Batch: 320; loss: 1.31; acc: 0.61
Batch: 340; loss: 1.01; acc: 0.64
Batch: 360; loss: 1.17; acc: 0.62
Batch: 380; loss: 0.79; acc: 0.73
Batch: 400; loss: 0.83; acc: 0.7
Batch: 420; loss: 0.99; acc: 0.7
Batch: 440; loss: 1.04; acc: 0.7
Batch: 460; loss: 0.83; acc: 0.66
Batch: 480; loss: 1.16; acc: 0.66
Batch: 500; loss: 0.79; acc: 0.77
Batch: 520; loss: 1.05; acc: 0.62
Batch: 540; loss: 0.94; acc: 0.62
Batch: 560; loss: 1.0; acc: 0.7
Batch: 580; loss: 1.0; acc: 0.67
Batch: 600; loss: 0.96; acc: 0.67
Batch: 620; loss: 0.92; acc: 0.7
Batch: 640; loss: 0.92; acc: 0.66
Batch: 660; loss: 1.1; acc: 0.67
Batch: 680; loss: 0.83; acc: 0.77
Batch: 700; loss: 0.94; acc: 0.7
Batch: 720; loss: 0.87; acc: 0.69
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 0.93; acc: 0.62
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.98; train_accuracy: 0.67 

Batch: 0; loss: 1.16; acc: 0.62
Batch: 20; loss: 1.35; acc: 0.52
Batch: 40; loss: 0.81; acc: 0.73
Batch: 60; loss: 1.01; acc: 0.66
Batch: 80; loss: 0.95; acc: 0.77
Batch: 100; loss: 1.12; acc: 0.62
Batch: 120; loss: 1.27; acc: 0.64
Batch: 140; loss: 0.9; acc: 0.64
Val Epoch over. val_loss: 1.1453690999632429; val_accuracy: 0.6217157643312102 

The current subspace-distance is: 4.884826557827182e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.58; acc: 0.55
Batch: 20; loss: 0.92; acc: 0.7
Batch: 40; loss: 1.03; acc: 0.59
Batch: 60; loss: 0.92; acc: 0.72
Batch: 80; loss: 1.1; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.66
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.76; acc: 0.77
Batch: 160; loss: 1.23; acc: 0.61
Batch: 180; loss: 0.82; acc: 0.75
Batch: 200; loss: 0.9; acc: 0.7
Batch: 220; loss: 0.68; acc: 0.78
Batch: 240; loss: 1.28; acc: 0.66
Batch: 260; loss: 0.94; acc: 0.77
Batch: 280; loss: 1.05; acc: 0.67
Batch: 300; loss: 0.95; acc: 0.66
Batch: 320; loss: 1.08; acc: 0.61
Batch: 340; loss: 1.0; acc: 0.69
Batch: 360; loss: 0.87; acc: 0.7
Batch: 380; loss: 0.91; acc: 0.67
Batch: 400; loss: 0.75; acc: 0.72
Batch: 420; loss: 1.14; acc: 0.59
Batch: 440; loss: 0.73; acc: 0.73
Batch: 460; loss: 1.13; acc: 0.62
Batch: 480; loss: 0.81; acc: 0.75
Batch: 500; loss: 0.92; acc: 0.7
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.73
Batch: 560; loss: 0.73; acc: 0.7
Batch: 580; loss: 0.89; acc: 0.7
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 0.78; acc: 0.75
Batch: 640; loss: 0.96; acc: 0.61
Batch: 660; loss: 1.03; acc: 0.67
Batch: 680; loss: 0.97; acc: 0.69
Batch: 700; loss: 1.27; acc: 0.61
Batch: 720; loss: 0.86; acc: 0.67
Batch: 740; loss: 0.79; acc: 0.78
Batch: 760; loss: 1.07; acc: 0.64
Batch: 780; loss: 1.02; acc: 0.66
Train Epoch over. train_loss: 0.91; train_accuracy: 0.7 

Batch: 0; loss: 0.92; acc: 0.67
Batch: 20; loss: 1.09; acc: 0.59
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.72; acc: 0.73
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 0.94; acc: 0.66
Batch: 140; loss: 0.47; acc: 0.84
Val Epoch over. val_loss: 0.8498059750362567; val_accuracy: 0.7182523885350318 

The current subspace-distance is: 5.343015072867274e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 0.93; acc: 0.67
Batch: 60; loss: 0.66; acc: 0.77
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.71; acc: 0.78
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.89; acc: 0.7
Batch: 160; loss: 0.69; acc: 0.73
Batch: 180; loss: 0.98; acc: 0.66
Batch: 200; loss: 0.84; acc: 0.69
Batch: 220; loss: 1.36; acc: 0.61
Batch: 240; loss: 0.79; acc: 0.7
Batch: 260; loss: 0.73; acc: 0.78
Batch: 280; loss: 1.08; acc: 0.69
Batch: 300; loss: 0.55; acc: 0.84
Batch: 320; loss: 1.11; acc: 0.67
Batch: 340; loss: 0.78; acc: 0.75
Batch: 360; loss: 0.99; acc: 0.66
Batch: 380; loss: 0.98; acc: 0.66
Batch: 400; loss: 0.8; acc: 0.73
Batch: 420; loss: 1.29; acc: 0.53
Batch: 440; loss: 0.85; acc: 0.72
Batch: 460; loss: 1.16; acc: 0.66
Batch: 480; loss: 1.07; acc: 0.61
Batch: 500; loss: 0.96; acc: 0.67
Batch: 520; loss: 0.98; acc: 0.62
Batch: 540; loss: 0.82; acc: 0.73
Batch: 560; loss: 1.21; acc: 0.62
Batch: 580; loss: 1.14; acc: 0.58
Batch: 600; loss: 0.77; acc: 0.72
Batch: 620; loss: 1.07; acc: 0.69
Batch: 640; loss: 1.0; acc: 0.75
Batch: 660; loss: 0.77; acc: 0.72
Batch: 680; loss: 0.81; acc: 0.67
Batch: 700; loss: 0.97; acc: 0.69
Batch: 720; loss: 0.81; acc: 0.73
Batch: 740; loss: 0.92; acc: 0.69
Batch: 760; loss: 0.78; acc: 0.72
Batch: 780; loss: 0.83; acc: 0.67
Train Epoch over. train_loss: 0.88; train_accuracy: 0.71 

Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.2; acc: 0.55
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 0.95; acc: 0.67
Batch: 140; loss: 0.54; acc: 0.8
Val Epoch over. val_loss: 0.8425136540725733; val_accuracy: 0.7238256369426752 

The current subspace-distance is: 5.720926492358558e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.07; acc: 0.66
Batch: 20; loss: 0.89; acc: 0.69
Batch: 40; loss: 0.85; acc: 0.7
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.85; acc: 0.67
Batch: 100; loss: 0.76; acc: 0.75
Batch: 120; loss: 0.81; acc: 0.67
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.92; acc: 0.7
Batch: 180; loss: 0.97; acc: 0.67
Batch: 200; loss: 0.97; acc: 0.69
Batch: 220; loss: 0.99; acc: 0.72
Batch: 240; loss: 0.88; acc: 0.64
Batch: 260; loss: 0.69; acc: 0.78
Batch: 280; loss: 1.01; acc: 0.62
Batch: 300; loss: 0.82; acc: 0.67
Batch: 320; loss: 0.64; acc: 0.84
Batch: 340; loss: 0.78; acc: 0.77
Batch: 360; loss: 0.72; acc: 0.78
Batch: 380; loss: 0.8; acc: 0.75
Batch: 400; loss: 0.73; acc: 0.75
Batch: 420; loss: 0.86; acc: 0.64
Batch: 440; loss: 0.7; acc: 0.7
Batch: 460; loss: 0.91; acc: 0.73
Batch: 480; loss: 1.01; acc: 0.61
Batch: 500; loss: 0.67; acc: 0.69
Batch: 520; loss: 0.89; acc: 0.75
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.82; acc: 0.73
Batch: 580; loss: 0.62; acc: 0.86
Batch: 600; loss: 0.99; acc: 0.73
Batch: 620; loss: 0.87; acc: 0.7
Batch: 640; loss: 0.81; acc: 0.75
Batch: 660; loss: 0.93; acc: 0.69
Batch: 680; loss: 0.92; acc: 0.69
Batch: 700; loss: 0.64; acc: 0.8
Batch: 720; loss: 1.09; acc: 0.55
Batch: 740; loss: 0.78; acc: 0.77
Batch: 760; loss: 1.09; acc: 0.64
Batch: 780; loss: 1.15; acc: 0.61
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.62
Batch: 20; loss: 1.09; acc: 0.61
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.73
Batch: 80; loss: 0.81; acc: 0.75
Batch: 100; loss: 0.9; acc: 0.73
Batch: 120; loss: 0.91; acc: 0.67
Batch: 140; loss: 0.47; acc: 0.83
Val Epoch over. val_loss: 0.8110520746677544; val_accuracy: 0.7315883757961783 

The current subspace-distance is: 6.125828804215416e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.74; acc: 0.72
Batch: 20; loss: 0.72; acc: 0.7
Batch: 40; loss: 0.66; acc: 0.72
Batch: 60; loss: 0.84; acc: 0.7
Batch: 80; loss: 0.94; acc: 0.72
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 0.79; acc: 0.66
Batch: 140; loss: 0.86; acc: 0.73
Batch: 160; loss: 0.92; acc: 0.69
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.84; acc: 0.7
Batch: 220; loss: 0.72; acc: 0.69
Batch: 240; loss: 0.81; acc: 0.77
Batch: 260; loss: 0.73; acc: 0.73
Batch: 280; loss: 0.91; acc: 0.7
Batch: 300; loss: 0.74; acc: 0.73
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 1.0; acc: 0.64
Batch: 360; loss: 0.63; acc: 0.77
Batch: 380; loss: 0.83; acc: 0.72
Batch: 400; loss: 0.58; acc: 0.75
Batch: 420; loss: 0.97; acc: 0.7
Batch: 440; loss: 0.9; acc: 0.73
Batch: 460; loss: 0.93; acc: 0.73
Batch: 480; loss: 0.91; acc: 0.75
Batch: 500; loss: 0.79; acc: 0.72
Batch: 520; loss: 0.91; acc: 0.69
Batch: 540; loss: 0.69; acc: 0.75
Batch: 560; loss: 0.81; acc: 0.77
Batch: 580; loss: 0.85; acc: 0.78
Batch: 600; loss: 0.77; acc: 0.73
Batch: 620; loss: 0.76; acc: 0.81
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.95; acc: 0.73
Batch: 700; loss: 1.0; acc: 0.64
Batch: 720; loss: 0.85; acc: 0.67
Batch: 740; loss: 0.79; acc: 0.73
Batch: 760; loss: 0.64; acc: 0.77
Batch: 780; loss: 0.94; acc: 0.64
Train Epoch over. train_loss: 0.84; train_accuracy: 0.72 

Batch: 0; loss: 1.0; acc: 0.64
Batch: 20; loss: 1.09; acc: 0.66
Batch: 40; loss: 0.44; acc: 0.91
Batch: 60; loss: 0.76; acc: 0.72
Batch: 80; loss: 0.79; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.77
Batch: 120; loss: 0.97; acc: 0.62
Batch: 140; loss: 0.5; acc: 0.81
Val Epoch over. val_loss: 0.8105245293325679; val_accuracy: 0.7269108280254777 

The current subspace-distance is: 6.384452717611566e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.76; acc: 0.69
Batch: 40; loss: 0.74; acc: 0.8
Batch: 60; loss: 0.61; acc: 0.73
Batch: 80; loss: 0.68; acc: 0.77
Batch: 100; loss: 0.88; acc: 0.7
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 1.05; acc: 0.72
Batch: 180; loss: 0.67; acc: 0.77
Batch: 200; loss: 0.7; acc: 0.8
Batch: 220; loss: 1.04; acc: 0.66
Batch: 240; loss: 0.96; acc: 0.75
Batch: 260; loss: 0.65; acc: 0.78
Batch: 280; loss: 0.78; acc: 0.7
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.76; acc: 0.75
Batch: 340; loss: 0.82; acc: 0.77
Batch: 360; loss: 0.68; acc: 0.67
Batch: 380; loss: 0.68; acc: 0.75
Batch: 400; loss: 0.83; acc: 0.72
Batch: 420; loss: 0.88; acc: 0.73
Batch: 440; loss: 0.99; acc: 0.67
Batch: 460; loss: 0.99; acc: 0.7
Batch: 480; loss: 0.81; acc: 0.75
Batch: 500; loss: 0.87; acc: 0.73
Batch: 520; loss: 0.83; acc: 0.66
Batch: 540; loss: 0.79; acc: 0.77
Batch: 560; loss: 0.71; acc: 0.7
Batch: 580; loss: 0.75; acc: 0.72
Batch: 600; loss: 1.13; acc: 0.64
Batch: 620; loss: 0.79; acc: 0.73
Batch: 640; loss: 0.8; acc: 0.72
Batch: 660; loss: 0.75; acc: 0.84
Batch: 680; loss: 1.23; acc: 0.62
Batch: 700; loss: 0.98; acc: 0.7
Batch: 720; loss: 0.73; acc: 0.77
Batch: 740; loss: 0.84; acc: 0.75
Batch: 760; loss: 0.86; acc: 0.77
Batch: 780; loss: 0.86; acc: 0.7
Train Epoch over. train_loss: 0.84; train_accuracy: 0.73 

Batch: 0; loss: 0.97; acc: 0.67
Batch: 20; loss: 1.04; acc: 0.62
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.77
Batch: 120; loss: 0.9; acc: 0.67
Batch: 140; loss: 0.44; acc: 0.88
Val Epoch over. val_loss: 0.78376290467894; val_accuracy: 0.7449243630573248 

The current subspace-distance is: 6.579612090718001e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.72; acc: 0.72
Batch: 60; loss: 0.63; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.78
Batch: 100; loss: 0.63; acc: 0.77
Batch: 120; loss: 0.81; acc: 0.67
Batch: 140; loss: 0.97; acc: 0.66
Batch: 160; loss: 0.85; acc: 0.69
Batch: 180; loss: 0.95; acc: 0.62
Batch: 200; loss: 0.69; acc: 0.77
Batch: 220; loss: 0.94; acc: 0.7
Batch: 240; loss: 0.69; acc: 0.77
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.99; acc: 0.64
Batch: 300; loss: 0.85; acc: 0.72
Batch: 320; loss: 0.8; acc: 0.67
Batch: 340; loss: 1.02; acc: 0.69
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.81; acc: 0.67
Batch: 400; loss: 0.71; acc: 0.8
Batch: 420; loss: 0.59; acc: 0.8
Batch: 440; loss: 1.13; acc: 0.69
Batch: 460; loss: 0.91; acc: 0.69
Batch: 480; loss: 0.85; acc: 0.66
Batch: 500; loss: 0.91; acc: 0.72
Batch: 520; loss: 1.04; acc: 0.64
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.68; acc: 0.78
Batch: 580; loss: 1.06; acc: 0.62
Batch: 600; loss: 1.05; acc: 0.64
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.99; acc: 0.75
Batch: 660; loss: 1.06; acc: 0.66
Batch: 680; loss: 0.9; acc: 0.75
Batch: 700; loss: 0.77; acc: 0.73
Batch: 720; loss: 1.19; acc: 0.67
Batch: 740; loss: 0.85; acc: 0.8
Batch: 760; loss: 0.76; acc: 0.67
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.84; train_accuracy: 0.73 

Batch: 0; loss: 0.9; acc: 0.69
Batch: 20; loss: 1.02; acc: 0.59
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.77; acc: 0.72
Batch: 80; loss: 0.74; acc: 0.78
Batch: 100; loss: 0.93; acc: 0.75
Batch: 120; loss: 0.95; acc: 0.64
Batch: 140; loss: 0.49; acc: 0.81
Val Epoch over. val_loss: 0.7933984989193594; val_accuracy: 0.7383558917197452 

The current subspace-distance is: 6.90222586854361e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.62; acc: 0.75
Batch: 40; loss: 1.11; acc: 0.58
Batch: 60; loss: 0.75; acc: 0.7
Batch: 80; loss: 0.96; acc: 0.73
Batch: 100; loss: 0.88; acc: 0.75
Batch: 120; loss: 0.85; acc: 0.72
Batch: 140; loss: 0.86; acc: 0.77
Batch: 160; loss: 1.07; acc: 0.64
Batch: 180; loss: 1.06; acc: 0.67
Batch: 200; loss: 0.7; acc: 0.77
Batch: 220; loss: 0.84; acc: 0.67
Batch: 240; loss: 1.02; acc: 0.72
Batch: 260; loss: 0.74; acc: 0.78
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 0.62; acc: 0.8
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.83; acc: 0.73
Batch: 360; loss: 0.66; acc: 0.81
Batch: 380; loss: 0.77; acc: 0.7
Batch: 400; loss: 0.92; acc: 0.66
Batch: 420; loss: 0.92; acc: 0.66
Batch: 440; loss: 0.85; acc: 0.73
Batch: 460; loss: 1.11; acc: 0.61
Batch: 480; loss: 0.71; acc: 0.78
Batch: 500; loss: 0.89; acc: 0.81
Batch: 520; loss: 0.66; acc: 0.7
Batch: 540; loss: 0.77; acc: 0.78
Batch: 560; loss: 0.63; acc: 0.78
Batch: 580; loss: 0.84; acc: 0.66
Batch: 600; loss: 0.79; acc: 0.72
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 1.02; acc: 0.7
Batch: 680; loss: 0.96; acc: 0.67
Batch: 700; loss: 0.72; acc: 0.73
Batch: 720; loss: 0.63; acc: 0.8
Batch: 740; loss: 0.92; acc: 0.66
Batch: 760; loss: 0.68; acc: 0.83
Batch: 780; loss: 0.83; acc: 0.7
Train Epoch over. train_loss: 0.83; train_accuracy: 0.73 

Batch: 0; loss: 0.95; acc: 0.69
Batch: 20; loss: 1.0; acc: 0.62
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.75
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 0.91; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.88
Val Epoch over. val_loss: 0.7837808680762152; val_accuracy: 0.7485071656050956 

The current subspace-distance is: 7.141559035517275e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.9; acc: 0.73
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 1.02; acc: 0.62
Batch: 60; loss: 1.37; acc: 0.66
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 0.79; acc: 0.73
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 1.09; acc: 0.69
Batch: 160; loss: 0.72; acc: 0.73
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.78; acc: 0.75
Batch: 240; loss: 0.87; acc: 0.73
Batch: 260; loss: 0.86; acc: 0.7
Batch: 280; loss: 0.69; acc: 0.75
Batch: 300; loss: 0.8; acc: 0.7
Batch: 320; loss: 0.76; acc: 0.77
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.89; acc: 0.73
Batch: 400; loss: 0.68; acc: 0.81
Batch: 420; loss: 0.92; acc: 0.7
Batch: 440; loss: 0.75; acc: 0.7
Batch: 460; loss: 1.15; acc: 0.61
Batch: 480; loss: 0.82; acc: 0.77
Batch: 500; loss: 0.99; acc: 0.69
Batch: 520; loss: 0.55; acc: 0.83
Batch: 540; loss: 0.86; acc: 0.7
Batch: 560; loss: 0.8; acc: 0.62
Batch: 580; loss: 0.92; acc: 0.72
Batch: 600; loss: 0.78; acc: 0.72
Batch: 620; loss: 0.79; acc: 0.64
Batch: 640; loss: 0.66; acc: 0.78
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.7; acc: 0.75
Batch: 700; loss: 0.92; acc: 0.75
Batch: 720; loss: 0.82; acc: 0.69
Batch: 740; loss: 1.18; acc: 0.67
Batch: 760; loss: 0.63; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.84; train_accuracy: 0.73 

Batch: 0; loss: 0.94; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.62
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.73
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.91; acc: 0.75
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7964283404456582; val_accuracy: 0.7410429936305732 

The current subspace-distance is: 7.201163680292666e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.96; acc: 0.66
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.95; acc: 0.64
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.61; acc: 0.78
Batch: 140; loss: 1.01; acc: 0.61
Batch: 160; loss: 1.12; acc: 0.62
Batch: 180; loss: 0.65; acc: 0.8
Batch: 200; loss: 0.94; acc: 0.73
Batch: 220; loss: 0.77; acc: 0.73
Batch: 240; loss: 1.13; acc: 0.62
Batch: 260; loss: 0.62; acc: 0.77
Batch: 280; loss: 0.65; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.77
Batch: 320; loss: 0.92; acc: 0.75
Batch: 340; loss: 0.83; acc: 0.73
Batch: 360; loss: 0.65; acc: 0.83
Batch: 380; loss: 0.84; acc: 0.69
Batch: 400; loss: 0.94; acc: 0.77
Batch: 420; loss: 0.91; acc: 0.69
Batch: 440; loss: 0.98; acc: 0.7
Batch: 460; loss: 0.64; acc: 0.78
Batch: 480; loss: 0.67; acc: 0.73
Batch: 500; loss: 0.5; acc: 0.84
Batch: 520; loss: 1.01; acc: 0.69
Batch: 540; loss: 0.56; acc: 0.78
Batch: 560; loss: 0.84; acc: 0.72
Batch: 580; loss: 0.98; acc: 0.62
Batch: 600; loss: 0.72; acc: 0.75
Batch: 620; loss: 0.88; acc: 0.77
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 0.84; acc: 0.73
Batch: 680; loss: 0.61; acc: 0.78
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 0.87; acc: 0.69
Batch: 740; loss: 0.84; acc: 0.67
Batch: 760; loss: 0.87; acc: 0.7
Batch: 780; loss: 0.79; acc: 0.72
Train Epoch over. train_loss: 0.83; train_accuracy: 0.73 

Batch: 0; loss: 0.96; acc: 0.67
Batch: 20; loss: 1.02; acc: 0.64
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.91; acc: 0.72
Batch: 120; loss: 0.93; acc: 0.67
Batch: 140; loss: 0.47; acc: 0.84
Val Epoch over. val_loss: 0.7926062028499166; val_accuracy: 0.7433320063694268 

The current subspace-distance is: 7.545098196715117e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.82; acc: 0.72
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.97; acc: 0.66
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.72
Batch: 100; loss: 0.98; acc: 0.66
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.98; acc: 0.69
Batch: 160; loss: 1.0; acc: 0.75
Batch: 180; loss: 0.76; acc: 0.78
Batch: 200; loss: 0.65; acc: 0.69
Batch: 220; loss: 0.75; acc: 0.75
Batch: 240; loss: 0.85; acc: 0.7
Batch: 260; loss: 1.0; acc: 0.7
Batch: 280; loss: 0.59; acc: 0.83
Batch: 300; loss: 0.77; acc: 0.77
Batch: 320; loss: 0.89; acc: 0.75
Batch: 340; loss: 0.64; acc: 0.77
Batch: 360; loss: 0.85; acc: 0.69
Batch: 380; loss: 0.64; acc: 0.77
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.7; acc: 0.75
Batch: 440; loss: 0.8; acc: 0.73
Batch: 460; loss: 0.75; acc: 0.78
Batch: 480; loss: 1.08; acc: 0.64
Batch: 500; loss: 1.21; acc: 0.62
Batch: 520; loss: 0.87; acc: 0.73
Batch: 540; loss: 0.82; acc: 0.72
Batch: 560; loss: 0.81; acc: 0.7
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.8; acc: 0.7
Batch: 620; loss: 0.75; acc: 0.72
Batch: 640; loss: 0.97; acc: 0.72
Batch: 660; loss: 0.96; acc: 0.66
Batch: 680; loss: 0.87; acc: 0.67
Batch: 700; loss: 0.85; acc: 0.67
Batch: 720; loss: 0.92; acc: 0.78
Batch: 740; loss: 1.0; acc: 0.69
Batch: 760; loss: 0.84; acc: 0.72
Batch: 780; loss: 0.93; acc: 0.64
Train Epoch over. train_loss: 0.83; train_accuracy: 0.73 

Batch: 0; loss: 1.08; acc: 0.67
Batch: 20; loss: 1.13; acc: 0.61
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.93; acc: 0.78
Batch: 120; loss: 1.02; acc: 0.64
Batch: 140; loss: 0.48; acc: 0.84
Val Epoch over. val_loss: 0.8197174131110975; val_accuracy: 0.7385549363057324 

The current subspace-distance is: 7.589607412228361e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.72; acc: 0.77
Batch: 20; loss: 0.92; acc: 0.67
Batch: 40; loss: 0.8; acc: 0.77
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 0.78; acc: 0.72
Batch: 100; loss: 0.81; acc: 0.73
Batch: 120; loss: 1.14; acc: 0.58
Batch: 140; loss: 1.12; acc: 0.69
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 1.02; acc: 0.69
Batch: 200; loss: 0.78; acc: 0.75
Batch: 220; loss: 0.84; acc: 0.72
Batch: 240; loss: 0.94; acc: 0.67
Batch: 260; loss: 0.86; acc: 0.73
Batch: 280; loss: 0.87; acc: 0.67
Batch: 300; loss: 0.83; acc: 0.72
Batch: 320; loss: 0.77; acc: 0.81
Batch: 340; loss: 0.83; acc: 0.8
Batch: 360; loss: 0.83; acc: 0.72
Batch: 380; loss: 0.71; acc: 0.77
Batch: 400; loss: 0.83; acc: 0.7
Batch: 420; loss: 1.0; acc: 0.58
Batch: 440; loss: 0.81; acc: 0.78
Batch: 460; loss: 0.76; acc: 0.75
Batch: 480; loss: 0.81; acc: 0.62
Batch: 500; loss: 0.72; acc: 0.78
Batch: 520; loss: 0.75; acc: 0.75
Batch: 540; loss: 0.61; acc: 0.8
Batch: 560; loss: 0.81; acc: 0.73
Batch: 580; loss: 0.92; acc: 0.69
Batch: 600; loss: 0.97; acc: 0.66
Batch: 620; loss: 0.68; acc: 0.72
Batch: 640; loss: 0.89; acc: 0.73
Batch: 660; loss: 0.83; acc: 0.69
Batch: 680; loss: 0.92; acc: 0.67
Batch: 700; loss: 0.62; acc: 0.75
Batch: 720; loss: 0.73; acc: 0.77
Batch: 740; loss: 0.98; acc: 0.7
Batch: 760; loss: 0.66; acc: 0.88
Batch: 780; loss: 0.96; acc: 0.66
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.98; acc: 0.64
Batch: 20; loss: 0.98; acc: 0.61
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.8; acc: 0.77
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.66
Batch: 140; loss: 0.45; acc: 0.83
Val Epoch over. val_loss: 0.7766718775223774; val_accuracy: 0.7476114649681529 

The current subspace-distance is: 7.739043940091506e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.09; acc: 0.59
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.98; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.7
Batch: 80; loss: 0.8; acc: 0.75
Batch: 100; loss: 0.89; acc: 0.7
Batch: 120; loss: 0.84; acc: 0.7
Batch: 140; loss: 1.16; acc: 0.62
Batch: 160; loss: 0.82; acc: 0.77
Batch: 180; loss: 0.75; acc: 0.72
Batch: 200; loss: 0.69; acc: 0.7
Batch: 220; loss: 1.09; acc: 0.72
Batch: 240; loss: 0.82; acc: 0.75
Batch: 260; loss: 0.85; acc: 0.7
Batch: 280; loss: 1.19; acc: 0.61
Batch: 300; loss: 0.9; acc: 0.77
Batch: 320; loss: 0.91; acc: 0.7
Batch: 340; loss: 1.04; acc: 0.66
Batch: 360; loss: 0.95; acc: 0.73
Batch: 380; loss: 0.53; acc: 0.81
Batch: 400; loss: 0.7; acc: 0.77
Batch: 420; loss: 0.68; acc: 0.77
Batch: 440; loss: 0.68; acc: 0.78
Batch: 460; loss: 1.0; acc: 0.64
Batch: 480; loss: 0.84; acc: 0.7
Batch: 500; loss: 1.05; acc: 0.62
Batch: 520; loss: 0.78; acc: 0.78
Batch: 540; loss: 0.89; acc: 0.72
Batch: 560; loss: 0.9; acc: 0.69
Batch: 580; loss: 0.68; acc: 0.7
Batch: 600; loss: 0.82; acc: 0.75
Batch: 620; loss: 1.14; acc: 0.66
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 1.07; acc: 0.7
Batch: 680; loss: 0.83; acc: 0.69
Batch: 700; loss: 0.83; acc: 0.77
Batch: 720; loss: 0.62; acc: 0.83
Batch: 740; loss: 0.99; acc: 0.66
Batch: 760; loss: 0.82; acc: 0.73
Batch: 780; loss: 0.85; acc: 0.64
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.99; acc: 0.61
Batch: 20; loss: 1.01; acc: 0.64
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.73
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.9; acc: 0.75
Batch: 120; loss: 0.98; acc: 0.64
Batch: 140; loss: 0.48; acc: 0.81
Val Epoch over. val_loss: 0.7853643827757258; val_accuracy: 0.7430334394904459 

The current subspace-distance is: 7.820613973308355e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.11; acc: 0.75
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 1.03; acc: 0.58
Batch: 60; loss: 0.67; acc: 0.81
Batch: 80; loss: 0.89; acc: 0.73
Batch: 100; loss: 1.04; acc: 0.78
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 1.07; acc: 0.61
Batch: 160; loss: 0.89; acc: 0.73
Batch: 180; loss: 1.01; acc: 0.73
Batch: 200; loss: 0.85; acc: 0.67
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.7; acc: 0.7
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.7; acc: 0.75
Batch: 300; loss: 1.01; acc: 0.61
Batch: 320; loss: 0.9; acc: 0.73
Batch: 340; loss: 1.04; acc: 0.73
Batch: 360; loss: 0.84; acc: 0.75
Batch: 380; loss: 0.74; acc: 0.73
Batch: 400; loss: 0.71; acc: 0.75
Batch: 420; loss: 0.57; acc: 0.78
Batch: 440; loss: 1.14; acc: 0.61
Batch: 460; loss: 0.71; acc: 0.73
Batch: 480; loss: 0.74; acc: 0.73
Batch: 500; loss: 1.02; acc: 0.69
Batch: 520; loss: 0.95; acc: 0.72
Batch: 540; loss: 0.81; acc: 0.77
Batch: 560; loss: 0.7; acc: 0.72
Batch: 580; loss: 0.57; acc: 0.84
Batch: 600; loss: 0.71; acc: 0.75
Batch: 620; loss: 0.74; acc: 0.78
Batch: 640; loss: 0.74; acc: 0.77
Batch: 660; loss: 0.81; acc: 0.73
Batch: 680; loss: 0.94; acc: 0.72
Batch: 700; loss: 0.65; acc: 0.78
Batch: 720; loss: 0.81; acc: 0.72
Batch: 740; loss: 0.67; acc: 0.73
Batch: 760; loss: 0.89; acc: 0.7
Batch: 780; loss: 0.89; acc: 0.73
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 1.02; acc: 0.62
Batch: 20; loss: 1.0; acc: 0.64
Batch: 40; loss: 0.44; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.75
Batch: 80; loss: 0.85; acc: 0.8
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 0.98; acc: 0.7
Batch: 140; loss: 0.48; acc: 0.8
Val Epoch over. val_loss: 0.7957012719789128; val_accuracy: 0.7431329617834395 

The current subspace-distance is: 8.013148908503354e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.9; acc: 0.72
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.88; acc: 0.67
Batch: 80; loss: 0.92; acc: 0.78
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.79; acc: 0.7
Batch: 160; loss: 0.82; acc: 0.72
Batch: 180; loss: 1.22; acc: 0.66
Batch: 200; loss: 0.83; acc: 0.67
Batch: 220; loss: 0.96; acc: 0.77
Batch: 240; loss: 0.99; acc: 0.66
Batch: 260; loss: 0.85; acc: 0.72
Batch: 280; loss: 1.01; acc: 0.7
Batch: 300; loss: 0.77; acc: 0.77
Batch: 320; loss: 0.76; acc: 0.73
Batch: 340; loss: 0.53; acc: 0.8
Batch: 360; loss: 1.21; acc: 0.67
Batch: 380; loss: 0.78; acc: 0.77
Batch: 400; loss: 0.97; acc: 0.75
Batch: 420; loss: 0.86; acc: 0.77
Batch: 440; loss: 0.78; acc: 0.83
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.66; acc: 0.78
Batch: 520; loss: 0.89; acc: 0.78
Batch: 540; loss: 0.68; acc: 0.72
Batch: 560; loss: 0.74; acc: 0.72
Batch: 580; loss: 0.88; acc: 0.72
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 0.85; acc: 0.73
Batch: 640; loss: 0.77; acc: 0.7
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.84; acc: 0.67
Batch: 700; loss: 0.92; acc: 0.72
Batch: 720; loss: 0.84; acc: 0.75
Batch: 740; loss: 0.85; acc: 0.78
Batch: 760; loss: 0.64; acc: 0.75
Batch: 780; loss: 0.75; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 1.03; acc: 0.66
Batch: 20; loss: 1.06; acc: 0.59
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.79; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.8
Batch: 120; loss: 0.98; acc: 0.62
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7898983651665366; val_accuracy: 0.7446257961783439 

The current subspace-distance is: 8.059586980380118e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 0.52; acc: 0.89
Batch: 40; loss: 0.69; acc: 0.72
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.95; acc: 0.77
Batch: 120; loss: 0.66; acc: 0.75
Batch: 140; loss: 0.91; acc: 0.67
Batch: 160; loss: 0.87; acc: 0.77
Batch: 180; loss: 0.72; acc: 0.8
Batch: 200; loss: 1.19; acc: 0.61
Batch: 220; loss: 1.14; acc: 0.7
Batch: 240; loss: 1.52; acc: 0.62
Batch: 260; loss: 0.88; acc: 0.66
Batch: 280; loss: 0.91; acc: 0.66
Batch: 300; loss: 0.83; acc: 0.72
Batch: 320; loss: 0.78; acc: 0.7
Batch: 340; loss: 0.88; acc: 0.72
Batch: 360; loss: 0.92; acc: 0.7
Batch: 380; loss: 1.28; acc: 0.72
Batch: 400; loss: 0.67; acc: 0.75
Batch: 420; loss: 0.73; acc: 0.73
Batch: 440; loss: 0.81; acc: 0.72
Batch: 460; loss: 0.89; acc: 0.7
Batch: 480; loss: 0.75; acc: 0.73
Batch: 500; loss: 0.98; acc: 0.73
Batch: 520; loss: 0.93; acc: 0.7
Batch: 540; loss: 0.79; acc: 0.73
Batch: 560; loss: 1.1; acc: 0.73
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.91; acc: 0.75
Batch: 620; loss: 0.87; acc: 0.69
Batch: 640; loss: 0.85; acc: 0.73
Batch: 660; loss: 0.68; acc: 0.8
Batch: 680; loss: 0.78; acc: 0.75
Batch: 700; loss: 0.7; acc: 0.72
Batch: 720; loss: 1.1; acc: 0.67
Batch: 740; loss: 0.98; acc: 0.66
Batch: 760; loss: 0.8; acc: 0.72
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.98; acc: 0.66
Batch: 20; loss: 1.0; acc: 0.67
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.73; acc: 0.75
Batch: 80; loss: 0.83; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.81
Batch: 120; loss: 0.94; acc: 0.64
Batch: 140; loss: 0.46; acc: 0.88
Val Epoch over. val_loss: 0.7891474349103915; val_accuracy: 0.7448248407643312 

The current subspace-distance is: 8.317493484355509e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.79; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.72
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.97; acc: 0.67
Batch: 100; loss: 0.99; acc: 0.66
Batch: 120; loss: 1.18; acc: 0.59
Batch: 140; loss: 0.87; acc: 0.75
Batch: 160; loss: 0.79; acc: 0.78
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.89; acc: 0.69
Batch: 220; loss: 0.97; acc: 0.66
Batch: 240; loss: 0.71; acc: 0.84
Batch: 260; loss: 0.97; acc: 0.67
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.87; acc: 0.75
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 0.76; acc: 0.83
Batch: 360; loss: 0.95; acc: 0.69
Batch: 380; loss: 0.74; acc: 0.77
Batch: 400; loss: 0.63; acc: 0.8
Batch: 420; loss: 0.9; acc: 0.8
Batch: 440; loss: 0.66; acc: 0.78
Batch: 460; loss: 0.85; acc: 0.67
Batch: 480; loss: 0.88; acc: 0.72
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.87; acc: 0.73
Batch: 540; loss: 0.79; acc: 0.73
Batch: 560; loss: 0.65; acc: 0.77
Batch: 580; loss: 0.56; acc: 0.78
Batch: 600; loss: 0.85; acc: 0.73
Batch: 620; loss: 0.76; acc: 0.8
Batch: 640; loss: 0.8; acc: 0.7
Batch: 660; loss: 0.77; acc: 0.75
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.67; acc: 0.78
Batch: 720; loss: 0.8; acc: 0.77
Batch: 740; loss: 1.12; acc: 0.67
Batch: 760; loss: 0.92; acc: 0.77
Batch: 780; loss: 0.69; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 1.0; acc: 0.62
Batch: 20; loss: 1.01; acc: 0.62
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.72; acc: 0.77
Batch: 80; loss: 0.83; acc: 0.77
Batch: 100; loss: 0.89; acc: 0.83
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7824598140777297; val_accuracy: 0.7455214968152867 

The current subspace-distance is: 8.43209563754499e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.71; acc: 0.8
Batch: 20; loss: 1.0; acc: 0.7
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.89; acc: 0.7
Batch: 80; loss: 0.64; acc: 0.77
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.78; acc: 0.77
Batch: 160; loss: 0.93; acc: 0.69
Batch: 180; loss: 0.8; acc: 0.69
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.81; acc: 0.77
Batch: 240; loss: 1.02; acc: 0.59
Batch: 260; loss: 0.76; acc: 0.69
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 0.84; acc: 0.72
Batch: 320; loss: 1.03; acc: 0.77
Batch: 340; loss: 0.68; acc: 0.77
Batch: 360; loss: 0.88; acc: 0.69
Batch: 380; loss: 0.84; acc: 0.78
Batch: 400; loss: 1.07; acc: 0.66
Batch: 420; loss: 0.77; acc: 0.7
Batch: 440; loss: 0.81; acc: 0.72
Batch: 460; loss: 1.12; acc: 0.67
Batch: 480; loss: 0.67; acc: 0.8
Batch: 500; loss: 0.9; acc: 0.67
Batch: 520; loss: 1.1; acc: 0.69
Batch: 540; loss: 0.76; acc: 0.7
Batch: 560; loss: 0.85; acc: 0.7
Batch: 580; loss: 0.96; acc: 0.73
Batch: 600; loss: 0.72; acc: 0.75
Batch: 620; loss: 1.1; acc: 0.7
Batch: 640; loss: 0.93; acc: 0.7
Batch: 660; loss: 1.18; acc: 0.64
Batch: 680; loss: 0.62; acc: 0.84
Batch: 700; loss: 0.76; acc: 0.75
Batch: 720; loss: 0.96; acc: 0.72
Batch: 740; loss: 0.71; acc: 0.75
Batch: 760; loss: 0.74; acc: 0.75
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.97; acc: 0.66
Batch: 20; loss: 1.02; acc: 0.61
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.77; acc: 0.77
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.64
Batch: 140; loss: 0.47; acc: 0.81
Val Epoch over. val_loss: 0.7773035986787954; val_accuracy: 0.7469148089171974 

The current subspace-distance is: 8.540682756574824e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.8; acc: 0.78
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.77; acc: 0.7
Batch: 60; loss: 0.94; acc: 0.62
Batch: 80; loss: 0.77; acc: 0.8
Batch: 100; loss: 0.83; acc: 0.72
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.99; acc: 0.59
Batch: 160; loss: 0.77; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.8
Batch: 200; loss: 1.12; acc: 0.64
Batch: 220; loss: 0.76; acc: 0.69
Batch: 240; loss: 0.7; acc: 0.75
Batch: 260; loss: 0.75; acc: 0.72
Batch: 280; loss: 0.65; acc: 0.77
Batch: 300; loss: 0.93; acc: 0.78
Batch: 320; loss: 1.16; acc: 0.62
Batch: 340; loss: 0.73; acc: 0.77
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 1.06; acc: 0.7
Batch: 400; loss: 0.99; acc: 0.72
Batch: 420; loss: 0.64; acc: 0.73
Batch: 440; loss: 0.88; acc: 0.72
Batch: 460; loss: 0.94; acc: 0.77
Batch: 480; loss: 0.9; acc: 0.69
Batch: 500; loss: 0.71; acc: 0.7
Batch: 520; loss: 0.68; acc: 0.78
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 1.18; acc: 0.67
Batch: 580; loss: 0.77; acc: 0.69
Batch: 600; loss: 0.77; acc: 0.7
Batch: 620; loss: 0.71; acc: 0.81
Batch: 640; loss: 0.93; acc: 0.77
Batch: 660; loss: 0.81; acc: 0.67
Batch: 680; loss: 0.52; acc: 0.88
Batch: 700; loss: 0.81; acc: 0.75
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.97; acc: 0.61
Batch: 760; loss: 0.76; acc: 0.77
Batch: 780; loss: 0.58; acc: 0.84
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.99; acc: 0.66
Batch: 20; loss: 1.04; acc: 0.59
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.89; acc: 0.8
Batch: 120; loss: 0.97; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.83
Val Epoch over. val_loss: 0.780103312176504; val_accuracy: 0.747312898089172 

The current subspace-distance is: 8.941979467635974e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.7; acc: 0.78
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.81; acc: 0.72
Batch: 60; loss: 0.86; acc: 0.72
Batch: 80; loss: 0.78; acc: 0.77
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 1.06; acc: 0.7
Batch: 160; loss: 0.91; acc: 0.67
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.84; acc: 0.75
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.64; acc: 0.73
Batch: 260; loss: 0.65; acc: 0.78
Batch: 280; loss: 0.91; acc: 0.7
Batch: 300; loss: 0.82; acc: 0.75
Batch: 320; loss: 0.8; acc: 0.73
Batch: 340; loss: 0.6; acc: 0.77
Batch: 360; loss: 0.84; acc: 0.78
Batch: 380; loss: 0.73; acc: 0.73
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.69; acc: 0.73
Batch: 440; loss: 0.84; acc: 0.75
Batch: 460; loss: 0.83; acc: 0.75
Batch: 480; loss: 0.89; acc: 0.72
Batch: 500; loss: 0.62; acc: 0.78
Batch: 520; loss: 0.67; acc: 0.81
Batch: 540; loss: 0.72; acc: 0.77
Batch: 560; loss: 1.05; acc: 0.69
Batch: 580; loss: 0.7; acc: 0.8
Batch: 600; loss: 0.95; acc: 0.67
Batch: 620; loss: 0.77; acc: 0.73
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.75; acc: 0.77
Batch: 680; loss: 0.63; acc: 0.78
Batch: 700; loss: 0.99; acc: 0.7
Batch: 720; loss: 0.76; acc: 0.77
Batch: 740; loss: 0.54; acc: 0.78
Batch: 760; loss: 1.11; acc: 0.69
Batch: 780; loss: 1.21; acc: 0.62
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 1.02; acc: 0.64
Batch: 20; loss: 1.05; acc: 0.62
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.89; acc: 0.77
Batch: 120; loss: 1.0; acc: 0.64
Batch: 140; loss: 0.47; acc: 0.83
Val Epoch over. val_loss: 0.7859957590224637; val_accuracy: 0.7444267515923567 

The current subspace-distance is: 9.061467426363379e-05 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.22; acc: 0.69
Batch: 20; loss: 0.9; acc: 0.64
Batch: 40; loss: 0.83; acc: 0.78
Batch: 60; loss: 0.84; acc: 0.75
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 1.01; acc: 0.69
Batch: 120; loss: 1.02; acc: 0.67
Batch: 140; loss: 0.79; acc: 0.67
Batch: 160; loss: 0.72; acc: 0.73
Batch: 180; loss: 0.69; acc: 0.77
Batch: 200; loss: 0.7; acc: 0.77
Batch: 220; loss: 0.8; acc: 0.78
Batch: 240; loss: 0.81; acc: 0.72
Batch: 260; loss: 0.8; acc: 0.7
Batch: 280; loss: 1.05; acc: 0.7
Batch: 300; loss: 0.84; acc: 0.73
Batch: 320; loss: 0.78; acc: 0.7
Batch: 340; loss: 1.06; acc: 0.66
Batch: 360; loss: 0.69; acc: 0.75
Batch: 380; loss: 0.7; acc: 0.8
Batch: 400; loss: 0.72; acc: 0.75
Batch: 420; loss: 0.82; acc: 0.73
Batch: 440; loss: 0.66; acc: 0.73
Batch: 460; loss: 0.77; acc: 0.7
Batch: 480; loss: 0.76; acc: 0.69
Batch: 500; loss: 0.55; acc: 0.88
Batch: 520; loss: 0.85; acc: 0.67
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 1.02; acc: 0.66
Batch: 580; loss: 0.82; acc: 0.7
Batch: 600; loss: 0.84; acc: 0.8
Batch: 620; loss: 0.91; acc: 0.72
Batch: 640; loss: 1.06; acc: 0.64
Batch: 660; loss: 0.78; acc: 0.77
Batch: 680; loss: 0.89; acc: 0.7
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 1.04; acc: 0.69
Batch: 740; loss: 0.46; acc: 0.81
Batch: 760; loss: 0.81; acc: 0.8
Batch: 780; loss: 0.89; acc: 0.77
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.95; acc: 0.64
Batch: 20; loss: 1.02; acc: 0.62
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.74; acc: 0.77
Batch: 100; loss: 0.89; acc: 0.77
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.47; acc: 0.83
Val Epoch over. val_loss: 0.7831290871094746; val_accuracy: 0.7426353503184714 

The current subspace-distance is: 9.0268105850555e-05 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.06; acc: 0.7
Batch: 20; loss: 0.66; acc: 0.72
Batch: 40; loss: 0.79; acc: 0.75
Batch: 60; loss: 1.39; acc: 0.59
Batch: 80; loss: 0.92; acc: 0.67
Batch: 100; loss: 0.75; acc: 0.75
Batch: 120; loss: 0.84; acc: 0.69
Batch: 140; loss: 0.89; acc: 0.69
Batch: 160; loss: 1.01; acc: 0.64
Batch: 180; loss: 0.66; acc: 0.83
Batch: 200; loss: 0.67; acc: 0.77
Batch: 220; loss: 0.93; acc: 0.78
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.9; acc: 0.72
Batch: 280; loss: 0.86; acc: 0.73
Batch: 300; loss: 0.81; acc: 0.77
Batch: 320; loss: 0.86; acc: 0.7
Batch: 340; loss: 0.74; acc: 0.69
Batch: 360; loss: 0.68; acc: 0.77
Batch: 380; loss: 1.03; acc: 0.59
Batch: 400; loss: 0.72; acc: 0.77
Batch: 420; loss: 1.03; acc: 0.67
Batch: 440; loss: 0.81; acc: 0.73
Batch: 460; loss: 0.79; acc: 0.75
Batch: 480; loss: 0.75; acc: 0.8
Batch: 500; loss: 0.95; acc: 0.73
Batch: 520; loss: 0.78; acc: 0.67
Batch: 540; loss: 0.8; acc: 0.8
Batch: 560; loss: 0.55; acc: 0.83
Batch: 580; loss: 0.99; acc: 0.7
Batch: 600; loss: 0.76; acc: 0.7
Batch: 620; loss: 0.59; acc: 0.88
Batch: 640; loss: 1.06; acc: 0.64
Batch: 660; loss: 0.9; acc: 0.77
Batch: 680; loss: 0.85; acc: 0.75
Batch: 700; loss: 0.7; acc: 0.73
Batch: 720; loss: 0.96; acc: 0.66
Batch: 740; loss: 0.71; acc: 0.78
Batch: 760; loss: 0.96; acc: 0.66
Batch: 780; loss: 0.78; acc: 0.67
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.66
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.46; acc: 0.81
Val Epoch over. val_loss: 0.7741186260038121; val_accuracy: 0.75109474522293 

The current subspace-distance is: 8.984326268546283e-05 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.85; acc: 0.78
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.78
Batch: 80; loss: 0.84; acc: 0.73
Batch: 100; loss: 1.1; acc: 0.69
Batch: 120; loss: 0.92; acc: 0.7
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 1.0; acc: 0.67
Batch: 180; loss: 1.07; acc: 0.69
Batch: 200; loss: 0.58; acc: 0.75
Batch: 220; loss: 0.71; acc: 0.73
Batch: 240; loss: 1.0; acc: 0.73
Batch: 260; loss: 0.67; acc: 0.83
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.96; acc: 0.73
Batch: 320; loss: 0.93; acc: 0.69
Batch: 340; loss: 0.96; acc: 0.69
Batch: 360; loss: 0.59; acc: 0.78
Batch: 380; loss: 0.79; acc: 0.75
Batch: 400; loss: 0.69; acc: 0.77
Batch: 420; loss: 0.9; acc: 0.69
Batch: 440; loss: 0.7; acc: 0.75
Batch: 460; loss: 0.87; acc: 0.75
Batch: 480; loss: 0.92; acc: 0.7
Batch: 500; loss: 0.62; acc: 0.75
Batch: 520; loss: 0.58; acc: 0.81
Batch: 540; loss: 0.71; acc: 0.66
Batch: 560; loss: 0.78; acc: 0.77
Batch: 580; loss: 0.72; acc: 0.73
Batch: 600; loss: 0.73; acc: 0.73
Batch: 620; loss: 0.81; acc: 0.8
Batch: 640; loss: 0.83; acc: 0.67
Batch: 660; loss: 0.72; acc: 0.78
Batch: 680; loss: 0.82; acc: 0.69
Batch: 700; loss: 1.03; acc: 0.73
Batch: 720; loss: 0.81; acc: 0.72
Batch: 740; loss: 0.74; acc: 0.8
Batch: 760; loss: 0.77; acc: 0.77
Batch: 780; loss: 0.95; acc: 0.61
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.95; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.64
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.7
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7729969884559607; val_accuracy: 0.7502985668789809 

The current subspace-distance is: 9.087474609259516e-05 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.9; acc: 0.67
Batch: 20; loss: 0.83; acc: 0.75
Batch: 40; loss: 0.78; acc: 0.7
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.74; acc: 0.73
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.73
Batch: 140; loss: 0.74; acc: 0.78
Batch: 160; loss: 0.74; acc: 0.73
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 1.22; acc: 0.64
Batch: 220; loss: 0.73; acc: 0.73
Batch: 240; loss: 0.73; acc: 0.78
Batch: 260; loss: 0.62; acc: 0.8
Batch: 280; loss: 0.84; acc: 0.69
Batch: 300; loss: 0.77; acc: 0.75
Batch: 320; loss: 0.79; acc: 0.8
Batch: 340; loss: 1.07; acc: 0.67
Batch: 360; loss: 1.14; acc: 0.72
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 1.0; acc: 0.66
Batch: 420; loss: 0.89; acc: 0.75
Batch: 440; loss: 1.0; acc: 0.72
Batch: 460; loss: 0.78; acc: 0.81
Batch: 480; loss: 0.74; acc: 0.77
Batch: 500; loss: 1.04; acc: 0.62
Batch: 520; loss: 0.85; acc: 0.75
Batch: 540; loss: 0.73; acc: 0.77
Batch: 560; loss: 0.85; acc: 0.72
Batch: 580; loss: 0.74; acc: 0.77
Batch: 600; loss: 0.97; acc: 0.66
Batch: 620; loss: 0.59; acc: 0.8
Batch: 640; loss: 0.6; acc: 0.75
Batch: 660; loss: 0.8; acc: 0.8
Batch: 680; loss: 0.75; acc: 0.8
Batch: 700; loss: 0.84; acc: 0.75
Batch: 720; loss: 0.84; acc: 0.72
Batch: 740; loss: 0.81; acc: 0.75
Batch: 760; loss: 0.94; acc: 0.72
Batch: 780; loss: 0.88; acc: 0.73
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.95; acc: 0.66
Batch: 20; loss: 1.01; acc: 0.62
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.83
Val Epoch over. val_loss: 0.7761778635963513; val_accuracy: 0.7490047770700637 

The current subspace-distance is: 9.233009768649936e-05 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.99; acc: 0.7
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.92; acc: 0.72
Batch: 60; loss: 1.28; acc: 0.58
Batch: 80; loss: 0.84; acc: 0.72
Batch: 100; loss: 1.01; acc: 0.72
Batch: 120; loss: 0.79; acc: 0.7
Batch: 140; loss: 0.85; acc: 0.75
Batch: 160; loss: 1.01; acc: 0.73
Batch: 180; loss: 0.72; acc: 0.81
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 0.76; acc: 0.75
Batch: 240; loss: 0.91; acc: 0.72
Batch: 260; loss: 0.91; acc: 0.72
Batch: 280; loss: 0.96; acc: 0.67
Batch: 300; loss: 0.94; acc: 0.69
Batch: 320; loss: 0.82; acc: 0.75
Batch: 340; loss: 0.9; acc: 0.72
Batch: 360; loss: 0.62; acc: 0.83
Batch: 380; loss: 1.11; acc: 0.75
Batch: 400; loss: 0.74; acc: 0.77
Batch: 420; loss: 0.71; acc: 0.78
Batch: 440; loss: 1.0; acc: 0.64
Batch: 460; loss: 0.9; acc: 0.72
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.83; acc: 0.7
Batch: 520; loss: 0.81; acc: 0.67
Batch: 540; loss: 0.71; acc: 0.73
Batch: 560; loss: 1.03; acc: 0.69
Batch: 580; loss: 0.87; acc: 0.75
Batch: 600; loss: 0.6; acc: 0.81
Batch: 620; loss: 0.79; acc: 0.73
Batch: 640; loss: 1.1; acc: 0.69
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 0.87; acc: 0.73
Batch: 700; loss: 0.76; acc: 0.86
Batch: 720; loss: 1.07; acc: 0.67
Batch: 740; loss: 0.96; acc: 0.69
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.83; acc: 0.73
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.97; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.62
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.83
Val Epoch over. val_loss: 0.7753249664974821; val_accuracy: 0.74890525477707 

The current subspace-distance is: 9.42597325774841e-05 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.79; acc: 0.67
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.7; acc: 0.73
Batch: 80; loss: 0.71; acc: 0.73
Batch: 100; loss: 1.32; acc: 0.62
Batch: 120; loss: 0.75; acc: 0.69
Batch: 140; loss: 0.64; acc: 0.83
Batch: 160; loss: 0.73; acc: 0.8
Batch: 180; loss: 0.96; acc: 0.66
Batch: 200; loss: 0.72; acc: 0.78
Batch: 220; loss: 0.6; acc: 0.78
Batch: 240; loss: 0.7; acc: 0.8
Batch: 260; loss: 1.05; acc: 0.64
Batch: 280; loss: 0.75; acc: 0.69
Batch: 300; loss: 0.86; acc: 0.78
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 1.01; acc: 0.7
Batch: 360; loss: 0.71; acc: 0.75
Batch: 380; loss: 0.73; acc: 0.72
Batch: 400; loss: 0.72; acc: 0.81
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.8; acc: 0.77
Batch: 460; loss: 0.91; acc: 0.73
Batch: 480; loss: 0.81; acc: 0.75
Batch: 500; loss: 0.91; acc: 0.72
Batch: 520; loss: 0.87; acc: 0.75
Batch: 540; loss: 0.96; acc: 0.77
Batch: 560; loss: 0.69; acc: 0.75
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 1.08; acc: 0.62
Batch: 620; loss: 0.74; acc: 0.72
Batch: 640; loss: 0.81; acc: 0.75
Batch: 660; loss: 0.94; acc: 0.7
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 0.95; acc: 0.72
Batch: 720; loss: 1.09; acc: 0.58
Batch: 740; loss: 1.22; acc: 0.67
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.66; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 1.01; acc: 0.64
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.83
Val Epoch over. val_loss: 0.774523836791895; val_accuracy: 0.75109474522293 

The current subspace-distance is: 9.398250404046848e-05 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.73
Batch: 20; loss: 0.72; acc: 0.73
Batch: 40; loss: 0.85; acc: 0.75
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.99; acc: 0.7
Batch: 100; loss: 0.93; acc: 0.72
Batch: 120; loss: 1.17; acc: 0.62
Batch: 140; loss: 0.9; acc: 0.72
Batch: 160; loss: 0.82; acc: 0.75
Batch: 180; loss: 1.34; acc: 0.62
Batch: 200; loss: 0.8; acc: 0.75
Batch: 220; loss: 1.01; acc: 0.66
Batch: 240; loss: 0.75; acc: 0.72
Batch: 260; loss: 0.85; acc: 0.7
Batch: 280; loss: 1.02; acc: 0.67
Batch: 300; loss: 1.05; acc: 0.69
Batch: 320; loss: 0.71; acc: 0.8
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.67; acc: 0.75
Batch: 400; loss: 0.83; acc: 0.75
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 1.21; acc: 0.7
Batch: 460; loss: 1.46; acc: 0.59
Batch: 480; loss: 0.98; acc: 0.64
Batch: 500; loss: 0.75; acc: 0.8
Batch: 520; loss: 0.91; acc: 0.66
Batch: 540; loss: 0.91; acc: 0.61
Batch: 560; loss: 0.63; acc: 0.77
Batch: 580; loss: 0.77; acc: 0.73
Batch: 600; loss: 0.82; acc: 0.77
Batch: 620; loss: 0.98; acc: 0.72
Batch: 640; loss: 0.79; acc: 0.73
Batch: 660; loss: 0.85; acc: 0.72
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 1.01; acc: 0.67
Batch: 720; loss: 0.65; acc: 0.77
Batch: 740; loss: 0.82; acc: 0.75
Batch: 760; loss: 0.75; acc: 0.77
Batch: 780; loss: 0.79; acc: 0.73
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.64
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.94; acc: 0.69
Batch: 140; loss: 0.46; acc: 0.86
Val Epoch over. val_loss: 0.7739242958794733; val_accuracy: 0.75109474522293 

The current subspace-distance is: 9.464233880862594e-05 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.03; acc: 0.69
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.44; acc: 0.89
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.72; acc: 0.73
Batch: 100; loss: 0.61; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.73; acc: 0.75
Batch: 160; loss: 0.87; acc: 0.75
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.93; acc: 0.75
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 1.22; acc: 0.56
Batch: 300; loss: 0.92; acc: 0.73
Batch: 320; loss: 0.65; acc: 0.81
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.75
Batch: 380; loss: 0.69; acc: 0.77
Batch: 400; loss: 0.68; acc: 0.77
Batch: 420; loss: 0.86; acc: 0.72
Batch: 440; loss: 0.88; acc: 0.7
Batch: 460; loss: 1.02; acc: 0.69
Batch: 480; loss: 0.88; acc: 0.66
Batch: 500; loss: 0.56; acc: 0.8
Batch: 520; loss: 0.93; acc: 0.67
Batch: 540; loss: 1.01; acc: 0.73
Batch: 560; loss: 1.1; acc: 0.59
Batch: 580; loss: 0.8; acc: 0.72
Batch: 600; loss: 0.69; acc: 0.78
Batch: 620; loss: 1.05; acc: 0.72
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.91; acc: 0.73
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.91; acc: 0.61
Batch: 720; loss: 0.65; acc: 0.75
Batch: 740; loss: 0.97; acc: 0.7
Batch: 760; loss: 0.69; acc: 0.75
Batch: 780; loss: 0.65; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.95; acc: 0.66
Batch: 20; loss: 1.0; acc: 0.62
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.77; acc: 0.77
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 0.95; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.83
Val Epoch over. val_loss: 0.775386585741286; val_accuracy: 0.7499004777070064 

The current subspace-distance is: 9.539204620523378e-05 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.98; acc: 0.73
Batch: 20; loss: 0.73; acc: 0.75
Batch: 40; loss: 0.67; acc: 0.77
Batch: 60; loss: 1.02; acc: 0.62
Batch: 80; loss: 0.67; acc: 0.81
Batch: 100; loss: 0.89; acc: 0.69
Batch: 120; loss: 0.71; acc: 0.86
Batch: 140; loss: 0.89; acc: 0.73
Batch: 160; loss: 0.9; acc: 0.77
Batch: 180; loss: 0.89; acc: 0.69
Batch: 200; loss: 0.83; acc: 0.75
Batch: 220; loss: 0.84; acc: 0.75
Batch: 240; loss: 0.81; acc: 0.7
Batch: 260; loss: 0.76; acc: 0.7
Batch: 280; loss: 1.01; acc: 0.69
Batch: 300; loss: 1.13; acc: 0.64
Batch: 320; loss: 0.98; acc: 0.64
Batch: 340; loss: 0.82; acc: 0.69
Batch: 360; loss: 0.91; acc: 0.67
Batch: 380; loss: 0.68; acc: 0.77
Batch: 400; loss: 0.71; acc: 0.75
Batch: 420; loss: 0.99; acc: 0.72
Batch: 440; loss: 0.73; acc: 0.75
Batch: 460; loss: 0.95; acc: 0.7
Batch: 480; loss: 0.97; acc: 0.67
Batch: 500; loss: 0.84; acc: 0.8
Batch: 520; loss: 1.06; acc: 0.66
Batch: 540; loss: 0.75; acc: 0.72
Batch: 560; loss: 0.73; acc: 0.77
Batch: 580; loss: 0.93; acc: 0.67
Batch: 600; loss: 0.77; acc: 0.73
Batch: 620; loss: 0.72; acc: 0.77
Batch: 640; loss: 0.88; acc: 0.73
Batch: 660; loss: 0.86; acc: 0.75
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.57; acc: 0.78
Batch: 720; loss: 0.75; acc: 0.7
Batch: 740; loss: 0.81; acc: 0.69
Batch: 760; loss: 1.03; acc: 0.7
Batch: 780; loss: 0.72; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 1.0; acc: 0.64
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.47; acc: 0.83
Val Epoch over. val_loss: 0.775913339511604; val_accuracy: 0.7476114649681529 

The current subspace-distance is: 9.731221507536247e-05 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.95; acc: 0.69
Batch: 20; loss: 1.02; acc: 0.66
Batch: 40; loss: 0.98; acc: 0.73
Batch: 60; loss: 0.91; acc: 0.7
Batch: 80; loss: 0.83; acc: 0.72
Batch: 100; loss: 0.98; acc: 0.73
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.69; acc: 0.81
Batch: 160; loss: 0.69; acc: 0.77
Batch: 180; loss: 1.06; acc: 0.67
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.84; acc: 0.66
Batch: 240; loss: 0.88; acc: 0.62
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.68; acc: 0.75
Batch: 300; loss: 0.78; acc: 0.77
Batch: 320; loss: 0.82; acc: 0.72
Batch: 340; loss: 0.88; acc: 0.67
Batch: 360; loss: 0.91; acc: 0.75
Batch: 380; loss: 0.8; acc: 0.75
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.9; acc: 0.77
Batch: 440; loss: 0.8; acc: 0.72
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.94; acc: 0.62
Batch: 500; loss: 0.82; acc: 0.69
Batch: 520; loss: 0.68; acc: 0.77
Batch: 540; loss: 0.86; acc: 0.72
Batch: 560; loss: 0.97; acc: 0.7
Batch: 580; loss: 0.85; acc: 0.72
Batch: 600; loss: 0.91; acc: 0.67
Batch: 620; loss: 0.84; acc: 0.73
Batch: 640; loss: 0.58; acc: 0.83
Batch: 660; loss: 0.74; acc: 0.78
Batch: 680; loss: 0.81; acc: 0.78
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.53; acc: 0.8
Batch: 740; loss: 0.76; acc: 0.7
Batch: 760; loss: 0.59; acc: 0.78
Batch: 780; loss: 0.79; acc: 0.8
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.64
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.79; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.67
Batch: 140; loss: 0.47; acc: 0.84
Val Epoch over. val_loss: 0.7729218047895249; val_accuracy: 0.7488057324840764 

The current subspace-distance is: 9.84663565759547e-05 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.86; acc: 0.73
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.74; acc: 0.75
Batch: 60; loss: 0.9; acc: 0.7
Batch: 80; loss: 1.06; acc: 0.73
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.68; acc: 0.72
Batch: 140; loss: 0.64; acc: 0.8
Batch: 160; loss: 0.68; acc: 0.78
Batch: 180; loss: 0.65; acc: 0.81
Batch: 200; loss: 0.74; acc: 0.72
Batch: 220; loss: 0.63; acc: 0.75
Batch: 240; loss: 0.89; acc: 0.7
Batch: 260; loss: 1.15; acc: 0.67
Batch: 280; loss: 1.19; acc: 0.66
Batch: 300; loss: 0.9; acc: 0.75
Batch: 320; loss: 0.71; acc: 0.77
Batch: 340; loss: 0.76; acc: 0.73
Batch: 360; loss: 1.0; acc: 0.67
Batch: 380; loss: 0.77; acc: 0.77
Batch: 400; loss: 0.79; acc: 0.69
Batch: 420; loss: 0.99; acc: 0.64
Batch: 440; loss: 0.72; acc: 0.72
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.71; acc: 0.77
Batch: 500; loss: 0.89; acc: 0.58
Batch: 520; loss: 0.65; acc: 0.77
Batch: 540; loss: 0.74; acc: 0.72
Batch: 560; loss: 1.12; acc: 0.64
Batch: 580; loss: 1.04; acc: 0.73
Batch: 600; loss: 0.84; acc: 0.69
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.73; acc: 0.78
Batch: 660; loss: 0.8; acc: 0.77
Batch: 680; loss: 0.73; acc: 0.8
Batch: 700; loss: 0.76; acc: 0.75
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 1.02; acc: 0.62
Batch: 760; loss: 0.85; acc: 0.73
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.67
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.75
Batch: 80; loss: 0.8; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7752548006309825; val_accuracy: 0.7499004777070064 

The current subspace-distance is: 9.960243187379092e-05 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.07; acc: 0.69
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 1.21; acc: 0.62
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.89; acc: 0.67
Batch: 100; loss: 0.66; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.73
Batch: 140; loss: 0.92; acc: 0.67
Batch: 160; loss: 0.85; acc: 0.72
Batch: 180; loss: 0.65; acc: 0.78
Batch: 200; loss: 0.74; acc: 0.75
Batch: 220; loss: 0.96; acc: 0.64
Batch: 240; loss: 0.88; acc: 0.73
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 0.81; acc: 0.77
Batch: 300; loss: 0.73; acc: 0.75
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 0.85; acc: 0.67
Batch: 360; loss: 0.71; acc: 0.84
Batch: 380; loss: 0.76; acc: 0.75
Batch: 400; loss: 0.94; acc: 0.67
Batch: 420; loss: 0.69; acc: 0.75
Batch: 440; loss: 0.99; acc: 0.64
Batch: 460; loss: 0.93; acc: 0.7
Batch: 480; loss: 0.78; acc: 0.75
Batch: 500; loss: 0.74; acc: 0.77
Batch: 520; loss: 0.73; acc: 0.72
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.97; acc: 0.69
Batch: 580; loss: 0.95; acc: 0.66
Batch: 600; loss: 1.17; acc: 0.69
Batch: 620; loss: 0.89; acc: 0.73
Batch: 640; loss: 0.68; acc: 0.77
Batch: 660; loss: 0.89; acc: 0.69
Batch: 680; loss: 0.9; acc: 0.77
Batch: 700; loss: 0.8; acc: 0.72
Batch: 720; loss: 0.75; acc: 0.73
Batch: 740; loss: 0.79; acc: 0.73
Batch: 760; loss: 1.02; acc: 0.62
Batch: 780; loss: 0.97; acc: 0.66
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.95; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.62
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7730496637760453; val_accuracy: 0.7512937898089171 

The current subspace-distance is: 0.00010025982192019 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.97; acc: 0.75
Batch: 20; loss: 0.93; acc: 0.69
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 0.69; acc: 0.77
Batch: 80; loss: 0.6; acc: 0.8
Batch: 100; loss: 0.73; acc: 0.8
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.66; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.77
Batch: 200; loss: 0.67; acc: 0.78
Batch: 220; loss: 0.62; acc: 0.77
Batch: 240; loss: 0.98; acc: 0.75
Batch: 260; loss: 0.69; acc: 0.83
Batch: 280; loss: 0.78; acc: 0.75
Batch: 300; loss: 0.76; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.8
Batch: 340; loss: 0.96; acc: 0.75
Batch: 360; loss: 0.91; acc: 0.7
Batch: 380; loss: 0.84; acc: 0.72
Batch: 400; loss: 0.74; acc: 0.83
Batch: 420; loss: 0.8; acc: 0.75
Batch: 440; loss: 0.79; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.99; acc: 0.73
Batch: 500; loss: 0.73; acc: 0.75
Batch: 520; loss: 0.76; acc: 0.75
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.82; acc: 0.73
Batch: 580; loss: 0.86; acc: 0.7
Batch: 600; loss: 0.66; acc: 0.77
Batch: 620; loss: 0.99; acc: 0.7
Batch: 640; loss: 0.87; acc: 0.75
Batch: 660; loss: 0.74; acc: 0.69
Batch: 680; loss: 0.79; acc: 0.67
Batch: 700; loss: 0.67; acc: 0.73
Batch: 720; loss: 0.74; acc: 0.69
Batch: 740; loss: 0.86; acc: 0.67
Batch: 760; loss: 0.82; acc: 0.7
Batch: 780; loss: 0.93; acc: 0.64
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.95; acc: 0.67
Batch: 20; loss: 0.99; acc: 0.62
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7724459055502704; val_accuracy: 0.7513933121019108 

The current subspace-distance is: 0.00010197798110311851 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.95; acc: 0.69
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.88
Batch: 80; loss: 1.08; acc: 0.61
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.7
Batch: 140; loss: 0.72; acc: 0.75
Batch: 160; loss: 0.94; acc: 0.69
Batch: 180; loss: 0.75; acc: 0.73
Batch: 200; loss: 1.03; acc: 0.66
Batch: 220; loss: 0.93; acc: 0.7
Batch: 240; loss: 0.76; acc: 0.72
Batch: 260; loss: 1.09; acc: 0.66
Batch: 280; loss: 0.74; acc: 0.77
Batch: 300; loss: 0.83; acc: 0.72
Batch: 320; loss: 0.8; acc: 0.7
Batch: 340; loss: 0.7; acc: 0.86
Batch: 360; loss: 1.08; acc: 0.64
Batch: 380; loss: 0.75; acc: 0.73
Batch: 400; loss: 0.66; acc: 0.75
Batch: 420; loss: 0.72; acc: 0.77
Batch: 440; loss: 0.74; acc: 0.72
Batch: 460; loss: 0.87; acc: 0.7
Batch: 480; loss: 0.72; acc: 0.78
Batch: 500; loss: 0.69; acc: 0.72
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.66; acc: 0.8
Batch: 560; loss: 1.27; acc: 0.69
Batch: 580; loss: 0.75; acc: 0.78
Batch: 600; loss: 0.65; acc: 0.75
Batch: 620; loss: 0.85; acc: 0.75
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.75; acc: 0.78
Batch: 680; loss: 1.1; acc: 0.69
Batch: 700; loss: 0.92; acc: 0.67
Batch: 720; loss: 0.87; acc: 0.75
Batch: 740; loss: 0.68; acc: 0.73
Batch: 760; loss: 0.84; acc: 0.67
Batch: 780; loss: 0.6; acc: 0.78
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.96; acc: 0.67
Batch: 20; loss: 1.01; acc: 0.62
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.66
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7742369586874724; val_accuracy: 0.7512937898089171 

The current subspace-distance is: 0.00010497364564798772 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.97; acc: 0.66
Batch: 20; loss: 0.86; acc: 0.7
Batch: 40; loss: 1.05; acc: 0.62
Batch: 60; loss: 1.02; acc: 0.67
Batch: 80; loss: 0.96; acc: 0.67
Batch: 100; loss: 0.9; acc: 0.7
Batch: 120; loss: 1.11; acc: 0.69
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 0.65; acc: 0.77
Batch: 180; loss: 0.8; acc: 0.73
Batch: 200; loss: 1.27; acc: 0.64
Batch: 220; loss: 0.61; acc: 0.81
Batch: 240; loss: 0.85; acc: 0.75
Batch: 260; loss: 0.93; acc: 0.62
Batch: 280; loss: 0.62; acc: 0.77
Batch: 300; loss: 0.68; acc: 0.81
Batch: 320; loss: 0.86; acc: 0.75
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.84; acc: 0.7
Batch: 380; loss: 0.72; acc: 0.77
Batch: 400; loss: 1.09; acc: 0.67
Batch: 420; loss: 0.73; acc: 0.73
Batch: 440; loss: 0.76; acc: 0.78
Batch: 460; loss: 0.79; acc: 0.72
Batch: 480; loss: 0.78; acc: 0.75
Batch: 500; loss: 0.64; acc: 0.77
Batch: 520; loss: 0.66; acc: 0.84
Batch: 540; loss: 0.86; acc: 0.67
Batch: 560; loss: 0.83; acc: 0.75
Batch: 580; loss: 0.87; acc: 0.77
Batch: 600; loss: 1.07; acc: 0.73
Batch: 620; loss: 0.94; acc: 0.67
Batch: 640; loss: 0.83; acc: 0.73
Batch: 660; loss: 1.15; acc: 0.69
Batch: 680; loss: 0.94; acc: 0.7
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.76; acc: 0.75
Batch: 740; loss: 0.72; acc: 0.72
Batch: 760; loss: 0.97; acc: 0.67
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 0.96; acc: 0.66
Batch: 20; loss: 1.01; acc: 0.64
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.66
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7742179681541054; val_accuracy: 0.7487062101910829 

The current subspace-distance is: 0.00010659070539986715 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.99; acc: 0.66
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.97; acc: 0.66
Batch: 60; loss: 0.83; acc: 0.7
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.95; acc: 0.73
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.72; acc: 0.78
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 0.85; acc: 0.72
Batch: 240; loss: 0.93; acc: 0.77
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.67; acc: 0.73
Batch: 300; loss: 0.7; acc: 0.77
Batch: 320; loss: 0.63; acc: 0.81
Batch: 340; loss: 0.8; acc: 0.75
Batch: 360; loss: 0.69; acc: 0.78
Batch: 380; loss: 0.62; acc: 0.8
Batch: 400; loss: 0.9; acc: 0.69
Batch: 420; loss: 0.73; acc: 0.7
Batch: 440; loss: 1.06; acc: 0.73
Batch: 460; loss: 0.88; acc: 0.66
Batch: 480; loss: 0.82; acc: 0.69
Batch: 500; loss: 0.91; acc: 0.75
Batch: 520; loss: 0.87; acc: 0.75
Batch: 540; loss: 0.65; acc: 0.8
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.75; acc: 0.78
Batch: 600; loss: 0.9; acc: 0.67
Batch: 620; loss: 0.58; acc: 0.81
Batch: 640; loss: 1.0; acc: 0.72
Batch: 660; loss: 0.91; acc: 0.73
Batch: 680; loss: 0.91; acc: 0.69
Batch: 700; loss: 1.13; acc: 0.73
Batch: 720; loss: 0.85; acc: 0.73
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.77; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.69
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.95; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.62
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7723998632400658; val_accuracy: 0.7533837579617835 

The current subspace-distance is: 0.00010797837603604421 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.95; acc: 0.69
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.81; acc: 0.7
Batch: 80; loss: 1.05; acc: 0.62
Batch: 100; loss: 1.07; acc: 0.56
Batch: 120; loss: 0.74; acc: 0.73
Batch: 140; loss: 0.72; acc: 0.8
Batch: 160; loss: 1.05; acc: 0.67
Batch: 180; loss: 0.77; acc: 0.73
Batch: 200; loss: 1.06; acc: 0.7
Batch: 220; loss: 1.07; acc: 0.61
Batch: 240; loss: 0.77; acc: 0.7
Batch: 260; loss: 0.85; acc: 0.7
Batch: 280; loss: 1.11; acc: 0.66
Batch: 300; loss: 0.92; acc: 0.7
Batch: 320; loss: 0.93; acc: 0.73
Batch: 340; loss: 0.74; acc: 0.77
Batch: 360; loss: 1.26; acc: 0.69
Batch: 380; loss: 0.59; acc: 0.78
Batch: 400; loss: 0.87; acc: 0.72
Batch: 420; loss: 1.06; acc: 0.56
Batch: 440; loss: 0.98; acc: 0.78
Batch: 460; loss: 0.88; acc: 0.7
Batch: 480; loss: 0.88; acc: 0.77
Batch: 500; loss: 1.06; acc: 0.69
Batch: 520; loss: 1.06; acc: 0.67
Batch: 540; loss: 0.53; acc: 0.86
Batch: 560; loss: 0.64; acc: 0.84
Batch: 580; loss: 1.22; acc: 0.67
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.72; acc: 0.83
Batch: 640; loss: 1.1; acc: 0.64
Batch: 660; loss: 1.13; acc: 0.72
Batch: 680; loss: 0.86; acc: 0.78
Batch: 700; loss: 0.86; acc: 0.73
Batch: 720; loss: 1.08; acc: 0.67
Batch: 740; loss: 0.8; acc: 0.75
Batch: 760; loss: 0.92; acc: 0.73
Batch: 780; loss: 0.92; acc: 0.69
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.95; acc: 0.64
Batch: 20; loss: 1.0; acc: 0.64
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7733724346965741; val_accuracy: 0.7523885350318471 

The current subspace-distance is: 0.00010974748147418723 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.73; acc: 0.8
Batch: 20; loss: 0.58; acc: 0.75
Batch: 40; loss: 0.89; acc: 0.7
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.77; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.77; acc: 0.8
Batch: 160; loss: 1.05; acc: 0.64
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.96; acc: 0.69
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.86; acc: 0.72
Batch: 260; loss: 0.73; acc: 0.78
Batch: 280; loss: 0.82; acc: 0.78
Batch: 300; loss: 0.75; acc: 0.72
Batch: 320; loss: 1.01; acc: 0.75
Batch: 340; loss: 0.57; acc: 0.75
Batch: 360; loss: 0.8; acc: 0.75
Batch: 380; loss: 0.73; acc: 0.73
Batch: 400; loss: 0.97; acc: 0.62
Batch: 420; loss: 0.74; acc: 0.77
Batch: 440; loss: 0.77; acc: 0.77
Batch: 460; loss: 0.77; acc: 0.72
Batch: 480; loss: 0.65; acc: 0.78
Batch: 500; loss: 1.12; acc: 0.7
Batch: 520; loss: 0.85; acc: 0.75
Batch: 540; loss: 1.08; acc: 0.73
Batch: 560; loss: 0.94; acc: 0.69
Batch: 580; loss: 0.77; acc: 0.78
Batch: 600; loss: 0.81; acc: 0.73
Batch: 620; loss: 0.78; acc: 0.77
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.68; acc: 0.73
Batch: 680; loss: 0.78; acc: 0.73
Batch: 700; loss: 0.62; acc: 0.8
Batch: 720; loss: 1.1; acc: 0.69
Batch: 740; loss: 0.89; acc: 0.7
Batch: 760; loss: 1.06; acc: 0.64
Batch: 780; loss: 0.78; acc: 0.75
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.94; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.62
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.69
Batch: 140; loss: 0.46; acc: 0.84
Val Epoch over. val_loss: 0.7723332210710854; val_accuracy: 0.7517914012738853 

The current subspace-distance is: 0.00011139112029923126 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.77; acc: 0.83
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.72; acc: 0.8
Batch: 60; loss: 0.9; acc: 0.72
Batch: 80; loss: 0.72; acc: 0.75
Batch: 100; loss: 0.72; acc: 0.73
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 1.1; acc: 0.66
Batch: 160; loss: 0.93; acc: 0.73
Batch: 180; loss: 1.24; acc: 0.64
Batch: 200; loss: 0.72; acc: 0.77
Batch: 220; loss: 1.27; acc: 0.66
Batch: 240; loss: 0.84; acc: 0.7
Batch: 260; loss: 0.54; acc: 0.8
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.66; acc: 0.78
Batch: 320; loss: 0.89; acc: 0.72
Batch: 340; loss: 0.93; acc: 0.72
Batch: 360; loss: 0.9; acc: 0.67
Batch: 380; loss: 0.64; acc: 0.78
Batch: 400; loss: 0.65; acc: 0.75
Batch: 420; loss: 1.01; acc: 0.7
Batch: 440; loss: 0.71; acc: 0.73
Batch: 460; loss: 1.1; acc: 0.66
Batch: 480; loss: 0.76; acc: 0.75
Batch: 500; loss: 0.8; acc: 0.72
Batch: 520; loss: 0.81; acc: 0.69
Batch: 540; loss: 1.01; acc: 0.7
Batch: 560; loss: 0.94; acc: 0.72
Batch: 580; loss: 0.59; acc: 0.78
Batch: 600; loss: 0.9; acc: 0.72
Batch: 620; loss: 1.14; acc: 0.61
Batch: 640; loss: 0.74; acc: 0.78
Batch: 660; loss: 0.84; acc: 0.72
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 1.05; acc: 0.69
Batch: 720; loss: 1.08; acc: 0.61
Batch: 740; loss: 1.09; acc: 0.58
Batch: 760; loss: 0.96; acc: 0.72
Batch: 780; loss: 0.7; acc: 0.77
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.95; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.64
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.95; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.83
Val Epoch over. val_loss: 0.7723681273733735; val_accuracy: 0.7511942675159236 

The current subspace-distance is: 0.00011287343659205362 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.76; acc: 0.73
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 1.17; acc: 0.66
Batch: 60; loss: 0.99; acc: 0.7
Batch: 80; loss: 0.85; acc: 0.77
Batch: 100; loss: 0.93; acc: 0.7
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.97; acc: 0.69
Batch: 160; loss: 0.52; acc: 0.81
Batch: 180; loss: 0.81; acc: 0.69
Batch: 200; loss: 0.69; acc: 0.77
Batch: 220; loss: 1.02; acc: 0.64
Batch: 240; loss: 0.88; acc: 0.75
Batch: 260; loss: 0.78; acc: 0.81
Batch: 280; loss: 0.95; acc: 0.67
Batch: 300; loss: 0.98; acc: 0.66
Batch: 320; loss: 1.02; acc: 0.64
Batch: 340; loss: 0.94; acc: 0.69
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 0.85; acc: 0.72
Batch: 400; loss: 0.98; acc: 0.69
Batch: 420; loss: 0.81; acc: 0.7
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.98; acc: 0.73
Batch: 480; loss: 0.59; acc: 0.78
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.73; acc: 0.77
Batch: 540; loss: 0.8; acc: 0.83
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 1.23; acc: 0.61
Batch: 600; loss: 1.12; acc: 0.7
Batch: 620; loss: 0.76; acc: 0.78
Batch: 640; loss: 0.87; acc: 0.7
Batch: 660; loss: 0.92; acc: 0.7
Batch: 680; loss: 0.79; acc: 0.73
Batch: 700; loss: 0.54; acc: 0.8
Batch: 720; loss: 1.13; acc: 0.59
Batch: 740; loss: 1.02; acc: 0.67
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.72; acc: 0.72
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.94; acc: 0.66
Batch: 20; loss: 0.99; acc: 0.62
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.8
Batch: 80; loss: 0.77; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 0.95; acc: 0.67
Batch: 140; loss: 0.47; acc: 0.83
Val Epoch over. val_loss: 0.7719525740404797; val_accuracy: 0.7513933121019108 

The current subspace-distance is: 0.00011420415830798447 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.07; acc: 0.64
Batch: 20; loss: 0.81; acc: 0.77
Batch: 40; loss: 0.77; acc: 0.78
Batch: 60; loss: 0.88; acc: 0.7
Batch: 80; loss: 0.91; acc: 0.69
Batch: 100; loss: 0.94; acc: 0.75
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.97; acc: 0.67
Batch: 160; loss: 0.66; acc: 0.8
Batch: 180; loss: 0.73; acc: 0.73
Batch: 200; loss: 0.84; acc: 0.73
Batch: 220; loss: 0.84; acc: 0.72
Batch: 240; loss: 1.14; acc: 0.66
Batch: 260; loss: 1.06; acc: 0.69
Batch: 280; loss: 0.6; acc: 0.78
Batch: 300; loss: 0.81; acc: 0.73
Batch: 320; loss: 0.86; acc: 0.78
Batch: 340; loss: 0.97; acc: 0.73
Batch: 360; loss: 0.81; acc: 0.67
Batch: 380; loss: 0.6; acc: 0.81
Batch: 400; loss: 0.78; acc: 0.7
Batch: 420; loss: 0.89; acc: 0.78
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.8; acc: 0.75
Batch: 480; loss: 0.97; acc: 0.81
Batch: 500; loss: 0.86; acc: 0.7
Batch: 520; loss: 0.76; acc: 0.7
Batch: 540; loss: 0.97; acc: 0.69
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.74; acc: 0.75
Batch: 600; loss: 0.77; acc: 0.77
Batch: 620; loss: 1.02; acc: 0.66
Batch: 640; loss: 0.73; acc: 0.83
Batch: 660; loss: 0.89; acc: 0.72
Batch: 680; loss: 0.64; acc: 0.75
Batch: 700; loss: 0.85; acc: 0.7
Batch: 720; loss: 0.85; acc: 0.72
Batch: 740; loss: 0.62; acc: 0.75
Batch: 760; loss: 0.66; acc: 0.81
Batch: 780; loss: 1.13; acc: 0.64
Train Epoch over. train_loss: 0.82; train_accuracy: 0.74 

Batch: 0; loss: 0.95; acc: 0.67
Batch: 20; loss: 0.99; acc: 0.64
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.78; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.47; acc: 0.83
Val Epoch over. val_loss: 0.7724376389175464; val_accuracy: 0.7505971337579618 

The current subspace-distance is: 0.00011484157585073262 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_75_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 20859
elements in E: 4442600
fraction nonzero: 0.004695223517759871
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.32; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.05
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.32; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.08
Batch: 140; loss: 2.3; acc: 0.11
Batch: 160; loss: 2.29; acc: 0.11
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.03
Batch: 220; loss: 2.29; acc: 0.06
Batch: 240; loss: 2.29; acc: 0.09
Batch: 260; loss: 2.28; acc: 0.09
Batch: 280; loss: 2.29; acc: 0.09
Batch: 300; loss: 2.26; acc: 0.16
Batch: 320; loss: 2.29; acc: 0.05
Batch: 340; loss: 2.27; acc: 0.16
Batch: 360; loss: 2.27; acc: 0.17
Batch: 380; loss: 2.27; acc: 0.19
Batch: 400; loss: 2.27; acc: 0.11
Batch: 420; loss: 2.27; acc: 0.14
Batch: 440; loss: 2.27; acc: 0.17
Batch: 460; loss: 2.26; acc: 0.22
Batch: 480; loss: 2.27; acc: 0.17
Batch: 500; loss: 2.27; acc: 0.19
Batch: 520; loss: 2.27; acc: 0.14
Batch: 540; loss: 2.24; acc: 0.25
Batch: 560; loss: 2.25; acc: 0.27
Batch: 580; loss: 2.26; acc: 0.17
Batch: 600; loss: 2.24; acc: 0.25
Batch: 620; loss: 2.22; acc: 0.28
Batch: 640; loss: 2.26; acc: 0.14
Batch: 660; loss: 2.2; acc: 0.31
Batch: 680; loss: 2.23; acc: 0.22
Batch: 700; loss: 2.2; acc: 0.22
Batch: 720; loss: 2.14; acc: 0.31
Batch: 740; loss: 2.17; acc: 0.23
Batch: 760; loss: 2.13; acc: 0.34
Batch: 780; loss: 2.11; acc: 0.31
Train Epoch over. train_loss: 2.26; train_accuracy: 0.17 

Batch: 0; loss: 2.12; acc: 0.34
Batch: 20; loss: 2.02; acc: 0.44
Batch: 40; loss: 2.02; acc: 0.42
Batch: 60; loss: 2.06; acc: 0.34
Batch: 80; loss: 2.06; acc: 0.34
Batch: 100; loss: 2.08; acc: 0.33
Batch: 120; loss: 2.09; acc: 0.36
Batch: 140; loss: 2.06; acc: 0.33
Val Epoch over. val_loss: 2.0916040945964256; val_accuracy: 0.3115047770700637 

The current subspace-distance is: 5.168418738321634e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.16; acc: 0.19
Batch: 20; loss: 2.05; acc: 0.38
Batch: 40; loss: 2.08; acc: 0.28
Batch: 60; loss: 2.01; acc: 0.31
Batch: 80; loss: 1.83; acc: 0.41
Batch: 100; loss: 1.76; acc: 0.38
Batch: 120; loss: 1.7; acc: 0.45
Batch: 140; loss: 1.66; acc: 0.38
Batch: 160; loss: 1.39; acc: 0.52
Batch: 180; loss: 1.49; acc: 0.45
Batch: 200; loss: 1.17; acc: 0.61
Batch: 220; loss: 1.27; acc: 0.52
Batch: 240; loss: 1.51; acc: 0.41
Batch: 260; loss: 1.23; acc: 0.62
Batch: 280; loss: 1.23; acc: 0.58
Batch: 300; loss: 1.02; acc: 0.61
Batch: 320; loss: 1.0; acc: 0.62
Batch: 340; loss: 1.15; acc: 0.66
Batch: 360; loss: 1.22; acc: 0.61
Batch: 380; loss: 0.84; acc: 0.67
Batch: 400; loss: 1.07; acc: 0.56
Batch: 420; loss: 0.82; acc: 0.73
Batch: 440; loss: 1.13; acc: 0.67
Batch: 460; loss: 0.9; acc: 0.72
Batch: 480; loss: 1.14; acc: 0.62
Batch: 500; loss: 1.29; acc: 0.58
Batch: 520; loss: 0.82; acc: 0.7
Batch: 540; loss: 1.18; acc: 0.64
Batch: 560; loss: 1.14; acc: 0.7
Batch: 580; loss: 0.97; acc: 0.72
Batch: 600; loss: 0.99; acc: 0.69
Batch: 620; loss: 0.81; acc: 0.77
Batch: 640; loss: 0.94; acc: 0.72
Batch: 660; loss: 1.1; acc: 0.64
Batch: 680; loss: 1.08; acc: 0.66
Batch: 700; loss: 1.1; acc: 0.67
Batch: 720; loss: 1.08; acc: 0.62
Batch: 740; loss: 0.99; acc: 0.61
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.01; acc: 0.66
Train Epoch over. train_loss: 1.22; train_accuracy: 0.6 

Batch: 0; loss: 1.19; acc: 0.58
Batch: 20; loss: 1.07; acc: 0.58
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 1.24; acc: 0.59
Batch: 80; loss: 1.15; acc: 0.62
Batch: 100; loss: 1.31; acc: 0.58
Batch: 120; loss: 1.31; acc: 0.55
Batch: 140; loss: 0.91; acc: 0.64
Val Epoch over. val_loss: 1.162707919148123; val_accuracy: 0.6101711783439491 

The current subspace-distance is: 1.690877616056241e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.12; acc: 0.56
Batch: 20; loss: 0.96; acc: 0.67
Batch: 40; loss: 0.86; acc: 0.77
Batch: 60; loss: 0.99; acc: 0.69
Batch: 80; loss: 0.85; acc: 0.73
Batch: 100; loss: 0.96; acc: 0.64
Batch: 120; loss: 0.75; acc: 0.81
Batch: 140; loss: 1.04; acc: 0.64
Batch: 160; loss: 1.01; acc: 0.62
Batch: 180; loss: 1.22; acc: 0.62
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 0.9; acc: 0.77
Batch: 240; loss: 0.99; acc: 0.69
Batch: 260; loss: 1.03; acc: 0.67
Batch: 280; loss: 0.72; acc: 0.75
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.89; acc: 0.66
Batch: 340; loss: 1.11; acc: 0.67
Batch: 360; loss: 1.0; acc: 0.67
Batch: 380; loss: 0.99; acc: 0.69
Batch: 400; loss: 0.78; acc: 0.7
Batch: 420; loss: 1.06; acc: 0.67
Batch: 440; loss: 0.75; acc: 0.73
Batch: 460; loss: 1.08; acc: 0.64
Batch: 480; loss: 1.03; acc: 0.66
Batch: 500; loss: 1.02; acc: 0.62
Batch: 520; loss: 0.76; acc: 0.7
Batch: 540; loss: 0.64; acc: 0.8
Batch: 560; loss: 0.79; acc: 0.67
Batch: 580; loss: 0.77; acc: 0.77
Batch: 600; loss: 1.05; acc: 0.62
Batch: 620; loss: 1.01; acc: 0.67
Batch: 640; loss: 0.9; acc: 0.75
Batch: 660; loss: 1.08; acc: 0.62
Batch: 680; loss: 0.86; acc: 0.67
Batch: 700; loss: 0.69; acc: 0.8
Batch: 720; loss: 0.79; acc: 0.72
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 1.01; acc: 0.64
Train Epoch over. train_loss: 0.91; train_accuracy: 0.7 

Batch: 0; loss: 1.17; acc: 0.62
Batch: 20; loss: 1.14; acc: 0.53
Batch: 40; loss: 0.67; acc: 0.75
Batch: 60; loss: 1.11; acc: 0.62
Batch: 80; loss: 1.03; acc: 0.64
Batch: 100; loss: 1.06; acc: 0.64
Batch: 120; loss: 0.99; acc: 0.66
Batch: 140; loss: 0.91; acc: 0.62
Val Epoch over. val_loss: 1.0801124018468675; val_accuracy: 0.6490843949044586 

The current subspace-distance is: 2.5368508431711234e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 0.73; acc: 0.78
Batch: 40; loss: 0.93; acc: 0.7
Batch: 60; loss: 0.76; acc: 0.77
Batch: 80; loss: 0.88; acc: 0.69
Batch: 100; loss: 0.81; acc: 0.7
Batch: 120; loss: 0.83; acc: 0.72
Batch: 140; loss: 0.89; acc: 0.75
Batch: 160; loss: 0.87; acc: 0.77
Batch: 180; loss: 0.72; acc: 0.77
Batch: 200; loss: 0.99; acc: 0.7
Batch: 220; loss: 0.79; acc: 0.67
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.88; acc: 0.75
Batch: 280; loss: 1.02; acc: 0.73
Batch: 300; loss: 0.7; acc: 0.75
Batch: 320; loss: 0.71; acc: 0.73
Batch: 340; loss: 0.87; acc: 0.7
Batch: 360; loss: 0.77; acc: 0.72
Batch: 380; loss: 0.88; acc: 0.77
Batch: 400; loss: 0.85; acc: 0.72
Batch: 420; loss: 1.09; acc: 0.72
Batch: 440; loss: 0.82; acc: 0.69
Batch: 460; loss: 0.84; acc: 0.69
Batch: 480; loss: 0.83; acc: 0.72
Batch: 500; loss: 0.71; acc: 0.77
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.85; acc: 0.77
Batch: 560; loss: 1.04; acc: 0.67
Batch: 580; loss: 0.99; acc: 0.61
Batch: 600; loss: 0.71; acc: 0.72
Batch: 620; loss: 0.94; acc: 0.78
Batch: 640; loss: 1.05; acc: 0.69
Batch: 660; loss: 1.07; acc: 0.58
Batch: 680; loss: 0.96; acc: 0.69
Batch: 700; loss: 1.0; acc: 0.77
Batch: 720; loss: 0.93; acc: 0.7
Batch: 740; loss: 0.94; acc: 0.7
Batch: 760; loss: 0.8; acc: 0.75
Batch: 780; loss: 0.57; acc: 0.86
Train Epoch over. train_loss: 0.85; train_accuracy: 0.72 

Batch: 0; loss: 0.8; acc: 0.7
Batch: 20; loss: 0.82; acc: 0.67
Batch: 40; loss: 0.72; acc: 0.73
Batch: 60; loss: 0.95; acc: 0.67
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.88; acc: 0.69
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.75; acc: 0.73
Val Epoch over. val_loss: 0.9047440149981505; val_accuracy: 0.6906847133757962 

The current subspace-distance is: 2.7744290491682477e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.13; acc: 0.56
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.87; acc: 0.78
Batch: 60; loss: 0.61; acc: 0.77
Batch: 80; loss: 0.65; acc: 0.81
Batch: 100; loss: 1.06; acc: 0.64
Batch: 120; loss: 1.03; acc: 0.61
Batch: 140; loss: 0.82; acc: 0.73
Batch: 160; loss: 0.83; acc: 0.8
Batch: 180; loss: 0.6; acc: 0.73
Batch: 200; loss: 0.73; acc: 0.75
Batch: 220; loss: 0.91; acc: 0.73
Batch: 240; loss: 0.83; acc: 0.77
Batch: 260; loss: 0.86; acc: 0.7
Batch: 280; loss: 0.99; acc: 0.66
Batch: 300; loss: 0.99; acc: 0.59
Batch: 320; loss: 0.86; acc: 0.72
Batch: 340; loss: 0.75; acc: 0.73
Batch: 360; loss: 1.03; acc: 0.66
Batch: 380; loss: 0.63; acc: 0.75
Batch: 400; loss: 0.66; acc: 0.78
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.95; acc: 0.72
Batch: 460; loss: 0.62; acc: 0.8
Batch: 480; loss: 1.0; acc: 0.69
Batch: 500; loss: 0.71; acc: 0.77
Batch: 520; loss: 0.78; acc: 0.7
Batch: 540; loss: 0.73; acc: 0.73
Batch: 560; loss: 0.69; acc: 0.8
Batch: 580; loss: 0.81; acc: 0.83
Batch: 600; loss: 0.95; acc: 0.72
Batch: 620; loss: 0.76; acc: 0.72
Batch: 640; loss: 0.85; acc: 0.72
Batch: 660; loss: 1.29; acc: 0.59
Batch: 680; loss: 0.73; acc: 0.77
Batch: 700; loss: 0.92; acc: 0.73
Batch: 720; loss: 1.15; acc: 0.61
Batch: 740; loss: 0.74; acc: 0.7
Batch: 760; loss: 0.77; acc: 0.75
Batch: 780; loss: 0.84; acc: 0.64
Train Epoch over. train_loss: 0.82; train_accuracy: 0.73 

Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.03; acc: 0.62
Batch: 40; loss: 0.95; acc: 0.72
Batch: 60; loss: 0.92; acc: 0.72
Batch: 80; loss: 0.88; acc: 0.73
Batch: 100; loss: 0.89; acc: 0.61
Batch: 120; loss: 1.07; acc: 0.67
Batch: 140; loss: 0.62; acc: 0.75
Val Epoch over. val_loss: 0.9879925776818755; val_accuracy: 0.6830214968152867 

The current subspace-distance is: 3.131559060420841e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.66
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.95; acc: 0.75
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.96; acc: 0.61
Batch: 100; loss: 0.86; acc: 0.75
Batch: 120; loss: 0.74; acc: 0.67
Batch: 140; loss: 0.69; acc: 0.77
Batch: 160; loss: 0.78; acc: 0.77
Batch: 180; loss: 0.94; acc: 0.72
Batch: 200; loss: 0.82; acc: 0.77
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.78; acc: 0.73
Batch: 260; loss: 0.69; acc: 0.78
Batch: 280; loss: 0.6; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.75
Batch: 320; loss: 0.71; acc: 0.78
Batch: 340; loss: 0.67; acc: 0.77
Batch: 360; loss: 0.87; acc: 0.69
Batch: 380; loss: 0.71; acc: 0.73
Batch: 400; loss: 0.74; acc: 0.75
Batch: 420; loss: 0.79; acc: 0.72
Batch: 440; loss: 0.93; acc: 0.75
Batch: 460; loss: 0.91; acc: 0.7
Batch: 480; loss: 0.81; acc: 0.73
Batch: 500; loss: 0.69; acc: 0.75
Batch: 520; loss: 0.69; acc: 0.77
Batch: 540; loss: 0.96; acc: 0.66
Batch: 560; loss: 1.07; acc: 0.7
Batch: 580; loss: 1.0; acc: 0.75
Batch: 600; loss: 0.78; acc: 0.73
Batch: 620; loss: 0.7; acc: 0.81
Batch: 640; loss: 0.66; acc: 0.78
Batch: 660; loss: 0.75; acc: 0.72
Batch: 680; loss: 0.93; acc: 0.73
Batch: 700; loss: 0.73; acc: 0.72
Batch: 720; loss: 0.89; acc: 0.7
Batch: 740; loss: 1.25; acc: 0.67
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.64; acc: 0.75
Train Epoch over. train_loss: 0.8; train_accuracy: 0.74 

Batch: 0; loss: 0.94; acc: 0.67
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.86; acc: 0.7
Batch: 60; loss: 0.9; acc: 0.72
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.69; acc: 0.73
Batch: 120; loss: 1.06; acc: 0.53
Batch: 140; loss: 0.55; acc: 0.81
Val Epoch over. val_loss: 0.827934525005377; val_accuracy: 0.7302945859872612 

The current subspace-distance is: 3.488478978397325e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.07; acc: 0.7
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.75; acc: 0.73
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.93; acc: 0.62
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.95; acc: 0.8
Batch: 140; loss: 0.82; acc: 0.8
Batch: 160; loss: 0.93; acc: 0.77
Batch: 180; loss: 1.12; acc: 0.69
Batch: 200; loss: 0.67; acc: 0.77
Batch: 220; loss: 0.71; acc: 0.73
Batch: 240; loss: 0.77; acc: 0.72
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.95; acc: 0.73
Batch: 300; loss: 0.75; acc: 0.78
Batch: 320; loss: 0.96; acc: 0.69
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.82; acc: 0.7
Batch: 380; loss: 0.8; acc: 0.73
Batch: 400; loss: 0.85; acc: 0.69
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.68; acc: 0.78
Batch: 480; loss: 0.97; acc: 0.66
Batch: 500; loss: 0.64; acc: 0.81
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.83; acc: 0.66
Batch: 560; loss: 0.95; acc: 0.67
Batch: 580; loss: 0.66; acc: 0.78
Batch: 600; loss: 0.95; acc: 0.64
Batch: 620; loss: 1.19; acc: 0.62
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 0.73; acc: 0.75
Batch: 680; loss: 0.88; acc: 0.67
Batch: 700; loss: 0.61; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.78; acc: 0.72
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.8; acc: 0.69
Train Epoch over. train_loss: 0.79; train_accuracy: 0.74 

Batch: 0; loss: 0.87; acc: 0.69
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.74; acc: 0.73
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.8
Batch: 120; loss: 0.81; acc: 0.67
Batch: 140; loss: 0.49; acc: 0.83
Val Epoch over. val_loss: 0.7266532962868928; val_accuracy: 0.7546775477707006 

The current subspace-distance is: 3.67494321835693e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.66
Batch: 20; loss: 1.08; acc: 0.61
Batch: 40; loss: 0.69; acc: 0.73
Batch: 60; loss: 0.97; acc: 0.66
Batch: 80; loss: 0.88; acc: 0.7
Batch: 100; loss: 0.72; acc: 0.78
Batch: 120; loss: 0.64; acc: 0.75
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.72; acc: 0.78
Batch: 180; loss: 1.12; acc: 0.67
Batch: 200; loss: 0.59; acc: 0.84
Batch: 220; loss: 0.7; acc: 0.75
Batch: 240; loss: 0.75; acc: 0.73
Batch: 260; loss: 0.78; acc: 0.78
Batch: 280; loss: 0.86; acc: 0.66
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.8; acc: 0.64
Batch: 340; loss: 0.86; acc: 0.77
Batch: 360; loss: 0.79; acc: 0.8
Batch: 380; loss: 0.75; acc: 0.77
Batch: 400; loss: 0.69; acc: 0.8
Batch: 420; loss: 0.77; acc: 0.72
Batch: 440; loss: 0.75; acc: 0.77
Batch: 460; loss: 0.94; acc: 0.66
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.82; acc: 0.67
Batch: 520; loss: 0.89; acc: 0.75
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.68; acc: 0.77
Batch: 580; loss: 0.77; acc: 0.75
Batch: 600; loss: 0.77; acc: 0.78
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.77; acc: 0.77
Batch: 660; loss: 0.83; acc: 0.72
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.93; acc: 0.69
Batch: 720; loss: 0.82; acc: 0.73
Batch: 740; loss: 0.87; acc: 0.7
Batch: 760; loss: 1.02; acc: 0.59
Batch: 780; loss: 0.72; acc: 0.78
Train Epoch over. train_loss: 0.77; train_accuracy: 0.75 

Batch: 0; loss: 0.91; acc: 0.67
Batch: 20; loss: 0.87; acc: 0.7
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.77
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 0.5; acc: 0.83
Val Epoch over. val_loss: 0.789414352861939; val_accuracy: 0.7432324840764332 

The current subspace-distance is: 3.985657895100303e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.85; acc: 0.67
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.87; acc: 0.72
Batch: 60; loss: 0.81; acc: 0.69
Batch: 80; loss: 0.79; acc: 0.75
Batch: 100; loss: 0.83; acc: 0.66
Batch: 120; loss: 0.84; acc: 0.81
Batch: 140; loss: 0.84; acc: 0.78
Batch: 160; loss: 0.51; acc: 0.81
Batch: 180; loss: 0.64; acc: 0.81
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.69; acc: 0.7
Batch: 240; loss: 0.69; acc: 0.75
Batch: 260; loss: 0.59; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.78
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.84; acc: 0.72
Batch: 340; loss: 0.78; acc: 0.75
Batch: 360; loss: 0.94; acc: 0.7
Batch: 380; loss: 0.8; acc: 0.78
Batch: 400; loss: 0.9; acc: 0.69
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.83; acc: 0.67
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.67; acc: 0.77
Batch: 500; loss: 0.76; acc: 0.78
Batch: 520; loss: 1.01; acc: 0.73
Batch: 540; loss: 1.06; acc: 0.69
Batch: 560; loss: 0.94; acc: 0.7
Batch: 580; loss: 0.66; acc: 0.77
Batch: 600; loss: 0.65; acc: 0.81
Batch: 620; loss: 0.69; acc: 0.78
Batch: 640; loss: 0.58; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.73; acc: 0.77
Batch: 700; loss: 0.67; acc: 0.77
Batch: 720; loss: 0.85; acc: 0.69
Batch: 740; loss: 0.76; acc: 0.73
Batch: 760; loss: 0.69; acc: 0.81
Batch: 780; loss: 0.73; acc: 0.81
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 1.09; acc: 0.61
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.96; acc: 0.64
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.6; acc: 0.8
Batch: 100; loss: 0.78; acc: 0.7
Batch: 120; loss: 1.18; acc: 0.58
Batch: 140; loss: 0.62; acc: 0.83
Val Epoch over. val_loss: 0.8890182503089783; val_accuracy: 0.7043192675159236 

The current subspace-distance is: 4.47055499535054e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.03; acc: 0.7
Batch: 20; loss: 0.94; acc: 0.67
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.77; acc: 0.67
Batch: 80; loss: 0.61; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.72
Batch: 120; loss: 0.93; acc: 0.69
Batch: 140; loss: 0.97; acc: 0.72
Batch: 160; loss: 0.74; acc: 0.72
Batch: 180; loss: 0.91; acc: 0.73
Batch: 200; loss: 0.74; acc: 0.75
Batch: 220; loss: 0.59; acc: 0.75
Batch: 240; loss: 0.98; acc: 0.67
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.55; acc: 0.83
Batch: 300; loss: 0.48; acc: 0.81
Batch: 320; loss: 0.92; acc: 0.69
Batch: 340; loss: 0.74; acc: 0.77
Batch: 360; loss: 0.85; acc: 0.69
Batch: 380; loss: 0.54; acc: 0.88
Batch: 400; loss: 0.69; acc: 0.77
Batch: 420; loss: 0.71; acc: 0.72
Batch: 440; loss: 0.68; acc: 0.78
Batch: 460; loss: 0.78; acc: 0.7
Batch: 480; loss: 0.96; acc: 0.67
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.8; acc: 0.73
Batch: 560; loss: 0.63; acc: 0.84
Batch: 580; loss: 0.92; acc: 0.64
Batch: 600; loss: 0.89; acc: 0.78
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.89; acc: 0.72
Batch: 660; loss: 0.85; acc: 0.77
Batch: 680; loss: 0.83; acc: 0.72
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.83; acc: 0.72
Batch: 740; loss: 0.78; acc: 0.72
Batch: 760; loss: 0.83; acc: 0.73
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.74; train_accuracy: 0.76 

Batch: 0; loss: 0.88; acc: 0.67
Batch: 20; loss: 0.71; acc: 0.81
Batch: 40; loss: 0.64; acc: 0.78
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.48; acc: 0.83
Val Epoch over. val_loss: 0.715016053740386; val_accuracy: 0.7611464968152867 

The current subspace-distance is: 4.8815763875609264e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.13; acc: 0.67
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.75
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.71; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.72
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.7; acc: 0.8
Batch: 240; loss: 0.92; acc: 0.69
Batch: 260; loss: 0.67; acc: 0.78
Batch: 280; loss: 0.76; acc: 0.75
Batch: 300; loss: 0.94; acc: 0.67
Batch: 320; loss: 0.88; acc: 0.69
Batch: 340; loss: 0.93; acc: 0.72
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.98; acc: 0.77
Batch: 400; loss: 0.84; acc: 0.78
Batch: 420; loss: 1.0; acc: 0.72
Batch: 440; loss: 0.7; acc: 0.75
Batch: 460; loss: 0.88; acc: 0.73
Batch: 480; loss: 0.72; acc: 0.73
Batch: 500; loss: 0.68; acc: 0.75
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.55; acc: 0.8
Batch: 560; loss: 0.48; acc: 0.78
Batch: 580; loss: 0.65; acc: 0.81
Batch: 600; loss: 0.56; acc: 0.83
Batch: 620; loss: 0.73; acc: 0.73
Batch: 640; loss: 0.82; acc: 0.77
Batch: 660; loss: 0.96; acc: 0.78
Batch: 680; loss: 0.83; acc: 0.72
Batch: 700; loss: 0.94; acc: 0.78
Batch: 720; loss: 0.78; acc: 0.78
Batch: 740; loss: 0.82; acc: 0.64
Batch: 760; loss: 0.78; acc: 0.72
Batch: 780; loss: 0.89; acc: 0.67
Train Epoch over. train_loss: 0.71; train_accuracy: 0.77 

Batch: 0; loss: 0.9; acc: 0.67
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.63; acc: 0.77
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.75
Batch: 120; loss: 0.76; acc: 0.72
Batch: 140; loss: 0.39; acc: 0.91
Val Epoch over. val_loss: 0.6697843302587035; val_accuracy: 0.7844347133757962 

The current subspace-distance is: 5.2189243433531374e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 1.07; acc: 0.75
Batch: 40; loss: 0.91; acc: 0.67
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.73; acc: 0.77
Batch: 120; loss: 0.86; acc: 0.67
Batch: 140; loss: 0.67; acc: 0.77
Batch: 160; loss: 0.71; acc: 0.73
Batch: 180; loss: 0.62; acc: 0.78
Batch: 200; loss: 0.76; acc: 0.83
Batch: 220; loss: 0.71; acc: 0.8
Batch: 240; loss: 0.79; acc: 0.69
Batch: 260; loss: 0.87; acc: 0.69
Batch: 280; loss: 0.73; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.77
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.75; acc: 0.7
Batch: 360; loss: 0.85; acc: 0.77
Batch: 380; loss: 0.7; acc: 0.75
Batch: 400; loss: 0.65; acc: 0.78
Batch: 420; loss: 0.78; acc: 0.78
Batch: 440; loss: 0.66; acc: 0.75
Batch: 460; loss: 0.87; acc: 0.69
Batch: 480; loss: 0.79; acc: 0.78
Batch: 500; loss: 1.02; acc: 0.77
Batch: 520; loss: 0.79; acc: 0.77
Batch: 540; loss: 0.89; acc: 0.77
Batch: 560; loss: 1.03; acc: 0.69
Batch: 580; loss: 0.81; acc: 0.77
Batch: 600; loss: 0.68; acc: 0.83
Batch: 620; loss: 0.73; acc: 0.78
Batch: 640; loss: 0.82; acc: 0.77
Batch: 660; loss: 0.62; acc: 0.84
Batch: 680; loss: 0.85; acc: 0.77
Batch: 700; loss: 0.59; acc: 0.8
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.76; acc: 0.8
Batch: 780; loss: 0.7; acc: 0.81
Train Epoch over. train_loss: 0.71; train_accuracy: 0.77 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.56; acc: 0.75
Batch: 120; loss: 0.83; acc: 0.69
Batch: 140; loss: 0.43; acc: 0.91
Val Epoch over. val_loss: 0.6782159946716515; val_accuracy: 0.7808519108280255 

The current subspace-distance is: 5.381636947277002e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.91; acc: 0.75
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 0.79; acc: 0.8
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.78
Batch: 120; loss: 0.55; acc: 0.8
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.7; acc: 0.78
Batch: 180; loss: 0.74; acc: 0.77
Batch: 200; loss: 0.7; acc: 0.75
Batch: 220; loss: 0.94; acc: 0.75
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.73; acc: 0.73
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.59; acc: 0.8
Batch: 320; loss: 0.72; acc: 0.78
Batch: 340; loss: 0.78; acc: 0.73
Batch: 360; loss: 0.72; acc: 0.77
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.74; acc: 0.78
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.74; acc: 0.75
Batch: 460; loss: 0.8; acc: 0.72
Batch: 480; loss: 0.96; acc: 0.67
Batch: 500; loss: 0.72; acc: 0.75
Batch: 520; loss: 0.69; acc: 0.84
Batch: 540; loss: 0.68; acc: 0.77
Batch: 560; loss: 0.82; acc: 0.78
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.73; acc: 0.81
Batch: 620; loss: 0.56; acc: 0.83
Batch: 640; loss: 0.64; acc: 0.8
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.94; acc: 0.72
Batch: 700; loss: 0.67; acc: 0.81
Batch: 720; loss: 0.96; acc: 0.77
Batch: 740; loss: 0.85; acc: 0.75
Batch: 760; loss: 0.86; acc: 0.7
Batch: 780; loss: 0.84; acc: 0.7
Train Epoch over. train_loss: 0.71; train_accuracy: 0.77 

Batch: 0; loss: 0.82; acc: 0.7
Batch: 20; loss: 0.57; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.78
Batch: 60; loss: 0.53; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.56; acc: 0.8
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.94
Val Epoch over. val_loss: 0.6674922049804858; val_accuracy: 0.7834394904458599 

The current subspace-distance is: 5.6413635320495814e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.75; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.8
Batch: 60; loss: 0.77; acc: 0.67
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.8
Batch: 120; loss: 0.58; acc: 0.78
Batch: 140; loss: 0.77; acc: 0.72
Batch: 160; loss: 0.8; acc: 0.73
Batch: 180; loss: 0.61; acc: 0.83
Batch: 200; loss: 0.74; acc: 0.75
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.73; acc: 0.73
Batch: 260; loss: 0.59; acc: 0.83
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.77
Batch: 340; loss: 0.94; acc: 0.66
Batch: 360; loss: 0.64; acc: 0.78
Batch: 380; loss: 0.56; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.73
Batch: 420; loss: 0.77; acc: 0.8
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.79; acc: 0.78
Batch: 480; loss: 0.99; acc: 0.77
Batch: 500; loss: 0.58; acc: 0.86
Batch: 520; loss: 0.8; acc: 0.75
Batch: 540; loss: 0.59; acc: 0.84
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.58; acc: 0.77
Batch: 600; loss: 0.92; acc: 0.72
Batch: 620; loss: 0.89; acc: 0.77
Batch: 640; loss: 0.63; acc: 0.83
Batch: 660; loss: 0.8; acc: 0.77
Batch: 680; loss: 1.11; acc: 0.64
Batch: 700; loss: 0.8; acc: 0.67
Batch: 720; loss: 0.59; acc: 0.77
Batch: 740; loss: 0.7; acc: 0.75
Batch: 760; loss: 0.63; acc: 0.77
Batch: 780; loss: 0.89; acc: 0.8
Train Epoch over. train_loss: 0.71; train_accuracy: 0.77 

Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.77
Batch: 80; loss: 0.45; acc: 0.92
Batch: 100; loss: 0.61; acc: 0.78
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.39; acc: 0.84
Val Epoch over. val_loss: 0.672008440467962; val_accuracy: 0.7878184713375797 

The current subspace-distance is: 5.9514393797144294e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.63; acc: 0.78
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.62; acc: 0.81
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.7; acc: 0.83
Batch: 160; loss: 1.02; acc: 0.69
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.68; acc: 0.75
Batch: 220; loss: 0.9; acc: 0.69
Batch: 240; loss: 0.75; acc: 0.8
Batch: 260; loss: 0.79; acc: 0.75
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.65; acc: 0.81
Batch: 340; loss: 0.59; acc: 0.81
Batch: 360; loss: 0.73; acc: 0.73
Batch: 380; loss: 0.68; acc: 0.73
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.8; acc: 0.72
Batch: 440; loss: 0.56; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.77
Batch: 480; loss: 0.72; acc: 0.75
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.7; acc: 0.77
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.73; acc: 0.78
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.67; acc: 0.81
Batch: 640; loss: 0.79; acc: 0.77
Batch: 660; loss: 0.72; acc: 0.75
Batch: 680; loss: 0.93; acc: 0.69
Batch: 700; loss: 0.91; acc: 0.72
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.62; acc: 0.81
Batch: 780; loss: 0.89; acc: 0.72
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.88; acc: 0.69
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.83
Batch: 60; loss: 0.52; acc: 0.77
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.57; acc: 0.75
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.37; acc: 0.89
Val Epoch over. val_loss: 0.673951541921895; val_accuracy: 0.7880175159235668 

The current subspace-distance is: 6.334877252811566e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.68; acc: 0.75
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.83
Batch: 100; loss: 0.65; acc: 0.75
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.92; acc: 0.75
Batch: 160; loss: 0.66; acc: 0.75
Batch: 180; loss: 0.7; acc: 0.75
Batch: 200; loss: 0.47; acc: 0.86
Batch: 220; loss: 0.99; acc: 0.7
Batch: 240; loss: 0.73; acc: 0.78
Batch: 260; loss: 0.68; acc: 0.81
Batch: 280; loss: 0.83; acc: 0.75
Batch: 300; loss: 0.88; acc: 0.69
Batch: 320; loss: 0.75; acc: 0.77
Batch: 340; loss: 1.09; acc: 0.66
Batch: 360; loss: 0.74; acc: 0.77
Batch: 380; loss: 0.71; acc: 0.78
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.92; acc: 0.7
Batch: 460; loss: 1.12; acc: 0.67
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.92; acc: 0.77
Batch: 520; loss: 0.88; acc: 0.72
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.65; acc: 0.81
Batch: 580; loss: 0.64; acc: 0.77
Batch: 600; loss: 1.02; acc: 0.73
Batch: 620; loss: 0.51; acc: 0.88
Batch: 640; loss: 0.8; acc: 0.73
Batch: 660; loss: 0.86; acc: 0.77
Batch: 680; loss: 0.62; acc: 0.8
Batch: 700; loss: 0.61; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.81
Batch: 740; loss: 1.0; acc: 0.73
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.83
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.84; acc: 0.7
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.77
Batch: 120; loss: 0.73; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.6476333168851343; val_accuracy: 0.7953821656050956 

The current subspace-distance is: 6.550472608068958e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.65; acc: 0.78
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.82; acc: 0.72
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.84; acc: 0.7
Batch: 160; loss: 1.02; acc: 0.73
Batch: 180; loss: 0.86; acc: 0.75
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.62; acc: 0.75
Batch: 240; loss: 0.85; acc: 0.75
Batch: 260; loss: 0.63; acc: 0.78
Batch: 280; loss: 0.49; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.8
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.74; acc: 0.72
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.8; acc: 0.77
Batch: 400; loss: 0.71; acc: 0.75
Batch: 420; loss: 0.64; acc: 0.77
Batch: 440; loss: 0.48; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.78
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.87; acc: 0.78
Batch: 520; loss: 0.54; acc: 0.83
Batch: 540; loss: 0.53; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.58; acc: 0.77
Batch: 620; loss: 0.54; acc: 0.8
Batch: 640; loss: 0.74; acc: 0.78
Batch: 660; loss: 0.59; acc: 0.8
Batch: 680; loss: 0.96; acc: 0.72
Batch: 700; loss: 0.65; acc: 0.78
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.81; acc: 0.78
Batch: 760; loss: 0.56; acc: 0.83
Batch: 780; loss: 0.77; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.54; acc: 0.77
Batch: 80; loss: 0.42; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.77
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.38; acc: 0.91
Val Epoch over. val_loss: 0.653966730187653; val_accuracy: 0.7911027070063694 

The current subspace-distance is: 6.944020424271002e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.16; acc: 0.64
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.9; acc: 0.75
Batch: 60; loss: 1.01; acc: 0.72
Batch: 80; loss: 0.63; acc: 0.77
Batch: 100; loss: 0.42; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.75
Batch: 140; loss: 0.84; acc: 0.69
Batch: 160; loss: 0.73; acc: 0.72
Batch: 180; loss: 0.66; acc: 0.8
Batch: 200; loss: 0.59; acc: 0.81
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.49; acc: 0.86
Batch: 320; loss: 0.69; acc: 0.78
Batch: 340; loss: 0.6; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.77; acc: 0.78
Batch: 420; loss: 1.11; acc: 0.61
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.91; acc: 0.69
Batch: 480; loss: 0.63; acc: 0.77
Batch: 500; loss: 0.86; acc: 0.77
Batch: 520; loss: 0.59; acc: 0.78
Batch: 540; loss: 0.69; acc: 0.72
Batch: 560; loss: 0.9; acc: 0.75
Batch: 580; loss: 0.79; acc: 0.72
Batch: 600; loss: 0.69; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.77
Batch: 740; loss: 1.19; acc: 0.64
Batch: 760; loss: 0.66; acc: 0.78
Batch: 780; loss: 0.56; acc: 0.83
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.82; acc: 0.75
Batch: 20; loss: 0.69; acc: 0.75
Batch: 40; loss: 0.53; acc: 0.77
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.81
Batch: 100; loss: 0.59; acc: 0.75
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.38; acc: 0.92
Val Epoch over. val_loss: 0.6665502282654404; val_accuracy: 0.7899084394904459 

The current subspace-distance is: 7.011365960352123e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.68; acc: 0.75
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.77; acc: 0.73
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.69; acc: 0.73
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.7; acc: 0.72
Batch: 140; loss: 0.78; acc: 0.78
Batch: 160; loss: 0.58; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.85; acc: 0.7
Batch: 220; loss: 0.85; acc: 0.69
Batch: 240; loss: 0.77; acc: 0.7
Batch: 260; loss: 0.55; acc: 0.84
Batch: 280; loss: 0.54; acc: 0.8
Batch: 300; loss: 0.78; acc: 0.81
Batch: 320; loss: 0.78; acc: 0.78
Batch: 340; loss: 0.61; acc: 0.8
Batch: 360; loss: 0.65; acc: 0.81
Batch: 380; loss: 0.86; acc: 0.69
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.74; acc: 0.77
Batch: 440; loss: 0.68; acc: 0.7
Batch: 460; loss: 0.7; acc: 0.78
Batch: 480; loss: 0.98; acc: 0.72
Batch: 500; loss: 0.7; acc: 0.75
Batch: 520; loss: 0.81; acc: 0.69
Batch: 540; loss: 0.38; acc: 0.91
Batch: 560; loss: 0.68; acc: 0.78
Batch: 580; loss: 0.82; acc: 0.72
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.58; acc: 0.81
Batch: 640; loss: 0.71; acc: 0.83
Batch: 660; loss: 0.87; acc: 0.72
Batch: 680; loss: 0.51; acc: 0.84
Batch: 700; loss: 0.78; acc: 0.78
Batch: 720; loss: 0.58; acc: 0.78
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.78; acc: 0.73
Batch: 780; loss: 0.72; acc: 0.73
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.85; acc: 0.72
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.6; acc: 0.77
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.64; acc: 0.7
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.39; acc: 0.92
Val Epoch over. val_loss: 0.6874805259856449; val_accuracy: 0.7813495222929936 

The current subspace-distance is: 7.472273136954755e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.7; acc: 0.72
Batch: 60; loss: 0.66; acc: 0.86
Batch: 80; loss: 0.59; acc: 0.81
Batch: 100; loss: 0.92; acc: 0.73
Batch: 120; loss: 0.72; acc: 0.8
Batch: 140; loss: 0.84; acc: 0.72
Batch: 160; loss: 0.73; acc: 0.78
Batch: 180; loss: 0.69; acc: 0.75
Batch: 200; loss: 0.59; acc: 0.8
Batch: 220; loss: 1.06; acc: 0.67
Batch: 240; loss: 1.01; acc: 0.75
Batch: 260; loss: 0.68; acc: 0.78
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.72; acc: 0.77
Batch: 320; loss: 0.8; acc: 0.72
Batch: 340; loss: 0.67; acc: 0.78
Batch: 360; loss: 0.77; acc: 0.73
Batch: 380; loss: 0.66; acc: 0.77
Batch: 400; loss: 0.68; acc: 0.75
Batch: 420; loss: 0.57; acc: 0.81
Batch: 440; loss: 0.75; acc: 0.77
Batch: 460; loss: 0.99; acc: 0.69
Batch: 480; loss: 0.72; acc: 0.75
Batch: 500; loss: 0.78; acc: 0.8
Batch: 520; loss: 0.74; acc: 0.75
Batch: 540; loss: 0.82; acc: 0.72
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.78; acc: 0.78
Batch: 620; loss: 0.76; acc: 0.72
Batch: 640; loss: 0.8; acc: 0.78
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.66; acc: 0.77
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.87; acc: 0.8
Batch: 740; loss: 0.99; acc: 0.72
Batch: 760; loss: 0.73; acc: 0.78
Batch: 780; loss: 0.74; acc: 0.77
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.81; acc: 0.73
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.78
Batch: 60; loss: 0.55; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.67; acc: 0.73
Batch: 120; loss: 0.75; acc: 0.72
Batch: 140; loss: 0.37; acc: 0.92
Val Epoch over. val_loss: 0.6544594690678226; val_accuracy: 0.7939888535031847 

The current subspace-distance is: 7.442010974045843e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.82; acc: 0.78
Batch: 20; loss: 0.59; acc: 0.8
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 1.06; acc: 0.73
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 0.77; acc: 0.69
Batch: 160; loss: 0.66; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.78
Batch: 200; loss: 0.9; acc: 0.69
Batch: 220; loss: 0.88; acc: 0.75
Batch: 240; loss: 0.74; acc: 0.81
Batch: 260; loss: 0.72; acc: 0.75
Batch: 280; loss: 0.74; acc: 0.84
Batch: 300; loss: 0.58; acc: 0.81
Batch: 320; loss: 0.6; acc: 0.81
Batch: 340; loss: 0.52; acc: 0.88
Batch: 360; loss: 0.55; acc: 0.8
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.83; acc: 0.75
Batch: 420; loss: 0.82; acc: 0.78
Batch: 440; loss: 0.71; acc: 0.78
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.78; acc: 0.8
Batch: 500; loss: 0.64; acc: 0.73
Batch: 520; loss: 0.77; acc: 0.8
Batch: 540; loss: 0.69; acc: 0.75
Batch: 560; loss: 0.81; acc: 0.72
Batch: 580; loss: 0.74; acc: 0.77
Batch: 600; loss: 0.71; acc: 0.78
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.81
Batch: 660; loss: 0.7; acc: 0.73
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.65; acc: 0.77
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.81; acc: 0.73
Batch: 760; loss: 0.74; acc: 0.77
Batch: 780; loss: 0.86; acc: 0.66
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.78; acc: 0.73
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.77
Batch: 120; loss: 0.71; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.95
Val Epoch over. val_loss: 0.6429540306139904; val_accuracy: 0.7989649681528662 

The current subspace-distance is: 7.649449253221974e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.88; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.72
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.64; acc: 0.75
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 1.19; acc: 0.67
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.62; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.71; acc: 0.75
Batch: 260; loss: 0.62; acc: 0.73
Batch: 280; loss: 0.81; acc: 0.78
Batch: 300; loss: 0.76; acc: 0.69
Batch: 320; loss: 1.02; acc: 0.72
Batch: 340; loss: 0.66; acc: 0.8
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.57; acc: 0.8
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.64; acc: 0.81
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.98; acc: 0.73
Batch: 480; loss: 0.78; acc: 0.8
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.7; acc: 0.8
Batch: 540; loss: 0.85; acc: 0.7
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.71; acc: 0.81
Batch: 640; loss: 0.71; acc: 0.81
Batch: 660; loss: 0.85; acc: 0.72
Batch: 680; loss: 0.66; acc: 0.83
Batch: 700; loss: 0.78; acc: 0.7
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.75; acc: 0.77
Batch: 760; loss: 0.54; acc: 0.78
Batch: 780; loss: 0.83; acc: 0.77
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.79; acc: 0.72
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.75
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.6433441359905681; val_accuracy: 0.7977707006369427 

The current subspace-distance is: 8.023993723327294e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.69; acc: 0.77
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.83; acc: 0.73
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.79; acc: 0.67
Batch: 100; loss: 0.8; acc: 0.73
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.75; acc: 0.77
Batch: 160; loss: 0.69; acc: 0.77
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.61; acc: 0.73
Batch: 220; loss: 0.71; acc: 0.78
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.58; acc: 0.75
Batch: 280; loss: 0.54; acc: 0.81
Batch: 300; loss: 0.74; acc: 0.73
Batch: 320; loss: 0.83; acc: 0.75
Batch: 340; loss: 0.74; acc: 0.77
Batch: 360; loss: 1.06; acc: 0.67
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.49; acc: 0.81
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.81; acc: 0.73
Batch: 460; loss: 0.7; acc: 0.75
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.79; acc: 0.78
Batch: 520; loss: 0.71; acc: 0.78
Batch: 540; loss: 0.78; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.86
Batch: 580; loss: 0.59; acc: 0.84
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.52; acc: 0.81
Batch: 640; loss: 0.81; acc: 0.8
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 1.0; acc: 0.66
Batch: 740; loss: 0.65; acc: 0.86
Batch: 760; loss: 0.66; acc: 0.75
Batch: 780; loss: 0.82; acc: 0.8
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.76; acc: 0.75
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.77
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.6415028508491577; val_accuracy: 0.8001592356687898 

The current subspace-distance is: 8.280295878648758e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.6; acc: 0.75
Batch: 40; loss: 0.61; acc: 0.86
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 0.57; acc: 0.84
Batch: 120; loss: 0.65; acc: 0.7
Batch: 140; loss: 0.65; acc: 0.77
Batch: 160; loss: 0.62; acc: 0.78
Batch: 180; loss: 0.94; acc: 0.73
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 0.65; acc: 0.78
Batch: 240; loss: 0.84; acc: 0.77
Batch: 260; loss: 0.61; acc: 0.83
Batch: 280; loss: 0.82; acc: 0.7
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.64; acc: 0.83
Batch: 340; loss: 0.82; acc: 0.78
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.54; acc: 0.8
Batch: 400; loss: 0.61; acc: 0.84
Batch: 420; loss: 0.66; acc: 0.8
Batch: 440; loss: 0.58; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.77
Batch: 480; loss: 0.72; acc: 0.77
Batch: 500; loss: 0.52; acc: 0.83
Batch: 520; loss: 0.64; acc: 0.78
Batch: 540; loss: 0.87; acc: 0.75
Batch: 560; loss: 0.66; acc: 0.73
Batch: 580; loss: 0.72; acc: 0.72
Batch: 600; loss: 0.55; acc: 0.84
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.69; acc: 0.78
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.73; acc: 0.77
Batch: 700; loss: 0.62; acc: 0.78
Batch: 720; loss: 0.88; acc: 0.75
Batch: 740; loss: 0.66; acc: 0.8
Batch: 760; loss: 0.72; acc: 0.77
Batch: 780; loss: 1.04; acc: 0.67
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.77; acc: 0.73
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.32; acc: 0.91
Val Epoch over. val_loss: 0.6422580387561944; val_accuracy: 0.7976711783439491 

The current subspace-distance is: 8.504917059326544e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.94; acc: 0.73
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.62; acc: 0.73
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.79; acc: 0.73
Batch: 100; loss: 0.63; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.82; acc: 0.73
Batch: 160; loss: 0.68; acc: 0.78
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.69; acc: 0.73
Batch: 220; loss: 0.85; acc: 0.8
Batch: 240; loss: 0.98; acc: 0.64
Batch: 260; loss: 0.75; acc: 0.78
Batch: 280; loss: 0.9; acc: 0.73
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.68; acc: 0.73
Batch: 340; loss: 0.52; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.88
Batch: 380; loss: 1.27; acc: 0.69
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.68; acc: 0.77
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.9; acc: 0.7
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.79; acc: 0.8
Batch: 520; loss: 0.73; acc: 0.75
Batch: 540; loss: 0.72; acc: 0.77
Batch: 560; loss: 0.94; acc: 0.72
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.66; acc: 0.73
Batch: 660; loss: 0.81; acc: 0.72
Batch: 680; loss: 0.87; acc: 0.73
Batch: 700; loss: 0.72; acc: 0.73
Batch: 720; loss: 0.74; acc: 0.75
Batch: 740; loss: 0.88; acc: 0.75
Batch: 760; loss: 0.67; acc: 0.8
Batch: 780; loss: 0.74; acc: 0.73
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.78; acc: 0.77
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.34; acc: 0.89
Val Epoch over. val_loss: 0.6448283288509223; val_accuracy: 0.7988654458598726 

The current subspace-distance is: 8.575705578550696e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.85; acc: 0.77
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 0.83; acc: 0.69
Batch: 60; loss: 0.64; acc: 0.77
Batch: 80; loss: 0.73; acc: 0.81
Batch: 100; loss: 0.77; acc: 0.77
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.71; acc: 0.73
Batch: 160; loss: 1.0; acc: 0.66
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.77; acc: 0.7
Batch: 220; loss: 0.49; acc: 0.81
Batch: 240; loss: 0.71; acc: 0.8
Batch: 260; loss: 0.88; acc: 0.77
Batch: 280; loss: 0.64; acc: 0.81
Batch: 300; loss: 0.8; acc: 0.78
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.99; acc: 0.75
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.78; acc: 0.73
Batch: 440; loss: 0.87; acc: 0.77
Batch: 460; loss: 0.71; acc: 0.73
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.84; acc: 0.75
Batch: 520; loss: 0.53; acc: 0.91
Batch: 540; loss: 0.71; acc: 0.78
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.78; acc: 0.77
Batch: 640; loss: 0.65; acc: 0.83
Batch: 660; loss: 0.62; acc: 0.8
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.72; acc: 0.73
Batch: 740; loss: 0.86; acc: 0.72
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.54; acc: 0.81
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.61; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.33; acc: 0.89
Val Epoch over. val_loss: 0.6395229942100064; val_accuracy: 0.8019506369426752 

The current subspace-distance is: 8.518371760146692e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.94; acc: 0.69
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.5; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.73
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.53; acc: 0.89
Batch: 180; loss: 0.85; acc: 0.77
Batch: 200; loss: 0.79; acc: 0.75
Batch: 220; loss: 0.71; acc: 0.73
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.64; acc: 0.75
Batch: 280; loss: 0.81; acc: 0.81
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 0.73; acc: 0.77
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.8
Batch: 380; loss: 0.52; acc: 0.88
Batch: 400; loss: 0.97; acc: 0.69
Batch: 420; loss: 0.89; acc: 0.75
Batch: 440; loss: 0.83; acc: 0.67
Batch: 460; loss: 0.86; acc: 0.77
Batch: 480; loss: 0.87; acc: 0.73
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.67; acc: 0.73
Batch: 540; loss: 0.6; acc: 0.81
Batch: 560; loss: 0.66; acc: 0.77
Batch: 580; loss: 0.83; acc: 0.72
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.8; acc: 0.72
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 0.53; acc: 0.84
Batch: 700; loss: 0.85; acc: 0.78
Batch: 720; loss: 0.76; acc: 0.8
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.72; acc: 0.73
Batch: 780; loss: 0.44; acc: 0.8
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.77
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.77
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.6410701195145868; val_accuracy: 0.7982683121019108 

The current subspace-distance is: 8.647284266771749e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.89; acc: 0.75
Batch: 20; loss: 0.58; acc: 0.78
Batch: 40; loss: 0.83; acc: 0.77
Batch: 60; loss: 0.66; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.75
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.68; acc: 0.81
Batch: 140; loss: 0.77; acc: 0.8
Batch: 160; loss: 0.7; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.75
Batch: 200; loss: 0.52; acc: 0.83
Batch: 220; loss: 0.66; acc: 0.81
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.88; acc: 0.73
Batch: 280; loss: 0.62; acc: 0.77
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.99; acc: 0.73
Batch: 340; loss: 0.62; acc: 0.78
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.93; acc: 0.72
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.64; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.89; acc: 0.73
Batch: 500; loss: 0.68; acc: 0.78
Batch: 520; loss: 0.74; acc: 0.75
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.93; acc: 0.66
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.65; acc: 0.8
Batch: 660; loss: 0.41; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.8
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.96; acc: 0.73
Batch: 760; loss: 0.62; acc: 0.77
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.75
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.89
Val Epoch over. val_loss: 0.6389370920361986; val_accuracy: 0.7981687898089171 

The current subspace-distance is: 8.873374463291839e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.63; acc: 0.77
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.77; acc: 0.78
Batch: 60; loss: 0.74; acc: 0.73
Batch: 80; loss: 0.74; acc: 0.83
Batch: 100; loss: 0.74; acc: 0.73
Batch: 120; loss: 0.53; acc: 0.78
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.94; acc: 0.64
Batch: 180; loss: 0.8; acc: 0.73
Batch: 200; loss: 0.72; acc: 0.77
Batch: 220; loss: 0.62; acc: 0.84
Batch: 240; loss: 0.54; acc: 0.81
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.93; acc: 0.8
Batch: 300; loss: 0.57; acc: 0.84
Batch: 320; loss: 0.69; acc: 0.78
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.66; acc: 0.77
Batch: 380; loss: 0.68; acc: 0.78
Batch: 400; loss: 0.53; acc: 0.8
Batch: 420; loss: 0.63; acc: 0.8
Batch: 440; loss: 0.56; acc: 0.8
Batch: 460; loss: 0.62; acc: 0.78
Batch: 480; loss: 0.62; acc: 0.81
Batch: 500; loss: 0.61; acc: 0.78
Batch: 520; loss: 0.64; acc: 0.75
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.94; acc: 0.78
Batch: 580; loss: 0.68; acc: 0.8
Batch: 600; loss: 0.72; acc: 0.7
Batch: 620; loss: 0.58; acc: 0.8
Batch: 640; loss: 0.7; acc: 0.81
Batch: 660; loss: 0.46; acc: 0.81
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.85; acc: 0.78
Batch: 720; loss: 0.71; acc: 0.78
Batch: 740; loss: 0.69; acc: 0.75
Batch: 760; loss: 0.68; acc: 0.78
Batch: 780; loss: 1.03; acc: 0.67
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.6374371114430154; val_accuracy: 0.8022492038216561 

The current subspace-distance is: 9.259558282792568e-05 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.73; acc: 0.77
Batch: 60; loss: 0.76; acc: 0.81
Batch: 80; loss: 0.65; acc: 0.77
Batch: 100; loss: 0.7; acc: 0.75
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.81
Batch: 220; loss: 0.85; acc: 0.77
Batch: 240; loss: 0.62; acc: 0.77
Batch: 260; loss: 0.53; acc: 0.88
Batch: 280; loss: 0.78; acc: 0.78
Batch: 300; loss: 1.03; acc: 0.73
Batch: 320; loss: 0.67; acc: 0.8
Batch: 340; loss: 0.74; acc: 0.78
Batch: 360; loss: 0.74; acc: 0.73
Batch: 380; loss: 0.58; acc: 0.81
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.89; acc: 0.7
Batch: 440; loss: 0.76; acc: 0.78
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.74; acc: 0.75
Batch: 500; loss: 0.53; acc: 0.8
Batch: 520; loss: 0.74; acc: 0.69
Batch: 540; loss: 0.64; acc: 0.78
Batch: 560; loss: 0.82; acc: 0.73
Batch: 580; loss: 0.65; acc: 0.81
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.72; acc: 0.78
Batch: 640; loss: 0.59; acc: 0.8
Batch: 660; loss: 0.74; acc: 0.83
Batch: 680; loss: 0.76; acc: 0.8
Batch: 700; loss: 0.65; acc: 0.83
Batch: 720; loss: 0.88; acc: 0.73
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.66; acc: 0.78
Train Epoch over. train_loss: 0.69; train_accuracy: 0.78 

Batch: 0; loss: 0.75; acc: 0.72
Batch: 20; loss: 0.66; acc: 0.73
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.84
Batch: 100; loss: 0.66; acc: 0.75
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.31; acc: 0.91
Val Epoch over. val_loss: 0.6438144646632443; val_accuracy: 0.7954816878980892 

The current subspace-distance is: 9.401904389960691e-05 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.95; acc: 0.77
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.67; acc: 0.77
Batch: 60; loss: 1.02; acc: 0.66
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.67; acc: 0.78
Batch: 120; loss: 1.04; acc: 0.72
Batch: 140; loss: 0.79; acc: 0.73
Batch: 160; loss: 0.8; acc: 0.78
Batch: 180; loss: 0.71; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.97; acc: 0.72
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.64; acc: 0.72
Batch: 280; loss: 0.59; acc: 0.81
Batch: 300; loss: 0.85; acc: 0.81
Batch: 320; loss: 0.77; acc: 0.78
Batch: 340; loss: 0.65; acc: 0.8
Batch: 360; loss: 0.62; acc: 0.78
Batch: 380; loss: 0.81; acc: 0.72
Batch: 400; loss: 0.58; acc: 0.86
Batch: 420; loss: 0.65; acc: 0.78
Batch: 440; loss: 0.73; acc: 0.78
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.76; acc: 0.77
Batch: 520; loss: 0.86; acc: 0.78
Batch: 540; loss: 0.87; acc: 0.7
Batch: 560; loss: 0.67; acc: 0.8
Batch: 580; loss: 0.81; acc: 0.77
Batch: 600; loss: 0.66; acc: 0.81
Batch: 620; loss: 0.73; acc: 0.73
Batch: 640; loss: 0.83; acc: 0.72
Batch: 660; loss: 0.62; acc: 0.77
Batch: 680; loss: 0.78; acc: 0.72
Batch: 700; loss: 0.57; acc: 0.8
Batch: 720; loss: 0.77; acc: 0.78
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.81; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.91
Val Epoch over. val_loss: 0.6335499263872766; val_accuracy: 0.8000597133757962 

The current subspace-distance is: 9.425514144822955e-05 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.7; acc: 0.7
Batch: 160; loss: 0.74; acc: 0.84
Batch: 180; loss: 0.56; acc: 0.77
Batch: 200; loss: 0.69; acc: 0.78
Batch: 220; loss: 0.45; acc: 0.83
Batch: 240; loss: 0.71; acc: 0.78
Batch: 260; loss: 0.89; acc: 0.78
Batch: 280; loss: 0.72; acc: 0.72
Batch: 300; loss: 0.75; acc: 0.75
Batch: 320; loss: 0.77; acc: 0.72
Batch: 340; loss: 0.66; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.81
Batch: 380; loss: 0.69; acc: 0.78
Batch: 400; loss: 0.53; acc: 0.81
Batch: 420; loss: 0.85; acc: 0.73
Batch: 440; loss: 0.57; acc: 0.86
Batch: 460; loss: 0.64; acc: 0.81
Batch: 480; loss: 0.77; acc: 0.77
Batch: 500; loss: 0.63; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.83
Batch: 540; loss: 0.69; acc: 0.77
Batch: 560; loss: 0.85; acc: 0.67
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.83
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.79; acc: 0.69
Batch: 660; loss: 0.6; acc: 0.75
Batch: 680; loss: 0.65; acc: 0.84
Batch: 700; loss: 0.82; acc: 0.75
Batch: 720; loss: 0.77; acc: 0.77
Batch: 740; loss: 0.69; acc: 0.77
Batch: 760; loss: 0.75; acc: 0.75
Batch: 780; loss: 0.83; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.91
Val Epoch over. val_loss: 0.6324523256462851; val_accuracy: 0.8007563694267515 

The current subspace-distance is: 9.476546983933076e-05 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.76; acc: 0.8
Batch: 20; loss: 0.67; acc: 0.8
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.76; acc: 0.8
Batch: 100; loss: 0.81; acc: 0.72
Batch: 120; loss: 0.85; acc: 0.7
Batch: 140; loss: 0.7; acc: 0.73
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.46; acc: 0.88
Batch: 200; loss: 0.89; acc: 0.73
Batch: 220; loss: 0.69; acc: 0.75
Batch: 240; loss: 0.72; acc: 0.73
Batch: 260; loss: 0.82; acc: 0.75
Batch: 280; loss: 0.75; acc: 0.75
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.55; acc: 0.81
Batch: 340; loss: 0.73; acc: 0.8
Batch: 360; loss: 0.94; acc: 0.67
Batch: 380; loss: 0.75; acc: 0.7
Batch: 400; loss: 0.67; acc: 0.73
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.72
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.7; acc: 0.73
Batch: 500; loss: 0.58; acc: 0.78
Batch: 520; loss: 0.9; acc: 0.77
Batch: 540; loss: 0.79; acc: 0.77
Batch: 560; loss: 0.69; acc: 0.78
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.9; acc: 0.72
Batch: 620; loss: 0.84; acc: 0.78
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.73; acc: 0.72
Batch: 680; loss: 0.59; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.78
Batch: 720; loss: 0.82; acc: 0.73
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.59; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.91
Val Epoch over. val_loss: 0.6328290858466155; val_accuracy: 0.8001592356687898 

The current subspace-distance is: 9.627789404476061e-05 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.9; acc: 0.78
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.89; acc: 0.7
Batch: 60; loss: 1.08; acc: 0.73
Batch: 80; loss: 0.8; acc: 0.72
Batch: 100; loss: 1.15; acc: 0.69
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 1.07; acc: 0.75
Batch: 180; loss: 0.6; acc: 0.8
Batch: 200; loss: 0.71; acc: 0.78
Batch: 220; loss: 0.68; acc: 0.73
Batch: 240; loss: 0.59; acc: 0.83
Batch: 260; loss: 0.62; acc: 0.78
Batch: 280; loss: 0.85; acc: 0.73
Batch: 300; loss: 0.7; acc: 0.77
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.94
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.91
Batch: 420; loss: 0.55; acc: 0.83
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.91; acc: 0.66
Batch: 480; loss: 0.65; acc: 0.75
Batch: 500; loss: 0.8; acc: 0.8
Batch: 520; loss: 0.65; acc: 0.75
Batch: 540; loss: 0.62; acc: 0.81
Batch: 560; loss: 0.83; acc: 0.75
Batch: 580; loss: 0.83; acc: 0.8
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.83
Batch: 640; loss: 0.65; acc: 0.8
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.78; acc: 0.78
Batch: 700; loss: 0.55; acc: 0.81
Batch: 720; loss: 0.79; acc: 0.77
Batch: 740; loss: 0.81; acc: 0.81
Batch: 760; loss: 0.63; acc: 0.78
Batch: 780; loss: 0.6; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.6314012933119088; val_accuracy: 0.8007563694267515 

The current subspace-distance is: 0.00010137054050574079 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.67; acc: 0.81
Batch: 20; loss: 0.93; acc: 0.7
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.66; acc: 0.75
Batch: 100; loss: 0.94; acc: 0.73
Batch: 120; loss: 0.68; acc: 0.73
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.61; acc: 0.77
Batch: 180; loss: 0.78; acc: 0.77
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.69; acc: 0.72
Batch: 240; loss: 0.56; acc: 0.81
Batch: 260; loss: 0.8; acc: 0.8
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.68; acc: 0.77
Batch: 340; loss: 0.73; acc: 0.77
Batch: 360; loss: 0.43; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.8
Batch: 400; loss: 0.53; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.81
Batch: 440; loss: 0.72; acc: 0.78
Batch: 460; loss: 0.66; acc: 0.8
Batch: 480; loss: 0.77; acc: 0.7
Batch: 500; loss: 0.88; acc: 0.72
Batch: 520; loss: 0.82; acc: 0.69
Batch: 540; loss: 0.69; acc: 0.77
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.65; acc: 0.75
Batch: 600; loss: 0.75; acc: 0.81
Batch: 620; loss: 0.53; acc: 0.89
Batch: 640; loss: 0.64; acc: 0.78
Batch: 660; loss: 0.71; acc: 0.8
Batch: 680; loss: 0.66; acc: 0.8
Batch: 700; loss: 0.66; acc: 0.77
Batch: 720; loss: 0.66; acc: 0.83
Batch: 740; loss: 0.85; acc: 0.7
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.89
Val Epoch over. val_loss: 0.6318824690808157; val_accuracy: 0.7988654458598726 

The current subspace-distance is: 9.96959424810484e-05 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.71; acc: 0.77
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.65; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.83
Batch: 120; loss: 0.83; acc: 0.7
Batch: 140; loss: 0.6; acc: 0.83
Batch: 160; loss: 0.78; acc: 0.8
Batch: 180; loss: 0.9; acc: 0.72
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.82; acc: 0.72
Batch: 240; loss: 0.85; acc: 0.77
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.89; acc: 0.73
Batch: 300; loss: 0.86; acc: 0.75
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.94; acc: 0.73
Batch: 360; loss: 0.58; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.87; acc: 0.75
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 1.01; acc: 0.7
Batch: 460; loss: 0.88; acc: 0.7
Batch: 480; loss: 0.72; acc: 0.73
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.82; acc: 0.73
Batch: 540; loss: 0.61; acc: 0.75
Batch: 560; loss: 0.69; acc: 0.77
Batch: 580; loss: 0.73; acc: 0.75
Batch: 600; loss: 0.77; acc: 0.7
Batch: 620; loss: 0.63; acc: 0.8
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.68; acc: 0.75
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.84; acc: 0.75
Batch: 720; loss: 0.41; acc: 0.92
Batch: 740; loss: 0.76; acc: 0.75
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.63; acc: 0.81
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.75
Batch: 40; loss: 0.52; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.6308137915886132; val_accuracy: 0.798765923566879 

The current subspace-distance is: 0.00010215963993687183 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.7; acc: 0.75
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.66; acc: 0.77
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.69; acc: 0.8
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.88; acc: 0.75
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.79; acc: 0.78
Batch: 260; loss: 0.67; acc: 0.77
Batch: 280; loss: 0.8; acc: 0.72
Batch: 300; loss: 1.05; acc: 0.69
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.62; acc: 0.81
Batch: 360; loss: 0.75; acc: 0.75
Batch: 380; loss: 0.67; acc: 0.77
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.68; acc: 0.75
Batch: 440; loss: 0.82; acc: 0.69
Batch: 460; loss: 0.8; acc: 0.72
Batch: 480; loss: 0.59; acc: 0.8
Batch: 500; loss: 0.54; acc: 0.78
Batch: 520; loss: 0.75; acc: 0.77
Batch: 540; loss: 0.93; acc: 0.73
Batch: 560; loss: 0.88; acc: 0.7
Batch: 580; loss: 0.7; acc: 0.78
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 1.0; acc: 0.62
Batch: 640; loss: 0.61; acc: 0.86
Batch: 660; loss: 0.86; acc: 0.73
Batch: 680; loss: 0.61; acc: 0.84
Batch: 700; loss: 1.01; acc: 0.69
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.56; acc: 0.8
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.64; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.74; acc: 0.73
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.78
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.78
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.6333220091405188; val_accuracy: 0.7996616242038217 

The current subspace-distance is: 0.00010526228288654238 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.7; acc: 0.75
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 1.04; acc: 0.73
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.69; acc: 0.72
Batch: 160; loss: 0.63; acc: 0.78
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.78
Batch: 220; loss: 0.92; acc: 0.75
Batch: 240; loss: 0.67; acc: 0.7
Batch: 260; loss: 0.89; acc: 0.7
Batch: 280; loss: 1.04; acc: 0.66
Batch: 300; loss: 0.8; acc: 0.75
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.8; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.66; acc: 0.77
Batch: 420; loss: 0.72; acc: 0.67
Batch: 440; loss: 0.62; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.65; acc: 0.83
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.86; acc: 0.73
Batch: 540; loss: 0.67; acc: 0.78
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.83; acc: 0.77
Batch: 640; loss: 0.8; acc: 0.8
Batch: 660; loss: 0.55; acc: 0.8
Batch: 680; loss: 0.47; acc: 0.8
Batch: 700; loss: 0.58; acc: 0.83
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.64; acc: 0.8
Batch: 760; loss: 0.69; acc: 0.77
Batch: 780; loss: 0.59; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.62; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.6335657374684218; val_accuracy: 0.8004578025477707 

The current subspace-distance is: 0.00010825112258316949 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.68; acc: 0.78
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 1.09; acc: 0.72
Batch: 100; loss: 0.74; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.72
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.75; acc: 0.8
Batch: 180; loss: 0.67; acc: 0.72
Batch: 200; loss: 0.58; acc: 0.86
Batch: 220; loss: 0.71; acc: 0.77
Batch: 240; loss: 0.73; acc: 0.78
Batch: 260; loss: 0.57; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.66; acc: 0.8
Batch: 320; loss: 0.85; acc: 0.81
Batch: 340; loss: 0.65; acc: 0.78
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.84
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.75; acc: 0.72
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.66; acc: 0.81
Batch: 480; loss: 0.76; acc: 0.73
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.75; acc: 0.8
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.67; acc: 0.78
Batch: 580; loss: 0.51; acc: 0.83
Batch: 600; loss: 0.55; acc: 0.78
Batch: 620; loss: 0.75; acc: 0.8
Batch: 640; loss: 0.46; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.77
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.64; acc: 0.77
Batch: 720; loss: 0.78; acc: 0.73
Batch: 740; loss: 0.63; acc: 0.8
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.94; acc: 0.7
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.6314171215720997; val_accuracy: 0.8003582802547771 

The current subspace-distance is: 0.00011128383630421013 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.62; acc: 0.83
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.64; acc: 0.84
Batch: 60; loss: 0.58; acc: 0.8
Batch: 80; loss: 0.87; acc: 0.7
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.76; acc: 0.73
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.85; acc: 0.72
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.56; acc: 0.81
Batch: 260; loss: 0.96; acc: 0.7
Batch: 280; loss: 0.74; acc: 0.72
Batch: 300; loss: 0.76; acc: 0.73
Batch: 320; loss: 0.56; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.57; acc: 0.81
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.72
Batch: 420; loss: 0.53; acc: 0.77
Batch: 440; loss: 0.79; acc: 0.77
Batch: 460; loss: 0.43; acc: 0.83
Batch: 480; loss: 0.67; acc: 0.77
Batch: 500; loss: 0.72; acc: 0.8
Batch: 520; loss: 0.51; acc: 0.88
Batch: 540; loss: 0.75; acc: 0.75
Batch: 560; loss: 0.67; acc: 0.88
Batch: 580; loss: 0.77; acc: 0.75
Batch: 600; loss: 0.63; acc: 0.83
Batch: 620; loss: 0.66; acc: 0.8
Batch: 640; loss: 0.67; acc: 0.75
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.71; acc: 0.77
Batch: 700; loss: 0.68; acc: 0.78
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.89; acc: 0.77
Batch: 760; loss: 0.63; acc: 0.77
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.6; acc: 0.77
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.88
Val Epoch over. val_loss: 0.6337083145311684; val_accuracy: 0.796875 

The current subspace-distance is: 0.00011320186604280025 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.78
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.7; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.84
Batch: 140; loss: 0.53; acc: 0.77
Batch: 160; loss: 0.61; acc: 0.78
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.82; acc: 0.77
Batch: 220; loss: 0.78; acc: 0.77
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.65; acc: 0.75
Batch: 280; loss: 0.77; acc: 0.73
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.8; acc: 0.77
Batch: 340; loss: 0.6; acc: 0.78
Batch: 360; loss: 0.72; acc: 0.75
Batch: 380; loss: 0.53; acc: 0.8
Batch: 400; loss: 0.7; acc: 0.78
Batch: 420; loss: 0.75; acc: 0.78
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.59; acc: 0.78
Batch: 500; loss: 0.59; acc: 0.81
Batch: 520; loss: 0.72; acc: 0.75
Batch: 540; loss: 0.51; acc: 0.81
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.82; acc: 0.7
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 0.79; acc: 0.81
Batch: 640; loss: 0.73; acc: 0.78
Batch: 660; loss: 0.73; acc: 0.75
Batch: 680; loss: 0.42; acc: 0.92
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.78; acc: 0.78
Batch: 740; loss: 0.79; acc: 0.78
Batch: 760; loss: 0.89; acc: 0.7
Batch: 780; loss: 0.69; acc: 0.75
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.6308131160060312; val_accuracy: 0.7994625796178344 

The current subspace-distance is: 0.00011219729640288278 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.65; acc: 0.77
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.71; acc: 0.8
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.75; acc: 0.72
Batch: 240; loss: 0.95; acc: 0.73
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.68; acc: 0.73
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.82; acc: 0.7
Batch: 360; loss: 0.54; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.81; acc: 0.73
Batch: 420; loss: 0.63; acc: 0.77
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.87; acc: 0.67
Batch: 500; loss: 0.69; acc: 0.75
Batch: 520; loss: 0.49; acc: 0.88
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.79; acc: 0.75
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.53; acc: 0.75
Batch: 620; loss: 0.81; acc: 0.78
Batch: 640; loss: 0.88; acc: 0.75
Batch: 660; loss: 0.7; acc: 0.73
Batch: 680; loss: 0.72; acc: 0.83
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.8; acc: 0.78
Batch: 740; loss: 0.65; acc: 0.83
Batch: 760; loss: 0.7; acc: 0.8
Batch: 780; loss: 0.57; acc: 0.78
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.6314595191721704; val_accuracy: 0.7991640127388535 

The current subspace-distance is: 0.00011337910109432414 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.77; acc: 0.72
Batch: 20; loss: 0.65; acc: 0.78
Batch: 40; loss: 0.78; acc: 0.78
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.78
Batch: 160; loss: 0.79; acc: 0.77
Batch: 180; loss: 0.53; acc: 0.81
Batch: 200; loss: 1.04; acc: 0.61
Batch: 220; loss: 0.68; acc: 0.73
Batch: 240; loss: 0.57; acc: 0.84
Batch: 260; loss: 0.62; acc: 0.83
Batch: 280; loss: 0.85; acc: 0.7
Batch: 300; loss: 0.7; acc: 0.8
Batch: 320; loss: 0.69; acc: 0.78
Batch: 340; loss: 0.76; acc: 0.78
Batch: 360; loss: 0.87; acc: 0.75
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.73; acc: 0.78
Batch: 440; loss: 0.59; acc: 0.83
Batch: 460; loss: 0.73; acc: 0.75
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.69; acc: 0.73
Batch: 540; loss: 0.8; acc: 0.75
Batch: 560; loss: 0.87; acc: 0.75
Batch: 580; loss: 0.59; acc: 0.8
Batch: 600; loss: 0.6; acc: 0.81
Batch: 620; loss: 0.83; acc: 0.72
Batch: 640; loss: 0.61; acc: 0.8
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.81; acc: 0.7
Batch: 700; loss: 0.73; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.81
Batch: 740; loss: 0.74; acc: 0.75
Batch: 760; loss: 0.86; acc: 0.69
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.6298053210518163; val_accuracy: 0.801453025477707 

The current subspace-distance is: 0.00011889371671713889 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.9; acc: 0.73
Batch: 40; loss: 0.76; acc: 0.75
Batch: 60; loss: 0.63; acc: 0.77
Batch: 80; loss: 0.78; acc: 0.67
Batch: 100; loss: 1.0; acc: 0.64
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.66; acc: 0.77
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.64; acc: 0.8
Batch: 200; loss: 0.99; acc: 0.7
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.77
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.59; acc: 0.78
Batch: 320; loss: 0.87; acc: 0.66
Batch: 340; loss: 0.69; acc: 0.75
Batch: 360; loss: 0.59; acc: 0.8
Batch: 380; loss: 0.59; acc: 0.86
Batch: 400; loss: 0.85; acc: 0.72
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.64; acc: 0.81
Batch: 460; loss: 0.73; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.83
Batch: 500; loss: 0.93; acc: 0.7
Batch: 520; loss: 0.46; acc: 0.81
Batch: 540; loss: 0.65; acc: 0.78
Batch: 560; loss: 0.85; acc: 0.69
Batch: 580; loss: 0.57; acc: 0.8
Batch: 600; loss: 0.76; acc: 0.78
Batch: 620; loss: 0.85; acc: 0.75
Batch: 640; loss: 0.9; acc: 0.73
Batch: 660; loss: 0.74; acc: 0.78
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.65; acc: 0.72
Batch: 760; loss: 0.66; acc: 0.78
Batch: 780; loss: 0.64; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.6303238203381277; val_accuracy: 0.7994625796178344 

The current subspace-distance is: 0.00012081493332516402 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.71; acc: 0.78
Batch: 20; loss: 0.54; acc: 0.84
Batch: 40; loss: 0.58; acc: 0.81
Batch: 60; loss: 0.7; acc: 0.77
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.53; acc: 0.77
Batch: 160; loss: 0.65; acc: 0.86
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.68; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.92; acc: 0.72
Batch: 320; loss: 0.63; acc: 0.77
Batch: 340; loss: 0.66; acc: 0.84
Batch: 360; loss: 0.6; acc: 0.8
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.83; acc: 0.8
Batch: 420; loss: 0.64; acc: 0.77
Batch: 440; loss: 0.63; acc: 0.77
Batch: 460; loss: 0.94; acc: 0.78
Batch: 480; loss: 0.56; acc: 0.81
Batch: 500; loss: 0.95; acc: 0.73
Batch: 520; loss: 0.83; acc: 0.7
Batch: 540; loss: 0.69; acc: 0.8
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.63; acc: 0.8
Batch: 600; loss: 0.62; acc: 0.78
Batch: 620; loss: 0.81; acc: 0.77
Batch: 640; loss: 0.73; acc: 0.75
Batch: 660; loss: 0.86; acc: 0.73
Batch: 680; loss: 0.62; acc: 0.78
Batch: 700; loss: 0.98; acc: 0.7
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.64; acc: 0.78
Batch: 760; loss: 0.77; acc: 0.73
Batch: 780; loss: 0.8; acc: 0.73
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.58; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.92
Val Epoch over. val_loss: 0.6301266387769371; val_accuracy: 0.8007563694267515 

The current subspace-distance is: 0.00012001508730463684 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.65; acc: 0.72
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.89; acc: 0.75
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.51; acc: 0.81
Batch: 160; loss: 0.63; acc: 0.75
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.87; acc: 0.73
Batch: 220; loss: 0.88; acc: 0.67
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.74; acc: 0.75
Batch: 280; loss: 0.69; acc: 0.78
Batch: 300; loss: 0.56; acc: 0.78
Batch: 320; loss: 1.09; acc: 0.66
Batch: 340; loss: 0.84; acc: 0.84
Batch: 360; loss: 0.85; acc: 0.75
Batch: 380; loss: 0.46; acc: 0.83
Batch: 400; loss: 0.63; acc: 0.78
Batch: 420; loss: 0.9; acc: 0.64
Batch: 440; loss: 0.85; acc: 0.75
Batch: 460; loss: 0.63; acc: 0.88
Batch: 480; loss: 0.87; acc: 0.66
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.76; acc: 0.73
Batch: 540; loss: 0.58; acc: 0.8
Batch: 560; loss: 1.03; acc: 0.67
Batch: 580; loss: 0.93; acc: 0.78
Batch: 600; loss: 0.81; acc: 0.73
Batch: 620; loss: 0.65; acc: 0.81
Batch: 640; loss: 0.94; acc: 0.69
Batch: 660; loss: 0.57; acc: 0.84
Batch: 680; loss: 0.81; acc: 0.73
Batch: 700; loss: 0.59; acc: 0.84
Batch: 720; loss: 0.78; acc: 0.77
Batch: 740; loss: 0.66; acc: 0.81
Batch: 760; loss: 0.85; acc: 0.73
Batch: 780; loss: 0.72; acc: 0.77
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.6295829344137459; val_accuracy: 0.8024482484076433 

The current subspace-distance is: 0.00011960526171606034 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.68; acc: 0.86
Batch: 40; loss: 0.52; acc: 0.84
Batch: 60; loss: 0.53; acc: 0.8
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 0.64; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.88
Batch: 160; loss: 0.94; acc: 0.73
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.65; acc: 0.75
Batch: 220; loss: 0.74; acc: 0.75
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.72; acc: 0.83
Batch: 320; loss: 0.79; acc: 0.75
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.61; acc: 0.81
Batch: 380; loss: 0.8; acc: 0.75
Batch: 400; loss: 0.96; acc: 0.73
Batch: 420; loss: 0.66; acc: 0.77
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.73
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.87; acc: 0.77
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 1.04; acc: 0.69
Batch: 560; loss: 0.72; acc: 0.78
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.77; acc: 0.78
Batch: 640; loss: 0.65; acc: 0.73
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.74; acc: 0.81
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.86; acc: 0.78
Batch: 740; loss: 0.73; acc: 0.77
Batch: 760; loss: 0.69; acc: 0.73
Batch: 780; loss: 0.62; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.62; acc: 0.8
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.6299644684905459; val_accuracy: 0.8019506369426752 

The current subspace-distance is: 0.00011859368532896042 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.83; acc: 0.7
Batch: 40; loss: 0.55; acc: 0.81
Batch: 60; loss: 0.68; acc: 0.72
Batch: 80; loss: 0.53; acc: 0.78
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.82; acc: 0.75
Batch: 160; loss: 0.91; acc: 0.73
Batch: 180; loss: 0.87; acc: 0.72
Batch: 200; loss: 0.8; acc: 0.73
Batch: 220; loss: 0.9; acc: 0.73
Batch: 240; loss: 0.64; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.69; acc: 0.75
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.74; acc: 0.77
Batch: 340; loss: 0.83; acc: 0.77
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.74; acc: 0.8
Batch: 400; loss: 0.72; acc: 0.72
Batch: 420; loss: 0.95; acc: 0.73
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.8
Batch: 480; loss: 0.69; acc: 0.78
Batch: 500; loss: 0.54; acc: 0.78
Batch: 520; loss: 0.77; acc: 0.77
Batch: 540; loss: 0.76; acc: 0.78
Batch: 560; loss: 0.62; acc: 0.78
Batch: 580; loss: 0.48; acc: 0.8
Batch: 600; loss: 0.61; acc: 0.8
Batch: 620; loss: 0.91; acc: 0.7
Batch: 640; loss: 0.6; acc: 0.78
Batch: 660; loss: 0.79; acc: 0.78
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.71; acc: 0.72
Batch: 720; loss: 0.73; acc: 0.73
Batch: 740; loss: 0.82; acc: 0.73
Batch: 760; loss: 0.77; acc: 0.73
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.77
Batch: 20; loss: 0.62; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.66; acc: 0.77
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.91
Val Epoch over. val_loss: 0.6305169033206952; val_accuracy: 0.8005573248407644 

The current subspace-distance is: 0.00012363215500954539 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 0.66; acc: 0.8
Batch: 60; loss: 0.7; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.73
Batch: 100; loss: 0.92; acc: 0.72
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.88; acc: 0.7
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.77
Batch: 200; loss: 0.42; acc: 0.91
Batch: 220; loss: 0.85; acc: 0.73
Batch: 240; loss: 0.6; acc: 0.77
Batch: 260; loss: 0.64; acc: 0.75
Batch: 280; loss: 0.79; acc: 0.7
Batch: 300; loss: 0.79; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.75
Batch: 340; loss: 0.62; acc: 0.77
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.62; acc: 0.8
Batch: 440; loss: 0.65; acc: 0.78
Batch: 460; loss: 0.69; acc: 0.78
Batch: 480; loss: 0.62; acc: 0.8
Batch: 500; loss: 0.57; acc: 0.78
Batch: 520; loss: 0.88; acc: 0.72
Batch: 540; loss: 0.63; acc: 0.78
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.86; acc: 0.73
Batch: 600; loss: 0.95; acc: 0.77
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.59; acc: 0.8
Batch: 680; loss: 0.64; acc: 0.83
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.93; acc: 0.69
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.74; acc: 0.78
Batch: 780; loss: 0.54; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.8
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.6; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.78
Batch: 120; loss: 0.66; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.6317463676640942; val_accuracy: 0.8016520700636943 

The current subspace-distance is: 0.00012493261601775885 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.98; acc: 0.75
Batch: 100; loss: 0.65; acc: 0.75
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.61; acc: 0.83
Batch: 180; loss: 0.65; acc: 0.8
Batch: 200; loss: 0.85; acc: 0.73
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.67; acc: 0.78
Batch: 260; loss: 1.11; acc: 0.7
Batch: 280; loss: 0.86; acc: 0.8
Batch: 300; loss: 0.92; acc: 0.69
Batch: 320; loss: 0.8; acc: 0.77
Batch: 340; loss: 1.03; acc: 0.64
Batch: 360; loss: 0.71; acc: 0.77
Batch: 380; loss: 0.62; acc: 0.81
Batch: 400; loss: 0.69; acc: 0.83
Batch: 420; loss: 0.83; acc: 0.77
Batch: 440; loss: 0.5; acc: 0.89
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.76; acc: 0.77
Batch: 500; loss: 0.85; acc: 0.75
Batch: 520; loss: 0.76; acc: 0.8
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.62; acc: 0.75
Batch: 600; loss: 0.56; acc: 0.8
Batch: 620; loss: 0.96; acc: 0.73
Batch: 640; loss: 0.87; acc: 0.75
Batch: 660; loss: 0.6; acc: 0.77
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.74; acc: 0.8
Batch: 740; loss: 0.65; acc: 0.77
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.8; acc: 0.8
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.74; acc: 0.75
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.6297186959511155; val_accuracy: 0.8007563694267515 

The current subspace-distance is: 0.00012555030116345733 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 26290
elements in E: 5553250
fraction nonzero: 0.004734164678341511
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.31; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.12
Batch: 160; loss: 2.29; acc: 0.06
Batch: 180; loss: 2.31; acc: 0.11
Batch: 200; loss: 2.3; acc: 0.08
Batch: 220; loss: 2.28; acc: 0.17
Batch: 240; loss: 2.28; acc: 0.09
Batch: 260; loss: 2.28; acc: 0.16
Batch: 280; loss: 2.27; acc: 0.23
Batch: 300; loss: 2.26; acc: 0.27
Batch: 320; loss: 2.28; acc: 0.12
Batch: 340; loss: 2.26; acc: 0.36
Batch: 360; loss: 2.26; acc: 0.22
Batch: 380; loss: 2.27; acc: 0.23
Batch: 400; loss: 2.27; acc: 0.3
Batch: 420; loss: 2.26; acc: 0.2
Batch: 440; loss: 2.25; acc: 0.3
Batch: 460; loss: 2.25; acc: 0.36
Batch: 480; loss: 2.22; acc: 0.36
Batch: 500; loss: 2.22; acc: 0.3
Batch: 520; loss: 2.2; acc: 0.34
Batch: 540; loss: 2.19; acc: 0.38
Batch: 560; loss: 2.15; acc: 0.36
Batch: 580; loss: 2.17; acc: 0.31
Batch: 600; loss: 2.03; acc: 0.39
Batch: 620; loss: 2.01; acc: 0.42
Batch: 640; loss: 1.94; acc: 0.38
Batch: 660; loss: 1.81; acc: 0.42
Batch: 680; loss: 1.76; acc: 0.38
Batch: 700; loss: 1.7; acc: 0.41
Batch: 720; loss: 1.32; acc: 0.61
Batch: 740; loss: 1.39; acc: 0.45
Batch: 760; loss: 1.06; acc: 0.72
Batch: 780; loss: 1.11; acc: 0.59
Train Epoch over. train_loss: 2.1; train_accuracy: 0.27 

Batch: 0; loss: 1.09; acc: 0.64
Batch: 20; loss: 1.03; acc: 0.67
Batch: 40; loss: 0.9; acc: 0.73
Batch: 60; loss: 0.96; acc: 0.64
Batch: 80; loss: 1.0; acc: 0.62
Batch: 100; loss: 1.0; acc: 0.64
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.94; acc: 0.67
Val Epoch over. val_loss: 1.118565880189276; val_accuracy: 0.6171377388535032 

The current subspace-distance is: 8.93970991455717e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.94; acc: 0.67
Batch: 20; loss: 0.99; acc: 0.62
Batch: 40; loss: 1.24; acc: 0.56
Batch: 60; loss: 0.81; acc: 0.7
Batch: 80; loss: 0.92; acc: 0.66
Batch: 100; loss: 0.99; acc: 0.55
Batch: 120; loss: 0.9; acc: 0.69
Batch: 140; loss: 1.2; acc: 0.59
Batch: 160; loss: 0.73; acc: 0.78
Batch: 180; loss: 1.15; acc: 0.67
Batch: 200; loss: 0.83; acc: 0.72
Batch: 220; loss: 0.81; acc: 0.77
Batch: 240; loss: 1.25; acc: 0.66
Batch: 260; loss: 1.13; acc: 0.61
Batch: 280; loss: 0.83; acc: 0.72
Batch: 300; loss: 0.73; acc: 0.73
Batch: 320; loss: 0.96; acc: 0.67
Batch: 340; loss: 0.68; acc: 0.77
Batch: 360; loss: 0.99; acc: 0.64
Batch: 380; loss: 0.93; acc: 0.67
Batch: 400; loss: 0.73; acc: 0.78
Batch: 420; loss: 0.62; acc: 0.81
Batch: 440; loss: 0.82; acc: 0.72
Batch: 460; loss: 0.98; acc: 0.69
Batch: 480; loss: 0.9; acc: 0.7
Batch: 500; loss: 0.85; acc: 0.77
Batch: 520; loss: 0.92; acc: 0.77
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 1.16; acc: 0.58
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.84; acc: 0.72
Batch: 620; loss: 0.92; acc: 0.7
Batch: 640; loss: 0.83; acc: 0.78
Batch: 660; loss: 0.84; acc: 0.73
Batch: 680; loss: 0.88; acc: 0.7
Batch: 700; loss: 0.89; acc: 0.69
Batch: 720; loss: 1.02; acc: 0.66
Batch: 740; loss: 0.67; acc: 0.78
Batch: 760; loss: 0.88; acc: 0.7
Batch: 780; loss: 0.95; acc: 0.61
Train Epoch over. train_loss: 0.85; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.67
Batch: 20; loss: 0.8; acc: 0.77
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.75; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.28; acc: 0.95
Val Epoch over. val_loss: 0.7066853547551829; val_accuracy: 0.7696058917197452 

The current subspace-distance is: 1.935111140483059e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.82; acc: 0.7
Batch: 20; loss: 0.9; acc: 0.72
Batch: 40; loss: 0.76; acc: 0.8
Batch: 60; loss: 0.68; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.77
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 0.76; acc: 0.69
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.62; acc: 0.78
Batch: 180; loss: 0.78; acc: 0.7
Batch: 200; loss: 0.77; acc: 0.78
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.72; acc: 0.77
Batch: 260; loss: 0.68; acc: 0.77
Batch: 280; loss: 0.52; acc: 0.83
Batch: 300; loss: 0.58; acc: 0.73
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.81; acc: 0.78
Batch: 400; loss: 0.69; acc: 0.73
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.85; acc: 0.69
Batch: 480; loss: 0.75; acc: 0.72
Batch: 500; loss: 0.86; acc: 0.8
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.7; acc: 0.77
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.78; acc: 0.75
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.67; acc: 0.75
Batch: 660; loss: 0.96; acc: 0.69
Batch: 680; loss: 0.75; acc: 0.7
Batch: 700; loss: 0.48; acc: 0.83
Batch: 720; loss: 0.63; acc: 0.78
Batch: 740; loss: 0.76; acc: 0.75
Batch: 760; loss: 0.61; acc: 0.78
Batch: 780; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 0.73; train_accuracy: 0.76 

Batch: 0; loss: 0.8; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.66
Batch: 40; loss: 0.57; acc: 0.78
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.75
Batch: 140; loss: 0.36; acc: 0.89
Val Epoch over. val_loss: 0.7782949940034538; val_accuracy: 0.7495023885350318 

The current subspace-distance is: 2.580446016509086e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.73; acc: 0.75
Batch: 60; loss: 0.66; acc: 0.72
Batch: 80; loss: 0.54; acc: 0.8
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.41; acc: 0.92
Batch: 160; loss: 0.6; acc: 0.8
Batch: 180; loss: 0.54; acc: 0.8
Batch: 200; loss: 0.86; acc: 0.7
Batch: 220; loss: 0.67; acc: 0.78
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 1.07; acc: 0.73
Batch: 300; loss: 0.63; acc: 0.81
Batch: 320; loss: 0.54; acc: 0.8
Batch: 340; loss: 0.92; acc: 0.75
Batch: 360; loss: 0.48; acc: 0.8
Batch: 380; loss: 0.64; acc: 0.8
Batch: 400; loss: 0.54; acc: 0.81
Batch: 420; loss: 0.52; acc: 0.78
Batch: 440; loss: 0.67; acc: 0.81
Batch: 460; loss: 0.75; acc: 0.69
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 0.76; acc: 0.75
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.84; acc: 0.73
Batch: 640; loss: 0.82; acc: 0.78
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.67; acc: 0.75
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.68; train_accuracy: 0.78 

Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 0.97; acc: 0.67
Batch: 40; loss: 0.64; acc: 0.75
Batch: 60; loss: 1.12; acc: 0.7
Batch: 80; loss: 0.69; acc: 0.75
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.98; acc: 0.72
Batch: 140; loss: 0.44; acc: 0.84
Val Epoch over. val_loss: 0.8405412980325663; val_accuracy: 0.7159633757961783 

The current subspace-distance is: 3.0840052204439417e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.24; acc: 0.67
Batch: 20; loss: 0.53; acc: 0.81
Batch: 40; loss: 0.75; acc: 0.73
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.63; acc: 0.83
Batch: 100; loss: 0.76; acc: 0.75
Batch: 120; loss: 0.99; acc: 0.7
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.63; acc: 0.78
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.78; acc: 0.75
Batch: 240; loss: 0.64; acc: 0.77
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.77
Batch: 340; loss: 0.66; acc: 0.8
Batch: 360; loss: 0.59; acc: 0.77
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.56; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.78
Batch: 440; loss: 1.13; acc: 0.62
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.68; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.8
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.5; acc: 0.8
Batch: 560; loss: 0.75; acc: 0.75
Batch: 580; loss: 0.78; acc: 0.78
Batch: 600; loss: 0.54; acc: 0.77
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.77; acc: 0.78
Batch: 660; loss: 0.83; acc: 0.8
Batch: 680; loss: 0.84; acc: 0.77
Batch: 700; loss: 0.66; acc: 0.77
Batch: 720; loss: 1.0; acc: 0.66
Batch: 740; loss: 0.47; acc: 0.83
Batch: 760; loss: 0.49; acc: 0.88
Batch: 780; loss: 0.72; acc: 0.72
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 1.27; acc: 0.64
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.6; acc: 0.78
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 1.08; acc: 0.72
Batch: 140; loss: 0.45; acc: 0.84
Val Epoch over. val_loss: 0.7712970546856048; val_accuracy: 0.7584593949044586 

The current subspace-distance is: 3.8195586967049167e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.19; acc: 0.75
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.83; acc: 0.72
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.72
Batch: 120; loss: 0.88; acc: 0.73
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.67; acc: 0.81
Batch: 180; loss: 0.57; acc: 0.81
Batch: 200; loss: 0.33; acc: 0.86
Batch: 220; loss: 0.53; acc: 0.84
Batch: 240; loss: 0.64; acc: 0.78
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.77; acc: 0.77
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.54; acc: 0.78
Batch: 380; loss: 0.48; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.83
Batch: 420; loss: 0.85; acc: 0.84
Batch: 440; loss: 0.7; acc: 0.83
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.72
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.53; acc: 0.88
Batch: 540; loss: 0.92; acc: 0.69
Batch: 560; loss: 0.84; acc: 0.81
Batch: 580; loss: 0.9; acc: 0.72
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.7; acc: 0.77
Batch: 640; loss: 0.76; acc: 0.77
Batch: 660; loss: 0.66; acc: 0.78
Batch: 680; loss: 0.62; acc: 0.78
Batch: 700; loss: 0.67; acc: 0.77
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.73; acc: 0.77
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.57; acc: 0.81
Train Epoch over. train_loss: 0.62; train_accuracy: 0.81 

Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.77; acc: 0.69
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.73; acc: 0.8
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.9; acc: 0.69
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.5631904724487073; val_accuracy: 0.8229498407643312 

The current subspace-distance is: 4.176617585471831e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.75; acc: 0.81
Batch: 20; loss: 0.63; acc: 0.78
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.77
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.61; acc: 0.84
Batch: 160; loss: 0.57; acc: 0.84
Batch: 180; loss: 0.82; acc: 0.75
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.75; acc: 0.73
Batch: 260; loss: 0.4; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.84
Batch: 300; loss: 0.59; acc: 0.83
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.73; acc: 0.75
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.78; acc: 0.73
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.82; acc: 0.77
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.88; acc: 0.73
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.64; acc: 0.77
Batch: 560; loss: 0.63; acc: 0.77
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.51; acc: 0.8
Batch: 660; loss: 0.62; acc: 0.78
Batch: 680; loss: 0.72; acc: 0.78
Batch: 700; loss: 0.72; acc: 0.8
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.62; acc: 0.86
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.78
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.89
Val Epoch over. val_loss: 0.5192356562348688; val_accuracy: 0.8434514331210191 

The current subspace-distance is: 4.54670334875118e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 0.83; acc: 0.72
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.77
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.84
Batch: 180; loss: 0.77; acc: 0.72
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.61; acc: 0.78
Batch: 240; loss: 0.52; acc: 0.8
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.57; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.81
Batch: 320; loss: 0.59; acc: 0.8
Batch: 340; loss: 0.61; acc: 0.84
Batch: 360; loss: 0.52; acc: 0.81
Batch: 380; loss: 0.78; acc: 0.77
Batch: 400; loss: 0.66; acc: 0.77
Batch: 420; loss: 0.61; acc: 0.75
Batch: 440; loss: 0.53; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.73
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.77
Batch: 520; loss: 0.63; acc: 0.78
Batch: 540; loss: 0.87; acc: 0.77
Batch: 560; loss: 0.59; acc: 0.73
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.8
Batch: 660; loss: 0.62; acc: 0.86
Batch: 680; loss: 0.59; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.75
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.74; acc: 0.78
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.65; acc: 0.75
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.75
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.7
Batch: 140; loss: 0.44; acc: 0.83
Val Epoch over. val_loss: 0.7800804456328131; val_accuracy: 0.7573646496815286 

The current subspace-distance is: 4.807053483091295e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.01; acc: 0.73
Batch: 20; loss: 0.74; acc: 0.8
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.86
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.75; acc: 0.8
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.75
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.83
Batch: 280; loss: 0.63; acc: 0.81
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.73; acc: 0.69
Batch: 340; loss: 0.75; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.67; acc: 0.8
Batch: 420; loss: 0.94; acc: 0.8
Batch: 440; loss: 0.57; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.8
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.52; acc: 0.84
Batch: 520; loss: 0.76; acc: 0.77
Batch: 540; loss: 0.83; acc: 0.78
Batch: 560; loss: 0.76; acc: 0.75
Batch: 580; loss: 0.57; acc: 0.8
Batch: 600; loss: 0.43; acc: 0.83
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.6; acc: 0.8
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.57; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.88
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.77
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.8
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.5169748910673105; val_accuracy: 0.8407643312101911 

The current subspace-distance is: 5.1468821766320616e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.86; acc: 0.67
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.45; acc: 0.81
Batch: 180; loss: 0.88; acc: 0.7
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.7; acc: 0.78
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.55; acc: 0.83
Batch: 320; loss: 0.82; acc: 0.84
Batch: 340; loss: 0.84; acc: 0.72
Batch: 360; loss: 0.78; acc: 0.81
Batch: 380; loss: 0.38; acc: 0.89
Batch: 400; loss: 0.57; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.81
Batch: 440; loss: 0.77; acc: 0.75
Batch: 460; loss: 0.33; acc: 0.94
Batch: 480; loss: 0.83; acc: 0.75
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 1.01; acc: 0.7
Batch: 540; loss: 0.48; acc: 0.78
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.91; acc: 0.7
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.7; acc: 0.83
Batch: 680; loss: 0.84; acc: 0.77
Batch: 700; loss: 0.51; acc: 0.81
Batch: 720; loss: 0.42; acc: 0.84
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.44; acc: 0.86
Batch: 780; loss: 0.29; acc: 0.94
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.67; acc: 0.77
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.38; acc: 0.81
Val Epoch over. val_loss: 0.6074257979916918; val_accuracy: 0.8124004777070064 

The current subspace-distance is: 5.417860666057095e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.93; acc: 0.75
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.67; acc: 0.86
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.91
Batch: 280; loss: 0.64; acc: 0.86
Batch: 300; loss: 0.66; acc: 0.86
Batch: 320; loss: 0.72; acc: 0.8
Batch: 340; loss: 0.68; acc: 0.77
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.6; acc: 0.81
Batch: 420; loss: 0.99; acc: 0.67
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.83
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.78
Batch: 600; loss: 0.56; acc: 0.84
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.8
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.57; acc: 0.84
Batch: 700; loss: 0.83; acc: 0.75
Batch: 720; loss: 0.69; acc: 0.81
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.73
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.72
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.507459648143334; val_accuracy: 0.8440485668789809 

The current subspace-distance is: 5.6499484344385564e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.57; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.74; acc: 0.78
Batch: 160; loss: 0.49; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.64; acc: 0.81
Batch: 220; loss: 0.66; acc: 0.8
Batch: 240; loss: 0.66; acc: 0.75
Batch: 260; loss: 0.57; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.34; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.81
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.66; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.66; acc: 0.84
Batch: 520; loss: 0.65; acc: 0.78
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.95; acc: 0.73
Batch: 580; loss: 0.75; acc: 0.8
Batch: 600; loss: 0.42; acc: 0.83
Batch: 620; loss: 0.61; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.51; acc: 0.88
Batch: 700; loss: 0.5; acc: 0.83
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.66; acc: 0.73
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.52; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 0.31; acc: 0.86
Val Epoch over. val_loss: 0.5106836891003476; val_accuracy: 0.8407643312101911 

The current subspace-distance is: 6.065192792448215e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.84
Batch: 20; loss: 0.77; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.58; acc: 0.83
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.53; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.67; acc: 0.75
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.46; acc: 0.88
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.35; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.78
Batch: 480; loss: 0.61; acc: 0.77
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.59; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.83
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.48; acc: 0.81
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.68; acc: 0.8
Batch: 740; loss: 0.65; acc: 0.8
Batch: 760; loss: 0.81; acc: 0.72
Batch: 780; loss: 0.71; acc: 0.81
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.73
Batch: 140; loss: 0.32; acc: 0.89
Val Epoch over. val_loss: 0.5276590003898949; val_accuracy: 0.839968152866242 

The current subspace-distance is: 6.39629943179898e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.65; acc: 0.78
Batch: 100; loss: 0.71; acc: 0.75
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.89; acc: 0.77
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.58; acc: 0.78
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.47; acc: 0.81
Batch: 280; loss: 0.35; acc: 0.91
Batch: 300; loss: 0.64; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.57; acc: 0.77
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.75; acc: 0.78
Batch: 440; loss: 0.44; acc: 0.81
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.69; acc: 0.77
Batch: 540; loss: 0.46; acc: 0.83
Batch: 560; loss: 0.48; acc: 0.88
Batch: 580; loss: 0.48; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.81
Batch: 620; loss: 0.37; acc: 0.88
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.84
Batch: 680; loss: 0.7; acc: 0.75
Batch: 700; loss: 0.7; acc: 0.8
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.97
Batch: 60; loss: 0.55; acc: 0.78
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.73
Batch: 140; loss: 0.3; acc: 0.86
Val Epoch over. val_loss: 0.5038718067726512; val_accuracy: 0.84484474522293 

The current subspace-distance is: 6.68689317535609e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.6; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.55; acc: 0.8
Batch: 100; loss: 0.64; acc: 0.75
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.79; acc: 0.73
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.69; acc: 0.8
Batch: 220; loss: 0.55; acc: 0.78
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.89
Batch: 280; loss: 0.46; acc: 0.88
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.4; acc: 0.84
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.55; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.64; acc: 0.78
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.56; acc: 0.84
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.62; acc: 0.81
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.45; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.75; acc: 0.83
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.7
Batch: 780; loss: 0.61; acc: 0.78
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.7
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.49973928411105634; val_accuracy: 0.8487261146496815 

The current subspace-distance is: 6.891277007525787e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.54; acc: 0.78
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.61; acc: 0.78
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.67; acc: 0.75
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.62; acc: 0.78
Batch: 460; loss: 0.87; acc: 0.77
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.64; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.78
Batch: 600; loss: 0.71; acc: 0.8
Batch: 620; loss: 0.48; acc: 0.8
Batch: 640; loss: 0.6; acc: 0.83
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.78
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.44; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.4961424605671767; val_accuracy: 0.846437101910828 

The current subspace-distance is: 6.986773223616183e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.64; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.75
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.55; acc: 0.8
Batch: 180; loss: 0.82; acc: 0.73
Batch: 200; loss: 0.53; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.8
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.58; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.88
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.83
Batch: 460; loss: 0.75; acc: 0.8
Batch: 480; loss: 0.45; acc: 0.81
Batch: 500; loss: 0.61; acc: 0.75
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.53; acc: 0.81
Batch: 560; loss: 0.43; acc: 0.84
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.68; acc: 0.78
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.83
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.6; acc: 0.81
Batch: 780; loss: 0.46; acc: 0.81
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.4896409083988256; val_accuracy: 0.8522093949044586 

The current subspace-distance is: 7.283285958692431e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.81
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.45; acc: 0.83
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.74; acc: 0.75
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.38; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.52; acc: 0.83
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.73; acc: 0.83
Batch: 320; loss: 0.81; acc: 0.73
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.46; acc: 0.83
Batch: 420; loss: 0.93; acc: 0.77
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.62; acc: 0.83
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.67; acc: 0.8
Batch: 560; loss: 0.59; acc: 0.78
Batch: 580; loss: 0.59; acc: 0.81
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.41; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.77
Batch: 740; loss: 0.76; acc: 0.78
Batch: 760; loss: 0.4; acc: 0.86
Batch: 780; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.72
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.48025936220481896; val_accuracy: 0.8506170382165605 

The current subspace-distance is: 7.452225690940395e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.59; acc: 0.84
Batch: 60; loss: 0.49; acc: 0.91
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.55; acc: 0.81
Batch: 160; loss: 0.38; acc: 0.84
Batch: 180; loss: 0.51; acc: 0.83
Batch: 200; loss: 0.6; acc: 0.81
Batch: 220; loss: 0.44; acc: 0.89
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.8
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.61; acc: 0.83
Batch: 360; loss: 0.49; acc: 0.86
Batch: 380; loss: 0.76; acc: 0.69
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.51; acc: 0.89
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.51; acc: 0.83
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.62; acc: 0.78
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.66; acc: 0.84
Batch: 580; loss: 0.65; acc: 0.8
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.37; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.59; acc: 0.83
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.71; acc: 0.73
Batch: 140; loss: 0.27; acc: 0.88
Val Epoch over. val_loss: 0.4903503346025564; val_accuracy: 0.846437101910828 

The current subspace-distance is: 7.604670099681243e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.86
Batch: 60; loss: 0.43; acc: 0.84
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.61; acc: 0.78
Batch: 160; loss: 0.57; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.8
Batch: 240; loss: 0.72; acc: 0.81
Batch: 260; loss: 0.67; acc: 0.86
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.67; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.44; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.33; acc: 0.84
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.72; acc: 0.84
Batch: 460; loss: 0.7; acc: 0.8
Batch: 480; loss: 0.6; acc: 0.78
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.51; acc: 0.8
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.84
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.88
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.6; acc: 0.78
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.76; acc: 0.73
Batch: 780; loss: 0.67; acc: 0.72
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.48; acc: 0.8
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 0.27; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.34; acc: 0.88
Val Epoch over. val_loss: 0.5441362612946018; val_accuracy: 0.824343152866242 

The current subspace-distance is: 7.743593596387655e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.74; acc: 0.81
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.88
Batch: 140; loss: 0.63; acc: 0.81
Batch: 160; loss: 0.58; acc: 0.81
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.62; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.63; acc: 0.81
Batch: 360; loss: 0.61; acc: 0.73
Batch: 380; loss: 0.67; acc: 0.78
Batch: 400; loss: 0.57; acc: 0.83
Batch: 420; loss: 0.4; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.81
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.5; acc: 0.88
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.66; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.75
Batch: 600; loss: 0.6; acc: 0.78
Batch: 620; loss: 0.36; acc: 0.84
Batch: 640; loss: 0.53; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.49; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.81
Batch: 780; loss: 0.55; acc: 0.78
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.72
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.48541659050306696; val_accuracy: 0.8481289808917197 

The current subspace-distance is: 8.010524470591918e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 0.57; acc: 0.77
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.86
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.8; acc: 0.77
Batch: 160; loss: 0.62; acc: 0.86
Batch: 180; loss: 0.46; acc: 0.78
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.62; acc: 0.78
Batch: 240; loss: 0.79; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 0.46; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.62; acc: 0.83
Batch: 360; loss: 0.62; acc: 0.81
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.75; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.52; acc: 0.86
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.61; acc: 0.81
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.78; acc: 0.8
Batch: 760; loss: 0.51; acc: 0.8
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.7
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.48132418793667653; val_accuracy: 0.8508160828025477 

The current subspace-distance is: 8.115886157611385e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.74; acc: 0.84
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.7; acc: 0.78
Batch: 120; loss: 0.58; acc: 0.77
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.8
Batch: 200; loss: 0.72; acc: 0.8
Batch: 220; loss: 0.51; acc: 0.8
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.89
Batch: 280; loss: 0.58; acc: 0.83
Batch: 300; loss: 0.96; acc: 0.75
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.6; acc: 0.84
Batch: 360; loss: 0.61; acc: 0.8
Batch: 380; loss: 0.5; acc: 0.81
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 0.7; acc: 0.77
Batch: 480; loss: 0.51; acc: 0.8
Batch: 500; loss: 0.58; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.7; acc: 0.81
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.21; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.61; acc: 0.78
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.58; acc: 0.81
Batch: 700; loss: 0.51; acc: 0.83
Batch: 720; loss: 0.7; acc: 0.77
Batch: 740; loss: 0.63; acc: 0.77
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.4731957901530205; val_accuracy: 0.8541003184713376 

The current subspace-distance is: 8.256105502368882e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.6; acc: 0.78
Batch: 180; loss: 0.66; acc: 0.75
Batch: 200; loss: 0.6; acc: 0.89
Batch: 220; loss: 0.7; acc: 0.78
Batch: 240; loss: 0.68; acc: 0.78
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.61; acc: 0.84
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.88
Batch: 360; loss: 0.67; acc: 0.8
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.66; acc: 0.77
Batch: 420; loss: 0.53; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.61; acc: 0.81
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.54; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.54; acc: 0.83
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.57; acc: 0.8
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.73; acc: 0.7
Batch: 720; loss: 0.88; acc: 0.77
Batch: 740; loss: 0.4; acc: 0.83
Batch: 760; loss: 0.61; acc: 0.77
Batch: 780; loss: 0.6; acc: 0.8
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.72; acc: 0.72
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.4739677233585886; val_accuracy: 0.853702229299363 

The current subspace-distance is: 8.507896563969553e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.65; acc: 0.8
Batch: 160; loss: 0.54; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.81
Batch: 200; loss: 0.71; acc: 0.8
Batch: 220; loss: 0.67; acc: 0.78
Batch: 240; loss: 0.76; acc: 0.78
Batch: 260; loss: 0.55; acc: 0.83
Batch: 280; loss: 0.71; acc: 0.8
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.96; acc: 0.75
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.7; acc: 0.8
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.59; acc: 0.81
Batch: 480; loss: 0.44; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.8
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.83
Batch: 720; loss: 0.69; acc: 0.8
Batch: 740; loss: 0.61; acc: 0.81
Batch: 760; loss: 0.61; acc: 0.78
Batch: 780; loss: 0.49; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.73
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.47704370519158185; val_accuracy: 0.8532046178343949 

The current subspace-distance is: 8.600205183029175e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.63; acc: 0.81
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.92
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.78
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.81; acc: 0.73
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.71; acc: 0.83
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.86
Batch: 360; loss: 0.62; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.59; acc: 0.84
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.55; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.8
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.86
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.83
Batch: 680; loss: 0.28; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.58; acc: 0.84
Batch: 740; loss: 0.84; acc: 0.77
Batch: 760; loss: 0.59; acc: 0.83
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4721504762578922; val_accuracy: 0.8553941082802548 

The current subspace-distance is: 8.759502088651061e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.67; acc: 0.78
Batch: 40; loss: 0.51; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.53; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.61; acc: 0.77
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.78; acc: 0.77
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.77
Batch: 340; loss: 0.51; acc: 0.81
Batch: 360; loss: 0.5; acc: 0.8
Batch: 380; loss: 0.61; acc: 0.83
Batch: 400; loss: 0.77; acc: 0.8
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.8
Batch: 460; loss: 1.05; acc: 0.78
Batch: 480; loss: 0.69; acc: 0.78
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.65; acc: 0.86
Batch: 540; loss: 0.41; acc: 0.83
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.6; acc: 0.78
Batch: 600; loss: 0.52; acc: 0.86
Batch: 620; loss: 0.52; acc: 0.83
Batch: 640; loss: 0.76; acc: 0.77
Batch: 660; loss: 0.59; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.65; acc: 0.8
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.23; acc: 0.97
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.26; acc: 0.92
Val Epoch over. val_loss: 0.47196179286689516; val_accuracy: 0.8545979299363057 

The current subspace-distance is: 8.874720515450463e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.79; acc: 0.78
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.8
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.35; acc: 0.84
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.68; acc: 0.8
Batch: 280; loss: 0.69; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.8
Batch: 320; loss: 1.09; acc: 0.7
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.62; acc: 0.83
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.81
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.7; acc: 0.8
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.79; acc: 0.7
Batch: 580; loss: 0.72; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.39; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.84
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.4776459143135198; val_accuracy: 0.8516122611464968 

The current subspace-distance is: 9.119308379013091e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.84
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.81
Batch: 160; loss: 0.62; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.73; acc: 0.83
Batch: 300; loss: 0.58; acc: 0.83
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.51; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.81
Batch: 460; loss: 0.4; acc: 0.83
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.53; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.8
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.69; acc: 0.75
Batch: 620; loss: 0.61; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.6; acc: 0.77
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.66; acc: 0.77
Batch: 780; loss: 0.71; acc: 0.77
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.42; acc: 0.89
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.91
Val Epoch over. val_loss: 0.4771281904096057; val_accuracy: 0.8532046178343949 

The current subspace-distance is: 9.315412899013609e-05 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.84; acc: 0.75
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.8
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.7; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.69; acc: 0.77
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.34; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.43; acc: 0.83
Batch: 560; loss: 0.62; acc: 0.83
Batch: 580; loss: 0.64; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.89
Batch: 620; loss: 0.74; acc: 0.78
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.51; acc: 0.89
Batch: 760; loss: 0.77; acc: 0.81
Batch: 780; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4793486907413811; val_accuracy: 0.8492237261146497 

The current subspace-distance is: 9.506961214356124e-05 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.71; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.72
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.57; acc: 0.81
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.61; acc: 0.83
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.61; acc: 0.78
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.8
Batch: 440; loss: 0.57; acc: 0.8
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.43; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.56; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.8
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.8; acc: 0.78
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.56; acc: 0.84
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.62; acc: 0.75
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.48; acc: 0.8
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.47002995166049644; val_accuracy: 0.8544984076433121 

The current subspace-distance is: 9.634286107029766e-05 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.91
Batch: 20; loss: 0.6; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.63; acc: 0.84
Batch: 140; loss: 0.57; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.6; acc: 0.81
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.45; acc: 0.8
Batch: 300; loss: 0.58; acc: 0.81
Batch: 320; loss: 0.53; acc: 0.81
Batch: 340; loss: 0.61; acc: 0.81
Batch: 360; loss: 0.53; acc: 0.89
Batch: 380; loss: 0.68; acc: 0.84
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.76; acc: 0.78
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.65; acc: 0.8
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.45; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.81
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.81
Batch: 700; loss: 0.72; acc: 0.83
Batch: 720; loss: 0.67; acc: 0.81
Batch: 740; loss: 0.41; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.84
Batch: 780; loss: 0.68; acc: 0.77
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.4701120200430512; val_accuracy: 0.8541998407643312 

The current subspace-distance is: 9.777913510333747e-05 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.31; acc: 0.86
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 1.08; acc: 0.75
Batch: 220; loss: 0.62; acc: 0.8
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.49; acc: 0.84
Batch: 280; loss: 0.51; acc: 0.81
Batch: 300; loss: 0.43; acc: 0.83
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.89; acc: 0.7
Batch: 380; loss: 0.45; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.8
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.59; acc: 0.84
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.96; acc: 0.77
Batch: 540; loss: 0.64; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.81
Batch: 600; loss: 0.56; acc: 0.83
Batch: 620; loss: 0.63; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.41; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4723800348628099; val_accuracy: 0.8519108280254777 

The current subspace-distance is: 0.00010019644832937047 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.72
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.75; acc: 0.72
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.77
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.9; acc: 0.77
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.52; acc: 0.81
Batch: 220; loss: 0.58; acc: 0.8
Batch: 240; loss: 0.56; acc: 0.84
Batch: 260; loss: 0.67; acc: 0.8
Batch: 280; loss: 1.0; acc: 0.72
Batch: 300; loss: 0.46; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.81
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.69; acc: 0.84
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.63; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.81
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.77
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.83
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.77; acc: 0.83
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.51; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.8
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.47157903063069484; val_accuracy: 0.8533041401273885 

The current subspace-distance is: 0.00010155759810004383 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.64; acc: 0.84
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.78; acc: 0.7
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.48; acc: 0.83
Batch: 220; loss: 0.61; acc: 0.8
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.8
Batch: 280; loss: 0.48; acc: 0.86
Batch: 300; loss: 0.56; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.83
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.51; acc: 0.86
Batch: 460; loss: 0.42; acc: 0.86
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.91
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.53; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.78
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.54; acc: 0.84
Batch: 700; loss: 0.6; acc: 0.78
Batch: 720; loss: 0.84; acc: 0.78
Batch: 740; loss: 0.55; acc: 0.84
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.47120378489137454; val_accuracy: 0.8524084394904459 

The current subspace-distance is: 0.00010201623081229627 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.63; acc: 0.81
Batch: 180; loss: 0.72; acc: 0.72
Batch: 200; loss: 0.49; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.8
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.65; acc: 0.81
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.68; acc: 0.78
Batch: 360; loss: 0.6; acc: 0.81
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.65; acc: 0.77
Batch: 420; loss: 0.55; acc: 0.81
Batch: 440; loss: 0.77; acc: 0.77
Batch: 460; loss: 0.8; acc: 0.78
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.53; acc: 0.86
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.5; acc: 0.78
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.56; acc: 0.84
Batch: 640; loss: 0.51; acc: 0.84
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.65; acc: 0.81
Batch: 720; loss: 0.35; acc: 0.94
Batch: 740; loss: 0.75; acc: 0.75
Batch: 760; loss: 0.46; acc: 0.83
Batch: 780; loss: 0.55; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.46907931585220775; val_accuracy: 0.855593152866242 

The current subspace-distance is: 0.00010331957309972495 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.54; acc: 0.8
Batch: 100; loss: 0.44; acc: 0.84
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.89
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.78
Batch: 260; loss: 0.55; acc: 0.81
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.82; acc: 0.78
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.56; acc: 0.81
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.59; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.59; acc: 0.83
Batch: 480; loss: 0.5; acc: 0.81
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.72; acc: 0.78
Batch: 540; loss: 0.74; acc: 0.83
Batch: 560; loss: 0.64; acc: 0.77
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.71; acc: 0.75
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.73; acc: 0.78
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.51; acc: 0.86
Batch: 760; loss: 0.65; acc: 0.78
Batch: 780; loss: 0.42; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.73
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4733175508155944; val_accuracy: 0.8527070063694268 

The current subspace-distance is: 0.00010628077143337578 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.49; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.71; acc: 0.8
Batch: 160; loss: 0.42; acc: 0.83
Batch: 180; loss: 0.55; acc: 0.8
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.47; acc: 0.81
Batch: 280; loss: 0.89; acc: 0.78
Batch: 300; loss: 0.84; acc: 0.75
Batch: 320; loss: 0.56; acc: 0.81
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.59; acc: 0.8
Batch: 400; loss: 0.37; acc: 0.86
Batch: 420; loss: 0.71; acc: 0.83
Batch: 440; loss: 0.39; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.8
Batch: 520; loss: 0.43; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.59; acc: 0.77
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.69; acc: 0.8
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.26; acc: 0.92
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.73
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.47161999297369817; val_accuracy: 0.8536027070063694 

The current subspace-distance is: 0.00010684437438612804 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.89; acc: 0.78
Batch: 100; loss: 0.79; acc: 0.72
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.91
Batch: 160; loss: 0.69; acc: 0.78
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.47; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.83
Batch: 240; loss: 0.59; acc: 0.81
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.46; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.67; acc: 0.8
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.66; acc: 0.78
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.75; acc: 0.78
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.8
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.81
Batch: 620; loss: 0.55; acc: 0.8
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.67; acc: 0.81
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.61; acc: 0.78
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.4703652146420661; val_accuracy: 0.854796974522293 

The current subspace-distance is: 0.00010790899250423536 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.72; acc: 0.75
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.92
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.42; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.88
Batch: 260; loss: 0.77; acc: 0.77
Batch: 280; loss: 0.7; acc: 0.81
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.73; acc: 0.75
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.78; acc: 0.75
Batch: 460; loss: 0.44; acc: 0.8
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.65; acc: 0.86
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.3; acc: 0.88
Batch: 640; loss: 0.45; acc: 0.84
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.66; acc: 0.78
Batch: 700; loss: 0.52; acc: 0.81
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.81; acc: 0.64
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.47118910297656513; val_accuracy: 0.8550955414012739 

The current subspace-distance is: 0.00010944464156636968 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.56; acc: 0.83
Batch: 20; loss: 0.75; acc: 0.77
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.66; acc: 0.77
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.55; acc: 0.8
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.74; acc: 0.8
Batch: 240; loss: 0.75; acc: 0.8
Batch: 260; loss: 0.36; acc: 0.84
Batch: 280; loss: 0.49; acc: 0.81
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.48; acc: 0.8
Batch: 360; loss: 0.69; acc: 0.84
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.76; acc: 0.78
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.63; acc: 0.72
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.57; acc: 0.84
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.49; acc: 0.91
Batch: 620; loss: 0.6; acc: 0.77
Batch: 640; loss: 0.49; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.66; acc: 0.78
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4689340630819084; val_accuracy: 0.8551950636942676 

The current subspace-distance is: 0.00011091361375292763 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.31; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.52; acc: 0.83
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.81
Batch: 400; loss: 0.5; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.89; acc: 0.77
Batch: 500; loss: 0.5; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.75; acc: 0.73
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.39; acc: 0.83
Batch: 620; loss: 0.64; acc: 0.78
Batch: 640; loss: 0.72; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.78
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.64; acc: 0.83
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4693764466199146; val_accuracy: 0.8538017515923567 

The current subspace-distance is: 0.00011210054799448699 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.59; acc: 0.78
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.6; acc: 0.78
Batch: 200; loss: 0.78; acc: 0.73
Batch: 220; loss: 0.64; acc: 0.78
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.78
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.42; acc: 0.8
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.7; acc: 0.83
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.71; acc: 0.8
Batch: 440; loss: 0.56; acc: 0.8
Batch: 460; loss: 0.59; acc: 0.88
Batch: 480; loss: 0.61; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.7; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.84
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.46; acc: 0.81
Batch: 660; loss: 0.33; acc: 0.84
Batch: 680; loss: 0.63; acc: 0.81
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.64; acc: 0.77
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.469708713376598; val_accuracy: 0.8536027070063694 

The current subspace-distance is: 0.00011478579108370468 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.91
Batch: 20; loss: 0.68; acc: 0.73
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.57; acc: 0.78
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.88
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.68; acc: 0.77
Batch: 260; loss: 0.64; acc: 0.78
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.78
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.71; acc: 0.78
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.51; acc: 0.8
Batch: 460; loss: 0.61; acc: 0.75
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.64; acc: 0.75
Batch: 560; loss: 0.62; acc: 0.77
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.73; acc: 0.8
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.78
Batch: 660; loss: 0.7; acc: 0.8
Batch: 680; loss: 0.59; acc: 0.8
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.73; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.77; acc: 0.75
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.46928207429161495; val_accuracy: 0.8550955414012739 

The current subspace-distance is: 0.00011574992095120251 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.77
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.83
Batch: 200; loss: 0.52; acc: 0.92
Batch: 220; loss: 0.67; acc: 0.77
Batch: 240; loss: 0.62; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.87; acc: 0.78
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.44; acc: 0.81
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.84
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.6; acc: 0.81
Batch: 460; loss: 0.8; acc: 0.77
Batch: 480; loss: 0.51; acc: 0.88
Batch: 500; loss: 0.81; acc: 0.75
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.46; acc: 0.83
Batch: 620; loss: 0.33; acc: 0.84
Batch: 640; loss: 0.5; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.64; acc: 0.8
Batch: 700; loss: 0.77; acc: 0.77
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.54; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.81
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.469068577952066; val_accuracy: 0.8549960191082803 

The current subspace-distance is: 0.00011681218165904284 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.78
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.33; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.75
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.64; acc: 0.8
Batch: 180; loss: 0.52; acc: 0.86
Batch: 200; loss: 0.73; acc: 0.75
Batch: 220; loss: 0.68; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.59; acc: 0.86
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.47; acc: 0.88
Batch: 360; loss: 0.8; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.69; acc: 0.81
Batch: 460; loss: 0.46; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.44; acc: 0.89
Batch: 520; loss: 0.58; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.61; acc: 0.83
Batch: 580; loss: 0.86; acc: 0.77
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.52; acc: 0.88
Batch: 680; loss: 0.64; acc: 0.81
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.66; acc: 0.8
Batch: 740; loss: 0.63; acc: 0.81
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.69; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.468608650954286; val_accuracy: 0.8554936305732485 

The current subspace-distance is: 0.00011806652037193999 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.94
Batch: 160; loss: 0.69; acc: 0.77
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.86
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.6; acc: 0.84
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.5; acc: 0.78
Batch: 320; loss: 0.58; acc: 0.83
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.53; acc: 0.8
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.91
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.75; acc: 0.73
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.61; acc: 0.88
Batch: 560; loss: 0.75; acc: 0.73
Batch: 580; loss: 0.52; acc: 0.78
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.83
Batch: 680; loss: 0.7; acc: 0.75
Batch: 700; loss: 0.57; acc: 0.84
Batch: 720; loss: 0.61; acc: 0.78
Batch: 740; loss: 0.46; acc: 0.84
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.46878395618716623; val_accuracy: 0.854796974522293 

The current subspace-distance is: 0.00011852127499878407 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.82; acc: 0.77
Batch: 160; loss: 0.68; acc: 0.78
Batch: 180; loss: 0.84; acc: 0.8
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.81; acc: 0.73
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.74; acc: 0.77
Batch: 360; loss: 0.58; acc: 0.81
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.7; acc: 0.81
Batch: 440; loss: 0.44; acc: 0.83
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.44; acc: 0.83
Batch: 520; loss: 0.65; acc: 0.77
Batch: 540; loss: 0.83; acc: 0.77
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.57; acc: 0.83
Batch: 620; loss: 0.55; acc: 0.81
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.65; acc: 0.78
Batch: 720; loss: 0.56; acc: 0.81
Batch: 740; loss: 0.54; acc: 0.81
Batch: 760; loss: 0.55; acc: 0.83
Batch: 780; loss: 0.25; acc: 0.98
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.46885637573565647; val_accuracy: 0.8556926751592356 

The current subspace-distance is: 0.00011929164611501619 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.78
Batch: 40; loss: 0.46; acc: 0.89
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.81
Batch: 100; loss: 0.69; acc: 0.78
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.74; acc: 0.8
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.73; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.8
Batch: 220; loss: 0.72; acc: 0.7
Batch: 240; loss: 0.46; acc: 0.81
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.54; acc: 0.84
Batch: 320; loss: 0.68; acc: 0.78
Batch: 340; loss: 0.45; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.55; acc: 0.83
Batch: 420; loss: 0.54; acc: 0.78
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.6; acc: 0.86
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.56; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.75
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.6; acc: 0.77
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.83
Batch: 680; loss: 0.68; acc: 0.81
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.81
Batch: 740; loss: 0.72; acc: 0.81
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.46825596153925936; val_accuracy: 0.8562898089171974 

The current subspace-distance is: 0.00011974353401456028 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.75; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.83
Batch: 60; loss: 0.46; acc: 0.83
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.8
Batch: 320; loss: 0.67; acc: 0.83
Batch: 340; loss: 0.84; acc: 0.75
Batch: 360; loss: 0.52; acc: 0.8
Batch: 380; loss: 0.55; acc: 0.8
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.56; acc: 0.78
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.67; acc: 0.88
Batch: 520; loss: 0.56; acc: 0.81
Batch: 540; loss: 0.56; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.81
Batch: 580; loss: 0.37; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.89
Batch: 620; loss: 0.6; acc: 0.81
Batch: 640; loss: 0.65; acc: 0.77
Batch: 660; loss: 0.53; acc: 0.8
Batch: 680; loss: 0.35; acc: 0.84
Batch: 700; loss: 0.61; acc: 0.78
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.68; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.92
Val Epoch over. val_loss: 0.46851832253538117; val_accuracy: 0.855593152866242 

The current subspace-distance is: 0.00012152887211414054 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_125_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 31568
elements in E: 6663900
fraction nonzero: 0.004737165923858401
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.12
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.31; acc: 0.12
Batch: 200; loss: 2.29; acc: 0.06
Batch: 220; loss: 2.28; acc: 0.16
Batch: 240; loss: 2.28; acc: 0.11
Batch: 260; loss: 2.27; acc: 0.12
Batch: 280; loss: 2.28; acc: 0.11
Batch: 300; loss: 2.25; acc: 0.22
Batch: 320; loss: 2.29; acc: 0.08
Batch: 340; loss: 2.26; acc: 0.19
Batch: 360; loss: 2.24; acc: 0.19
Batch: 380; loss: 2.25; acc: 0.16
Batch: 400; loss: 2.25; acc: 0.12
Batch: 420; loss: 2.23; acc: 0.22
Batch: 440; loss: 2.22; acc: 0.28
Batch: 460; loss: 2.21; acc: 0.2
Batch: 480; loss: 2.2; acc: 0.31
Batch: 500; loss: 2.21; acc: 0.23
Batch: 520; loss: 2.17; acc: 0.34
Batch: 540; loss: 2.08; acc: 0.42
Batch: 560; loss: 2.07; acc: 0.44
Batch: 580; loss: 2.03; acc: 0.36
Batch: 600; loss: 1.92; acc: 0.39
Batch: 620; loss: 1.79; acc: 0.5
Batch: 640; loss: 1.71; acc: 0.53
Batch: 660; loss: 1.52; acc: 0.53
Batch: 680; loss: 1.44; acc: 0.55
Batch: 700; loss: 1.41; acc: 0.53
Batch: 720; loss: 1.07; acc: 0.75
Batch: 740; loss: 1.14; acc: 0.62
Batch: 760; loss: 0.95; acc: 0.67
Batch: 780; loss: 0.83; acc: 0.7
Train Epoch over. train_loss: 2.03; train_accuracy: 0.27 

Batch: 0; loss: 1.26; acc: 0.52
Batch: 20; loss: 1.22; acc: 0.58
Batch: 40; loss: 0.88; acc: 0.67
Batch: 60; loss: 1.16; acc: 0.64
Batch: 80; loss: 0.94; acc: 0.64
Batch: 100; loss: 1.2; acc: 0.62
Batch: 120; loss: 1.11; acc: 0.53
Batch: 140; loss: 0.93; acc: 0.64
Val Epoch over. val_loss: 1.1402197635857163; val_accuracy: 0.6134554140127388 

The current subspace-distance is: 8.910090400604531e-06 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.15; acc: 0.58
Batch: 20; loss: 0.88; acc: 0.67
Batch: 40; loss: 1.2; acc: 0.58
Batch: 60; loss: 0.8; acc: 0.69
Batch: 80; loss: 1.04; acc: 0.7
Batch: 100; loss: 0.78; acc: 0.7
Batch: 120; loss: 0.92; acc: 0.67
Batch: 140; loss: 1.07; acc: 0.64
Batch: 160; loss: 1.02; acc: 0.64
Batch: 180; loss: 0.98; acc: 0.67
Batch: 200; loss: 1.06; acc: 0.69
Batch: 220; loss: 0.86; acc: 0.7
Batch: 240; loss: 1.06; acc: 0.7
Batch: 260; loss: 1.07; acc: 0.67
Batch: 280; loss: 0.72; acc: 0.77
Batch: 300; loss: 0.66; acc: 0.73
Batch: 320; loss: 0.77; acc: 0.77
Batch: 340; loss: 0.88; acc: 0.73
Batch: 360; loss: 1.04; acc: 0.67
Batch: 380; loss: 0.88; acc: 0.66
Batch: 400; loss: 0.73; acc: 0.78
Batch: 420; loss: 0.62; acc: 0.78
Batch: 440; loss: 0.74; acc: 0.8
Batch: 460; loss: 0.95; acc: 0.69
Batch: 480; loss: 0.93; acc: 0.73
Batch: 500; loss: 1.0; acc: 0.67
Batch: 520; loss: 0.72; acc: 0.7
Batch: 540; loss: 0.78; acc: 0.72
Batch: 560; loss: 0.85; acc: 0.78
Batch: 580; loss: 0.65; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.75
Batch: 620; loss: 0.96; acc: 0.75
Batch: 640; loss: 0.56; acc: 0.83
Batch: 660; loss: 0.73; acc: 0.8
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.78; acc: 0.72
Batch: 720; loss: 0.92; acc: 0.66
Batch: 740; loss: 0.82; acc: 0.73
Batch: 760; loss: 0.93; acc: 0.69
Batch: 780; loss: 0.71; acc: 0.75
Train Epoch over. train_loss: 0.8; train_accuracy: 0.74 

Batch: 0; loss: 1.07; acc: 0.66
Batch: 20; loss: 0.92; acc: 0.66
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 1.15; acc: 0.61
Batch: 120; loss: 1.16; acc: 0.61
Batch: 140; loss: 0.38; acc: 0.89
Val Epoch over. val_loss: 0.7586306994128379; val_accuracy: 0.7576632165605095 

The current subspace-distance is: 1.8565669961390086e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.69
Batch: 20; loss: 0.6; acc: 0.77
Batch: 40; loss: 0.6; acc: 0.77
Batch: 60; loss: 0.69; acc: 0.75
Batch: 80; loss: 0.81; acc: 0.73
Batch: 100; loss: 0.74; acc: 0.72
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.6; acc: 0.78
Batch: 160; loss: 0.64; acc: 0.81
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.57; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.55; acc: 0.75
Batch: 260; loss: 0.71; acc: 0.75
Batch: 280; loss: 0.52; acc: 0.8
Batch: 300; loss: 0.62; acc: 0.81
Batch: 320; loss: 0.58; acc: 0.81
Batch: 340; loss: 0.68; acc: 0.73
Batch: 360; loss: 0.59; acc: 0.81
Batch: 380; loss: 0.69; acc: 0.78
Batch: 400; loss: 0.53; acc: 0.8
Batch: 420; loss: 0.65; acc: 0.7
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.88
Batch: 480; loss: 0.85; acc: 0.7
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.57; acc: 0.8
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.72; acc: 0.75
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.63; acc: 0.78
Batch: 620; loss: 0.68; acc: 0.73
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.93; acc: 0.73
Batch: 680; loss: 0.59; acc: 0.78
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.7; acc: 0.75
Batch: 740; loss: 0.88; acc: 0.75
Batch: 760; loss: 0.74; acc: 0.8
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.66; train_accuracy: 0.78 

Batch: 0; loss: 0.9; acc: 0.75
Batch: 20; loss: 0.76; acc: 0.73
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.94
Batch: 100; loss: 0.81; acc: 0.77
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.44; acc: 0.83
Val Epoch over. val_loss: 0.6526488063821367; val_accuracy: 0.7849323248407644 

The current subspace-distance is: 2.7030760975321755e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.53; acc: 0.78
Batch: 20; loss: 0.66; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.83
Batch: 60; loss: 0.71; acc: 0.77
Batch: 80; loss: 0.77; acc: 0.75
Batch: 100; loss: 0.77; acc: 0.75
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.46; acc: 0.86
Batch: 160; loss: 0.54; acc: 0.75
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.82; acc: 0.75
Batch: 220; loss: 0.59; acc: 0.84
Batch: 240; loss: 0.7; acc: 0.75
Batch: 260; loss: 0.59; acc: 0.75
Batch: 280; loss: 1.05; acc: 0.78
Batch: 300; loss: 0.68; acc: 0.83
Batch: 320; loss: 0.69; acc: 0.8
Batch: 340; loss: 0.77; acc: 0.77
Batch: 360; loss: 0.53; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.8
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.56; acc: 0.75
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.84; acc: 0.67
Batch: 480; loss: 0.43; acc: 0.83
Batch: 500; loss: 0.5; acc: 0.81
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.56; acc: 0.8
Batch: 560; loss: 0.91; acc: 0.72
Batch: 580; loss: 0.57; acc: 0.78
Batch: 600; loss: 0.6; acc: 0.77
Batch: 620; loss: 0.87; acc: 0.77
Batch: 640; loss: 0.92; acc: 0.69
Batch: 660; loss: 0.81; acc: 0.75
Batch: 680; loss: 0.53; acc: 0.81
Batch: 700; loss: 0.58; acc: 0.84
Batch: 720; loss: 0.65; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.65; train_accuracy: 0.79 

Batch: 0; loss: 1.07; acc: 0.72
Batch: 20; loss: 0.87; acc: 0.72
Batch: 40; loss: 0.38; acc: 0.84
Batch: 60; loss: 0.89; acc: 0.69
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 0.93; acc: 0.61
Batch: 140; loss: 0.57; acc: 0.75
Val Epoch over. val_loss: 0.7403265188454063; val_accuracy: 0.7569665605095541 

The current subspace-distance is: 3.279290103819221e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.98; acc: 0.73
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.67; acc: 0.77
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.78
Batch: 100; loss: 0.98; acc: 0.7
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.43; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.76; acc: 0.8
Batch: 240; loss: 0.73; acc: 0.78
Batch: 260; loss: 0.61; acc: 0.8
Batch: 280; loss: 0.64; acc: 0.8
Batch: 300; loss: 0.72; acc: 0.72
Batch: 320; loss: 0.59; acc: 0.78
Batch: 340; loss: 0.52; acc: 0.8
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.48; acc: 0.81
Batch: 400; loss: 0.61; acc: 0.75
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.93; acc: 0.69
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.55; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.94; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.8
Batch: 600; loss: 0.64; acc: 0.83
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.97; acc: 0.77
Batch: 660; loss: 0.79; acc: 0.75
Batch: 680; loss: 0.85; acc: 0.72
Batch: 700; loss: 0.67; acc: 0.83
Batch: 720; loss: 0.88; acc: 0.7
Batch: 740; loss: 0.9; acc: 0.69
Batch: 760; loss: 0.54; acc: 0.75
Batch: 780; loss: 0.56; acc: 0.8
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 1.1; acc: 0.55
Batch: 20; loss: 1.17; acc: 0.56
Batch: 40; loss: 0.49; acc: 0.8
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.64; acc: 0.73
Batch: 100; loss: 1.41; acc: 0.61
Batch: 120; loss: 1.18; acc: 0.67
Batch: 140; loss: 0.46; acc: 0.78
Val Epoch over. val_loss: 0.8340590949271135; val_accuracy: 0.713077229299363 

The current subspace-distance is: 3.680841109598987e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.23; acc: 0.62
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 0.56; acc: 0.8
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.69; acc: 0.81
Batch: 180; loss: 0.71; acc: 0.73
Batch: 200; loss: 0.51; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.69; acc: 0.75
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.82; acc: 0.77
Batch: 320; loss: 0.61; acc: 0.78
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.65; acc: 0.81
Batch: 380; loss: 0.57; acc: 0.84
Batch: 400; loss: 0.6; acc: 0.84
Batch: 420; loss: 0.75; acc: 0.77
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.49; acc: 0.86
Batch: 480; loss: 0.86; acc: 0.73
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.84
Batch: 540; loss: 1.18; acc: 0.69
Batch: 560; loss: 0.65; acc: 0.8
Batch: 580; loss: 0.56; acc: 0.86
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.56; acc: 0.8
Batch: 640; loss: 0.45; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.76; acc: 0.75
Batch: 700; loss: 0.59; acc: 0.78
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.67; acc: 0.88
Batch: 760; loss: 0.42; acc: 0.83
Batch: 780; loss: 0.5; acc: 0.83
Train Epoch over. train_loss: 0.61; train_accuracy: 0.81 

Batch: 0; loss: 0.79; acc: 0.7
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.68; acc: 0.8
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.74; acc: 0.75
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.41; acc: 0.86
Val Epoch over. val_loss: 0.5658202024212309; val_accuracy: 0.8222531847133758 

The current subspace-distance is: 4.33998393418733e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.75
Batch: 20; loss: 0.57; acc: 0.81
Batch: 40; loss: 0.61; acc: 0.81
Batch: 60; loss: 0.58; acc: 0.84
Batch: 80; loss: 0.67; acc: 0.75
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.68; acc: 0.77
Batch: 180; loss: 0.78; acc: 0.84
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.74; acc: 0.77
Batch: 240; loss: 0.86; acc: 0.66
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.52; acc: 0.81
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.68; acc: 0.81
Batch: 340; loss: 0.49; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.78
Batch: 380; loss: 0.61; acc: 0.84
Batch: 400; loss: 0.92; acc: 0.66
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.51; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.86
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.59; acc: 0.75
Batch: 560; loss: 0.52; acc: 0.77
Batch: 580; loss: 0.49; acc: 0.81
Batch: 600; loss: 0.89; acc: 0.72
Batch: 620; loss: 0.87; acc: 0.75
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.54; acc: 0.89
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.72; acc: 0.77
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.6; acc: 0.75
Train Epoch over. train_loss: 0.59; train_accuracy: 0.81 

Batch: 0; loss: 0.8; acc: 0.72
Batch: 20; loss: 0.69; acc: 0.77
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.64; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.61; acc: 0.77
Batch: 140; loss: 0.32; acc: 0.89
Val Epoch over. val_loss: 0.5739696595319517; val_accuracy: 0.8202627388535032 

The current subspace-distance is: 4.599063322530128e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.73
Batch: 20; loss: 0.67; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.5; acc: 0.81
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.84
Batch: 180; loss: 0.69; acc: 0.83
Batch: 200; loss: 0.5; acc: 0.83
Batch: 220; loss: 0.73; acc: 0.83
Batch: 240; loss: 0.5; acc: 0.81
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.77; acc: 0.75
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.6; acc: 0.77
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.8; acc: 0.8
Batch: 420; loss: 0.6; acc: 0.8
Batch: 440; loss: 0.86; acc: 0.75
Batch: 460; loss: 0.73; acc: 0.75
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.67; acc: 0.8
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.72; acc: 0.8
Batch: 600; loss: 0.46; acc: 0.8
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.81
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.66; acc: 0.78
Batch: 720; loss: 0.84; acc: 0.72
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.46; acc: 0.84
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.56; train_accuracy: 0.82 

Batch: 0; loss: 0.82; acc: 0.69
Batch: 20; loss: 1.02; acc: 0.67
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.56; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.76; acc: 0.78
Batch: 120; loss: 1.02; acc: 0.67
Batch: 140; loss: 0.42; acc: 0.83
Val Epoch over. val_loss: 0.6868664107884571; val_accuracy: 0.7838375796178344 

The current subspace-distance is: 4.901776264887303e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.73
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.49; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.84
Batch: 120; loss: 0.7; acc: 0.81
Batch: 140; loss: 0.63; acc: 0.84
Batch: 160; loss: 0.54; acc: 0.8
Batch: 180; loss: 0.46; acc: 0.83
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.48; acc: 0.75
Batch: 240; loss: 0.57; acc: 0.8
Batch: 260; loss: 0.52; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.53; acc: 0.88
Batch: 340; loss: 0.6; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.81
Batch: 380; loss: 0.58; acc: 0.78
Batch: 400; loss: 0.7; acc: 0.73
Batch: 420; loss: 0.63; acc: 0.81
Batch: 440; loss: 0.6; acc: 0.78
Batch: 460; loss: 0.61; acc: 0.77
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.86
Batch: 520; loss: 0.57; acc: 0.75
Batch: 540; loss: 0.63; acc: 0.84
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.55; acc: 0.89
Batch: 600; loss: 0.51; acc: 0.8
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.39; acc: 0.86
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.39; acc: 0.83
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.55; acc: 0.81
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.31; acc: 0.95
Batch: 100; loss: 0.52; acc: 0.86
Batch: 120; loss: 0.7; acc: 0.75
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.474793534369985; val_accuracy: 0.8516122611464968 

The current subspace-distance is: 5.1440438255667686e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.83; acc: 0.72
Batch: 60; loss: 0.58; acc: 0.78
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.73; acc: 0.81
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.78
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.55; acc: 0.84
Batch: 240; loss: 0.91; acc: 0.8
Batch: 260; loss: 0.46; acc: 0.83
Batch: 280; loss: 0.32; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.75
Batch: 340; loss: 0.83; acc: 0.81
Batch: 360; loss: 0.84; acc: 0.75
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.45; acc: 0.81
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.43; acc: 0.81
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.71; acc: 0.77
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.67; acc: 0.8
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.64; acc: 0.78
Batch: 600; loss: 0.54; acc: 0.84
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.64; acc: 0.8
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.62; acc: 0.73
Batch: 700; loss: 0.35; acc: 0.88
Batch: 720; loss: 0.62; acc: 0.78
Batch: 740; loss: 0.48; acc: 0.8
Batch: 760; loss: 0.58; acc: 0.77
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.63; acc: 0.75
Batch: 20; loss: 0.9; acc: 0.7
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.6; acc: 0.77
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.5762136117288261; val_accuracy: 0.8088176751592356 

The current subspace-distance is: 5.529224290512502e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.97; acc: 0.75
Batch: 20; loss: 0.4; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.46; acc: 0.83
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.8
Batch: 280; loss: 0.76; acc: 0.75
Batch: 300; loss: 0.48; acc: 0.83
Batch: 320; loss: 0.54; acc: 0.83
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.44; acc: 0.81
Batch: 420; loss: 1.09; acc: 0.7
Batch: 440; loss: 0.46; acc: 0.84
Batch: 460; loss: 0.54; acc: 0.81
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.69; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.8
Batch: 700; loss: 0.6; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.8
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.95
Val Epoch over. val_loss: 0.42418566222783105; val_accuracy: 0.8681329617834395 

The current subspace-distance is: 5.8586098020896316e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.39; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.57; acc: 0.83
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.57; acc: 0.8
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.8
Batch: 340; loss: 0.66; acc: 0.8
Batch: 360; loss: 0.6; acc: 0.77
Batch: 380; loss: 0.54; acc: 0.84
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.63; acc: 0.84
Batch: 440; loss: 0.61; acc: 0.84
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.49; acc: 0.84
Batch: 500; loss: 0.66; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.73
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.81
Batch: 680; loss: 0.53; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.88
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.48; acc: 0.86
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.52; acc: 0.78
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.23; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4490706840897821; val_accuracy: 0.8638535031847133 

The current subspace-distance is: 6.150058470666409e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.37; acc: 0.92
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.86
Batch: 160; loss: 0.43; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.84
Batch: 200; loss: 0.48; acc: 0.8
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.86
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.61; acc: 0.88
Batch: 420; loss: 0.37; acc: 0.92
Batch: 440; loss: 0.47; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.65; acc: 0.81
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.88
Batch: 560; loss: 0.5; acc: 0.88
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.38; acc: 0.83
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.57; acc: 0.83
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.74; acc: 0.73
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.57; acc: 0.83
Batch: 20; loss: 0.62; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4906134122306374; val_accuracy: 0.8443471337579618 

The current subspace-distance is: 6.449742068070918e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.54; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.42; acc: 0.86
Batch: 160; loss: 0.55; acc: 0.78
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.84
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.55; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.8
Batch: 320; loss: 0.42; acc: 0.83
Batch: 340; loss: 0.56; acc: 0.78
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.61; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.81
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.66; acc: 0.8
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.54; acc: 0.8
Batch: 700; loss: 0.4; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.86
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.31; acc: 0.88
Batch: 780; loss: 0.54; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.64; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.95
Val Epoch over. val_loss: 0.45485198858437265; val_accuracy: 0.8594745222929936 

The current subspace-distance is: 6.764126010239124e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.5; acc: 0.8
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.81
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.8
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.48; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.8
Batch: 240; loss: 0.57; acc: 0.83
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.77
Batch: 440; loss: 0.34; acc: 0.84
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.4; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.81
Batch: 560; loss: 0.38; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.58; acc: 0.81
Batch: 640; loss: 0.51; acc: 0.83
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.72; acc: 0.78
Batch: 720; loss: 0.59; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.44; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.44399679789117946; val_accuracy: 0.8618630573248408 

The current subspace-distance is: 6.975258293095976e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.8
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.6; acc: 0.78
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.75; acc: 0.8
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.78; acc: 0.75
Batch: 480; loss: 0.53; acc: 0.84
Batch: 500; loss: 0.59; acc: 0.84
Batch: 520; loss: 0.66; acc: 0.8
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.68; acc: 0.78
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.46; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.72; acc: 0.75
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.81
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.97
Val Epoch over. val_loss: 0.4348604200751918; val_accuracy: 0.8676353503184714 

The current subspace-distance is: 7.146961434045807e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.61; acc: 0.84
Batch: 60; loss: 0.55; acc: 0.84
Batch: 80; loss: 0.54; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.61; acc: 0.81
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.83
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.8
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.33; acc: 0.94
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.8
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.32; acc: 0.86
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.79; acc: 0.8
Batch: 700; loss: 0.37; acc: 0.84
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.45; acc: 0.81
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.65; acc: 0.8
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.43698122276432194; val_accuracy: 0.8654458598726115 

The current subspace-distance is: 7.446473318850622e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.97; acc: 0.69
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.69; acc: 0.78
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.86
Batch: 300; loss: 0.44; acc: 0.91
Batch: 320; loss: 0.79; acc: 0.78
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.6; acc: 0.83
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.87; acc: 0.7
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.56; acc: 0.77
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.5; acc: 0.81
Batch: 560; loss: 0.46; acc: 0.83
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.62; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.8
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.51; acc: 0.84
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.92; acc: 0.78
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.81
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.42358498816277573; val_accuracy: 0.871218152866242 

The current subspace-distance is: 7.80866394052282e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.83
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.31; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.55; acc: 0.81
Batch: 240; loss: 0.62; acc: 0.81
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.65; acc: 0.83
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.38; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.78
Batch: 540; loss: 0.32; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.63; acc: 0.84
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.4; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4163317388030374; val_accuracy: 0.8735071656050956 

The current subspace-distance is: 8.025988790905103e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.48; acc: 0.89
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.5; acc: 0.81
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.53; acc: 0.8
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.36; acc: 0.91
Batch: 340; loss: 0.52; acc: 0.83
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.43; acc: 0.89
Batch: 420; loss: 0.46; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.69; acc: 0.75
Batch: 480; loss: 0.75; acc: 0.7
Batch: 500; loss: 0.52; acc: 0.86
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.83
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.83
Batch: 660; loss: 0.46; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.91
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.86
Train Epoch over. train_loss: 0.46; train_accuracy: 0.85 

Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.4464049166554858; val_accuracy: 0.8649482484076433 

The current subspace-distance is: 8.144398452714086e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.86
Batch: 60; loss: 0.65; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.55; acc: 0.83
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.47; acc: 0.91
Batch: 200; loss: 0.63; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.8
Batch: 260; loss: 0.49; acc: 0.8
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.92
Batch: 400; loss: 0.58; acc: 0.77
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.57; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.78
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.4135384581461074; val_accuracy: 0.8756966560509554 

The current subspace-distance is: 8.417935896432027e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.54; acc: 0.86
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.85; acc: 0.83
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.49; acc: 0.8
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.55; acc: 0.84
Batch: 360; loss: 0.68; acc: 0.78
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.92
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.65; acc: 0.83
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.78; acc: 0.81
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.88
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4158320524700128; val_accuracy: 0.8729100318471338 

The current subspace-distance is: 8.712941780686378e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.8
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.83
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.55; acc: 0.84
Batch: 340; loss: 0.49; acc: 0.81
Batch: 360; loss: 0.8; acc: 0.78
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.6; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.91
Batch: 660; loss: 0.51; acc: 0.83
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.72; acc: 0.81
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.41474187554447517; val_accuracy: 0.8747014331210191 

The current subspace-distance is: 8.754881127970293e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.83
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.86
Batch: 180; loss: 0.7; acc: 0.77
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 0.63; acc: 0.8
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.6; acc: 0.75
Batch: 300; loss: 0.37; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.53; acc: 0.84
Batch: 360; loss: 0.48; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.81
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.88
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.84
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.64; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.65; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.4272156181229148; val_accuracy: 0.8689291401273885 

The current subspace-distance is: 9.075767593458295e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.89; acc: 0.78
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.47; acc: 0.81
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.48; acc: 0.81
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 1.18; acc: 0.67
Batch: 260; loss: 0.34; acc: 0.81
Batch: 280; loss: 0.61; acc: 0.84
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.46; acc: 0.81
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.85; acc: 0.78
Batch: 400; loss: 0.4; acc: 0.84
Batch: 420; loss: 0.66; acc: 0.75
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.83
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.55; acc: 0.78
Batch: 520; loss: 0.53; acc: 0.86
Batch: 540; loss: 0.46; acc: 0.81
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.31; acc: 0.88
Batch: 600; loss: 0.36; acc: 0.86
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.52; acc: 0.83
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.78; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.6; acc: 0.8
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4091960520596261; val_accuracy: 0.8763933121019108 

The current subspace-distance is: 9.42429542192258e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.83; acc: 0.73
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.35; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.43; acc: 0.88
Batch: 160; loss: 0.64; acc: 0.8
Batch: 180; loss: 0.24; acc: 0.95
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.66; acc: 0.83
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.81
Batch: 320; loss: 0.43; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.84
Batch: 360; loss: 0.51; acc: 0.84
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.81
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.86
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.55; acc: 0.81
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.31; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.94
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.7; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.8
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4116185484987915; val_accuracy: 0.8754976114649682 

The current subspace-distance is: 9.516843419987708e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.35; acc: 0.94
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.83
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.84
Batch: 380; loss: 0.5; acc: 0.88
Batch: 400; loss: 0.64; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.76; acc: 0.72
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.87; acc: 0.75
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.55; acc: 0.83
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.7; acc: 0.75
Batch: 640; loss: 0.78; acc: 0.78
Batch: 660; loss: 0.57; acc: 0.81
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.81
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.22; acc: 0.95
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.59; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.92
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.4163295697824211; val_accuracy: 0.873109076433121 

The current subspace-distance is: 9.702194074634463e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.52; acc: 0.88
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.63; acc: 0.81
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.59; acc: 0.83
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.71; acc: 0.86
Batch: 400; loss: 0.27; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.52; acc: 0.86
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.73; acc: 0.73
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.45; acc: 0.83
Batch: 700; loss: 0.49; acc: 0.81
Batch: 720; loss: 0.33; acc: 0.94
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.76; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.41972668877073155; val_accuracy: 0.871218152866242 

The current subspace-distance is: 9.961952309822664e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.52; acc: 0.81
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.58; acc: 0.81
Batch: 160; loss: 0.67; acc: 0.8
Batch: 180; loss: 0.44; acc: 0.83
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.57; acc: 0.84
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.6; acc: 0.78
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.78
Batch: 580; loss: 0.36; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.37; acc: 0.94
Batch: 640; loss: 0.47; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.84
Batch: 700; loss: 0.62; acc: 0.84
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.5; acc: 0.83
Batch: 780; loss: 0.61; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.4191264914479225; val_accuracy: 0.8720143312101911 

The current subspace-distance is: 0.00010232601198367774 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.78; acc: 0.78
Batch: 20; loss: 0.46; acc: 0.77
Batch: 40; loss: 0.75; acc: 0.73
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.34; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.46; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.66; acc: 0.81
Batch: 300; loss: 0.59; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.43; acc: 0.83
Batch: 380; loss: 0.42; acc: 0.86
Batch: 400; loss: 0.61; acc: 0.89
Batch: 420; loss: 0.52; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.84
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.33; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.54; acc: 0.88
Batch: 560; loss: 0.6; acc: 0.83
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.54; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.89
Batch: 640; loss: 0.66; acc: 0.84
Batch: 660; loss: 0.49; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.8
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.81
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.81
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.4197226294856163; val_accuracy: 0.8717157643312102 

The current subspace-distance is: 0.00010339730215491727 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.73; acc: 0.78
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.39; acc: 0.83
Batch: 60; loss: 0.69; acc: 0.78
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.66; acc: 0.81
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.81
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.91
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.81
Batch: 280; loss: 0.3; acc: 0.88
Batch: 300; loss: 0.52; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.78
Batch: 340; loss: 0.38; acc: 0.81
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.86; acc: 0.84
Batch: 440; loss: 0.65; acc: 0.83
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.63; acc: 0.78
Batch: 540; loss: 0.61; acc: 0.83
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.66; acc: 0.83
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.62; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.81
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.77
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4059660063142989; val_accuracy: 0.8764928343949044 

The current subspace-distance is: 0.00010378401930211112 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.75; acc: 0.78
Batch: 180; loss: 0.44; acc: 0.91
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.44; acc: 0.81
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.4; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.81
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.86
Batch: 480; loss: 0.55; acc: 0.83
Batch: 500; loss: 0.42; acc: 0.83
Batch: 520; loss: 0.42; acc: 0.83
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.8
Batch: 580; loss: 0.36; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.84
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.81
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.25; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.75
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.407550845176551; val_accuracy: 0.8759952229299363 

The current subspace-distance is: 0.00010620692046359181 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.78
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.64; acc: 0.78
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.4; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.81
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.6; acc: 0.8
Batch: 520; loss: 0.79; acc: 0.78
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.5; acc: 0.8
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.59; acc: 0.86
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.54; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.41015698784475874; val_accuracy: 0.8763933121019108 

The current subspace-distance is: 0.00010625678260112181 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.58; acc: 0.77
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.84; acc: 0.75
Batch: 180; loss: 0.43; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.81
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.58; acc: 0.86
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.86
Batch: 360; loss: 0.46; acc: 0.91
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.8
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.57; acc: 0.83
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.58; acc: 0.83
Batch: 580; loss: 0.54; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.91
Batch: 640; loss: 0.6; acc: 0.8
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.61; acc: 0.81
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.68; acc: 0.78
Batch: 740; loss: 0.6; acc: 0.83
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.47; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.56; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.41179205069116726; val_accuracy: 0.8758957006369427 

The current subspace-distance is: 0.00010832305997610092 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.78
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.86
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.51; acc: 0.84
Batch: 200; loss: 0.49; acc: 0.8
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.5; acc: 0.84
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.72; acc: 0.77
Batch: 360; loss: 0.43; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.8
Batch: 480; loss: 0.63; acc: 0.78
Batch: 500; loss: 0.5; acc: 0.8
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.54; acc: 0.83
Batch: 560; loss: 0.58; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.68; acc: 0.77
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4078008207925566; val_accuracy: 0.8747014331210191 

The current subspace-distance is: 0.00011043401900678873 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.54; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.47; acc: 0.86
Batch: 160; loss: 0.64; acc: 0.83
Batch: 180; loss: 0.68; acc: 0.83
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.63; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.8
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.46; acc: 0.8
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.51; acc: 0.86
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.83
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.49; acc: 0.84
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.75
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.84
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.41; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.46; acc: 0.81
Batch: 700; loss: 0.56; acc: 0.8
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.83
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.40540089372806487; val_accuracy: 0.8764928343949044 

The current subspace-distance is: 0.00011093826469732448 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.41; acc: 0.91
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.56; acc: 0.8
Batch: 300; loss: 0.63; acc: 0.78
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.83
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.54; acc: 0.88
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.35; acc: 0.88
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.44; acc: 0.89
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.69; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.58; acc: 0.8
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4089244557603909; val_accuracy: 0.8755971337579618 

The current subspace-distance is: 0.00011307279055472463 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.88
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.62; acc: 0.86
Batch: 280; loss: 0.84; acc: 0.75
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 0.4; acc: 0.84
Batch: 340; loss: 0.65; acc: 0.77
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.59; acc: 0.83
Batch: 440; loss: 0.65; acc: 0.78
Batch: 460; loss: 0.34; acc: 0.83
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.78
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.62; acc: 0.77
Batch: 560; loss: 0.46; acc: 0.86
Batch: 580; loss: 0.46; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.83
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.59; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.406970306851302; val_accuracy: 0.87609474522293 

The current subspace-distance is: 0.00011501296830829233 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.9; acc: 0.78
Batch: 100; loss: 0.48; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.78
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.63; acc: 0.8
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.43; acc: 0.89
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.62; acc: 0.84
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.31; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.81
Batch: 440; loss: 0.54; acc: 0.83
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.8
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.56; acc: 0.8
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.89
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.4053709761352296; val_accuracy: 0.8783837579617835 

The current subspace-distance is: 0.00011681754403980449 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.46; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.81
Batch: 260; loss: 0.49; acc: 0.83
Batch: 280; loss: 0.73; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.8
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.58; acc: 0.86
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.61; acc: 0.8
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.4; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.59; acc: 0.81
Batch: 760; loss: 0.53; acc: 0.8
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4066389575126065; val_accuracy: 0.8761942675159236 

The current subspace-distance is: 0.00011850094597321004 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.76; acc: 0.84
Batch: 40; loss: 0.46; acc: 0.83
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.62; acc: 0.83
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.43; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.88
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.83
Batch: 440; loss: 0.75; acc: 0.83
Batch: 460; loss: 0.57; acc: 0.75
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.56; acc: 0.8
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.8
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.71; acc: 0.77
Batch: 640; loss: 0.6; acc: 0.88
Batch: 660; loss: 0.63; acc: 0.77
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.48; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.98
Val Epoch over. val_loss: 0.4053605631658226; val_accuracy: 0.8770899681528662 

The current subspace-distance is: 0.00012097429862478748 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.61; acc: 0.83
Batch: 260; loss: 0.34; acc: 0.83
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.45; acc: 0.81
Batch: 380; loss: 0.26; acc: 0.95
Batch: 400; loss: 0.69; acc: 0.84
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.83
Batch: 480; loss: 0.64; acc: 0.78
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.6; acc: 0.86
Batch: 740; loss: 0.25; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4049077989758959; val_accuracy: 0.8766918789808917 

The current subspace-distance is: 0.00012265308760106564 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.46; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.84
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.68; acc: 0.73
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.59; acc: 0.8
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.92
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.65; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.89
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.81
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.63; acc: 0.83
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.40573883986776804; val_accuracy: 0.8763933121019108 

The current subspace-distance is: 0.0001238731201738119 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.89
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.42; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.84
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.37; acc: 0.91
Batch: 200; loss: 0.75; acc: 0.81
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.56; acc: 0.84
Batch: 280; loss: 0.51; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.95
Batch: 380; loss: 0.57; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.54; acc: 0.84
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.55; acc: 0.77
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.56; acc: 0.78
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.81
Batch: 660; loss: 0.71; acc: 0.8
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4049534261891037; val_accuracy: 0.8767914012738853 

The current subspace-distance is: 0.0001252119691343978 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.71; acc: 0.78
Batch: 100; loss: 0.53; acc: 0.78
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.72; acc: 0.8
Batch: 260; loss: 0.51; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.57; acc: 0.78
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.3; acc: 0.86
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.91
Batch: 440; loss: 0.64; acc: 0.84
Batch: 460; loss: 0.75; acc: 0.8
Batch: 480; loss: 0.37; acc: 0.81
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.5; acc: 0.8
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.67; acc: 0.8
Batch: 660; loss: 0.44; acc: 0.83
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.69; acc: 0.78
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.55; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4053154354737063; val_accuracy: 0.8775875796178344 

The current subspace-distance is: 0.00012633719597943127 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.83
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.54; acc: 0.83
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.34; acc: 0.88
Batch: 200; loss: 0.54; acc: 0.8
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.78
Batch: 280; loss: 0.5; acc: 0.8
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.7; acc: 0.8
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.76; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.86
Batch: 480; loss: 0.61; acc: 0.8
Batch: 500; loss: 0.59; acc: 0.78
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.46; acc: 0.81
Batch: 560; loss: 0.58; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.84
Batch: 640; loss: 0.57; acc: 0.83
Batch: 660; loss: 0.51; acc: 0.84
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.64; acc: 0.8
Batch: 740; loss: 0.35; acc: 0.84
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.62; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.40480377183408495; val_accuracy: 0.8772890127388535 

The current subspace-distance is: 0.00012798779061995447 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.77; acc: 0.81
Batch: 180; loss: 0.53; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.56; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.52; acc: 0.8
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.63; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.52; acc: 0.84
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.83
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.81
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4047586987162851; val_accuracy: 0.8769904458598726 

The current subspace-distance is: 0.00012971310934517533 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.83
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.61; acc: 0.8
Batch: 160; loss: 0.41; acc: 0.86
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.55; acc: 0.81
Batch: 300; loss: 0.34; acc: 0.83
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.62; acc: 0.8
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.81
Batch: 440; loss: 0.29; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.3; acc: 0.86
Batch: 500; loss: 0.37; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.53; acc: 0.8
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.49; acc: 0.8
Batch: 620; loss: 0.65; acc: 0.78
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.86
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.95
Batch: 760; loss: 0.44; acc: 0.84
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.40431088166441886; val_accuracy: 0.877687101910828 

The current subspace-distance is: 0.0001300454168813303 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.24; acc: 0.89
Batch: 180; loss: 0.45; acc: 0.86
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.83
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.84
Batch: 300; loss: 0.6; acc: 0.83
Batch: 320; loss: 0.61; acc: 0.81
Batch: 340; loss: 0.5; acc: 0.91
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.57; acc: 0.81
Batch: 560; loss: 0.37; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.72; acc: 0.75
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4048619086670268; val_accuracy: 0.8766918789808917 

The current subspace-distance is: 0.00012999222963117063 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.51; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.44; acc: 0.81
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.77; acc: 0.81
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.38; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.81
Batch: 300; loss: 0.51; acc: 0.83
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.54; acc: 0.83
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.81
Batch: 420; loss: 0.7; acc: 0.86
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.55; acc: 0.73
Batch: 520; loss: 0.51; acc: 0.83
Batch: 540; loss: 0.45; acc: 0.83
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.54; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.47; acc: 0.89
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.43; acc: 0.81
Batch: 720; loss: 0.55; acc: 0.8
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.79; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.92
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.98
Val Epoch over. val_loss: 0.4055191445502506; val_accuracy: 0.8764928343949044 

The current subspace-distance is: 0.0001308806095039472 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_150_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 36612
elements in E: 7774550
fraction nonzero: 0.004709211465615373
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.08
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.31; acc: 0.05
Batch: 140; loss: 2.29; acc: 0.14
Batch: 160; loss: 2.28; acc: 0.09
Batch: 180; loss: 2.3; acc: 0.09
Batch: 200; loss: 2.29; acc: 0.06
Batch: 220; loss: 2.27; acc: 0.12
Batch: 240; loss: 2.26; acc: 0.17
Batch: 260; loss: 2.26; acc: 0.14
Batch: 280; loss: 2.26; acc: 0.27
Batch: 300; loss: 2.23; acc: 0.34
Batch: 320; loss: 2.25; acc: 0.27
Batch: 340; loss: 2.21; acc: 0.34
Batch: 360; loss: 2.22; acc: 0.36
Batch: 380; loss: 2.21; acc: 0.28
Batch: 400; loss: 2.15; acc: 0.36
Batch: 420; loss: 2.16; acc: 0.34
Batch: 440; loss: 2.09; acc: 0.44
Batch: 460; loss: 2.1; acc: 0.36
Batch: 480; loss: 1.98; acc: 0.41
Batch: 500; loss: 1.89; acc: 0.44
Batch: 520; loss: 1.71; acc: 0.61
Batch: 540; loss: 1.5; acc: 0.59
Batch: 560; loss: 1.38; acc: 0.61
Batch: 580; loss: 1.28; acc: 0.59
Batch: 600; loss: 1.07; acc: 0.61
Batch: 620; loss: 1.11; acc: 0.62
Batch: 640; loss: 1.18; acc: 0.58
Batch: 660; loss: 1.1; acc: 0.62
Batch: 680; loss: 1.06; acc: 0.56
Batch: 700; loss: 1.24; acc: 0.64
Batch: 720; loss: 0.75; acc: 0.73
Batch: 740; loss: 0.82; acc: 0.75
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.87; acc: 0.72
Train Epoch over. train_loss: 1.84; train_accuracy: 0.36 

Batch: 0; loss: 0.81; acc: 0.75
Batch: 20; loss: 0.86; acc: 0.62
Batch: 40; loss: 0.51; acc: 0.81
Batch: 60; loss: 0.6; acc: 0.77
Batch: 80; loss: 0.64; acc: 0.78
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.59; acc: 0.8
Val Epoch over. val_loss: 0.7631244211439874; val_accuracy: 0.758359872611465 

The current subspace-distance is: 1.0727735570981167e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.75
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 1.18; acc: 0.61
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.98; acc: 0.66
Batch: 100; loss: 0.8; acc: 0.72
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.94; acc: 0.73
Batch: 160; loss: 0.72; acc: 0.8
Batch: 180; loss: 0.95; acc: 0.7
Batch: 200; loss: 0.85; acc: 0.72
Batch: 220; loss: 0.87; acc: 0.72
Batch: 240; loss: 0.99; acc: 0.62
Batch: 260; loss: 0.83; acc: 0.7
Batch: 280; loss: 0.73; acc: 0.8
Batch: 300; loss: 0.63; acc: 0.77
Batch: 320; loss: 0.66; acc: 0.77
Batch: 340; loss: 0.78; acc: 0.72
Batch: 360; loss: 0.86; acc: 0.73
Batch: 380; loss: 0.87; acc: 0.75
Batch: 400; loss: 0.53; acc: 0.83
Batch: 420; loss: 0.7; acc: 0.78
Batch: 440; loss: 0.65; acc: 0.77
Batch: 460; loss: 0.68; acc: 0.73
Batch: 480; loss: 0.81; acc: 0.75
Batch: 500; loss: 0.88; acc: 0.72
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.53; acc: 0.84
Batch: 560; loss: 0.82; acc: 0.75
Batch: 580; loss: 0.73; acc: 0.8
Batch: 600; loss: 0.63; acc: 0.8
Batch: 620; loss: 0.66; acc: 0.83
Batch: 640; loss: 0.58; acc: 0.78
Batch: 660; loss: 0.87; acc: 0.7
Batch: 680; loss: 0.81; acc: 0.75
Batch: 700; loss: 0.73; acc: 0.73
Batch: 720; loss: 0.88; acc: 0.72
Batch: 740; loss: 0.56; acc: 0.78
Batch: 760; loss: 0.78; acc: 0.72
Batch: 780; loss: 0.77; acc: 0.8
Train Epoch over. train_loss: 0.7; train_accuracy: 0.77 

Batch: 0; loss: 0.72; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.4; acc: 0.83
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.82; acc: 0.75
Batch: 140; loss: 0.62; acc: 0.77
Val Epoch over. val_loss: 0.6481442028170179; val_accuracy: 0.7883160828025477 

The current subspace-distance is: 2.0068780941073783e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.87; acc: 0.7
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.72; acc: 0.81
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 0.64; acc: 0.77
Batch: 100; loss: 0.6; acc: 0.75
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.89
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.6; acc: 0.8
Batch: 220; loss: 0.76; acc: 0.78
Batch: 240; loss: 0.53; acc: 0.8
Batch: 260; loss: 0.68; acc: 0.72
Batch: 280; loss: 0.42; acc: 0.81
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.75; acc: 0.7
Batch: 340; loss: 0.76; acc: 0.77
Batch: 360; loss: 0.58; acc: 0.77
Batch: 380; loss: 0.74; acc: 0.73
Batch: 400; loss: 0.51; acc: 0.8
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.64; acc: 0.75
Batch: 480; loss: 0.63; acc: 0.8
Batch: 500; loss: 0.65; acc: 0.84
Batch: 520; loss: 0.84; acc: 0.7
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.7; acc: 0.81
Batch: 620; loss: 0.57; acc: 0.81
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 1.0; acc: 0.64
Batch: 680; loss: 0.62; acc: 0.77
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.73
Batch: 740; loss: 0.85; acc: 0.81
Batch: 760; loss: 0.49; acc: 0.78
Batch: 780; loss: 0.66; acc: 0.84
Train Epoch over. train_loss: 0.62; train_accuracy: 0.8 

Batch: 0; loss: 0.6; acc: 0.75
Batch: 20; loss: 0.59; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.73; acc: 0.75
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.45; acc: 0.86
Val Epoch over. val_loss: 0.5957537356075967; val_accuracy: 0.8082205414012739 

The current subspace-distance is: 2.7213078283239156e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.46; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.59; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.81
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.54; acc: 0.88
Batch: 200; loss: 0.7; acc: 0.75
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.68; acc: 0.78
Batch: 260; loss: 0.7; acc: 0.8
Batch: 280; loss: 0.77; acc: 0.81
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.63; acc: 0.77
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.7; acc: 0.75
Batch: 400; loss: 0.72; acc: 0.78
Batch: 420; loss: 0.64; acc: 0.73
Batch: 440; loss: 0.41; acc: 0.84
Batch: 460; loss: 0.83; acc: 0.72
Batch: 480; loss: 0.69; acc: 0.83
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.77; acc: 0.75
Batch: 560; loss: 0.81; acc: 0.75
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.73; acc: 0.8
Batch: 640; loss: 0.67; acc: 0.84
Batch: 660; loss: 0.8; acc: 0.73
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.71; acc: 0.83
Batch: 740; loss: 0.61; acc: 0.8
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.58; train_accuracy: 0.81 

Batch: 0; loss: 1.23; acc: 0.56
Batch: 20; loss: 1.12; acc: 0.59
Batch: 40; loss: 0.83; acc: 0.75
Batch: 60; loss: 1.5; acc: 0.64
Batch: 80; loss: 1.18; acc: 0.7
Batch: 100; loss: 1.4; acc: 0.69
Batch: 120; loss: 1.39; acc: 0.64
Batch: 140; loss: 1.11; acc: 0.66
Val Epoch over. val_loss: 1.2272975209412302; val_accuracy: 0.6282842356687898 

The current subspace-distance is: 3.271187233622186e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.46; acc: 0.56
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.78; acc: 0.73
Batch: 120; loss: 0.67; acc: 0.73
Batch: 140; loss: 0.51; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.86
Batch: 200; loss: 0.54; acc: 0.83
Batch: 220; loss: 0.86; acc: 0.77
Batch: 240; loss: 0.56; acc: 0.78
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.6; acc: 0.8
Batch: 300; loss: 0.55; acc: 0.73
Batch: 320; loss: 0.68; acc: 0.75
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.54; acc: 0.81
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.6; acc: 0.78
Batch: 460; loss: 0.43; acc: 0.83
Batch: 480; loss: 0.82; acc: 0.81
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.84
Batch: 540; loss: 0.6; acc: 0.78
Batch: 560; loss: 0.53; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.81
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.85; acc: 0.8
Batch: 660; loss: 0.85; acc: 0.75
Batch: 680; loss: 0.55; acc: 0.83
Batch: 700; loss: 0.55; acc: 0.78
Batch: 720; loss: 1.08; acc: 0.64
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.6; acc: 0.78
Train Epoch over. train_loss: 0.57; train_accuracy: 0.82 

Batch: 0; loss: 0.64; acc: 0.78
Batch: 20; loss: 0.69; acc: 0.75
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.87; acc: 0.78
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.48; acc: 0.84
Val Epoch over. val_loss: 0.5822160224056547; val_accuracy: 0.8070262738853503 

The current subspace-distance is: 3.826788815786131e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.75
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.82; acc: 0.78
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.56; acc: 0.78
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.79; acc: 0.78
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.65; acc: 0.84
Batch: 240; loss: 0.68; acc: 0.78
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.64; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.81
Batch: 480; loss: 0.57; acc: 0.81
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 0.62; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.83
Batch: 700; loss: 0.7; acc: 0.8
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.47; acc: 0.81
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.77
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.86
Batch: 100; loss: 0.77; acc: 0.84
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.5191959444504635; val_accuracy: 0.8350915605095541 

The current subspace-distance is: 4.233993968227878e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.77
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.54; acc: 0.86
Batch: 160; loss: 0.68; acc: 0.81
Batch: 180; loss: 0.79; acc: 0.77
Batch: 200; loss: 0.52; acc: 0.83
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.77
Batch: 260; loss: 0.39; acc: 0.83
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.88
Batch: 320; loss: 0.65; acc: 0.81
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.63; acc: 0.73
Batch: 380; loss: 0.48; acc: 0.84
Batch: 400; loss: 0.48; acc: 0.88
Batch: 420; loss: 0.52; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.58; acc: 0.78
Batch: 480; loss: 0.56; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.78
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.55; acc: 0.77
Batch: 560; loss: 0.57; acc: 0.86
Batch: 580; loss: 0.58; acc: 0.83
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.61; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.81
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.81
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.62; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.29; acc: 0.94
Val Epoch over. val_loss: 0.47722694467587073; val_accuracy: 0.8528065286624203 

The current subspace-distance is: 4.680897473008372e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.78
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.86
Batch: 180; loss: 0.63; acc: 0.81
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.84
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.84
Batch: 380; loss: 0.49; acc: 0.89
Batch: 400; loss: 0.61; acc: 0.83
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.61; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.83
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.78
Batch: 660; loss: 0.4; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.52; acc: 0.81
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 0.29; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.7; acc: 0.81
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 0.51; acc: 0.84
Val Epoch over. val_loss: 0.6316300534708484; val_accuracy: 0.801453025477707 

The current subspace-distance is: 5.097109897178598e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.8
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.61; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.49; acc: 0.91
Batch: 160; loss: 0.32; acc: 0.88
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.49; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.79; acc: 0.75
Batch: 280; loss: 0.34; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.81
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.53; acc: 0.81
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.86
Batch: 400; loss: 0.72; acc: 0.73
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.67; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.56; acc: 0.86
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.72; acc: 0.77
Batch: 560; loss: 0.86; acc: 0.75
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.94
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.54; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.55; acc: 0.8
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.42; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.35; acc: 0.89
Val Epoch over. val_loss: 0.46455785736536526; val_accuracy: 0.8607683121019108 

The current subspace-distance is: 5.401071030064486e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.54; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.59; acc: 0.8
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.71; acc: 0.75
Batch: 180; loss: 0.77; acc: 0.78
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.62; acc: 0.77
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.6; acc: 0.84
Batch: 340; loss: 0.74; acc: 0.84
Batch: 360; loss: 0.71; acc: 0.78
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.49; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.69; acc: 0.78
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.8
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.92
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.65; acc: 0.8
Batch: 700; loss: 0.35; acc: 0.86
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.38; acc: 0.88
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.78; acc: 0.73
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.8; acc: 0.72
Batch: 120; loss: 0.79; acc: 0.72
Batch: 140; loss: 0.53; acc: 0.8
Val Epoch over. val_loss: 0.5599173339688854; val_accuracy: 0.8229498407643312 

The current subspace-distance is: 5.7013618061318994e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.72
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.88
Batch: 180; loss: 0.7; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.88
Batch: 260; loss: 0.49; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.86
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.64; acc: 0.78
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.51; acc: 0.89
Batch: 420; loss: 0.74; acc: 0.78
Batch: 440; loss: 0.49; acc: 0.83
Batch: 460; loss: 0.59; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.38; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.83
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.49; acc: 0.86
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.59; acc: 0.84
Batch: 660; loss: 0.58; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 0.51; acc: 0.81
Batch: 740; loss: 0.41; acc: 0.83
Batch: 760; loss: 0.45; acc: 0.81
Batch: 780; loss: 0.69; acc: 0.77
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.34; acc: 0.88
Val Epoch over. val_loss: 0.4384063037147947; val_accuracy: 0.8641520700636943 

The current subspace-distance is: 6.084157575969584e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.55; acc: 0.78
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.84
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.5; acc: 0.88
Batch: 380; loss: 0.52; acc: 0.8
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.63; acc: 0.86
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.6; acc: 0.81
Batch: 480; loss: 0.48; acc: 0.81
Batch: 500; loss: 0.71; acc: 0.8
Batch: 520; loss: 0.48; acc: 0.84
Batch: 540; loss: 0.4; acc: 0.84
Batch: 560; loss: 0.67; acc: 0.78
Batch: 580; loss: 0.51; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.94
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.48; acc: 0.89
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.88
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.37; acc: 0.89
Val Epoch over. val_loss: 0.45478224934666023; val_accuracy: 0.8599721337579618 

The current subspace-distance is: 6.397701508831233e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.69; acc: 0.81
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.54; acc: 0.84
Batch: 100; loss: 0.25; acc: 0.88
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.37; acc: 0.83
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.92
Batch: 280; loss: 0.54; acc: 0.83
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.54; acc: 0.8
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.65; acc: 0.86
Batch: 480; loss: 0.77; acc: 0.83
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.68; acc: 0.78
Batch: 560; loss: 0.35; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.61; acc: 0.84
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.72; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.95
Batch: 760; loss: 0.57; acc: 0.86
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.43; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.3; acc: 0.92
Val Epoch over. val_loss: 0.44501428624057465; val_accuracy: 0.8667396496815286 

The current subspace-distance is: 6.685645348625258e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.71; acc: 0.81
Batch: 60; loss: 0.6; acc: 0.84
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.81
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.43; acc: 0.81
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.88
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.68; acc: 0.81
Batch: 460; loss: 0.52; acc: 0.84
Batch: 480; loss: 0.59; acc: 0.83
Batch: 500; loss: 0.37; acc: 0.86
Batch: 520; loss: 0.41; acc: 0.83
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.29; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.88
Batch: 600; loss: 0.44; acc: 0.86
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.56; acc: 0.8
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.48; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.87 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.89
Batch: 120; loss: 0.75; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.89
Val Epoch over. val_loss: 0.43391006549072875; val_accuracy: 0.8667396496815286 

The current subspace-distance is: 7.057520997477695e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.86
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.36; acc: 0.84
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.53; acc: 0.8
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.39; acc: 0.84
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.55; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.41; acc: 0.84
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.87; acc: 0.78
Batch: 700; loss: 0.64; acc: 0.81
Batch: 720; loss: 0.44; acc: 0.89
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.55; acc: 0.81
Batch: 780; loss: 0.6; acc: 0.81
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.5; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.4316153506374663; val_accuracy: 0.8670382165605095 

The current subspace-distance is: 7.290373469004408e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.59; acc: 0.83
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.95
Batch: 140; loss: 0.53; acc: 0.83
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.73; acc: 0.8
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.86; acc: 0.77
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.62; acc: 0.78
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.6; acc: 0.77
Batch: 600; loss: 0.52; acc: 0.8
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.65; acc: 0.78
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.81; acc: 0.81
Batch: 740; loss: 0.67; acc: 0.8
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.34; acc: 0.94
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.66; acc: 0.77
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.86
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.72
Batch: 140; loss: 0.34; acc: 0.88
Val Epoch over. val_loss: 0.4294696086721056; val_accuracy: 0.8692277070063694 

The current subspace-distance is: 7.534721225965768e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.56; acc: 0.8
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.57; acc: 0.86
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.33; acc: 0.92
Batch: 240; loss: 0.61; acc: 0.8
Batch: 260; loss: 0.53; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.46; acc: 0.84
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.36; acc: 0.92
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.56; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.84
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.7; acc: 0.81
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.8
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.86
Batch: 120; loss: 0.84; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.4094412923333751; val_accuracy: 0.8763933121019108 

The current subspace-distance is: 7.774728874210268e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.63; acc: 0.75
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.42; acc: 0.88
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.51; acc: 0.89
Batch: 280; loss: 0.44; acc: 0.83
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.84
Batch: 440; loss: 0.23; acc: 0.91
Batch: 460; loss: 0.82; acc: 0.75
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.65; acc: 0.7
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.57; acc: 0.83
Batch: 580; loss: 0.67; acc: 0.8
Batch: 600; loss: 0.62; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.58; acc: 0.8
Batch: 740; loss: 0.82; acc: 0.78
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.86
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.24; acc: 0.92
Val Epoch over. val_loss: 0.4160260359288021; val_accuracy: 0.8717157643312102 

The current subspace-distance is: 8.050725591601804e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.55; acc: 0.83
Batch: 220; loss: 0.54; acc: 0.81
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.91
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.61; acc: 0.84
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.91
Batch: 540; loss: 0.31; acc: 0.86
Batch: 560; loss: 0.37; acc: 0.92
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.32; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.86
Batch: 660; loss: 0.51; acc: 0.8
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.95
Val Epoch over. val_loss: 0.40400662220967043; val_accuracy: 0.8772890127388535 

The current subspace-distance is: 8.323879592353478e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.6; acc: 0.81
Batch: 280; loss: 0.44; acc: 0.86
Batch: 300; loss: 0.57; acc: 0.77
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.39; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.37; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.7; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.32; acc: 0.86
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.95
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.62; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.92
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.42026441425654537; val_accuracy: 0.8713176751592356 

The current subspace-distance is: 8.592916128691286e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.63; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.86
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.53; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.97
Batch: 300; loss: 0.29; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.75
Batch: 580; loss: 0.36; acc: 0.84
Batch: 600; loss: 0.47; acc: 0.8
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.59; acc: 0.83
Batch: 660; loss: 0.42; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.43; acc: 0.83
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.390518525935662; val_accuracy: 0.880672770700637 

The current subspace-distance is: 8.895181235857308e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.57; acc: 0.81
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.62; acc: 0.78
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.86
Batch: 400; loss: 0.38; acc: 0.84
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.66; acc: 0.84
Batch: 480; loss: 0.48; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.73; acc: 0.8
Batch: 560; loss: 0.69; acc: 0.8
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.88
Batch: 640; loss: 0.55; acc: 0.78
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.8
Batch: 760; loss: 0.41; acc: 0.88
Batch: 780; loss: 0.64; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.87; acc: 0.77
Batch: 140; loss: 0.23; acc: 0.91
Val Epoch over. val_loss: 0.40335108382496865; val_accuracy: 0.8749004777070064 

The current subspace-distance is: 9.107559162657708e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.83
Batch: 140; loss: 0.57; acc: 0.8
Batch: 160; loss: 0.48; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.81
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.86
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.82; acc: 0.8
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.44; acc: 0.83
Batch: 660; loss: 0.47; acc: 0.81
Batch: 680; loss: 0.31; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.4; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.38877586621767396; val_accuracy: 0.8816679936305732 

The current subspace-distance is: 9.376229718327522e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.54; acc: 0.78
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.46; acc: 0.84
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.66; acc: 0.78
Batch: 200; loss: 0.65; acc: 0.84
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.56; acc: 0.88
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.46; acc: 0.91
Batch: 320; loss: 0.55; acc: 0.86
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.58; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.33; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.43; acc: 0.83
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.83
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.5; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3981859648398533; val_accuracy: 0.8754976114649682 

The current subspace-distance is: 9.529567614663392e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.61; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.86
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.8
Batch: 160; loss: 0.51; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.71; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.84
Batch: 240; loss: 0.9; acc: 0.8
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.69; acc: 0.8
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.25; acc: 0.88
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 1.13; acc: 0.75
Batch: 400; loss: 0.35; acc: 0.92
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.43; acc: 0.83
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.62; acc: 0.83
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.39; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.39438603984512344; val_accuracy: 0.8791799363057324 

The current subspace-distance is: 9.822969150263816e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.52; acc: 0.77
Batch: 280; loss: 0.37; acc: 0.92
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.36; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.35; acc: 0.84
Batch: 760; loss: 0.49; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.8
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.39532658277423516; val_accuracy: 0.8782842356687898 

The current subspace-distance is: 0.00010086939437314868 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.45; acc: 0.84
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.92
Batch: 160; loss: 0.44; acc: 0.81
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.48; acc: 0.84
Batch: 240; loss: 0.55; acc: 0.83
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.83
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.5; acc: 0.81
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.8
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.49; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.78
Batch: 660; loss: 0.64; acc: 0.8
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.64; acc: 0.81
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.52; acc: 0.84
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.22; acc: 0.92
Val Epoch over. val_loss: 0.39256020350630877; val_accuracy: 0.879578025477707 

The current subspace-distance is: 0.0001036459143506363 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.92
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.76; acc: 0.81
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.92
Batch: 420; loss: 0.24; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.54; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.67; acc: 0.81
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.83
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.88
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.51; acc: 0.86
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.39795786199296357; val_accuracy: 0.8773885350318471 

The current subspace-distance is: 0.00010601523536024615 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.4; acc: 0.83
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.51; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.98
Batch: 480; loss: 0.43; acc: 0.81
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.76; acc: 0.81
Batch: 580; loss: 0.53; acc: 0.8
Batch: 600; loss: 0.72; acc: 0.81
Batch: 620; loss: 0.48; acc: 0.83
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.63; acc: 0.81
Batch: 720; loss: 0.6; acc: 0.8
Batch: 740; loss: 0.44; acc: 0.83
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.8
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.38755044018387036; val_accuracy: 0.8812699044585988 

The current subspace-distance is: 0.00010677355021471158 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.6; acc: 0.8
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.47; acc: 0.84
Batch: 60; loss: 0.65; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.83
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.88
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.69; acc: 0.84
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.43; acc: 0.94
Batch: 300; loss: 0.57; acc: 0.8
Batch: 320; loss: 0.41; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.62; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.37; acc: 0.89
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.83
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.48; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 0.24; acc: 0.94
Val Epoch over. val_loss: 0.40759457633563667; val_accuracy: 0.8722133757961783 

The current subspace-distance is: 0.00010865717922570184 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.81
Batch: 20; loss: 0.36; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.41; acc: 0.84
Batch: 200; loss: 0.2; acc: 0.89
Batch: 220; loss: 0.82; acc: 0.78
Batch: 240; loss: 0.36; acc: 0.94
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.46; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.84
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.88
Batch: 580; loss: 0.58; acc: 0.81
Batch: 600; loss: 0.65; acc: 0.75
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.88
Batch: 660; loss: 0.44; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.3; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.38734436272435885; val_accuracy: 0.8810708598726115 

The current subspace-distance is: 0.00010968906281050295 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.92
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.57; acc: 0.8
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.62; acc: 0.78
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.45; acc: 0.81
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.61; acc: 0.88
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.3875694162431796; val_accuracy: 0.8804737261146497 

The current subspace-distance is: 0.00011114071094198152 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.56; acc: 0.8
Batch: 100; loss: 0.38; acc: 0.81
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.76; acc: 0.8
Batch: 220; loss: 0.42; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.45; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.75; acc: 0.81
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.86
Batch: 460; loss: 0.6; acc: 0.84
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.96; acc: 0.8
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.83
Batch: 620; loss: 0.65; acc: 0.83
Batch: 640; loss: 0.31; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.38694645891523666; val_accuracy: 0.881468949044586 

The current subspace-distance is: 0.00011428697325754911 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.54; acc: 0.88
Batch: 60; loss: 0.82; acc: 0.73
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.64; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.78
Batch: 300; loss: 0.29; acc: 0.86
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.49; acc: 0.88
Batch: 400; loss: 0.27; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.77
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.37; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.88
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.89; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.51; acc: 0.8
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.54; acc: 0.84
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.38647507173810036; val_accuracy: 0.8831608280254777 

The current subspace-distance is: 0.00011618435382843018 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.86
Batch: 80; loss: 0.36; acc: 0.83
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.49; acc: 0.77
Batch: 240; loss: 0.23; acc: 0.89
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.92
Batch: 340; loss: 0.57; acc: 0.81
Batch: 360; loss: 0.44; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.38; acc: 0.84
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.5; acc: 0.8
Batch: 500; loss: 0.49; acc: 0.84
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.86
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.43; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.84
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.65; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.54; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.37; acc: 0.81
Batch: 760; loss: 0.28; acc: 0.89
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.49; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.38605547368906107; val_accuracy: 0.8820660828025477 

The current subspace-distance is: 0.00011865915439557284 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.48; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.83
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.83
Batch: 460; loss: 0.88; acc: 0.7
Batch: 480; loss: 0.38; acc: 0.84
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.62; acc: 0.78
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.57; acc: 0.86
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.71; acc: 0.81
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.24; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.48; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.3887469414977511; val_accuracy: 0.8810708598726115 

The current subspace-distance is: 0.00012031232472509146 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.84
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.65; acc: 0.8
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.91
Batch: 360; loss: 0.41; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.5; acc: 0.84
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.52; acc: 0.86
Batch: 560; loss: 0.54; acc: 0.81
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.86
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.83
Batch: 760; loss: 0.53; acc: 0.83
Batch: 780; loss: 0.26; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.89
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.92
Val Epoch over. val_loss: 0.3873928135652451; val_accuracy: 0.8796775477707006 

The current subspace-distance is: 0.00012171400885563344 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.88
Batch: 20; loss: 0.15; acc: 0.97
Batch: 40; loss: 0.52; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.86
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.92
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.75; acc: 0.78
Batch: 300; loss: 0.41; acc: 0.81
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.86
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.51; acc: 0.81
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.59; acc: 0.81
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.83
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.49; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.3909166084163508; val_accuracy: 0.8815684713375797 

The current subspace-distance is: 0.00012211005378048867 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.82; acc: 0.8
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.41; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.57; acc: 0.8
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.58; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.86
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.63; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.83
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.38526484389214; val_accuracy: 0.8824641719745223 

The current subspace-distance is: 0.00012430739297997206 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.96; acc: 0.73
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.43; acc: 0.83
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.4; acc: 0.83
Batch: 220; loss: 0.42; acc: 0.83
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.45; acc: 0.89
Batch: 280; loss: 0.68; acc: 0.84
Batch: 300; loss: 0.54; acc: 0.83
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.63; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.78
Batch: 440; loss: 0.65; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.23; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.63; acc: 0.8
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.5; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.91
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.8
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.38727806309226215; val_accuracy: 0.8794785031847133 

The current subspace-distance is: 0.00012594145664479584 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.51; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.81
Batch: 60; loss: 0.3; acc: 0.84
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.84
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.84
Batch: 220; loss: 0.62; acc: 0.83
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.44; acc: 0.81
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.54; acc: 0.84
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.39; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.48; acc: 0.83
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.83
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.47; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.38562856951526775; val_accuracy: 0.8816679936305732 

The current subspace-distance is: 0.00012632885773200542 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.84
Batch: 180; loss: 0.26; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.63; acc: 0.81
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.86
Batch: 320; loss: 0.33; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.38522684104313515; val_accuracy: 0.882265127388535 

The current subspace-distance is: 0.0001274188980460167 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.48; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.38; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.68; acc: 0.84
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.56; acc: 0.83
Batch: 280; loss: 0.49; acc: 0.8
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.73; acc: 0.75
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.58; acc: 0.84
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.47; acc: 0.83
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3848189170573168; val_accuracy: 0.8812699044585988 

The current subspace-distance is: 0.00012900264118798077 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.58; acc: 0.89
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.91
Batch: 320; loss: 0.56; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.42; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.2; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.83
Batch: 600; loss: 0.5; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.88
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.6; acc: 0.84
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.32; acc: 0.84
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.81
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3857037173999343; val_accuracy: 0.8812699044585988 

The current subspace-distance is: 0.00012981865438632667 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.39; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.97
Batch: 220; loss: 0.47; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.36; acc: 0.84
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.8
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.83
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.6; acc: 0.77
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.83
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3848496423974918; val_accuracy: 0.881468949044586 

The current subspace-distance is: 0.00013255138765089214 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.59; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.84
Batch: 180; loss: 0.24; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.84
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.34; acc: 0.86
Batch: 260; loss: 0.46; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.72; acc: 0.84
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.61; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.83
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.6; acc: 0.83
Batch: 660; loss: 0.49; acc: 0.89
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.48; acc: 0.88
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.38460547249218463; val_accuracy: 0.8826632165605095 

The current subspace-distance is: 0.0001349873055005446 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.86
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.86; acc: 0.8
Batch: 180; loss: 0.56; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.94
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.46; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.73; acc: 0.78
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.81
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.42; acc: 0.88
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.34; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.38518338360983856; val_accuracy: 0.8823646496815286 

The current subspace-distance is: 0.0001357935689156875 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.31; acc: 0.84
Batch: 60; loss: 0.54; acc: 0.88
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.67; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.64; acc: 0.84
Batch: 200; loss: 0.43; acc: 0.8
Batch: 220; loss: 0.79; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.83
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.39; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.49; acc: 0.81
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.81
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.59; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.56; acc: 0.83
Batch: 560; loss: 0.45; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.91
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.52; acc: 0.83
Batch: 700; loss: 0.6; acc: 0.77
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.84
Batch: 760; loss: 0.32; acc: 0.86
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.38479169443914085; val_accuracy: 0.8827627388535032 

The current subspace-distance is: 0.00013734666572418064 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.5; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.12; acc: 1.0
Batch: 140; loss: 0.71; acc: 0.81
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.56; acc: 0.88
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.81
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.44; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.81
Batch: 580; loss: 0.79; acc: 0.81
Batch: 600; loss: 0.77; acc: 0.81
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.5; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.47; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3855035912933623; val_accuracy: 0.8816679936305732 

The current subspace-distance is: 0.0001392754929838702 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.83
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.95
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.69; acc: 0.8
Batch: 360; loss: 0.34; acc: 0.86
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.81
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.92
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.46; acc: 0.84
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.46; acc: 0.8
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.65; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.84
Batch: 780; loss: 0.73; acc: 0.83
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.46; acc: 0.91
Batch: 120; loss: 0.84; acc: 0.77
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.3848362947060804; val_accuracy: 0.8819665605095541 

The current subspace-distance is: 0.00014098525571171194 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_175_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 42223
elements in E: 8885200
fraction nonzero: 0.004752059604735966
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.05
Batch: 80; loss: 2.3; acc: 0.11
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.08
Batch: 140; loss: 2.29; acc: 0.12
Batch: 160; loss: 2.28; acc: 0.16
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.29; acc: 0.05
Batch: 220; loss: 2.28; acc: 0.14
Batch: 240; loss: 2.27; acc: 0.19
Batch: 260; loss: 2.27; acc: 0.17
Batch: 280; loss: 2.26; acc: 0.2
Batch: 300; loss: 2.24; acc: 0.33
Batch: 320; loss: 2.26; acc: 0.22
Batch: 340; loss: 2.25; acc: 0.23
Batch: 360; loss: 2.23; acc: 0.39
Batch: 380; loss: 2.21; acc: 0.33
Batch: 400; loss: 2.22; acc: 0.22
Batch: 420; loss: 2.18; acc: 0.36
Batch: 440; loss: 2.11; acc: 0.42
Batch: 460; loss: 2.08; acc: 0.38
Batch: 480; loss: 2.06; acc: 0.28
Batch: 500; loss: 2.0; acc: 0.3
Batch: 520; loss: 1.84; acc: 0.36
Batch: 540; loss: 1.65; acc: 0.44
Batch: 560; loss: 1.4; acc: 0.61
Batch: 580; loss: 1.41; acc: 0.53
Batch: 600; loss: 1.14; acc: 0.7
Batch: 620; loss: 1.09; acc: 0.69
Batch: 640; loss: 1.08; acc: 0.66
Batch: 660; loss: 0.97; acc: 0.69
Batch: 680; loss: 1.49; acc: 0.53
Batch: 700; loss: 1.09; acc: 0.62
Batch: 720; loss: 0.56; acc: 0.86
Batch: 740; loss: 0.98; acc: 0.7
Batch: 760; loss: 0.77; acc: 0.73
Batch: 780; loss: 0.8; acc: 0.7
Train Epoch over. train_loss: 1.86; train_accuracy: 0.36 

Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 0.82; acc: 0.7
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.85; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.62; acc: 0.83
Batch: 120; loss: 0.9; acc: 0.69
Batch: 140; loss: 0.52; acc: 0.83
Val Epoch over. val_loss: 0.7900443880041693; val_accuracy: 0.7390525477707006 

The current subspace-distance is: 1.0099961400555912e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.95; acc: 0.73
Batch: 20; loss: 0.77; acc: 0.77
Batch: 40; loss: 1.23; acc: 0.69
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.79; acc: 0.73
Batch: 100; loss: 0.78; acc: 0.77
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.89; acc: 0.69
Batch: 160; loss: 0.71; acc: 0.77
Batch: 180; loss: 0.78; acc: 0.81
Batch: 200; loss: 0.66; acc: 0.8
Batch: 220; loss: 0.72; acc: 0.84
Batch: 240; loss: 0.93; acc: 0.67
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.79; acc: 0.83
Batch: 300; loss: 0.63; acc: 0.78
Batch: 320; loss: 0.49; acc: 0.8
Batch: 340; loss: 0.6; acc: 0.81
Batch: 360; loss: 0.8; acc: 0.78
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.57; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.59; acc: 0.86
Batch: 460; loss: 0.62; acc: 0.73
Batch: 480; loss: 0.84; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.75
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.63; acc: 0.81
Batch: 600; loss: 0.51; acc: 0.83
Batch: 620; loss: 0.59; acc: 0.83
Batch: 640; loss: 0.45; acc: 0.88
Batch: 660; loss: 0.64; acc: 0.78
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.65; acc: 0.75
Batch: 720; loss: 0.67; acc: 0.77
Batch: 740; loss: 0.58; acc: 0.78
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.64; train_accuracy: 0.8 

Batch: 0; loss: 0.64; acc: 0.72
Batch: 20; loss: 0.54; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.83
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.58; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.73
Batch: 140; loss: 0.31; acc: 0.88
Val Epoch over. val_loss: 0.49384198048312195; val_accuracy: 0.8423566878980892 

The current subspace-distance is: 1.961696398211643e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.56; acc: 0.78
Batch: 20; loss: 0.48; acc: 0.83
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.8
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.63; acc: 0.75
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.41; acc: 0.89
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.61; acc: 0.81
Batch: 200; loss: 0.56; acc: 0.84
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.5; acc: 0.78
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.51; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.64; acc: 0.78
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.53; acc: 0.84
Batch: 400; loss: 0.42; acc: 0.86
Batch: 420; loss: 0.58; acc: 0.84
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.47; acc: 0.81
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.83
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.62; acc: 0.8
Batch: 580; loss: 0.4; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.6; acc: 0.83
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.76; acc: 0.8
Batch: 680; loss: 0.64; acc: 0.78
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.8; acc: 0.81
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 0.93; acc: 0.69
Batch: 20; loss: 0.79; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.78
Batch: 60; loss: 0.88; acc: 0.72
Batch: 80; loss: 0.9; acc: 0.73
Batch: 100; loss: 0.78; acc: 0.8
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.38; acc: 0.86
Val Epoch over. val_loss: 0.83082201108811; val_accuracy: 0.7508957006369427 

The current subspace-distance is: 2.6479270673007704e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.8
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.48; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.52; acc: 0.77
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.81
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.45; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.86
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.84
Batch: 400; loss: 0.5; acc: 0.84
Batch: 420; loss: 0.65; acc: 0.78
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.51; acc: 0.78
Batch: 480; loss: 0.46; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.33; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.55; acc: 0.86
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.55; acc: 0.8
Batch: 680; loss: 0.54; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.52; acc: 0.84
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.43; train_accuracy: 0.86 

Batch: 0; loss: 0.93; acc: 0.72
Batch: 20; loss: 0.86; acc: 0.75
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 1.14; acc: 0.69
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.52; acc: 0.77
Val Epoch over. val_loss: 0.7761828639325062; val_accuracy: 0.7538813694267515 

The current subspace-distance is: 3.261934034526348e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.99; acc: 0.69
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.94
Batch: 100; loss: 0.58; acc: 0.84
Batch: 120; loss: 0.58; acc: 0.81
Batch: 140; loss: 0.29; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.86
Batch: 200; loss: 0.45; acc: 0.83
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.56; acc: 0.86
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.83
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.86
Batch: 500; loss: 0.61; acc: 0.83
Batch: 520; loss: 0.22; acc: 0.91
Batch: 540; loss: 0.41; acc: 0.84
Batch: 560; loss: 0.54; acc: 0.84
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.5; acc: 0.83
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.76; acc: 0.77
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.83
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.81
Batch: 120; loss: 0.75; acc: 0.73
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.48185888431064644; val_accuracy: 0.8425557324840764 

The current subspace-distance is: 3.825106250587851e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.81
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.74; acc: 0.75
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.65; acc: 0.84
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.37; acc: 0.83
Batch: 180; loss: 0.5; acc: 0.88
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.86
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.73; acc: 0.75
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.56; acc: 0.84
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.43; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.89
Batch: 740; loss: 0.55; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.44; acc: 0.89
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.56; acc: 0.84
Batch: 20; loss: 0.63; acc: 0.83
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.74; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.88
Batch: 100; loss: 0.53; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.78
Batch: 140; loss: 0.26; acc: 0.89
Val Epoch over. val_loss: 0.5283824947609264; val_accuracy: 0.8344944267515924 

The current subspace-distance is: 4.2909494368359447e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.69
Batch: 20; loss: 0.46; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.84
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.47; acc: 0.89
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.47; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.92
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.45; acc: 0.81
Batch: 380; loss: 0.39; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.83
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.34; acc: 0.88
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.61; acc: 0.83
Batch: 640; loss: 0.25; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.48; acc: 0.78
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.94
Val Epoch over. val_loss: 0.3708061216410938; val_accuracy: 0.882265127388535 

The current subspace-distance is: 4.777331560035236e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.83
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.4; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.43; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.94
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.67; acc: 0.77
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.8
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.38; acc: 0.86
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.91
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.49; acc: 0.88
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.51; acc: 0.84
Batch: 760; loss: 0.54; acc: 0.81
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.45; acc: 0.83
Batch: 20; loss: 0.8; acc: 0.78
Batch: 40; loss: 0.16; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.4580531478355265; val_accuracy: 0.8560907643312102 

The current subspace-distance is: 5.1082693971693516e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.55; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.56; acc: 0.84
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.86
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.8
Batch: 340; loss: 0.42; acc: 0.81
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.68; acc: 0.7
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.83
Batch: 540; loss: 0.49; acc: 0.8
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.45; acc: 0.83
Batch: 620; loss: 0.45; acc: 0.84
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.38; acc: 0.83
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.35469202189498644; val_accuracy: 0.89171974522293 

The current subspace-distance is: 5.5165808589663357e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.62; acc: 0.78
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.48; acc: 0.84
Batch: 80; loss: 0.47; acc: 0.84
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.83; acc: 0.77
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.25; acc: 0.89
Batch: 320; loss: 0.65; acc: 0.77
Batch: 340; loss: 0.71; acc: 0.86
Batch: 360; loss: 0.91; acc: 0.77
Batch: 380; loss: 0.42; acc: 0.84
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.57; acc: 0.78
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.51; acc: 0.86
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.5; acc: 0.84
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 0.2; acc: 0.97
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3950156522974087; val_accuracy: 0.8789808917197452 

The current subspace-distance is: 5.8943442127201706e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.59; acc: 0.84
Batch: 340; loss: 0.4; acc: 0.83
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.46; acc: 0.81
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.36; acc: 0.84
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.81
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.339503258205713; val_accuracy: 0.897093949044586 

The current subspace-distance is: 6.212687731022015e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.4; acc: 0.84
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.83
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.95
Batch: 360; loss: 0.51; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.45; acc: 0.92
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.61; acc: 0.81
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.41; acc: 0.81
Batch: 640; loss: 0.42; acc: 0.92
Batch: 660; loss: 0.23; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.22; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3486131308896906; val_accuracy: 0.8947054140127388 

The current subspace-distance is: 6.492207467090338e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.91
Batch: 220; loss: 0.5; acc: 0.8
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.45; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.59; acc: 0.84
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.45; acc: 0.83
Batch: 480; loss: 0.48; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.5; acc: 0.88
Batch: 540; loss: 0.41; acc: 0.86
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.89
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.51; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.51; acc: 0.89
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3361761884135046; val_accuracy: 0.8991839171974523 

The current subspace-distance is: 6.76703784847632e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.83
Batch: 100; loss: 0.46; acc: 0.89
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.17; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.14; acc: 1.0
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.95
Batch: 340; loss: 0.55; acc: 0.81
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.53; acc: 0.86
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.54; acc: 0.81
Batch: 540; loss: 0.25; acc: 0.88
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.89
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3160168664755335; val_accuracy: 0.9028662420382165 

The current subspace-distance is: 7.009568071225658e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.45; acc: 0.91
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.83
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.25; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.84
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.41; acc: 0.84
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.47; acc: 0.81
Batch: 560; loss: 0.16; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.48; acc: 0.81
Batch: 720; loss: 0.46; acc: 0.89
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.51; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.36; acc: 0.86
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.33009179524934973; val_accuracy: 0.8987858280254777 

The current subspace-distance is: 7.348658982664347e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.26; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.31; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.57; acc: 0.86
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.39; acc: 0.83
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.22; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.8
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.64; acc: 0.86
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.17; acc: 0.97
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.5; acc: 0.8
Batch: 600; loss: 0.54; acc: 0.81
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.6; acc: 0.83
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.35; acc: 0.89
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.97
Batch: 20; loss: 0.52; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.5; acc: 0.78
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3716173512730629; val_accuracy: 0.8844546178343949 

The current subspace-distance is: 7.706833275733516e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.17; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.89
Batch: 160; loss: 0.66; acc: 0.78
Batch: 180; loss: 0.33; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.83
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.83
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.3; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.81
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.29; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.321994059262382; val_accuracy: 0.901671974522293 

The current subspace-distance is: 7.902814832050353e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.52; acc: 0.8
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.56; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.89
Batch: 240; loss: 0.22; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.91
Batch: 320; loss: 0.51; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.21; acc: 0.95
Batch: 460; loss: 0.6; acc: 0.78
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.44; acc: 0.81
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.41; acc: 0.84
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.77; acc: 0.81
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.33656143081510903; val_accuracy: 0.896297770700637 

The current subspace-distance is: 8.13583392300643e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.49; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.51; acc: 0.8
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.86
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.91
Batch: 520; loss: 0.5; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.91
Batch: 580; loss: 0.55; acc: 0.77
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.34; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.2; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.09; acc: 1.0
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.33693264842413034; val_accuracy: 0.8967953821656051 

The current subspace-distance is: 8.314002479892224e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.88
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.84
Batch: 240; loss: 0.49; acc: 0.86
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.49; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.88
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.97
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.3; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.31978369371336735; val_accuracy: 0.9029657643312102 

The current subspace-distance is: 8.582041482441127e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.39; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.89
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.31; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.42; acc: 0.92
Batch: 680; loss: 0.55; acc: 0.86
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.94
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.30839395698658223; val_accuracy: 0.9060509554140127 

The current subspace-distance is: 8.787088154349476e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.86
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.62; acc: 0.83
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.97
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.5; acc: 0.84
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.5; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.97
Batch: 460; loss: 0.46; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.51; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.49; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3116975146683918; val_accuracy: 0.9046576433121019 

The current subspace-distance is: 8.984186570160091e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.65; acc: 0.81
Batch: 60; loss: 0.15; acc: 0.98
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.55; acc: 0.86
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.46; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.42; acc: 0.84
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.34; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.8
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.17; acc: 0.95
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.97
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.39; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.36; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.35; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.31937636712649065; val_accuracy: 0.9027667197452229 

The current subspace-distance is: 9.209488052874804e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.89
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.58; acc: 0.83
Batch: 200; loss: 0.39; acc: 0.86
Batch: 220; loss: 0.36; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.83
Batch: 260; loss: 0.28; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.83
Batch: 300; loss: 0.31; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.36; acc: 0.92
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.42; acc: 0.84
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.4; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.58; acc: 0.83
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.30676452155895295; val_accuracy: 0.9064490445859873 

The current subspace-distance is: 9.444356692256406e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.86
Batch: 200; loss: 0.53; acc: 0.89
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.55; acc: 0.84
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.86; acc: 0.75
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.83
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.84
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.97
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.91
Batch: 740; loss: 0.57; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.31596373586328164; val_accuracy: 0.9027667197452229 

The current subspace-distance is: 9.67568121268414e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.54; acc: 0.83
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.57; acc: 0.88
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.53; acc: 0.81
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.32; acc: 0.86
Batch: 500; loss: 0.45; acc: 0.78
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.29; acc: 0.94
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.31151244609029427; val_accuracy: 0.9075437898089171 

The current subspace-distance is: 9.880265861283988e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.8
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.33; acc: 0.88
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.55; acc: 0.8
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.62; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.49; acc: 0.81
Batch: 540; loss: 0.18; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.36; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.81
Batch: 620; loss: 0.43; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.4; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.67; acc: 0.84
Batch: 740; loss: 0.24; acc: 0.95
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.19; acc: 0.97
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3093526764375389; val_accuracy: 0.9068471337579618 

The current subspace-distance is: 0.00010125119297299534 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.91
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.84
Batch: 400; loss: 0.25; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.92
Batch: 520; loss: 0.35; acc: 0.94
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.83
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.38; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.48; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3145243652212392; val_accuracy: 0.9042595541401274 

The current subspace-distance is: 0.00010336792911402881 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.84
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.44; acc: 0.83
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.54; acc: 0.83
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.29; acc: 0.92
Batch: 240; loss: 0.18; acc: 0.98
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.64; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.92
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.88
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.88
Batch: 780; loss: 0.64; acc: 0.81
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3096701041053814; val_accuracy: 0.9071457006369427 

The current subspace-distance is: 0.00010527172707952559 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.81
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.52; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.17; acc: 0.95
Batch: 260; loss: 0.32; acc: 0.94
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.4; acc: 0.83
Batch: 320; loss: 0.29; acc: 0.86
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.91
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.97
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.92
Batch: 560; loss: 0.51; acc: 0.8
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.97
Batch: 680; loss: 0.51; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.42; acc: 0.84
Batch: 780; loss: 0.2; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.31680904072561084; val_accuracy: 0.9029657643312102 

The current subspace-distance is: 0.00010761436715256423 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.47; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.84
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.83
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.14; acc: 0.98
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.15; acc: 0.95
Batch: 260; loss: 0.52; acc: 0.84
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.54; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.58; acc: 0.83
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.42; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.92
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.38; acc: 0.83
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.44; acc: 0.86
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.4; acc: 0.92
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.30582970703483386; val_accuracy: 0.9054538216560509 

The current subspace-distance is: 0.00010873053543036804 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.51; acc: 0.84
Batch: 180; loss: 0.42; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.51; acc: 0.83
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.24; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.29; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.94
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.24; acc: 0.95
Batch: 780; loss: 0.44; acc: 0.83
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.30818102333196407; val_accuracy: 0.9052547770700637 

The current subspace-distance is: 0.00011062213889090344 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.45; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.46; acc: 0.88
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.4; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.91
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.67; acc: 0.83
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.38; acc: 0.83
Batch: 600; loss: 0.42; acc: 0.81
Batch: 620; loss: 0.43; acc: 0.92
Batch: 640; loss: 0.28; acc: 0.86
Batch: 660; loss: 0.31; acc: 0.89
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.21; acc: 0.92
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.30595814999026855; val_accuracy: 0.9057523885350318 

The current subspace-distance is: 0.00011211805394850671 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.92
Batch: 40; loss: 0.53; acc: 0.89
Batch: 60; loss: 0.73; acc: 0.81
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.74; acc: 0.83
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.89
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.88
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.57; acc: 0.83
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3076445561400644; val_accuracy: 0.9040605095541401 

The current subspace-distance is: 0.00011302671919111162 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.21; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.86
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.31; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.97
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.86
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.38; acc: 0.91
Batch: 660; loss: 0.65; acc: 0.83
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.92
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.14; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3061215674896149; val_accuracy: 0.9067476114649682 

The current subspace-distance is: 0.00011485295544844121 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.61; acc: 0.8
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.91
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.63; acc: 0.78
Batch: 460; loss: 0.67; acc: 0.8
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.54; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.49; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.44; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3041413159696919; val_accuracy: 0.908140923566879 

The current subspace-distance is: 0.00011694058775901794 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.83
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.46; acc: 0.81
Batch: 300; loss: 0.57; acc: 0.83
Batch: 320; loss: 0.19; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.51; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.84
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.83
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.2; acc: 0.97
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3090708356374388; val_accuracy: 0.90515525477707 

The current subspace-distance is: 0.00011859474761877209 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.25; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.29; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.72; acc: 0.84
Batch: 300; loss: 0.43; acc: 0.84
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.27; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.46; acc: 0.88
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.3; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.84
Batch: 760; loss: 0.45; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3072211404040361; val_accuracy: 0.9049562101910829 

The current subspace-distance is: 0.00012025164323858917 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.72; acc: 0.73
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.47; acc: 0.81
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.43; acc: 0.83
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.34; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.16; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3049935986092136; val_accuracy: 0.9072452229299363 

The current subspace-distance is: 0.00012092500401195139 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.39; acc: 0.86
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.76; acc: 0.83
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.86
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.26; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.42; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3061452243643202; val_accuracy: 0.9067476114649682 

The current subspace-distance is: 0.0001239584817085415 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.45; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.24; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.46; acc: 0.89
Batch: 220; loss: 0.59; acc: 0.86
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.89
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.44; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.88
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.84
Batch: 540; loss: 0.37; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.44; acc: 0.83
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.18; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3052563280056996; val_accuracy: 0.9060509554140127 

The current subspace-distance is: 0.00012610724661499262 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.58; acc: 0.83
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.97
Batch: 400; loss: 0.4; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.94
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.48; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.91
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.86
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3060948790353575; val_accuracy: 0.9053542993630573 

The current subspace-distance is: 0.00012761734251398593 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.28; acc: 0.89
Batch: 160; loss: 0.3; acc: 0.92
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.64; acc: 0.88
Batch: 220; loss: 0.48; acc: 0.86
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.48; acc: 0.91
Batch: 360; loss: 0.69; acc: 0.81
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.78
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.84
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.13; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.81
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3061660951964415; val_accuracy: 0.9052547770700637 

The current subspace-distance is: 0.0001282662124140188 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.83
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.94
Batch: 140; loss: 0.18; acc: 0.97
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.63; acc: 0.88
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.89
Batch: 320; loss: 0.5; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.81
Batch: 380; loss: 0.6; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.84
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.35; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.91
Batch: 660; loss: 0.68; acc: 0.83
Batch: 680; loss: 0.43; acc: 0.83
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.30619229433263184; val_accuracy: 0.9056528662420382 

The current subspace-distance is: 0.000129285515868105 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.44; acc: 0.86
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.86
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.41; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.88
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.3; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.31; acc: 0.88
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3044538467553011; val_accuracy: 0.908140923566879 

The current subspace-distance is: 0.00013193309132475406 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.83
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.4; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.45; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.86
Batch: 580; loss: 0.48; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.45; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.52; acc: 0.83
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.55; acc: 0.86
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3048735274725659; val_accuracy: 0.9077428343949044 

The current subspace-distance is: 0.00013281266728881747 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.66; acc: 0.81
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.63; acc: 0.83
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.5; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.36; acc: 0.92
Batch: 400; loss: 0.41; acc: 0.83
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.48; acc: 0.83
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.38; acc: 0.89
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.89
Batch: 680; loss: 0.45; acc: 0.86
Batch: 700; loss: 0.34; acc: 0.89
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3053198564489176; val_accuracy: 0.9072452229299363 

The current subspace-distance is: 0.000134894551592879 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.92
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.53; acc: 0.88
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.5; acc: 0.8
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.52; acc: 0.89
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.45; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.48; acc: 0.83
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.91
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.35; acc: 0.91
Batch: 580; loss: 0.28; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.39; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.89
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.47; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.19; acc: 0.92
Batch: 760; loss: 0.28; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.95
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3044515660708877; val_accuracy: 0.9069466560509554 

The current subspace-distance is: 0.00013633350317832083 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.5; acc: 0.86
Batch: 160; loss: 0.12; acc: 0.95
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.81
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.47; acc: 0.86
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.86
Batch: 420; loss: 0.27; acc: 0.95
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.11; acc: 1.0
Batch: 500; loss: 0.44; acc: 0.8
Batch: 520; loss: 0.53; acc: 0.8
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.48; acc: 0.83
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.62; acc: 0.83
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.1; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.30471483262101556; val_accuracy: 0.9074442675159236 

The current subspace-distance is: 0.00013683660654351115 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.43; acc: 0.81
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.28; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.21; acc: 0.88
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.42; acc: 0.92
Batch: 340; loss: 0.5; acc: 0.81
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.52; acc: 0.89
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.47; acc: 0.89
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.84
Batch: 780; loss: 0.46; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.305153926846328; val_accuracy: 0.9072452229299363 

The current subspace-distance is: 0.00013847986701875925 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 44376
elements in E: 9329460
fraction nonzero: 0.004756545394910316
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.31; acc: 0.06
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.12
Batch: 160; loss: 2.28; acc: 0.12
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.28; acc: 0.12
Batch: 220; loss: 2.28; acc: 0.14
Batch: 240; loss: 2.26; acc: 0.2
Batch: 260; loss: 2.25; acc: 0.23
Batch: 280; loss: 2.26; acc: 0.27
Batch: 300; loss: 2.22; acc: 0.36
Batch: 320; loss: 2.23; acc: 0.19
Batch: 340; loss: 2.24; acc: 0.06
Batch: 360; loss: 2.15; acc: 0.34
Batch: 380; loss: 2.12; acc: 0.36
Batch: 400; loss: 2.13; acc: 0.23
Batch: 420; loss: 1.99; acc: 0.42
Batch: 440; loss: 1.77; acc: 0.58
Batch: 460; loss: 1.61; acc: 0.56
Batch: 480; loss: 1.37; acc: 0.58
Batch: 500; loss: 1.06; acc: 0.58
Batch: 520; loss: 0.79; acc: 0.73
Batch: 540; loss: 0.91; acc: 0.73
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.86; acc: 0.73
Batch: 600; loss: 0.63; acc: 0.84
Batch: 620; loss: 0.71; acc: 0.77
Batch: 640; loss: 0.96; acc: 0.69
Batch: 660; loss: 0.77; acc: 0.72
Batch: 680; loss: 1.02; acc: 0.64
Batch: 700; loss: 0.92; acc: 0.77
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.88; acc: 0.72
Batch: 760; loss: 0.68; acc: 0.78
Batch: 780; loss: 1.12; acc: 0.69
Train Epoch over. train_loss: 1.67; train_accuracy: 0.42 

Batch: 0; loss: 0.7; acc: 0.75
Batch: 20; loss: 0.96; acc: 0.7
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.72; acc: 0.75
Batch: 80; loss: 0.53; acc: 0.81
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 1.14; acc: 0.62
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.6161143744637252; val_accuracy: 0.800656847133758 

The current subspace-distance is: 1.1525232366693672e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.75
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.97; acc: 0.72
Batch: 60; loss: 0.47; acc: 0.83
Batch: 80; loss: 0.96; acc: 0.72
Batch: 100; loss: 0.75; acc: 0.73
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.57; acc: 0.8
Batch: 180; loss: 0.91; acc: 0.75
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.64; acc: 0.8
Batch: 240; loss: 0.86; acc: 0.81
Batch: 260; loss: 0.75; acc: 0.75
Batch: 280; loss: 0.72; acc: 0.75
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.8
Batch: 360; loss: 0.64; acc: 0.73
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.69; acc: 0.78
Batch: 500; loss: 0.76; acc: 0.75
Batch: 520; loss: 0.67; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.65; acc: 0.81
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.75; acc: 0.77
Batch: 620; loss: 0.52; acc: 0.8
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.54; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.77
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.5; acc: 0.89
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.58; train_accuracy: 0.82 

Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.78; acc: 0.75
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.83
Batch: 120; loss: 0.7; acc: 0.8
Batch: 140; loss: 0.4; acc: 0.83
Val Epoch over. val_loss: 0.6272160523826149; val_accuracy: 0.7916003184713376 

The current subspace-distance is: 2.0053037587786093e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.91
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.54; acc: 0.8
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.8
Batch: 180; loss: 0.61; acc: 0.78
Batch: 200; loss: 0.63; acc: 0.86
Batch: 220; loss: 0.72; acc: 0.83
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.47; acc: 0.84
Batch: 280; loss: 0.23; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.86
Batch: 320; loss: 0.44; acc: 0.86
Batch: 340; loss: 0.61; acc: 0.78
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.47; acc: 0.77
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.81
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.58; acc: 0.88
Batch: 480; loss: 0.37; acc: 0.89
Batch: 500; loss: 0.48; acc: 0.89
Batch: 520; loss: 0.53; acc: 0.77
Batch: 540; loss: 0.41; acc: 0.95
Batch: 560; loss: 0.53; acc: 0.83
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.49; acc: 0.8
Batch: 660; loss: 0.85; acc: 0.7
Batch: 680; loss: 0.43; acc: 0.81
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.76; acc: 0.78
Batch: 740; loss: 0.85; acc: 0.73
Batch: 760; loss: 0.52; acc: 0.83
Batch: 780; loss: 0.51; acc: 0.81
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.8; acc: 0.77
Batch: 20; loss: 0.69; acc: 0.75
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.59; acc: 0.77
Batch: 80; loss: 0.49; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.88; acc: 0.72
Batch: 140; loss: 0.22; acc: 0.97
Val Epoch over. val_loss: 0.5348767387639185; val_accuracy: 0.8295183121019108 

The current subspace-distance is: 2.7305137336952612e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.65; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.78
Batch: 100; loss: 0.79; acc: 0.72
Batch: 120; loss: 0.7; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.8
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.37; acc: 0.86
Batch: 280; loss: 0.71; acc: 0.83
Batch: 300; loss: 0.49; acc: 0.83
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.7; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.89
Batch: 420; loss: 0.57; acc: 0.83
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.76; acc: 0.75
Batch: 480; loss: 0.63; acc: 0.81
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.92; acc: 0.73
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.92
Batch: 620; loss: 0.62; acc: 0.81
Batch: 640; loss: 0.85; acc: 0.73
Batch: 660; loss: 0.63; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.46; acc: 0.84
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.5; train_accuracy: 0.85 

Batch: 0; loss: 0.92; acc: 0.69
Batch: 20; loss: 0.9; acc: 0.7
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 1.1; acc: 0.7
Batch: 80; loss: 0.58; acc: 0.8
Batch: 100; loss: 0.78; acc: 0.75
Batch: 120; loss: 1.0; acc: 0.69
Batch: 140; loss: 0.76; acc: 0.75
Val Epoch over. val_loss: 0.7299938184820163; val_accuracy: 0.7586584394904459 

The current subspace-distance is: 3.3218828320968896e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.08; acc: 0.61
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.74; acc: 0.78
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.69; acc: 0.75
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.62; acc: 0.8
Batch: 160; loss: 0.59; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.53; acc: 0.81
Batch: 300; loss: 0.81; acc: 0.8
Batch: 320; loss: 0.51; acc: 0.81
Batch: 340; loss: 0.45; acc: 0.89
Batch: 360; loss: 0.56; acc: 0.78
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.67; acc: 0.77
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.64; acc: 0.83
Batch: 500; loss: 0.63; acc: 0.78
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.45; acc: 0.86
Batch: 560; loss: 0.8; acc: 0.75
Batch: 580; loss: 0.7; acc: 0.84
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.61; acc: 0.81
Batch: 660; loss: 0.67; acc: 0.78
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 1.19; acc: 0.66
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 1.52; acc: 0.59
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.87; acc: 0.72
Batch: 80; loss: 1.02; acc: 0.72
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 1.29; acc: 0.66
Batch: 140; loss: 0.59; acc: 0.8
Val Epoch over. val_loss: 0.9353977257658721; val_accuracy: 0.7201433121019108 

The current subspace-distance is: 3.880261647282168e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 1.14; acc: 0.7
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 1.0; acc: 0.78
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.41; acc: 0.88
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.53; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.51; acc: 0.8
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.56; acc: 0.83
Batch: 260; loss: 0.57; acc: 0.83
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.55; acc: 0.86
Batch: 360; loss: 0.48; acc: 0.84
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.89; acc: 0.73
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.52; acc: 0.88
Batch: 500; loss: 0.18; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.89
Batch: 540; loss: 0.74; acc: 0.81
Batch: 560; loss: 0.63; acc: 0.86
Batch: 580; loss: 0.51; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.84
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.34; acc: 0.91
Batch: 680; loss: 0.58; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.86
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.64; acc: 0.83
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.81
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.16; acc: 0.64
Batch: 40; loss: 0.51; acc: 0.78
Batch: 60; loss: 0.99; acc: 0.7
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.73; acc: 0.8
Batch: 120; loss: 1.4; acc: 0.66
Batch: 140; loss: 1.01; acc: 0.72
Val Epoch over. val_loss: 0.922886432925607; val_accuracy: 0.7110867834394905 

The current subspace-distance is: 4.3097592424601316e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.69
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.49; acc: 0.8
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.51; acc: 0.84
Batch: 160; loss: 0.6; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.73
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.66; acc: 0.75
Batch: 360; loss: 0.49; acc: 0.81
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.54; acc: 0.84
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.47; acc: 0.86
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.49; acc: 0.83
Batch: 560; loss: 0.28; acc: 0.88
Batch: 580; loss: 0.42; acc: 0.83
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.38; acc: 0.89
Batch: 660; loss: 0.49; acc: 0.81
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.44; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.86
Batch: 740; loss: 0.44; acc: 0.84
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.8
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.43; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.383472977834902; val_accuracy: 0.8771894904458599 

The current subspace-distance is: 4.715684553957544e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.7; acc: 0.83
Batch: 40; loss: 0.65; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.55; acc: 0.83
Batch: 200; loss: 0.38; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.49; acc: 0.8
Batch: 280; loss: 0.44; acc: 0.88
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.5; acc: 0.83
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.5; acc: 0.8
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.84; acc: 0.77
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.81; acc: 0.77
Batch: 520; loss: 0.47; acc: 0.81
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.15; acc: 0.98
Batch: 640; loss: 0.54; acc: 0.8
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.49; acc: 0.83
Batch: 720; loss: 0.56; acc: 0.77
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.54; acc: 0.84
Batch: 780; loss: 0.41; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 2.27; acc: 0.48
Batch: 20; loss: 2.96; acc: 0.44
Batch: 40; loss: 1.38; acc: 0.61
Batch: 60; loss: 1.58; acc: 0.52
Batch: 80; loss: 2.3; acc: 0.5
Batch: 100; loss: 1.52; acc: 0.58
Batch: 120; loss: 2.6; acc: 0.47
Batch: 140; loss: 2.22; acc: 0.5
Val Epoch over. val_loss: 2.1061088218810453; val_accuracy: 0.5084593949044586 

The current subspace-distance is: 4.954127507517114e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 2.48; acc: 0.41
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.71; acc: 0.72
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.69; acc: 0.8
Batch: 160; loss: 0.17; acc: 0.97
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.84
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.55; acc: 0.89
Batch: 280; loss: 0.3; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.32; acc: 0.91
Batch: 340; loss: 0.67; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.44; acc: 0.81
Batch: 400; loss: 0.73; acc: 0.78
Batch: 420; loss: 0.54; acc: 0.75
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.49; acc: 0.78
Batch: 540; loss: 0.83; acc: 0.73
Batch: 560; loss: 0.49; acc: 0.8
Batch: 580; loss: 0.48; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.84
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.75; acc: 0.78
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.86
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.39; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.73; acc: 0.72
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.75; acc: 0.73
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.54; acc: 0.86
Batch: 120; loss: 0.94; acc: 0.72
Batch: 140; loss: 0.32; acc: 0.89
Val Epoch over. val_loss: 0.4833859437780016; val_accuracy: 0.8457404458598726 

The current subspace-distance is: 5.268979293759912e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.83
Batch: 40; loss: 0.71; acc: 0.75
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.63; acc: 0.75
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.47; acc: 0.8
Batch: 180; loss: 0.58; acc: 0.78
Batch: 200; loss: 0.24; acc: 0.86
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.56; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.65; acc: 0.8
Batch: 340; loss: 0.68; acc: 0.83
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.21; acc: 0.97
Batch: 520; loss: 0.68; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.45; acc: 0.86
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.39; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.88
Batch: 660; loss: 0.55; acc: 0.81
Batch: 680; loss: 0.46; acc: 0.8
Batch: 700; loss: 0.34; acc: 0.81
Batch: 720; loss: 0.52; acc: 0.73
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.36; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.9; acc: 0.67
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.96; acc: 0.77
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.42834999379079053; val_accuracy: 0.8634554140127388 

The current subspace-distance is: 5.699713801732287e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.59; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.84
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.83
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.3; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.52; acc: 0.84
Batch: 420; loss: 0.67; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.47; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.86
Batch: 560; loss: 0.49; acc: 0.77
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.59; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.84
Batch: 700; loss: 0.33; acc: 0.88
Batch: 720; loss: 0.43; acc: 0.83
Batch: 740; loss: 0.34; acc: 0.83
Batch: 760; loss: 0.51; acc: 0.83
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.81
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.3712298771378341; val_accuracy: 0.8873407643312102 

The current subspace-distance is: 6.04357774136588e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.44; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.46; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.55; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.83
Batch: 340; loss: 0.47; acc: 0.83
Batch: 360; loss: 0.45; acc: 0.84
Batch: 380; loss: 0.46; acc: 0.88
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.95
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.39; acc: 0.89
Batch: 560; loss: 0.79; acc: 0.83
Batch: 580; loss: 0.42; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.81
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.77; acc: 0.84
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.58; acc: 0.8
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.86; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3768163883382348; val_accuracy: 0.8832603503184714 

The current subspace-distance is: 6.385830783983693e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.16; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.88
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.81
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.15; acc: 0.97
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.47; acc: 0.86
Batch: 420; loss: 0.24; acc: 0.95
Batch: 440; loss: 0.41; acc: 0.83
Batch: 460; loss: 0.69; acc: 0.83
Batch: 480; loss: 0.35; acc: 0.83
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.57; acc: 0.83
Batch: 540; loss: 0.46; acc: 0.83
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.86
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.68; acc: 0.88
Batch: 740; loss: 0.48; acc: 0.86
Batch: 760; loss: 0.64; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.83
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.52; acc: 0.78
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.39034893510827595; val_accuracy: 0.877687101910828 

The current subspace-distance is: 6.700758240185678e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.39; acc: 0.86
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.49; acc: 0.8
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.84
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.48; acc: 0.89
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.58; acc: 0.78
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.47; acc: 0.81
Batch: 460; loss: 0.55; acc: 0.89
Batch: 480; loss: 0.46; acc: 0.81
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.86
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.48; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.7
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.40959081295759053; val_accuracy: 0.8735071656050956 

The current subspace-distance is: 6.915340054547414e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.56; acc: 0.83
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.44; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.5; acc: 0.84
Batch: 440; loss: 0.27; acc: 0.88
Batch: 460; loss: 0.38; acc: 0.84
Batch: 480; loss: 0.38; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.84
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.86
Batch: 600; loss: 0.41; acc: 0.91
Batch: 620; loss: 0.54; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.91
Batch: 700; loss: 0.46; acc: 0.8
Batch: 720; loss: 0.5; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.83
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.55; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.81
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.21; acc: 0.97
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3643073651488799; val_accuracy: 0.8872412420382165 

The current subspace-distance is: 7.175084465416148e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.26; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.22; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.64; acc: 0.83
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.57; acc: 0.81
Batch: 280; loss: 0.63; acc: 0.8
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.66; acc: 0.81
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.88
Batch: 600; loss: 0.65; acc: 0.83
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.5; acc: 0.83
Batch: 680; loss: 0.33; acc: 0.92
Batch: 700; loss: 0.35; acc: 0.84
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.95
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.8; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.362834060030758; val_accuracy: 0.8899283439490446 

The current subspace-distance is: 7.44275821489282e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.45; acc: 0.86
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.36; acc: 0.86
Batch: 360; loss: 0.17; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.95
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.27; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.81
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.52; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.83
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.37; acc: 0.83
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.64; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.48; acc: 0.8
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.3726279807698195; val_accuracy: 0.8802746815286624 

The current subspace-distance is: 7.729024946456775e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.18; acc: 0.91
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.38; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.92
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.36; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.35; acc: 0.95
Batch: 320; loss: 0.61; acc: 0.83
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.93; acc: 0.8
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.69; acc: 0.81
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.88
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.64; acc: 0.84
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.84
Batch: 640; loss: 0.32; acc: 0.86
Batch: 660; loss: 0.4; acc: 0.84
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.62; acc: 0.77
Batch: 80; loss: 0.24; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.22; acc: 0.91
Val Epoch over. val_loss: 0.39274154987874305; val_accuracy: 0.8773885350318471 

The current subspace-distance is: 7.977491623023525e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.49; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.83
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.41; acc: 0.92
Batch: 200; loss: 0.51; acc: 0.84
Batch: 220; loss: 0.34; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.46; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.23; acc: 0.95
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.83
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.95
Batch: 440; loss: 0.34; acc: 0.84
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.41; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.83
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.41; acc: 0.84
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.48; acc: 0.83
Batch: 780; loss: 0.32; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.8
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.34581087999472954; val_accuracy: 0.88953025477707 

The current subspace-distance is: 8.21668072603643e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.21; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.43; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.51; acc: 0.86
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.5; acc: 0.83
Batch: 280; loss: 0.4; acc: 0.81
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.94
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.8
Batch: 420; loss: 0.18; acc: 0.95
Batch: 440; loss: 0.47; acc: 0.86
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.55; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.33; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.89
Batch: 720; loss: 0.63; acc: 0.81
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.86; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3480290747751856; val_accuracy: 0.8893312101910829 

The current subspace-distance is: 8.406797860516235e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.43; acc: 0.92
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.46; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.89
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.81
Batch: 420; loss: 0.45; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.26; acc: 0.95
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.3; acc: 0.89
Batch: 740; loss: 0.4; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.55; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.33855434004098744; val_accuracy: 0.8946058917197452 

The current subspace-distance is: 8.61302760313265e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.59; acc: 0.8
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.37; acc: 0.88
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.52; acc: 0.84
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.84
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.62; acc: 0.88
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.78; acc: 0.78
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.89
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.61; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.81; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3332936146361813; val_accuracy: 0.8990843949044586 

The current subspace-distance is: 8.746715320739895e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.6; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.8
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.58; acc: 0.83
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.89
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.35; acc: 0.86
Batch: 540; loss: 0.58; acc: 0.84
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.55; acc: 0.83
Batch: 740; loss: 0.43; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.81
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.339160507604195; val_accuracy: 0.8960987261146497 

The current subspace-distance is: 8.913160854717717e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.23; acc: 0.91
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.64; acc: 0.84
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.3; acc: 0.88
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.84
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.44; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.31; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.44; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.55; acc: 0.81
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3396069551491813; val_accuracy: 0.894406847133758 

The current subspace-distance is: 9.107220830628648e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.55; acc: 0.8
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.46; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.49; acc: 0.81
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.73; acc: 0.81
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.83
Batch: 300; loss: 0.18; acc: 0.97
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.96; acc: 0.77
Batch: 400; loss: 0.43; acc: 0.83
Batch: 420; loss: 0.55; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.48; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.29; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.84
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.44; acc: 0.84
Batch: 740; loss: 0.58; acc: 0.83
Batch: 760; loss: 0.38; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.32939291168834756; val_accuracy: 0.8982882165605095 

The current subspace-distance is: 9.313413465861231e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.84
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.92
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.84
Batch: 320; loss: 0.22; acc: 0.94
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.86
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.43; acc: 0.8
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.41; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.19; acc: 0.94
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.56; acc: 0.84
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.32; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.97
Val Epoch over. val_loss: 0.3314122396764482; val_accuracy: 0.8973925159235668 

The current subspace-distance is: 9.5465307822451e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.36; acc: 0.84
Batch: 160; loss: 0.32; acc: 0.89
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.42; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.52; acc: 0.86
Batch: 420; loss: 0.42; acc: 0.83
Batch: 440; loss: 0.42; acc: 0.91
Batch: 460; loss: 0.66; acc: 0.81
Batch: 480; loss: 0.29; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.88
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.36; acc: 0.92
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.48; acc: 0.84
Batch: 680; loss: 0.22; acc: 0.94
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.83
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.34375794155962147; val_accuracy: 0.8924164012738853 

The current subspace-distance is: 9.674282046034932e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.84
Batch: 140; loss: 0.39; acc: 0.83
Batch: 160; loss: 0.46; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.49; acc: 0.8
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.5; acc: 0.86
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.88
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.84
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.63; acc: 0.8
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.54; acc: 0.81
Batch: 580; loss: 0.49; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.94
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.44; acc: 0.86
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3592435144789659; val_accuracy: 0.8878383757961783 

The current subspace-distance is: 9.862392471404746e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.86
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.46; acc: 0.8
Batch: 340; loss: 0.4; acc: 0.89
Batch: 360; loss: 0.59; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.64; acc: 0.81
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.5; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.5; acc: 0.86
Batch: 780; loss: 0.7; acc: 0.8
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.52; acc: 0.8
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.38; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3477393292887196; val_accuracy: 0.8928144904458599 

The current subspace-distance is: 0.00010039849439635873 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.59; acc: 0.77
Batch: 20; loss: 0.52; acc: 0.78
Batch: 40; loss: 0.5; acc: 0.84
Batch: 60; loss: 0.37; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.41; acc: 0.81
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.38; acc: 0.81
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.92
Batch: 220; loss: 0.56; acc: 0.84
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.2; acc: 0.91
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.46; acc: 0.83
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.41; acc: 0.94
Batch: 620; loss: 0.56; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.77
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.34; acc: 0.86
Batch: 740; loss: 0.21; acc: 0.94
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.81
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.33996883148600343; val_accuracy: 0.8905254777070064 

The current subspace-distance is: 0.0001018749171635136 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.38; acc: 0.94
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.89
Batch: 220; loss: 0.75; acc: 0.86
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.49; acc: 0.88
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.83
Batch: 440; loss: 0.48; acc: 0.88
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.3; acc: 0.94
Batch: 500; loss: 0.27; acc: 0.95
Batch: 520; loss: 0.33; acc: 0.92
Batch: 540; loss: 0.57; acc: 0.8
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.39; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.29; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.54; acc: 0.8
Batch: 700; loss: 0.37; acc: 0.81
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.53; acc: 0.84
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.32620038841940036; val_accuracy: 0.8969944267515924 

The current subspace-distance is: 0.00010366261994931847 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.52; acc: 0.89
Batch: 100; loss: 0.47; acc: 0.86
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.83
Batch: 160; loss: 0.53; acc: 0.83
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.97
Batch: 240; loss: 0.47; acc: 0.8
Batch: 260; loss: 0.47; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.95
Batch: 300; loss: 0.52; acc: 0.84
Batch: 320; loss: 0.51; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.48; acc: 0.8
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.49; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.91
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.17; acc: 0.95
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.4; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.57; acc: 0.78
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.52; acc: 0.84
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.81
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.33218591048079693; val_accuracy: 0.8969944267515924 

The current subspace-distance is: 0.00010512272274354473 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.8
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.89
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.7; acc: 0.75
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.36; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.5; acc: 0.83
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.58; acc: 0.88
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.51; acc: 0.88
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.48; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.6; acc: 0.83
Batch: 540; loss: 0.5; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.62; acc: 0.83
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.4; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.32596121868415245; val_accuracy: 0.897093949044586 

The current subspace-distance is: 0.00010674463555915281 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.65; acc: 0.88
Batch: 60; loss: 0.71; acc: 0.8
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.61; acc: 0.84
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.58; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.44; acc: 0.84
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.25; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.84
Batch: 460; loss: 0.44; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.43; acc: 0.83
Batch: 580; loss: 0.66; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.91
Batch: 660; loss: 0.47; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3273390631909203; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.00010814096458489075 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.88
Batch: 20; loss: 0.58; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.48; acc: 0.78
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.88
Batch: 160; loss: 0.18; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.94
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.83
Batch: 360; loss: 0.35; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.91
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.83
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.89
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.94
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.43; acc: 0.86
Batch: 720; loss: 0.51; acc: 0.88
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.81
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.32733063779439137; val_accuracy: 0.8971934713375797 

The current subspace-distance is: 0.00010963155364152044 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.43; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.44; acc: 0.81
Batch: 60; loss: 0.47; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.34; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.39; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.49; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.89
Batch: 300; loss: 0.48; acc: 0.92
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.54; acc: 0.84
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.36; acc: 0.86
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.85; acc: 0.75
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.89
Batch: 680; loss: 0.22; acc: 0.91
Batch: 700; loss: 0.5; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3295094986108078; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.00011202337918803096 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.83
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.92
Batch: 200; loss: 0.54; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.53; acc: 0.86
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.37; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.47; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.84
Batch: 560; loss: 0.38; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.3311513461599684; val_accuracy: 0.8955015923566879 

The current subspace-distance is: 0.00011431166785769165 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.49; acc: 0.88
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.23; acc: 0.98
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.48; acc: 0.89
Batch: 280; loss: 0.71; acc: 0.81
Batch: 300; loss: 0.58; acc: 0.8
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.37; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.39; acc: 0.92
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.43; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.88
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3268702824594109; val_accuracy: 0.8975915605095541 

The current subspace-distance is: 0.00011652940884232521 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.86
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.43; acc: 0.86
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.94
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.63; acc: 0.88
Batch: 440; loss: 0.37; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.78
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.84
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.86
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.49; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.84
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.32661149086086616; val_accuracy: 0.8982882165605095 

The current subspace-distance is: 0.00011794080637628213 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.29; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.23; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.46; acc: 0.86
Batch: 220; loss: 0.29; acc: 0.86
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.91
Batch: 280; loss: 0.7; acc: 0.84
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.41; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.41; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.91
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.54; acc: 0.83
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.31; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.29; acc: 0.94
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.47; acc: 0.81
Batch: 760; loss: 0.52; acc: 0.86
Batch: 780; loss: 0.41; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.83
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3288525076476252; val_accuracy: 0.8981886942675159 

The current subspace-distance is: 0.00011875710333697498 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.65; acc: 0.83
Batch: 40; loss: 0.54; acc: 0.83
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.52; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.36; acc: 0.83
Batch: 180; loss: 0.31; acc: 0.95
Batch: 200; loss: 0.48; acc: 0.86
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.65; acc: 0.84
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.55; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.28; acc: 0.86
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.39; acc: 0.88
Batch: 620; loss: 0.83; acc: 0.8
Batch: 640; loss: 0.33; acc: 0.86
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.89
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.43; acc: 0.91
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.32430131045306565; val_accuracy: 0.897890127388535 

The current subspace-distance is: 0.00011978916882071644 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.13; acc: 0.98
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.48; acc: 0.83
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.86
Batch: 500; loss: 0.59; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.42; acc: 0.83
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.3258450811218684; val_accuracy: 0.8974920382165605 

The current subspace-distance is: 0.00012175889423815534 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.24; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.69; acc: 0.81
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.88
Batch: 280; loss: 0.45; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.1; acc: 1.0
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.55; acc: 0.81
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.3; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.6; acc: 0.78
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.83
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.78
Batch: 700; loss: 0.55; acc: 0.84
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.32498735329432854; val_accuracy: 0.8974920382165605 

The current subspace-distance is: 0.00012274690379854292 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.81
Batch: 80; loss: 0.47; acc: 0.92
Batch: 100; loss: 0.7; acc: 0.84
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.76; acc: 0.81
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.4; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.86
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.91
Batch: 420; loss: 0.27; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.91
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.95
Batch: 540; loss: 0.43; acc: 0.81
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.57; acc: 0.81
Batch: 620; loss: 0.48; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.31; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.91
Batch: 720; loss: 0.39; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.88
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.81
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.32568720666466244; val_accuracy: 0.8979896496815286 

The current subspace-distance is: 0.00012460308789741248 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.4; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.47; acc: 0.88
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.84
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.88
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.54; acc: 0.91
Batch: 480; loss: 0.21; acc: 0.95
Batch: 500; loss: 0.47; acc: 0.88
Batch: 520; loss: 0.52; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.88
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.42; acc: 0.89
Batch: 660; loss: 0.4; acc: 0.83
Batch: 680; loss: 0.5; acc: 0.81
Batch: 700; loss: 0.66; acc: 0.83
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.32440825395143713; val_accuracy: 0.897093949044586 

The current subspace-distance is: 0.00012592534767463803 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.18; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.51; acc: 0.89
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.42; acc: 0.84
Batch: 220; loss: 0.43; acc: 0.89
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.68; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.54; acc: 0.91
Batch: 360; loss: 0.64; acc: 0.83
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.45; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.86
Batch: 520; loss: 0.49; acc: 0.86
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.94
Batch: 600; loss: 0.42; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.88
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.46; acc: 0.88
Batch: 680; loss: 0.41; acc: 0.89
Batch: 700; loss: 0.26; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.39; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.32422162015821526; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.00012756316573359072 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.66; acc: 0.81
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.89
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.88
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.5; acc: 0.84
Batch: 400; loss: 0.42; acc: 0.88
Batch: 420; loss: 0.33; acc: 0.88
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.89
Batch: 560; loss: 0.54; acc: 0.81
Batch: 580; loss: 0.31; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.33; acc: 0.92
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.86
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.32427908821850066; val_accuracy: 0.8980891719745223 

The current subspace-distance is: 0.0001276069669984281 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.39; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.43; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.88
Batch: 180; loss: 0.75; acc: 0.86
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.54; acc: 0.91
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.5; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.57; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.95
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.27; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.64; acc: 0.84
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.46; acc: 0.86
Batch: 620; loss: 0.41; acc: 0.84
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.32483522328221875; val_accuracy: 0.8983877388535032 

The current subspace-distance is: 0.00012823488214053214 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.62; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.51; acc: 0.84
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.84
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.84
Batch: 400; loss: 0.59; acc: 0.83
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.3; acc: 0.89
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.6; acc: 0.84
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.84
Batch: 720; loss: 0.37; acc: 0.89
Batch: 740; loss: 0.72; acc: 0.75
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.39; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.06; acc: 0.98
Val Epoch over. val_loss: 0.3254699474971765; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 0.000129947075038217 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.94
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.86
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.57; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.58; acc: 0.78
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.32; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.83
Batch: 560; loss: 0.26; acc: 0.89
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.86
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.86
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.83
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3245677641432756; val_accuracy: 0.8976910828025477 

The current subspace-distance is: 0.00013193482300266623 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_210_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 46404
elements in E: 9773720
fraction nonzero: 0.004747833987468436
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.08
Batch: 160; loss: 2.27; acc: 0.16
Batch: 180; loss: 2.29; acc: 0.09
Batch: 200; loss: 2.28; acc: 0.14
Batch: 220; loss: 2.26; acc: 0.19
Batch: 240; loss: 2.25; acc: 0.23
Batch: 260; loss: 2.25; acc: 0.19
Batch: 280; loss: 2.23; acc: 0.38
Batch: 300; loss: 2.21; acc: 0.34
Batch: 320; loss: 2.22; acc: 0.27
Batch: 340; loss: 2.19; acc: 0.27
Batch: 360; loss: 2.14; acc: 0.5
Batch: 380; loss: 2.1; acc: 0.5
Batch: 400; loss: 2.0; acc: 0.36
Batch: 420; loss: 1.79; acc: 0.59
Batch: 440; loss: 1.53; acc: 0.53
Batch: 460; loss: 1.52; acc: 0.48
Batch: 480; loss: 1.2; acc: 0.62
Batch: 500; loss: 1.06; acc: 0.58
Batch: 520; loss: 0.96; acc: 0.66
Batch: 540; loss: 1.43; acc: 0.53
Batch: 560; loss: 0.73; acc: 0.75
Batch: 580; loss: 0.99; acc: 0.69
Batch: 600; loss: 0.81; acc: 0.73
Batch: 620; loss: 0.72; acc: 0.75
Batch: 640; loss: 0.95; acc: 0.7
Batch: 660; loss: 0.84; acc: 0.73
Batch: 680; loss: 0.99; acc: 0.73
Batch: 700; loss: 0.88; acc: 0.7
Batch: 720; loss: 0.75; acc: 0.73
Batch: 740; loss: 1.06; acc: 0.7
Batch: 760; loss: 0.57; acc: 0.78
Batch: 780; loss: 0.72; acc: 0.75
Train Epoch over. train_loss: 1.65; train_accuracy: 0.43 

Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.58; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.67; acc: 0.75
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.6; acc: 0.81
Batch: 120; loss: 1.03; acc: 0.66
Batch: 140; loss: 0.33; acc: 0.89
Val Epoch over. val_loss: 0.6219301795124248; val_accuracy: 0.8021496815286624 

The current subspace-distance is: 1.1319161785650067e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.77
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 1.07; acc: 0.67
Batch: 60; loss: 0.57; acc: 0.78
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.58; acc: 0.8
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.72; acc: 0.78
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.77; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.91
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.87; acc: 0.77
Batch: 260; loss: 0.76; acc: 0.78
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.57; acc: 0.75
Batch: 320; loss: 0.52; acc: 0.89
Batch: 340; loss: 0.69; acc: 0.81
Batch: 360; loss: 0.75; acc: 0.78
Batch: 380; loss: 0.56; acc: 0.83
Batch: 400; loss: 0.57; acc: 0.81
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.4; acc: 0.84
Batch: 460; loss: 0.69; acc: 0.78
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.62; acc: 0.81
Batch: 520; loss: 0.69; acc: 0.77
Batch: 540; loss: 0.4; acc: 0.81
Batch: 560; loss: 0.75; acc: 0.75
Batch: 580; loss: 0.62; acc: 0.83
Batch: 600; loss: 0.6; acc: 0.83
Batch: 620; loss: 0.55; acc: 0.83
Batch: 640; loss: 0.78; acc: 0.75
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.68; acc: 0.78
Batch: 700; loss: 0.5; acc: 0.81
Batch: 720; loss: 0.71; acc: 0.77
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.83
Train Epoch over. train_loss: 0.6; train_accuracy: 0.81 

Batch: 0; loss: 0.83; acc: 0.77
Batch: 20; loss: 0.87; acc: 0.77
Batch: 40; loss: 0.52; acc: 0.78
Batch: 60; loss: 0.89; acc: 0.72
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.72; acc: 0.77
Batch: 120; loss: 0.97; acc: 0.69
Batch: 140; loss: 0.66; acc: 0.69
Val Epoch over. val_loss: 0.7476094182889172; val_accuracy: 0.7580613057324841 

The current subspace-distance is: 2.018217674049083e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.02; acc: 0.69
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.65; acc: 0.75
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.45; acc: 0.88
Batch: 160; loss: 0.54; acc: 0.78
Batch: 180; loss: 0.48; acc: 0.84
Batch: 200; loss: 0.6; acc: 0.83
Batch: 220; loss: 0.82; acc: 0.72
Batch: 240; loss: 0.43; acc: 0.88
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.94
Batch: 300; loss: 0.45; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.84
Batch: 480; loss: 0.52; acc: 0.78
Batch: 500; loss: 0.68; acc: 0.83
Batch: 520; loss: 0.39; acc: 0.86
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.59; acc: 0.89
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.78; acc: 0.81
Batch: 680; loss: 0.83; acc: 0.72
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.86; acc: 0.78
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.54; acc: 0.81
Batch: 20; loss: 0.6; acc: 0.8
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.51; acc: 0.83
Batch: 120; loss: 0.69; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.5110694647404799; val_accuracy: 0.8396695859872612 

The current subspace-distance is: 2.6805988454725593e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.31; acc: 0.84
Batch: 20; loss: 0.46; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.81
Batch: 60; loss: 0.73; acc: 0.73
Batch: 80; loss: 0.46; acc: 0.86
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.83
Batch: 200; loss: 0.64; acc: 0.8
Batch: 220; loss: 0.43; acc: 0.83
Batch: 240; loss: 0.51; acc: 0.84
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 1.11; acc: 0.78
Batch: 300; loss: 0.63; acc: 0.8
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.7; acc: 0.84
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.49; acc: 0.84
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.81
Batch: 460; loss: 0.69; acc: 0.75
Batch: 480; loss: 0.38; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.89
Batch: 520; loss: 0.33; acc: 0.86
Batch: 540; loss: 0.57; acc: 0.84
Batch: 560; loss: 0.83; acc: 0.77
Batch: 580; loss: 0.4; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.95
Batch: 620; loss: 0.42; acc: 0.86
Batch: 640; loss: 0.7; acc: 0.8
Batch: 660; loss: 0.7; acc: 0.84
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.47; acc: 0.84
Batch: 760; loss: 0.55; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.49; train_accuracy: 0.85 

Batch: 0; loss: 1.01; acc: 0.73
Batch: 20; loss: 0.92; acc: 0.75
Batch: 40; loss: 0.64; acc: 0.77
Batch: 60; loss: 1.08; acc: 0.67
Batch: 80; loss: 0.76; acc: 0.84
Batch: 100; loss: 0.95; acc: 0.77
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.91; acc: 0.67
Val Epoch over. val_loss: 0.9430220504854895; val_accuracy: 0.7100915605095541 

The current subspace-distance is: 3.205436223652214e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.68; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.65; acc: 0.75
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.83
Batch: 180; loss: 0.35; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.86
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.36; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.94
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.39; acc: 0.89
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.54; acc: 0.75
Batch: 380; loss: 0.38; acc: 0.88
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.86
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.84
Batch: 500; loss: 0.69; acc: 0.78
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.43; acc: 0.88
Batch: 560; loss: 0.61; acc: 0.86
Batch: 580; loss: 0.38; acc: 0.84
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.47; acc: 0.86
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.62; acc: 0.78
Batch: 740; loss: 0.33; acc: 0.86
Batch: 760; loss: 0.35; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.68; acc: 0.81
Batch: 20; loss: 1.16; acc: 0.67
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 1.16; acc: 0.7
Batch: 80; loss: 0.96; acc: 0.78
Batch: 100; loss: 0.81; acc: 0.7
Batch: 120; loss: 1.25; acc: 0.69
Batch: 140; loss: 0.63; acc: 0.77
Val Epoch over. val_loss: 0.8093926204237968; val_accuracy: 0.7622412420382165 

The current subspace-distance is: 3.77291944460012e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.89; acc: 0.73
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.41; acc: 0.83
Batch: 140; loss: 0.27; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.89
Batch: 180; loss: 0.56; acc: 0.83
Batch: 200; loss: 0.25; acc: 0.95
Batch: 220; loss: 0.44; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.81
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.34; acc: 0.88
Batch: 340; loss: 0.35; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.83
Batch: 380; loss: 0.44; acc: 0.88
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.76; acc: 0.78
Batch: 440; loss: 0.41; acc: 0.89
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.33; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.88
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.55; acc: 0.8
Batch: 560; loss: 0.44; acc: 0.88
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.39; acc: 0.94
Batch: 620; loss: 0.49; acc: 0.84
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.84
Batch: 680; loss: 0.44; acc: 0.8
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.84
Batch: 740; loss: 0.55; acc: 0.81
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.75; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.86
Val Epoch over. val_loss: 0.42360510919124456; val_accuracy: 0.8675358280254777 

The current subspace-distance is: 4.187255763099529e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.47; acc: 0.84
Batch: 40; loss: 0.35; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.57; acc: 0.81
Batch: 100; loss: 0.4; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.61; acc: 0.83
Batch: 200; loss: 0.28; acc: 0.88
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.65; acc: 0.77
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.68; acc: 0.8
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.5; acc: 0.88
Batch: 480; loss: 0.48; acc: 0.8
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.78; acc: 0.72
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.89
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.44; acc: 0.81
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.53; acc: 0.84
Batch: 720; loss: 0.48; acc: 0.83
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.45; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.23; acc: 0.94
Val Epoch over. val_loss: 0.377280140141393; val_accuracy: 0.884952229299363 

The current subspace-distance is: 4.595003338181414e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.83
Batch: 20; loss: 0.67; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.47; acc: 0.83
Batch: 160; loss: 0.25; acc: 0.94
Batch: 180; loss: 0.54; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.83
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.78
Batch: 300; loss: 0.55; acc: 0.77
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.63; acc: 0.83
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.84
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.88
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.26; acc: 0.95
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.39; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.83
Batch: 620; loss: 0.21; acc: 0.95
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.54; acc: 0.83
Batch: 700; loss: 0.42; acc: 0.84
Batch: 720; loss: 0.42; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.43; train_accuracy: 0.87 

Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.64; acc: 0.75
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.77; acc: 0.73
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.83
Batch: 140; loss: 0.52; acc: 0.83
Val Epoch over. val_loss: 0.5217661449484005; val_accuracy: 0.8363853503184714 

The current subspace-distance is: 4.9289490561932325e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.78
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.67; acc: 0.78
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.61; acc: 0.81
Batch: 160; loss: 0.27; acc: 0.84
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.39; acc: 0.88
Batch: 240; loss: 0.33; acc: 0.81
Batch: 260; loss: 0.45; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.59; acc: 0.84
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.83
Batch: 520; loss: 0.59; acc: 0.78
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.78
Batch: 580; loss: 0.5; acc: 0.86
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.39; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.43; train_accuracy: 0.86 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.67; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.4056723723840562; val_accuracy: 0.8763933121019108 

The current subspace-distance is: 5.386405609897338e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.65; acc: 0.81
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.55; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.83
Batch: 160; loss: 0.35; acc: 0.88
Batch: 180; loss: 0.5; acc: 0.81
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.67; acc: 0.81
Batch: 260; loss: 0.51; acc: 0.81
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.56; acc: 0.86
Batch: 360; loss: 0.81; acc: 0.8
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.38; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.71; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.94
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.33; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.89
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.38; acc: 0.84
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.36; acc: 0.84
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.83
Batch: 760; loss: 0.56; acc: 0.8
Batch: 780; loss: 0.3; acc: 0.86
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.55; acc: 0.83
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.28; acc: 0.92
Val Epoch over. val_loss: 0.4152557755446738; val_accuracy: 0.8690286624203821 

The current subspace-distance is: 5.7538643886800855e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.81; acc: 0.8
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.5; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.46; acc: 0.81
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.46; acc: 0.89
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.84
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.86
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.43; acc: 0.83
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.78; acc: 0.8
Batch: 440; loss: 0.43; acc: 0.83
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.94
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.49; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.8
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.88
Train Epoch over. train_loss: 0.37; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.94
Val Epoch over. val_loss: 0.3549934877142025; val_accuracy: 0.8942078025477707 

The current subspace-distance is: 5.998280175845139e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.37; acc: 0.84
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.95
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.51; acc: 0.88
Batch: 260; loss: 0.43; acc: 0.84
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.3; acc: 0.86
Batch: 320; loss: 0.52; acc: 0.81
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.52; acc: 0.83
Batch: 380; loss: 0.48; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.57; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.41; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.73; acc: 0.8
Batch: 580; loss: 0.47; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.47; acc: 0.86
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.47; acc: 0.84
Batch: 720; loss: 0.31; acc: 0.94
Batch: 740; loss: 0.58; acc: 0.84
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.3649550331341233; val_accuracy: 0.886046974522293 

The current subspace-distance is: 6.268644210649654e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.63; acc: 0.86
Batch: 40; loss: 0.34; acc: 0.86
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.97
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.36; acc: 0.97
Batch: 280; loss: 0.51; acc: 0.86
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.43; acc: 0.84
Batch: 380; loss: 0.24; acc: 0.89
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.38; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.88
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.21; acc: 0.94
Batch: 520; loss: 0.48; acc: 0.86
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.86
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.57; acc: 0.88
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.56; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.97
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.34000448085320223; val_accuracy: 0.8977906050955414 

The current subspace-distance is: 6.56836709822528e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.47; acc: 0.83
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.84
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.86
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.91
Batch: 600; loss: 0.47; acc: 0.86
Batch: 620; loss: 0.43; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.53; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.97
Batch: 760; loss: 0.4; acc: 0.83
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.59; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.48; acc: 0.81
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.37095871738567476; val_accuracy: 0.8810708598726115 

The current subspace-distance is: 6.8645465944428e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.83
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.64; acc: 0.8
Batch: 120; loss: 0.13; acc: 0.97
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.54; acc: 0.8
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.5; acc: 0.84
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.31; acc: 0.86
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.33; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.89
Batch: 640; loss: 0.35; acc: 0.84
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.88
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.47; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.3519599926035116; val_accuracy: 0.8916202229299363 

The current subspace-distance is: 7.223202555906028e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.95
Batch: 80; loss: 0.5; acc: 0.86
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.54; acc: 0.84
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.43; acc: 0.86
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.5; acc: 0.88
Batch: 280; loss: 0.42; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.28; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.46; acc: 0.81
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.88
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.53; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.19; acc: 0.95
Batch: 660; loss: 0.39; acc: 0.8
Batch: 680; loss: 0.41; acc: 0.84
Batch: 700; loss: 0.33; acc: 0.86
Batch: 720; loss: 0.4; acc: 0.81
Batch: 740; loss: 0.53; acc: 0.83
Batch: 760; loss: 0.29; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.36025424460601657; val_accuracy: 0.8859474522292994 

The current subspace-distance is: 7.494429155485705e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.51; acc: 0.84
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.59; acc: 0.84
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.34; acc: 0.86
Batch: 420; loss: 0.25; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.54; acc: 0.86
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.47; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.43; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.84
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.91
Batch: 760; loss: 0.42; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.88
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.53; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.86
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.35039987218133206; val_accuracy: 0.8909235668789809 

The current subspace-distance is: 7.685973832849413e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.92
Batch: 140; loss: 0.59; acc: 0.83
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.26; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.84
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.56; acc: 0.81
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.69; acc: 0.78
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.44; acc: 0.88
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.52; acc: 0.81
Batch: 600; loss: 0.49; acc: 0.88
Batch: 620; loss: 0.26; acc: 0.88
Batch: 640; loss: 0.2; acc: 0.88
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.32; acc: 0.86
Batch: 700; loss: 0.59; acc: 0.94
Batch: 720; loss: 0.37; acc: 0.84
Batch: 740; loss: 0.62; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.41; acc: 0.89
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.5; acc: 0.81
Batch: 140; loss: 0.21; acc: 0.88
Val Epoch over. val_loss: 0.369050486167525; val_accuracy: 0.8846536624203821 

The current subspace-distance is: 7.926832040539011e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.94
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.34; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.88
Batch: 280; loss: 0.44; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.95
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.94
Batch: 400; loss: 0.38; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.83
Batch: 480; loss: 0.39; acc: 0.84
Batch: 500; loss: 0.28; acc: 0.94
Batch: 520; loss: 0.35; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.55; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.86
Batch: 600; loss: 0.12; acc: 0.98
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.28; acc: 0.94
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.37; acc: 0.94
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3323523365198427; val_accuracy: 0.8972929936305732 

The current subspace-distance is: 8.175916445907205e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.53; acc: 0.81
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.23; acc: 0.95
Batch: 220; loss: 0.57; acc: 0.81
Batch: 240; loss: 0.31; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.35; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.45; acc: 0.92
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.41; acc: 0.86
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.43; acc: 0.84
Batch: 500; loss: 0.58; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.86
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.37; acc: 0.84
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.81
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.88
Batch: 120; loss: 0.64; acc: 0.8
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3559032544778411; val_accuracy: 0.88953025477707 

The current subspace-distance is: 8.413326577283442e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.53; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.53; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.94
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.35; acc: 0.84
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.88
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.28; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.89
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.44; acc: 0.94
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.32472227276510496; val_accuracy: 0.8996815286624203 

The current subspace-distance is: 8.602156594861299e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.58; acc: 0.78
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.16; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.51; acc: 0.91
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.51; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.41; acc: 0.92
Batch: 460; loss: 0.55; acc: 0.8
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.59; acc: 0.88
Batch: 560; loss: 0.48; acc: 0.84
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.86
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.29; acc: 0.86
Batch: 700; loss: 0.39; acc: 0.91
Batch: 720; loss: 0.33; acc: 0.86
Batch: 740; loss: 0.36; acc: 0.84
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.5; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3303067361473278; val_accuracy: 0.8960987261146497 

The current subspace-distance is: 8.84277542354539e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.91
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.41; acc: 0.86
Batch: 140; loss: 0.36; acc: 0.84
Batch: 160; loss: 0.34; acc: 0.89
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.3; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.37; acc: 0.86
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.58; acc: 0.81
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.27; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.83
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.32; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.3269376149223109; val_accuracy: 0.8986863057324841 

The current subspace-distance is: 9.057973511517048e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.27; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.42; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.48; acc: 0.81
Batch: 200; loss: 0.52; acc: 0.84
Batch: 220; loss: 0.44; acc: 0.86
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.88
Batch: 280; loss: 0.52; acc: 0.8
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.91
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.46; acc: 0.81
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.92
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.95
Batch: 700; loss: 0.31; acc: 0.84
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.88
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.52; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.94
Val Epoch over. val_loss: 0.336305590714239; val_accuracy: 0.895203025477707 

The current subspace-distance is: 9.188349940814078e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.52; acc: 0.81
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.84
Batch: 160; loss: 0.47; acc: 0.92
Batch: 180; loss: 0.14; acc: 0.94
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.77; acc: 0.84
Batch: 260; loss: 0.38; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.84
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.79; acc: 0.83
Batch: 400; loss: 0.21; acc: 0.97
Batch: 420; loss: 0.59; acc: 0.83
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.39; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.29; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.52; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.24; acc: 0.89
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3293538322779024; val_accuracy: 0.8979896496815286 

The current subspace-distance is: 9.346532897325233e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.84
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.28; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.38; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.83
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.58; acc: 0.89
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.35; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.55; acc: 0.78
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.81
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.3221653481577612; val_accuracy: 0.8995820063694268 

The current subspace-distance is: 9.56548101385124e-05 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.24; acc: 0.94
Batch: 140; loss: 0.25; acc: 0.89
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.86
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.86
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.31; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.84
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.86
Batch: 640; loss: 0.61; acc: 0.81
Batch: 660; loss: 0.52; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.86
Batch: 740; loss: 0.27; acc: 0.94
Batch: 760; loss: 0.49; acc: 0.83
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.31918444033640964; val_accuracy: 0.9021695859872612 

The current subspace-distance is: 9.701724047772586e-05 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.91
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.36; acc: 0.84
Batch: 160; loss: 0.43; acc: 0.91
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.3; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.18; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.84
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.63; acc: 0.84
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.84
Batch: 580; loss: 0.43; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.3308941876147963; val_accuracy: 0.8964968152866242 

The current subspace-distance is: 9.885142935672775e-05 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.36; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.88
Batch: 80; loss: 0.53; acc: 0.8
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.91
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.28; acc: 0.91
Batch: 600; loss: 0.42; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.89
Batch: 780; loss: 0.54; acc: 0.8
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.3315786155308508; val_accuracy: 0.8975915605095541 

The current subspace-distance is: 0.00010093390301335603 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.55; acc: 0.86
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.54; acc: 0.88
Batch: 300; loss: 0.5; acc: 0.8
Batch: 320; loss: 0.25; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.89
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.39; acc: 0.84
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.47; acc: 0.81
Batch: 640; loss: 0.74; acc: 0.83
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.92
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.97
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.86
Batch: 40; loss: 0.15; acc: 0.92
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.88
Batch: 120; loss: 0.58; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3426149261130649; val_accuracy: 0.8914211783439491 

The current subspace-distance is: 0.00010313177335774526 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.17; acc: 0.97
Batch: 100; loss: 0.39; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.88
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.95
Batch: 220; loss: 0.69; acc: 0.77
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.94
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.28; acc: 0.91
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.37; acc: 0.95
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.31; acc: 0.86
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.58; acc: 0.81
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.35; acc: 0.88
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.5; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.89
Batch: 760; loss: 0.58; acc: 0.84
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.31534085835620856; val_accuracy: 0.9027667197452229 

The current subspace-distance is: 0.00010460160410730168 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.32; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.38; acc: 0.91
Batch: 340; loss: 0.42; acc: 0.88
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.51; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.88
Batch: 480; loss: 0.38; acc: 0.83
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.64; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.21; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.53; acc: 0.81
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.31862526937465; val_accuracy: 0.9000796178343949 

The current subspace-distance is: 0.00010589283192530274 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.89
Batch: 100; loss: 0.34; acc: 0.86
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.28; acc: 0.94
Batch: 240; loss: 0.24; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.28; acc: 0.95
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.52; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.62; acc: 0.78
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.51; acc: 0.81
Batch: 480; loss: 0.35; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.65; acc: 0.84
Batch: 540; loss: 0.58; acc: 0.89
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.83
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.83
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.31738166520549993; val_accuracy: 0.9013734076433121 

The current subspace-distance is: 0.000107931176899001 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.29; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.78
Batch: 80; loss: 0.37; acc: 0.91
Batch: 100; loss: 0.54; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.33; acc: 0.92
Batch: 160; loss: 0.72; acc: 0.81
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.62; acc: 0.83
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.28; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.86
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.43; acc: 0.86
Batch: 400; loss: 0.24; acc: 0.95
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.88
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.84
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.92
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.23; acc: 0.98
Batch: 720; loss: 0.46; acc: 0.81
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.31763225950443064; val_accuracy: 0.9012738853503185 

The current subspace-distance is: 0.0001087906930479221 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.94
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.94
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.41; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.52; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.89
Batch: 640; loss: 0.25; acc: 0.95
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3177818091716736; val_accuracy: 0.9021695859872612 

The current subspace-distance is: 0.0001106587951653637 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.83
Batch: 140; loss: 0.38; acc: 0.89
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.55; acc: 0.8
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.4; acc: 0.89
Batch: 260; loss: 0.31; acc: 0.92
Batch: 280; loss: 0.53; acc: 0.81
Batch: 300; loss: 0.41; acc: 0.84
Batch: 320; loss: 0.26; acc: 0.92
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.5; acc: 0.84
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.4; acc: 0.88
Batch: 440; loss: 0.73; acc: 0.84
Batch: 460; loss: 0.63; acc: 0.83
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.45; acc: 0.86
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.23; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.88
Batch: 700; loss: 0.39; acc: 0.89
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.47; acc: 0.86
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.88
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.21; acc: 0.97
Batch: 120; loss: 0.52; acc: 0.84
Batch: 140; loss: 0.13; acc: 0.97
Val Epoch over. val_loss: 0.31988212228960294; val_accuracy: 0.9007762738853503 

The current subspace-distance is: 0.00011253908451180905 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.83
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.89
Batch: 200; loss: 0.4; acc: 0.88
Batch: 220; loss: 0.31; acc: 0.84
Batch: 240; loss: 0.42; acc: 0.84
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.88
Batch: 440; loss: 0.25; acc: 0.95
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.58; acc: 0.86
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.52; acc: 0.78
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.35; acc: 0.92
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.86
Batch: 140; loss: 0.12; acc: 0.98
Val Epoch over. val_loss: 0.321897561050904; val_accuracy: 0.8994824840764332 

The current subspace-distance is: 0.00011385728430468589 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.59; acc: 0.83
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.36; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.48; acc: 0.84
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.35; acc: 0.86
Batch: 240; loss: 0.14; acc: 0.97
Batch: 260; loss: 0.48; acc: 0.88
Batch: 280; loss: 0.66; acc: 0.86
Batch: 300; loss: 0.38; acc: 0.83
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.86
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.35; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.59; acc: 0.81
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.36; acc: 0.84
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.39; acc: 0.89
Batch: 740; loss: 0.31; acc: 0.86
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.31701036490452517; val_accuracy: 0.9001791401273885 

The current subspace-distance is: 0.0001152370241470635 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.92
Batch: 40; loss: 0.3; acc: 0.97
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.61; acc: 0.81
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.95
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.94
Batch: 320; loss: 0.19; acc: 0.98
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.38; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.88
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.24; acc: 0.95
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.89
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.31; acc: 0.95
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.91
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3153843248061314; val_accuracy: 0.9027667197452229 

The current subspace-distance is: 0.00011650773376459256 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.47; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.18; acc: 0.92
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.44; acc: 0.88
Batch: 280; loss: 0.59; acc: 0.8
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.94
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.15; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.32; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.86
Batch: 520; loss: 0.29; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.41; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.95
Batch: 620; loss: 0.14; acc: 0.98
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.36; acc: 0.91
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.83
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.31692603705035655; val_accuracy: 0.9018710191082803 

The current subspace-distance is: 0.00011798490595538169 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.33; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.24; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.86
Batch: 320; loss: 0.21; acc: 0.91
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.37; acc: 0.83
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.3; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.92
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.34; acc: 0.84
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.44; acc: 0.88
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.25; acc: 0.89
Batch: 740; loss: 0.32; acc: 0.95
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.31493335758235047; val_accuracy: 0.9019705414012739 

The current subspace-distance is: 0.00011954950605286285 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.39; acc: 0.94
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.97
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.19; acc: 0.94
Batch: 140; loss: 0.24; acc: 0.97
Batch: 160; loss: 0.17; acc: 0.94
Batch: 180; loss: 0.27; acc: 0.89
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.17; acc: 0.97
Batch: 240; loss: 0.5; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.26; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.92
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.5; acc: 0.83
Batch: 500; loss: 0.36; acc: 0.89
Batch: 520; loss: 0.25; acc: 0.92
Batch: 540; loss: 0.25; acc: 0.94
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.2; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.36; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.35; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.31510929810773036; val_accuracy: 0.9019705414012739 

The current subspace-distance is: 0.00012141472689108923 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.29; acc: 0.94
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.86
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.58; acc: 0.88
Batch: 220; loss: 0.59; acc: 0.88
Batch: 240; loss: 0.17; acc: 0.94
Batch: 260; loss: 0.48; acc: 0.83
Batch: 280; loss: 0.26; acc: 0.91
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.97
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.42; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.91
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.3; acc: 0.88
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.88
Batch: 760; loss: 0.37; acc: 0.91
Batch: 780; loss: 0.11; acc: 1.0
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.83
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3147228423767029; val_accuracy: 0.901671974522293 

The current subspace-distance is: 0.00012263402459211648 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.4; acc: 0.86
Batch: 80; loss: 0.37; acc: 0.89
Batch: 100; loss: 0.48; acc: 0.83
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.37; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.98
Batch: 300; loss: 0.33; acc: 0.92
Batch: 320; loss: 0.35; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.89
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.48; acc: 0.89
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.32; acc: 0.92
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.33; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.81
Batch: 620; loss: 0.32; acc: 0.94
Batch: 640; loss: 0.35; acc: 0.86
Batch: 660; loss: 0.54; acc: 0.81
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.31619442482093335; val_accuracy: 0.9021695859872612 

The current subspace-distance is: 0.00012434361269697547 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.88
Batch: 100; loss: 0.38; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.32; acc: 0.86
Batch: 320; loss: 0.4; acc: 0.88
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.94
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.54; acc: 0.91
Batch: 520; loss: 0.44; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.17; acc: 0.97
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.8
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.88
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.89
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.32; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3145554936520613; val_accuracy: 0.9020700636942676 

The current subspace-distance is: 0.00012592347047757357 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.98
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.54; acc: 0.91
Batch: 120; loss: 0.39; acc: 0.88
Batch: 140; loss: 0.32; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.97
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.25; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.69; acc: 0.86
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.48; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.35; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.3; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.83
Batch: 600; loss: 0.23; acc: 0.95
Batch: 620; loss: 0.23; acc: 0.88
Batch: 640; loss: 0.47; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3148426023922908; val_accuracy: 0.9026671974522293 

The current subspace-distance is: 0.00012797162344213575 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.19; acc: 0.91
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.32; acc: 0.86
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.29; acc: 0.91
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.37; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.88
Batch: 460; loss: 0.21; acc: 0.95
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.88
Batch: 540; loss: 0.42; acc: 0.88
Batch: 560; loss: 0.35; acc: 0.84
Batch: 580; loss: 0.3; acc: 0.86
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.84
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.44; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.54; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.314114034697888; val_accuracy: 0.9009753184713376 

The current subspace-distance is: 0.00013095830217935145 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.49; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.83
Batch: 200; loss: 0.35; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.84
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.36; acc: 0.89
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.51; acc: 0.83
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.97
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.91
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.51; acc: 0.86
Batch: 540; loss: 0.47; acc: 0.88
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.52; acc: 0.81
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.81
Batch: 720; loss: 0.37; acc: 0.83
Batch: 740; loss: 0.33; acc: 0.88
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3144019038244418; val_accuracy: 0.9020700636942676 

The current subspace-distance is: 0.00013246704475022852 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.47; acc: 0.83
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.88
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.45; acc: 0.83
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.44; acc: 0.83
Batch: 240; loss: 0.27; acc: 0.88
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.92
Batch: 300; loss: 0.28; acc: 0.88
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.37; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.37; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.4; acc: 0.81
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.86
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.28; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.86
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.41; acc: 0.89
Batch: 740; loss: 0.62; acc: 0.8
Batch: 760; loss: 0.31; acc: 0.94
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.31489028943002606; val_accuracy: 0.9018710191082803 

The current subspace-distance is: 0.00013416509318631142 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.5; acc: 0.81
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.28; acc: 0.88
Batch: 280; loss: 0.28; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.95
Batch: 340; loss: 0.48; acc: 0.81
Batch: 360; loss: 0.25; acc: 0.89
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.35; acc: 0.89
Batch: 420; loss: 0.48; acc: 0.84
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.89
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.86
Batch: 520; loss: 0.27; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.84
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.92
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.71; acc: 0.83
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.84
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.3157095941863242; val_accuracy: 0.9007762738853503 

The current subspace-distance is: 0.00013430777471512556 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_220_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 48738
elements in E: 10217980
fraction nonzero: 0.00476982730441829
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.09
Batch: 60; loss: 2.3; acc: 0.06
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.31; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.05
Batch: 140; loss: 2.29; acc: 0.06
Batch: 160; loss: 2.26; acc: 0.25
Batch: 180; loss: 2.29; acc: 0.09
Batch: 200; loss: 2.27; acc: 0.25
Batch: 220; loss: 2.26; acc: 0.31
Batch: 240; loss: 2.24; acc: 0.31
Batch: 260; loss: 2.23; acc: 0.39
Batch: 280; loss: 2.22; acc: 0.42
Batch: 300; loss: 2.18; acc: 0.44
Batch: 320; loss: 2.2; acc: 0.39
Batch: 340; loss: 2.16; acc: 0.41
Batch: 360; loss: 2.09; acc: 0.48
Batch: 380; loss: 2.1; acc: 0.34
Batch: 400; loss: 2.06; acc: 0.31
Batch: 420; loss: 1.86; acc: 0.38
Batch: 440; loss: 1.69; acc: 0.58
Batch: 460; loss: 1.49; acc: 0.58
Batch: 480; loss: 1.2; acc: 0.61
Batch: 500; loss: 1.11; acc: 0.67
Batch: 520; loss: 0.77; acc: 0.73
Batch: 540; loss: 1.0; acc: 0.66
Batch: 560; loss: 0.87; acc: 0.78
Batch: 580; loss: 0.96; acc: 0.72
Batch: 600; loss: 0.79; acc: 0.75
Batch: 620; loss: 0.85; acc: 0.78
Batch: 640; loss: 0.97; acc: 0.58
Batch: 660; loss: 0.97; acc: 0.72
Batch: 680; loss: 1.12; acc: 0.67
Batch: 700; loss: 1.04; acc: 0.61
Batch: 720; loss: 0.47; acc: 0.86
Batch: 740; loss: 0.86; acc: 0.77
Batch: 760; loss: 0.63; acc: 0.81
Batch: 780; loss: 1.03; acc: 0.64
Train Epoch over. train_loss: 1.66; train_accuracy: 0.45 

Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.77; acc: 0.72
Batch: 40; loss: 0.5; acc: 0.83
Batch: 60; loss: 0.73; acc: 0.77
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.8; acc: 0.72
Batch: 140; loss: 0.48; acc: 0.81
Val Epoch over. val_loss: 0.6868137474272661; val_accuracy: 0.7752786624203821 

The current subspace-distance is: 1.1621050362009555e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.68; acc: 0.78
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 1.11; acc: 0.73
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.89; acc: 0.7
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.64; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.7
Batch: 160; loss: 0.72; acc: 0.75
Batch: 180; loss: 0.8; acc: 0.75
Batch: 200; loss: 0.59; acc: 0.75
Batch: 220; loss: 0.55; acc: 0.83
Batch: 240; loss: 0.75; acc: 0.75
Batch: 260; loss: 0.71; acc: 0.8
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.51; acc: 0.78
Batch: 320; loss: 0.45; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.86
Batch: 360; loss: 0.67; acc: 0.73
Batch: 380; loss: 0.61; acc: 0.81
Batch: 400; loss: 0.49; acc: 0.86
Batch: 420; loss: 0.34; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.58; acc: 0.81
Batch: 480; loss: 0.58; acc: 0.89
Batch: 500; loss: 0.85; acc: 0.72
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.75; acc: 0.77
Batch: 580; loss: 0.38; acc: 0.92
Batch: 600; loss: 0.58; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.51; acc: 0.86
Batch: 700; loss: 0.43; acc: 0.83
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.56; acc: 0.83
Batch: 760; loss: 0.71; acc: 0.78
Batch: 780; loss: 0.47; acc: 0.83
Train Epoch over. train_loss: 0.55; train_accuracy: 0.82 

Batch: 0; loss: 0.54; acc: 0.84
Batch: 20; loss: 0.7; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.75; acc: 0.83
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.41; acc: 0.91
Batch: 120; loss: 0.92; acc: 0.7
Batch: 140; loss: 0.2; acc: 0.95
Val Epoch over. val_loss: 0.5133639161184336; val_accuracy: 0.8435509554140127 

The current subspace-distance is: 2.0992529243812896e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.8; acc: 0.78
Batch: 20; loss: 0.4; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.6; acc: 0.77
Batch: 100; loss: 0.44; acc: 0.83
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.45; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.8
Batch: 180; loss: 0.71; acc: 0.81
Batch: 200; loss: 0.35; acc: 0.88
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.92
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.47; acc: 0.81
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.57; acc: 0.83
Batch: 400; loss: 0.42; acc: 0.84
Batch: 420; loss: 0.39; acc: 0.84
Batch: 440; loss: 0.33; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.81
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.31; acc: 0.94
Batch: 560; loss: 0.65; acc: 0.81
Batch: 580; loss: 0.41; acc: 0.88
Batch: 600; loss: 0.41; acc: 0.88
Batch: 620; loss: 0.34; acc: 0.83
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.68; acc: 0.78
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.84
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.94
Batch: 120; loss: 0.69; acc: 0.7
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4494469699206626; val_accuracy: 0.8588773885350318 

The current subspace-distance is: 2.6960542527376674e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.45; acc: 0.8
Batch: 120; loss: 0.41; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.38; acc: 0.88
Batch: 180; loss: 0.48; acc: 0.88
Batch: 200; loss: 0.53; acc: 0.86
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.29; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.61; acc: 0.83
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.47; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.89
Batch: 420; loss: 0.47; acc: 0.83
Batch: 440; loss: 0.36; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.77
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.88
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.88
Batch: 560; loss: 0.7; acc: 0.73
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.59; acc: 0.83
Batch: 640; loss: 0.41; acc: 0.86
Batch: 660; loss: 0.62; acc: 0.81
Batch: 680; loss: 0.27; acc: 0.95
Batch: 700; loss: 0.47; acc: 0.81
Batch: 720; loss: 0.47; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.88
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.42; train_accuracy: 0.87 

Batch: 0; loss: 0.67; acc: 0.73
Batch: 20; loss: 0.74; acc: 0.77
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 1.14; acc: 0.67
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.8
Batch: 120; loss: 0.82; acc: 0.78
Batch: 140; loss: 0.27; acc: 0.92
Val Epoch over. val_loss: 0.7081721865447463; val_accuracy: 0.7777667197452229 

The current subspace-distance is: 3.24571410601493e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.75
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.88
Batch: 100; loss: 0.65; acc: 0.77
Batch: 120; loss: 0.42; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.81
Batch: 220; loss: 0.41; acc: 0.89
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.88
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.84
Batch: 360; loss: 0.47; acc: 0.8
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.88
Batch: 420; loss: 0.39; acc: 0.84
Batch: 440; loss: 0.53; acc: 0.83
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.46; acc: 0.91
Batch: 500; loss: 0.84; acc: 0.8
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.41; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.91
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.84
Batch: 680; loss: 0.56; acc: 0.81
Batch: 700; loss: 0.59; acc: 0.81
Batch: 720; loss: 1.03; acc: 0.77
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.42; acc: 0.86
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.44; acc: 0.83
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 0.23; acc: 0.88
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.33; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.22; acc: 0.91
Val Epoch over. val_loss: 0.5159912358993178; val_accuracy: 0.8304140127388535 

The current subspace-distance is: 3.8078880606917664e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.77; acc: 0.77
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.68; acc: 0.8
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.48; acc: 0.88
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.5; acc: 0.89
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.36; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.35; acc: 0.89
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.91
Batch: 380; loss: 0.42; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.53; acc: 0.81
Batch: 460; loss: 0.34; acc: 0.86
Batch: 480; loss: 0.36; acc: 0.88
Batch: 500; loss: 0.35; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.91
Batch: 540; loss: 0.65; acc: 0.72
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.35; acc: 0.92
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.92
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.41; train_accuracy: 0.87 

Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.48; acc: 0.86
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.66; acc: 0.81
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.75; acc: 0.77
Batch: 140; loss: 0.1; acc: 1.0
Val Epoch over. val_loss: 0.36515328821957493; val_accuracy: 0.8854498407643312 

The current subspace-distance is: 4.223344149067998e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.66; acc: 0.84
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.31; acc: 0.94
Batch: 160; loss: 0.45; acc: 0.94
Batch: 180; loss: 0.67; acc: 0.83
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.89
Batch: 240; loss: 0.55; acc: 0.78
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.33; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.64; acc: 0.78
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.94
Batch: 440; loss: 0.31; acc: 0.88
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.68; acc: 0.81
Batch: 500; loss: 0.48; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.47; acc: 0.83
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.26; acc: 0.94
Batch: 660; loss: 0.54; acc: 0.84
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.84
Batch: 740; loss: 0.37; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.4; train_accuracy: 0.88 

Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.40328773376858157; val_accuracy: 0.8740047770700637 

The current subspace-distance is: 4.6870511141605675e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.8
Batch: 20; loss: 0.51; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.46; acc: 0.81
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.81
Batch: 160; loss: 0.24; acc: 0.89
Batch: 180; loss: 0.67; acc: 0.8
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.88
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.5; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.78; acc: 0.83
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.55; acc: 0.8
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.88
Batch: 600; loss: 0.42; acc: 0.92
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.33; acc: 0.86
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.34; acc: 0.88
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.53; acc: 0.8
Batch: 20; loss: 0.93; acc: 0.72
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.83; acc: 0.78
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.23; acc: 0.88
Batch: 120; loss: 1.1; acc: 0.73
Batch: 140; loss: 0.26; acc: 0.94
Val Epoch over. val_loss: 0.5208008637663665; val_accuracy: 0.841859076433121 

The current subspace-distance is: 5.134807724971324e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.81
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.37; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.86
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.84
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.42; acc: 0.83
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.39; acc: 0.84
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.69; acc: 0.75
Batch: 420; loss: 0.69; acc: 0.83
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.81
Batch: 540; loss: 0.51; acc: 0.86
Batch: 560; loss: 0.53; acc: 0.81
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.33; acc: 0.86
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.5; acc: 0.84
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.61; acc: 0.83
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.13; acc: 0.95
Val Epoch over. val_loss: 0.329116468145779; val_accuracy: 0.8988853503184714 

The current subspace-distance is: 5.526029781321995e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.81
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.42; acc: 0.84
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.38; acc: 0.89
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.43; acc: 0.83
Batch: 180; loss: 0.44; acc: 0.86
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.42; acc: 0.83
Batch: 240; loss: 0.54; acc: 0.86
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.89
Batch: 320; loss: 0.53; acc: 0.83
Batch: 340; loss: 0.51; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.88
Batch: 380; loss: 0.41; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.81
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.35; acc: 0.92
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.43; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.32; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.43; acc: 0.83
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.31; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.79; acc: 0.83
Batch: 40; loss: 0.11; acc: 0.97
Batch: 60; loss: 0.5; acc: 0.84
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.78
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.3467700620460662; val_accuracy: 0.890625 

The current subspace-distance is: 5.904875069973059e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.71; acc: 0.78
Batch: 20; loss: 0.27; acc: 0.95
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.92
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.49; acc: 0.88
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.22; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.48; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.88
Batch: 360; loss: 0.37; acc: 0.84
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.62; acc: 0.83
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.33; acc: 0.84
Batch: 480; loss: 0.33; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.88
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.38; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.38; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.14; acc: 0.92
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.61; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.98
Val Epoch over. val_loss: 0.35354883427832534; val_accuracy: 0.8889331210191083 

The current subspace-distance is: 6.352891068672761e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.53; acc: 0.81
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.89
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.29; acc: 0.89
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.48; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.47; acc: 0.81
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.88
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.64; acc: 0.81
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.88
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.88
Batch: 620; loss: 0.29; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.46; acc: 0.88
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.25; acc: 0.95
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.49; acc: 0.84
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.91; acc: 0.7
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.38729123410525595; val_accuracy: 0.8834593949044586 

The current subspace-distance is: 6.666735862381756e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.84
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.3; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.89
Batch: 260; loss: 0.35; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.42; acc: 0.89
Batch: 420; loss: 0.24; acc: 0.89
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.51; acc: 0.84
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.88
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.47; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.41; acc: 0.83
Batch: 20; loss: 0.63; acc: 0.81
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.78
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3707104284482397; val_accuracy: 0.8863455414012739 

The current subspace-distance is: 7.050118438201025e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.88
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.86
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.92
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.4; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.88
Batch: 220; loss: 0.21; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.44; acc: 0.84
Batch: 320; loss: 0.25; acc: 0.97
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.31; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.38; acc: 0.88
Batch: 460; loss: 0.46; acc: 0.86
Batch: 480; loss: 0.49; acc: 0.86
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.88
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.32; acc: 0.92
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.3; acc: 0.86
Batch: 20; loss: 0.75; acc: 0.72
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.51; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.97
Val Epoch over. val_loss: 0.36597536779513024; val_accuracy: 0.8857484076433121 

The current subspace-distance is: 7.287679909495637e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.89
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.62; acc: 0.88
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.86
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.94
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.38; acc: 0.86
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.95
Batch: 660; loss: 0.34; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.45; acc: 0.86
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.5; acc: 0.89
Batch: 780; loss: 0.4; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.65; acc: 0.81
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.46; acc: 0.86
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.58; acc: 0.83
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.31748042730199305; val_accuracy: 0.9021695859872612 

The current subspace-distance is: 7.471986464224756e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.91
Batch: 60; loss: 0.37; acc: 0.84
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.92
Batch: 140; loss: 0.4; acc: 0.84
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.45; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.86
Batch: 280; loss: 0.45; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.98
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.91
Batch: 440; loss: 0.41; acc: 0.88
Batch: 460; loss: 0.48; acc: 0.81
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.48; acc: 0.86
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.39; acc: 0.91
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.46; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.18; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.61; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.30055751725082186; val_accuracy: 0.9058519108280255 

The current subspace-distance is: 7.720349094597623e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.51; acc: 0.86
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.46; acc: 0.77
Batch: 260; loss: 0.4; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.27; acc: 0.89
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.43; acc: 0.89
Batch: 480; loss: 0.29; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.91
Batch: 540; loss: 0.6; acc: 0.86
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.36; acc: 0.86
Batch: 600; loss: 0.4; acc: 0.84
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.68; acc: 0.83
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.54; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.3119793149174019; val_accuracy: 0.9014729299363057 

The current subspace-distance is: 7.883292710175738e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.84
Batch: 20; loss: 0.14; acc: 0.98
Batch: 40; loss: 0.27; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.29; acc: 0.94
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.26; acc: 0.95
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.28; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.89
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.19; acc: 0.97
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.56; acc: 0.86
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.33; acc: 0.83
Batch: 500; loss: 0.43; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.28; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.89
Batch: 580; loss: 0.46; acc: 0.91
Batch: 600; loss: 0.44; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.24; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.91
Batch: 700; loss: 0.39; acc: 0.92
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.73; acc: 0.81
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.94
Val Epoch over. val_loss: 0.28725374945599563; val_accuracy: 0.9130175159235668 

The current subspace-distance is: 8.18680418888107e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.92
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.44; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.3; acc: 0.88
Batch: 200; loss: 0.2; acc: 0.95
Batch: 220; loss: 0.33; acc: 0.89
Batch: 240; loss: 0.47; acc: 0.81
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.31; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.88
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.38; acc: 0.84
Batch: 400; loss: 0.36; acc: 0.91
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.29; acc: 0.91
Batch: 500; loss: 0.19; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.88
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.29; acc: 0.92
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.27; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2858882839702497; val_accuracy: 0.9142117834394905 

The current subspace-distance is: 8.534041990060359e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.92
Batch: 160; loss: 0.24; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.94
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.46; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.42; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.94
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.33; acc: 0.86
Batch: 480; loss: 0.43; acc: 0.89
Batch: 500; loss: 0.47; acc: 0.91
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.92
Batch: 560; loss: 0.31; acc: 0.88
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.74; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.33; acc: 0.92
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.2; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.66; acc: 0.83
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.52; acc: 0.88
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.67; acc: 0.77
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.3033929912100552; val_accuracy: 0.9059514331210191 

The current subspace-distance is: 8.693067502463236e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.68; acc: 0.88
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.83
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.52; acc: 0.89
Batch: 200; loss: 0.19; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.51; acc: 0.81
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.44; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.46; acc: 0.89
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.31; acc: 0.89
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.86
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.8
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.27982710131034727; val_accuracy: 0.9150079617834395 

The current subspace-distance is: 8.96532874321565e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.18; acc: 0.97
Batch: 140; loss: 0.51; acc: 0.83
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.84
Batch: 340; loss: 0.31; acc: 0.92
Batch: 360; loss: 0.48; acc: 0.88
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.2; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.97
Batch: 460; loss: 0.54; acc: 0.88
Batch: 480; loss: 0.42; acc: 0.89
Batch: 500; loss: 0.31; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.43; acc: 0.84
Batch: 560; loss: 0.47; acc: 0.89
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.26; acc: 0.88
Batch: 760; loss: 0.22; acc: 0.95
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.28894854277657095; val_accuracy: 0.9117237261146497 

The current subspace-distance is: 9.1955327661708e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.26; acc: 0.86
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.26; acc: 0.95
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.18; acc: 0.89
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.88
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.3; acc: 0.88
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.95
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.92
Batch: 480; loss: 0.19; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.89
Batch: 540; loss: 0.71; acc: 0.83
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.14; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.2745425243428938; val_accuracy: 0.9159036624203821 

The current subspace-distance is: 9.46112340898253e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.41; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.83
Batch: 200; loss: 0.46; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.98
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.86
Batch: 720; loss: 0.42; acc: 0.89
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.34; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.08; acc: 1.0
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.28256736686275263; val_accuracy: 0.912718949044586 

The current subspace-distance is: 9.668461279943585e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.48; acc: 0.86
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.67; acc: 0.86
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.31; acc: 0.84
Batch: 360; loss: 0.41; acc: 0.92
Batch: 380; loss: 0.68; acc: 0.77
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.84
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.35; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.94
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.88
Batch: 660; loss: 0.31; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.89
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.65; acc: 0.88
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.89
Batch: 780; loss: 0.22; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.38; acc: 0.86
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.78
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.28048100258419467; val_accuracy: 0.9140127388535032 

The current subspace-distance is: 9.854501695372164e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.34; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.41; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.28; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.92
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.97
Batch: 520; loss: 0.08; acc: 1.0
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.97
Batch: 700; loss: 0.3; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.83
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.07; acc: 1.0
Batch: 60; loss: 0.45; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.27443653644080374; val_accuracy: 0.9166003184713376 

The current subspace-distance is: 0.00010065188689623028 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.98
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.24; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.47; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.57; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.83
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.3; acc: 0.89
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.52; acc: 0.88
Batch: 660; loss: 0.32; acc: 0.92
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.27120043781058045; val_accuracy: 0.9183917197452229 

The current subspace-distance is: 0.0001027264806907624 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.23; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.91
Batch: 140; loss: 0.51; acc: 0.8
Batch: 160; loss: 0.4; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.34; acc: 0.83
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.49; acc: 0.86
Batch: 280; loss: 0.23; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.54; acc: 0.86
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.89
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.57; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.89
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.89
Batch: 540; loss: 0.12; acc: 0.98
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.22; acc: 0.91
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.25; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.34; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.8
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3109499451935671; val_accuracy: 0.9032643312101911 

The current subspace-distance is: 0.00010511169239180163 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.36; acc: 0.94
Batch: 100; loss: 0.44; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.84
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.84
Batch: 180; loss: 0.32; acc: 0.88
Batch: 200; loss: 0.31; acc: 0.88
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.12; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.5; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.83
Batch: 320; loss: 0.49; acc: 0.84
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.39; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.95
Batch: 400; loss: 0.22; acc: 0.91
Batch: 420; loss: 0.19; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.25; acc: 0.92
Batch: 520; loss: 0.33; acc: 0.91
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.89
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.55; acc: 0.84
Batch: 660; loss: 0.3; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.94
Batch: 700; loss: 0.34; acc: 0.91
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.95
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.62; acc: 0.84
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.78
Batch: 140; loss: 0.15; acc: 0.95
Val Epoch over. val_loss: 0.29285486282151973; val_accuracy: 0.910031847133758 

The current subspace-distance is: 0.00010645185102475807 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.5; acc: 0.88
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.81
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.25; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.89
Batch: 180; loss: 0.34; acc: 0.86
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.24; acc: 0.91
Batch: 280; loss: 0.41; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.25; acc: 0.89
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.97
Batch: 420; loss: 0.45; acc: 0.86
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.98
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.53; acc: 0.86
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.6; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2811307711111512; val_accuracy: 0.9137141719745223 

The current subspace-distance is: 0.00010996296623488888 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.58; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.29; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.68; acc: 0.83
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.31; acc: 0.88
Batch: 300; loss: 0.32; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.18; acc: 0.97
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.38; acc: 0.84
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.92
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.92
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.27; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2685880543557322; val_accuracy: 0.9175955414012739 

The current subspace-distance is: 0.00011096306116087362 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.22; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.86
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.31; acc: 0.94
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.88
Batch: 260; loss: 0.31; acc: 0.88
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.89
Batch: 340; loss: 0.3; acc: 0.89
Batch: 360; loss: 0.17; acc: 0.97
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.92
Batch: 580; loss: 0.27; acc: 0.88
Batch: 600; loss: 0.26; acc: 0.92
Batch: 620; loss: 0.39; acc: 0.92
Batch: 640; loss: 0.25; acc: 0.89
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.88
Batch: 760; loss: 0.19; acc: 0.98
Batch: 780; loss: 0.24; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.2695695964773749; val_accuracy: 0.9182921974522293 

The current subspace-distance is: 0.0001128906587837264 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.43; acc: 0.86
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.97
Batch: 200; loss: 0.52; acc: 0.86
Batch: 220; loss: 0.32; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.34; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.89
Batch: 460; loss: 0.42; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.73; acc: 0.83
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.5; acc: 0.88
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.89
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.13; acc: 0.98
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2689513740408572; val_accuracy: 0.9178941082802548 

The current subspace-distance is: 0.00011529874609550461 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.91
Batch: 60; loss: 0.68; acc: 0.83
Batch: 80; loss: 0.25; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.54; acc: 0.84
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.34; acc: 0.88
Batch: 220; loss: 0.36; acc: 0.84
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.52; acc: 0.91
Batch: 280; loss: 0.45; acc: 0.83
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.46; acc: 0.89
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.88
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.91
Batch: 560; loss: 0.37; acc: 0.89
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.35; acc: 0.84
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.86
Batch: 740; loss: 0.39; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.86
Batch: 780; loss: 0.3; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.26913204101050736; val_accuracy: 0.919187898089172 

The current subspace-distance is: 0.00011764775990741327 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.97
Batch: 100; loss: 0.69; acc: 0.81
Batch: 120; loss: 0.31; acc: 0.89
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.33; acc: 0.88
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.2; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.88
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.21; acc: 0.89
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.4; acc: 0.92
Batch: 560; loss: 0.43; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.47; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.91
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.75; acc: 0.86
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.23; acc: 0.95
Batch: 720; loss: 0.4; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.83
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.56; acc: 0.83
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.27253125921176496; val_accuracy: 0.9161027070063694 

The current subspace-distance is: 0.0001189759059343487 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.92
Batch: 60; loss: 0.31; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.8
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.38; acc: 0.88
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.89
Batch: 240; loss: 0.41; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.25; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.27; acc: 0.92
Batch: 480; loss: 0.33; acc: 0.89
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.29; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.14; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.33; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.5; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.53; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.26986238642767735; val_accuracy: 0.918093152866242 

The current subspace-distance is: 0.0001206895976793021 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.91
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.27; acc: 0.91
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.95
Batch: 200; loss: 0.52; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.38; acc: 0.88
Batch: 320; loss: 0.12; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.97
Batch: 400; loss: 0.4; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.36; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.3; acc: 0.88
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.19; acc: 0.97
Batch: 660; loss: 0.45; acc: 0.88
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.8
Batch: 760; loss: 0.64; acc: 0.86
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.4; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.8
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.27306617772693087; val_accuracy: 0.9166003184713376 

The current subspace-distance is: 0.0001228482142323628 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.36; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.45; acc: 0.91
Batch: 280; loss: 0.59; acc: 0.77
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.35; acc: 0.86
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.32; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.43; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.37; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.29; acc: 0.88
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.12; acc: 0.95
Val Epoch over. val_loss: 0.2721529451857327; val_accuracy: 0.9156050955414012 

The current subspace-distance is: 0.00012584237265400589 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.92
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.42; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.47; acc: 0.84
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.27; acc: 0.92
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.28; acc: 0.95
Batch: 360; loss: 0.34; acc: 0.88
Batch: 380; loss: 0.25; acc: 0.92
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.26; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.91
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.15; acc: 0.98
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.42; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.2672300333524965; val_accuracy: 0.9185907643312102 

The current subspace-distance is: 0.00012829907063860446 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.97
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.72; acc: 0.8
Batch: 300; loss: 0.44; acc: 0.83
Batch: 320; loss: 0.23; acc: 0.89
Batch: 340; loss: 0.32; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.3; acc: 0.86
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.98
Batch: 500; loss: 0.35; acc: 0.88
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.31; acc: 0.94
Batch: 580; loss: 0.34; acc: 0.91
Batch: 600; loss: 0.34; acc: 0.86
Batch: 620; loss: 0.08; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.41; acc: 0.86
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.27201445006830677; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 0.00012920582958031446 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.83
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.26; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.32; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.27; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.47; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.88
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.81
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.36; acc: 0.89
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.43; acc: 0.91
Batch: 660; loss: 0.41; acc: 0.89
Batch: 680; loss: 0.38; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.28; acc: 0.95
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.26612236737540573; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 0.00012937966675963253 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.29; acc: 0.92
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.92
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.3; acc: 0.89
Batch: 300; loss: 0.25; acc: 0.88
Batch: 320; loss: 0.23; acc: 0.94
Batch: 340; loss: 0.35; acc: 0.88
Batch: 360; loss: 0.27; acc: 0.88
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.84
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.14; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.09; acc: 0.98
Batch: 600; loss: 0.18; acc: 0.95
Batch: 620; loss: 0.46; acc: 0.84
Batch: 640; loss: 0.27; acc: 0.92
Batch: 660; loss: 0.31; acc: 0.88
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.98
Batch: 720; loss: 0.4; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.89
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2665434647944702; val_accuracy: 0.9184912420382165 

The current subspace-distance is: 0.0001307808270212263 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.92
Batch: 140; loss: 0.35; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.27; acc: 0.91
Batch: 200; loss: 0.57; acc: 0.81
Batch: 220; loss: 0.43; acc: 0.92
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.4; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.95
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.21; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.32; acc: 0.89
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.27; acc: 0.91
Batch: 760; loss: 0.49; acc: 0.84
Batch: 780; loss: 0.11; acc: 0.97
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.51; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2667663156464221; val_accuracy: 0.9189888535031847 

The current subspace-distance is: 0.00013222619600128382 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.47; acc: 0.86
Batch: 40; loss: 0.31; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.38; acc: 0.84
Batch: 140; loss: 0.24; acc: 0.94
Batch: 160; loss: 0.09; acc: 1.0
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.38; acc: 0.92
Batch: 220; loss: 0.27; acc: 0.95
Batch: 240; loss: 0.24; acc: 0.92
Batch: 260; loss: 0.51; acc: 0.88
Batch: 280; loss: 0.24; acc: 0.95
Batch: 300; loss: 0.29; acc: 0.95
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.92
Batch: 420; loss: 0.23; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.29; acc: 0.97
Batch: 500; loss: 0.38; acc: 0.81
Batch: 520; loss: 0.17; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.89
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.33; acc: 0.89
Batch: 600; loss: 0.28; acc: 0.89
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.32; acc: 0.94
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.17; acc: 0.94
Batch: 760; loss: 0.3; acc: 0.88
Batch: 780; loss: 0.27; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.42; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.26761745269985715; val_accuracy: 0.918093152866242 

The current subspace-distance is: 0.00013345401384867728 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.95
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.84
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.2; acc: 0.95
Batch: 300; loss: 0.39; acc: 0.83
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.25; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.21; acc: 0.95
Batch: 420; loss: 0.29; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.77; acc: 0.86
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.41; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.31; acc: 0.91
Batch: 660; loss: 0.35; acc: 0.91
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.83; acc: 0.83
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.38; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.86
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.26711745688300226; val_accuracy: 0.9200835987261147 

The current subspace-distance is: 0.0001363517512800172 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.86
Batch: 20; loss: 0.24; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.98
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.43; acc: 0.92
Batch: 120; loss: 0.41; acc: 0.84
Batch: 140; loss: 0.32; acc: 0.94
Batch: 160; loss: 0.46; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.53; acc: 0.83
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.29; acc: 0.92
Batch: 260; loss: 0.35; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.89
Batch: 320; loss: 0.45; acc: 0.91
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.52; acc: 0.84
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.26; acc: 0.89
Batch: 440; loss: 0.18; acc: 0.95
Batch: 460; loss: 0.44; acc: 0.84
Batch: 480; loss: 0.42; acc: 0.83
Batch: 500; loss: 0.27; acc: 0.89
Batch: 520; loss: 0.46; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.89
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.53; acc: 0.83
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.43; acc: 0.88
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.49; acc: 0.86
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.92
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2662789810235333; val_accuracy: 0.9187898089171974 

The current subspace-distance is: 0.0001387471129419282 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.14; acc: 0.95
Batch: 160; loss: 0.51; acc: 0.83
Batch: 180; loss: 0.47; acc: 0.89
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.32; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.89
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.2; acc: 0.97
Batch: 400; loss: 0.33; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.26; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.97
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.17; acc: 0.92
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2660318880466519; val_accuracy: 0.9190883757961783 

The current subspace-distance is: 0.0001397866872139275 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.89
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.47; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.88
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.89
Batch: 340; loss: 0.34; acc: 0.91
Batch: 360; loss: 0.18; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.3; acc: 0.89
Batch: 480; loss: 0.19; acc: 0.97
Batch: 500; loss: 0.16; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.94
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.91
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.27; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.43; acc: 0.86
Batch: 700; loss: 0.28; acc: 0.91
Batch: 720; loss: 0.28; acc: 0.89
Batch: 740; loss: 0.35; acc: 0.89
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.09; acc: 0.98
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.84
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.26628217181772185; val_accuracy: 0.9188893312101911 

The current subspace-distance is: 0.0001425205555278808 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.33; acc: 0.91
Batch: 40; loss: 0.52; acc: 0.91
Batch: 60; loss: 0.47; acc: 0.88
Batch: 80; loss: 0.38; acc: 0.86
Batch: 100; loss: 0.39; acc: 0.83
Batch: 120; loss: 0.27; acc: 0.95
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.18; acc: 0.97
Batch: 180; loss: 0.33; acc: 0.88
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.46; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.23; acc: 0.89
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.32; acc: 0.91
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.98
Batch: 500; loss: 0.26; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.38; acc: 0.89
Batch: 600; loss: 0.37; acc: 0.86
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.7; acc: 0.8
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.2658623559459759; val_accuracy: 0.9192874203821656 

The current subspace-distance is: 0.0001440206542611122 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.33; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.29; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.94
Batch: 360; loss: 0.32; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.52; acc: 0.92
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.91
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.29; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.38; acc: 0.86
Batch: 640; loss: 0.31; acc: 0.88
Batch: 660; loss: 0.24; acc: 0.89
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.33; acc: 0.91
Batch: 720; loss: 0.47; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.57; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.07; acc: 0.98
Batch: 60; loss: 0.41; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.2684238887611468; val_accuracy: 0.9182921974522293 

The current subspace-distance is: 0.0001455897290725261 

plots/subspace_training/lenet/2020-01-22 14:35:09/d_dim_230_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 50719
elements in E: 10662240
fraction nonzero: 0.004756880355347469
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.09
Batch: 20; loss: 2.31; acc: 0.06
Batch: 40; loss: 2.31; acc: 0.08
Batch: 60; loss: 2.3; acc: 0.08
Batch: 80; loss: 2.3; acc: 0.09
Batch: 100; loss: 2.31; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.06
Batch: 140; loss: 2.29; acc: 0.11
Batch: 160; loss: 2.27; acc: 0.14
Batch: 180; loss: 2.29; acc: 0.11
Batch: 200; loss: 2.28; acc: 0.09
Batch: 220; loss: 2.26; acc: 0.17
Batch: 240; loss: 2.25; acc: 0.14
Batch: 260; loss: 2.24; acc: 0.17
Batch: 280; loss: 2.22; acc: 0.36
Batch: 300; loss: 2.18; acc: 0.44
Batch: 320; loss: 2.2; acc: 0.31
Batch: 340; loss: 2.14; acc: 0.39
Batch: 360; loss: 2.07; acc: 0.56
Batch: 380; loss: 1.98; acc: 0.53
Batch: 400; loss: 1.78; acc: 0.45
Batch: 420; loss: 1.57; acc: 0.56
Batch: 440; loss: 1.17; acc: 0.69
Batch: 460; loss: 1.29; acc: 0.58
Batch: 480; loss: 1.0; acc: 0.62
Batch: 500; loss: 0.8; acc: 0.73
Batch: 520; loss: 0.57; acc: 0.81
Batch: 540; loss: 0.77; acc: 0.69
Batch: 560; loss: 0.62; acc: 0.77
Batch: 580; loss: 0.83; acc: 0.81
Batch: 600; loss: 0.44; acc: 0.84
Batch: 620; loss: 0.7; acc: 0.8
Batch: 640; loss: 0.69; acc: 0.73
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.85; acc: 0.81
Batch: 700; loss: 0.75; acc: 0.73
Batch: 720; loss: 0.46; acc: 0.86
Batch: 740; loss: 0.71; acc: 0.77
Batch: 760; loss: 0.5; acc: 0.77
Batch: 780; loss: 0.72; acc: 0.78
Train Epoch over. train_loss: 1.54; train_accuracy: 0.47 

Batch: 0; loss: 0.44; acc: 0.81
Batch: 20; loss: 0.5; acc: 0.83
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.54; acc: 0.81
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.56; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.27; acc: 0.91
Val Epoch over. val_loss: 0.4883733424980929; val_accuracy: 0.8441480891719745 

The current subspace-distance is: 1.2145727851020638e-05 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 0.55; acc: 0.83
Batch: 20; loss: 0.5; acc: 0.86
Batch: 40; loss: 0.85; acc: 0.8
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.58; acc: 0.81
Batch: 100; loss: 0.56; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.72; acc: 0.78
Batch: 160; loss: 0.38; acc: 0.83
Batch: 180; loss: 0.71; acc: 0.75
Batch: 200; loss: 0.52; acc: 0.81
Batch: 220; loss: 0.42; acc: 0.84
Batch: 240; loss: 0.57; acc: 0.75
Batch: 260; loss: 0.78; acc: 0.83
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.56; acc: 0.84
Batch: 380; loss: 0.4; acc: 0.89
Batch: 400; loss: 0.41; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.56; acc: 0.86
Batch: 460; loss: 0.55; acc: 0.83
Batch: 480; loss: 0.51; acc: 0.84
Batch: 500; loss: 0.87; acc: 0.73
Batch: 520; loss: 0.58; acc: 0.78
Batch: 540; loss: 0.46; acc: 0.89
Batch: 560; loss: 0.58; acc: 0.8
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.53; acc: 0.86
Batch: 640; loss: 0.39; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.55; acc: 0.8
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.48; acc: 0.84
Batch: 740; loss: 0.48; acc: 0.89
Batch: 760; loss: 0.67; acc: 0.8
Batch: 780; loss: 0.42; acc: 0.84
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.58; acc: 0.81
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.57; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.3960268299338544; val_accuracy: 0.8741042993630573 

The current subspace-distance is: 2.124811362591572e-05 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.51; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.3; acc: 0.94
Batch: 160; loss: 0.44; acc: 0.84
Batch: 180; loss: 0.68; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.37; acc: 0.84
Batch: 260; loss: 0.31; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.84
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.26; acc: 0.92
Batch: 440; loss: 0.33; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.94
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.4; acc: 0.86
Batch: 580; loss: 0.35; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.48; acc: 0.88
Batch: 660; loss: 0.67; acc: 0.81
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.29; acc: 0.91
Batch: 740; loss: 0.86; acc: 0.75
Batch: 760; loss: 0.3; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.33; acc: 0.86
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.1; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.84
Batch: 140; loss: 0.07; acc: 1.0
Val Epoch over. val_loss: 0.34460747137570835; val_accuracy: 0.8939092356687898 

The current subspace-distance is: 2.7848307581734844e-05 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.48; acc: 0.91
Batch: 40; loss: 0.35; acc: 0.88
Batch: 60; loss: 0.48; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.37; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.56; acc: 0.84
Batch: 300; loss: 0.28; acc: 0.89
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.41; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.45; acc: 0.83
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.84
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.37; acc: 0.88
Batch: 620; loss: 0.59; acc: 0.86
Batch: 640; loss: 0.52; acc: 0.86
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.52; acc: 0.77
Batch: 20; loss: 0.47; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.7; acc: 0.75
Batch: 80; loss: 0.52; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.89
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.5579359089587904; val_accuracy: 0.8245421974522293 

The current subspace-distance is: 3.33728312398307e-05 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.92; acc: 0.7
Batch: 20; loss: 0.19; acc: 0.97
Batch: 40; loss: 0.4; acc: 0.86
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.68; acc: 0.83
Batch: 120; loss: 0.65; acc: 0.83
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.24; acc: 0.91
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.43; acc: 0.86
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.24; acc: 0.92
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.84
Batch: 340; loss: 0.33; acc: 0.89
Batch: 360; loss: 0.4; acc: 0.89
Batch: 380; loss: 0.3; acc: 0.92
Batch: 400; loss: 0.48; acc: 0.81
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.36; acc: 0.83
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.84
Batch: 500; loss: 0.47; acc: 0.83
Batch: 520; loss: 0.28; acc: 0.95
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.38; acc: 0.88
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.37; acc: 0.94
Batch: 620; loss: 0.32; acc: 0.88
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.53; acc: 0.88
Batch: 680; loss: 0.44; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.51; acc: 0.78
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.75; acc: 0.7
Batch: 20; loss: 0.71; acc: 0.67
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.62; acc: 0.8
Batch: 80; loss: 0.67; acc: 0.77
Batch: 100; loss: 0.58; acc: 0.78
Batch: 120; loss: 0.67; acc: 0.78
Batch: 140; loss: 0.18; acc: 0.89
Val Epoch over. val_loss: 0.5538939533719591; val_accuracy: 0.8169785031847133 

The current subspace-distance is: 3.865406688419171e-05 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.81; acc: 0.69
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.51; acc: 0.83
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.4; acc: 0.92
Batch: 180; loss: 0.41; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.43; acc: 0.83
Batch: 260; loss: 0.27; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.28; acc: 0.94
Batch: 320; loss: 0.22; acc: 0.95
Batch: 340; loss: 0.35; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.89
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.42; acc: 0.86
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.3; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.99; acc: 0.75
Batch: 560; loss: 0.49; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.89
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.47; acc: 0.84
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.41; acc: 0.84
Batch: 720; loss: 0.34; acc: 0.88
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.43; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.9 

Batch: 0; loss: 0.57; acc: 0.81
Batch: 20; loss: 0.46; acc: 0.84
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.48; acc: 0.83
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.52; acc: 0.83
Batch: 140; loss: 0.21; acc: 0.91
Val Epoch over. val_loss: 0.41697776369797956; val_accuracy: 0.8703224522292994 

The current subspace-distance is: 4.3440129957161844e-05 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.83
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.32; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.52; acc: 0.86
Batch: 180; loss: 0.66; acc: 0.86
Batch: 200; loss: 0.17; acc: 0.98
Batch: 220; loss: 0.3; acc: 0.91
Batch: 240; loss: 0.49; acc: 0.83
Batch: 260; loss: 0.22; acc: 0.92
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.29; acc: 0.92
Batch: 320; loss: 0.54; acc: 0.81
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.46; acc: 0.88
Batch: 380; loss: 0.45; acc: 0.89
Batch: 400; loss: 0.36; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.17; acc: 0.94
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.28; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.54; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.86
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.46; acc: 0.88
Batch: 620; loss: 0.37; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.86
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.36; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.33; train_accuracy: 0.9 

Batch: 0; loss: 0.43; acc: 0.78
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.38; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2783838297436192; val_accuracy: 0.9141122611464968 

The current subspace-distance is: 4.767440987052396e-05 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.88
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.3; acc: 0.88
Batch: 180; loss: 0.59; acc: 0.83
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.88
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.92
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.3; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.43; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.49; acc: 0.81
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.84
Batch: 520; loss: 0.28; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.89
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.45; acc: 0.88
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.83
Batch: 660; loss: 0.51; acc: 0.86
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.36; acc: 0.86
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.39; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.91
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.84; acc: 0.78
Batch: 20; loss: 0.75; acc: 0.73
Batch: 40; loss: 0.17; acc: 0.94
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 0.52; acc: 0.83
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.82; acc: 0.69
Batch: 140; loss: 0.22; acc: 0.89
Val Epoch over. val_loss: 0.5367895311136155; val_accuracy: 0.828125 

The current subspace-distance is: 5.094250445836224e-05 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.6; acc: 0.78
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.34; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.33; acc: 0.83
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.44; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.89
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.19; acc: 0.92
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.4; acc: 0.84
Batch: 420; loss: 0.61; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.33; acc: 0.92
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.26; acc: 0.92
Batch: 520; loss: 0.31; acc: 0.89
Batch: 540; loss: 0.57; acc: 0.88
Batch: 560; loss: 0.53; acc: 0.86
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.19; acc: 0.88
Batch: 640; loss: 0.28; acc: 0.91
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.51; acc: 0.84
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.48; acc: 0.81
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.43; acc: 0.8
Batch: 20; loss: 0.27; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.84
Batch: 60; loss: 0.44; acc: 0.81
Batch: 80; loss: 0.21; acc: 0.92
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.81
Batch: 140; loss: 0.14; acc: 0.92
Val Epoch over. val_loss: 0.3816610172296026; val_accuracy: 0.8743033439490446 

The current subspace-distance is: 5.5286458518821746e-05 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.67; acc: 0.77
Batch: 20; loss: 0.4; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.29; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.31; acc: 0.91
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.43; acc: 0.83
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.34; acc: 0.88
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.11; acc: 0.98
Batch: 300; loss: 0.25; acc: 0.88
Batch: 320; loss: 0.46; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.94
Batch: 360; loss: 0.75; acc: 0.8
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.94
Batch: 440; loss: 0.47; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.86
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.21; acc: 0.92
Batch: 520; loss: 0.38; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.86
Batch: 560; loss: 0.22; acc: 0.91
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.36; acc: 0.94
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.42; acc: 0.89
Batch: 680; loss: 0.4; acc: 0.86
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.86
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.42; acc: 0.84
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.30666733587718314; val_accuracy: 0.9076433121019108 

The current subspace-distance is: 5.918960596318357e-05 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.65; acc: 0.81
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.39; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.5; acc: 0.86
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.41; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.88
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.38; acc: 0.92
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.94
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.91
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.3; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.19; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.97
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.49; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.86
Batch: 700; loss: 0.35; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.94
Batch: 780; loss: 0.38; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.28433652082161537; val_accuracy: 0.9131170382165605 

The current subspace-distance is: 6.183954246807843e-05 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.06; acc: 1.0
Batch: 200; loss: 0.4; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.35; acc: 0.91
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.3; acc: 0.91
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.41; acc: 0.91
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.19; acc: 0.94
Batch: 500; loss: 0.38; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.83
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.26; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.84
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.27; acc: 0.91
Batch: 720; loss: 0.34; acc: 0.89
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.86
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.05; acc: 0.98
Val Epoch over. val_loss: 0.2739490698904369; val_accuracy: 0.9145103503184714 

The current subspace-distance is: 6.469204527093098e-05 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.3; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.33; acc: 0.86
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.26; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.27; acc: 0.97
Batch: 280; loss: 0.41; acc: 0.86
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.98
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.13; acc: 0.97
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.86
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.22; acc: 0.92
Batch: 600; loss: 0.3; acc: 0.88
Batch: 620; loss: 0.22; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.3; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.88
Batch: 700; loss: 0.32; acc: 0.94
Batch: 720; loss: 0.4; acc: 0.83
Batch: 740; loss: 0.19; acc: 0.97
Batch: 760; loss: 0.45; acc: 0.89
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.33; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2659620382129007; val_accuracy: 0.9219745222929936 

The current subspace-distance is: 6.815289816586301e-05 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.44; acc: 0.88
Batch: 160; loss: 0.47; acc: 0.88
Batch: 180; loss: 0.23; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.15; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.94
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.16; acc: 0.95
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.24; acc: 0.92
Batch: 380; loss: 0.29; acc: 0.89
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.36; acc: 0.91
Batch: 480; loss: 0.28; acc: 0.86
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.91
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.45; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.42; acc: 0.84
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.38; acc: 0.81
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.17; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.88
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.88
Batch: 140; loss: 0.05; acc: 1.0
Val Epoch over. val_loss: 0.2995068601741912; val_accuracy: 0.9065485668789809 

The current subspace-distance is: 7.093229214660823e-05 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.29; acc: 0.86
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.91
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.2; acc: 0.92
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.38; acc: 0.89
Batch: 240; loss: 0.2; acc: 0.91
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.25; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.92
Batch: 320; loss: 0.19; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.92
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.89
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.23; acc: 0.88
Batch: 620; loss: 0.49; acc: 0.91
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.88
Batch: 760; loss: 0.28; acc: 0.88
Batch: 780; loss: 0.53; acc: 0.84
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.26; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.29061523881877305; val_accuracy: 0.9084394904458599 

The current subspace-distance is: 7.401048060273752e-05 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.88
Batch: 60; loss: 0.16; acc: 0.97
Batch: 80; loss: 0.17; acc: 0.94
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.36; acc: 0.91
Batch: 220; loss: 0.53; acc: 0.88
Batch: 240; loss: 0.17; acc: 0.97
Batch: 260; loss: 0.41; acc: 0.89
Batch: 280; loss: 0.35; acc: 0.83
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.88
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.32; acc: 0.86
Batch: 460; loss: 0.53; acc: 0.83
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.46; acc: 0.91
Batch: 520; loss: 0.41; acc: 0.88
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.31; acc: 0.86
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.32; acc: 0.84
Batch: 680; loss: 0.29; acc: 0.89
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.89
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.94
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.2743928584323567; val_accuracy: 0.9167993630573248 

The current subspace-distance is: 7.694117084611207e-05 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.88
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.2; acc: 0.94
Batch: 140; loss: 0.31; acc: 0.92
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.4; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.88
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.92
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.59; acc: 0.86
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.54; acc: 0.81
Batch: 600; loss: 0.34; acc: 0.89
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.27; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.86
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.88
Batch: 40; loss: 0.13; acc: 0.94
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.26; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2573977617463868; val_accuracy: 0.9200835987261147 

The current subspace-distance is: 8.045146387303248e-05 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.84
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.52; acc: 0.83
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.34; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.52; acc: 0.86
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.81
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.36; acc: 0.88
Batch: 480; loss: 0.35; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.32; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.19; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.2; acc: 0.89
Batch: 680; loss: 0.27; acc: 0.89
Batch: 700; loss: 0.38; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.46; acc: 0.89
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.11; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2714505820612239; val_accuracy: 0.9186902866242038 

The current subspace-distance is: 8.321288623847067e-05 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.44; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.35; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.89
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.89
Batch: 240; loss: 0.35; acc: 0.91
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.94
Batch: 300; loss: 0.4; acc: 0.89
Batch: 320; loss: 0.21; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.09; acc: 0.98
Batch: 380; loss: 0.37; acc: 0.83
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.34; acc: 0.86
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.94
Batch: 480; loss: 0.33; acc: 0.86
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.39; acc: 0.89
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.44; acc: 0.86
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.22; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.26; acc: 0.97
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.17; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.86
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.24583076823289227; val_accuracy: 0.924562101910828 

The current subspace-distance is: 8.570680802222341e-05 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.98
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.27; acc: 0.89
Batch: 180; loss: 0.35; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.91
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.94
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.21; acc: 0.97
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.92
Batch: 480; loss: 0.35; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.97
Batch: 520; loss: 0.21; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.53; acc: 0.88
Batch: 660; loss: 0.5; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.47; acc: 0.84
Batch: 740; loss: 0.21; acc: 0.89
Batch: 760; loss: 0.28; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.92
Batch: 100; loss: 0.24; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.25669270580646336; val_accuracy: 0.9196855095541401 

The current subspace-distance is: 8.772792352829129e-05 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.2; acc: 0.92
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.97
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.47; acc: 0.88
Batch: 140; loss: 0.2; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.92
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.94
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.4; acc: 0.86
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.09; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.92
Batch: 360; loss: 0.21; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.94
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.24; acc: 0.95
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.89
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.38; acc: 0.91
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.98
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.94
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.24005549125800466; val_accuracy: 0.9250597133757962 

The current subspace-distance is: 9.01666862773709e-05 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.39; acc: 0.86
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.15; acc: 0.97
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.51; acc: 0.81
Batch: 160; loss: 0.17; acc: 0.95
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.14; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.43; acc: 0.86
Batch: 340; loss: 0.33; acc: 0.91
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.89
Batch: 480; loss: 0.16; acc: 0.94
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.29; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.3; acc: 0.88
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.15; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.35; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.18; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.24435978408926612; val_accuracy: 0.9243630573248408 

The current subspace-distance is: 9.200444037560374e-05 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.84
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.28; acc: 0.94
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.86
Batch: 280; loss: 0.29; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.42; acc: 0.91
Batch: 340; loss: 0.32; acc: 0.91
Batch: 360; loss: 0.33; acc: 0.84
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.21; acc: 0.95
Batch: 440; loss: 0.46; acc: 0.88
Batch: 460; loss: 0.33; acc: 0.91
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.15; acc: 0.94
Batch: 520; loss: 0.19; acc: 0.89
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.16; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.19; acc: 0.92
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.91
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.27; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2455064732652561; val_accuracy: 0.9230692675159236 

The current subspace-distance is: 9.452613448956981e-05 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.98
Batch: 140; loss: 0.11; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.89
Batch: 180; loss: 0.63; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.36; acc: 0.88
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.12; acc: 0.97
Batch: 320; loss: 0.43; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.16; acc: 0.92
Batch: 460; loss: 0.18; acc: 0.92
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.21; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.22; acc: 0.95
Batch: 560; loss: 0.27; acc: 0.88
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.17; acc: 0.94
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.39; acc: 0.92
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23734264716411094; val_accuracy: 0.9262539808917197 

The current subspace-distance is: 9.69381580944173e-05 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.21; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.32; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.92
Batch: 140; loss: 0.45; acc: 0.86
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.89
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.68; acc: 0.86
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.27; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.95
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.07; acc: 1.0
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.89
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.09; acc: 0.98
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.4; acc: 0.89
Batch: 700; loss: 0.13; acc: 0.95
Batch: 720; loss: 0.62; acc: 0.89
Batch: 740; loss: 0.44; acc: 0.91
Batch: 760; loss: 0.21; acc: 0.92
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.23686810198483194; val_accuracy: 0.9255573248407644 

The current subspace-distance is: 9.929329826263711e-05 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.28; acc: 0.86
Batch: 40; loss: 0.09; acc: 0.98
Batch: 60; loss: 0.16; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.91
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.27; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.94
Batch: 300; loss: 0.28; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.97
Batch: 340; loss: 0.24; acc: 0.95
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.25; acc: 0.95
Batch: 400; loss: 0.26; acc: 0.89
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.34; acc: 0.83
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.88
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23946155073821165; val_accuracy: 0.9255573248407644 

The current subspace-distance is: 0.0001007847095024772 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.12; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.98
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.92
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.15; acc: 0.97
Batch: 380; loss: 0.43; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.92
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.26; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.91
Batch: 480; loss: 0.33; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.24; acc: 0.89
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.31; acc: 0.89
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23665193107667243; val_accuracy: 0.9261544585987261 

The current subspace-distance is: 0.00010338272113585845 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.18; acc: 0.89
Batch: 220; loss: 0.2; acc: 0.91
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.91
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.09; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.36; acc: 0.88
Batch: 400; loss: 0.12; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.95
Batch: 440; loss: 0.25; acc: 0.89
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.16; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.86
Batch: 580; loss: 0.34; acc: 0.88
Batch: 600; loss: 0.24; acc: 0.92
Batch: 620; loss: 0.11; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.29; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.37; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.17; acc: 0.95
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.34; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2576538969993971; val_accuracy: 0.9187898089171974 

The current subspace-distance is: 0.00010471314453752711 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.28; acc: 0.88
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.97
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.21; acc: 0.91
Batch: 300; loss: 0.23; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.46; acc: 0.89
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.25; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.98
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.49; acc: 0.88
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.19; acc: 0.94
Batch: 540; loss: 0.11; acc: 0.97
Batch: 560; loss: 0.59; acc: 0.88
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.94
Batch: 660; loss: 0.15; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.95
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.29; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.24026013592815704; val_accuracy: 0.9253582802547771 

The current subspace-distance is: 0.00010618609667290002 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.56; acc: 0.88
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.38; acc: 0.91
Batch: 200; loss: 0.18; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.37; acc: 0.94
Batch: 320; loss: 0.28; acc: 0.94
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.92
Batch: 500; loss: 0.14; acc: 0.95
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.38; acc: 0.88
Batch: 560; loss: 0.43; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.88
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.95
Batch: 700; loss: 0.18; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23811448294266013; val_accuracy: 0.926453025477707 

The current subspace-distance is: 0.00010826707148225978 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.88
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.88
Batch: 140; loss: 0.25; acc: 0.91
Batch: 160; loss: 0.26; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.58; acc: 0.86
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.25; acc: 0.95
Batch: 320; loss: 0.23; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.31; acc: 0.91
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.27; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.12; acc: 0.95
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.16; acc: 0.98
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.4; acc: 0.92
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.19; acc: 0.94
Batch: 780; loss: 0.31; acc: 0.89
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2333899509565086; val_accuracy: 0.9267515923566879 

The current subspace-distance is: 0.00010971078154398128 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.94
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.86
Batch: 160; loss: 0.26; acc: 0.94
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.95
Batch: 260; loss: 0.34; acc: 0.92
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.21; acc: 0.95
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.38; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.33; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.91
Batch: 480; loss: 0.35; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.16; acc: 0.94
Batch: 540; loss: 0.37; acc: 0.91
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.27; acc: 0.89
Batch: 620; loss: 0.34; acc: 0.91
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.94
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.3; acc: 0.91
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.29; acc: 0.88
Batch: 780; loss: 0.46; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2332641479980414; val_accuracy: 0.9276472929936306 

The current subspace-distance is: 0.00011161368456669152 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.92
Batch: 40; loss: 0.27; acc: 0.86
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.38; acc: 0.92
Batch: 160; loss: 0.29; acc: 0.91
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.42; acc: 0.86
Batch: 220; loss: 0.23; acc: 0.89
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.86
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.21; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.44; acc: 0.91
Batch: 420; loss: 0.18; acc: 0.97
Batch: 440; loss: 0.29; acc: 0.92
Batch: 460; loss: 0.28; acc: 0.89
Batch: 480; loss: 0.38; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.89
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.32; acc: 0.91
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.38; acc: 0.91
Batch: 640; loss: 0.2; acc: 0.97
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.91
Batch: 700; loss: 0.3; acc: 0.86
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.18; acc: 0.97
Batch: 760; loss: 0.36; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.26; acc: 0.91
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2341847723931264; val_accuracy: 0.9271496815286624 

The current subspace-distance is: 0.00011382896627765149 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.49; acc: 0.89
Batch: 60; loss: 0.52; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.4; acc: 0.86
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.45; acc: 0.86
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.36; acc: 0.86
Batch: 260; loss: 0.3; acc: 0.89
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.29; acc: 0.95
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.94
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.94
Batch: 620; loss: 0.12; acc: 0.94
Batch: 640; loss: 0.5; acc: 0.83
Batch: 660; loss: 0.27; acc: 0.88
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.49; acc: 0.92
Batch: 760; loss: 0.25; acc: 0.91
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.25; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23131669481184072; val_accuracy: 0.9287420382165605 

The current subspace-distance is: 0.00011498122330522165 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.92
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.2; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.92
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.25; acc: 0.92
Batch: 280; loss: 0.26; acc: 0.92
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.41; acc: 0.92
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.37; acc: 0.88
Batch: 500; loss: 0.29; acc: 0.91
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.57; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.97
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.91
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23179333374663524; val_accuracy: 0.9282444267515924 

The current subspace-distance is: 0.00011689194798236713 

Epoch 36 start
The current lr is: 0.06400000000000002
