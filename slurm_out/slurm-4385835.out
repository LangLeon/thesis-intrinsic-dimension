Namespace(batch_size=64, chunked=False, ddim_vs_acc=True, dense=False, device=device(type='cuda'), lr=1.0, model='reg_lenet_2', n_epochs=50, non_wrapped=False, optimizer='SGD', parameter_correction=False, print_freq=20, print_prec=2, schedule=True, schedule_freq=10, schedule_gamma=0.4, seed=1, subspace_training=True, timestamp='2020-01-19 19:11:48')
nonzero elements in E: 7110
elements in E: 988700
fraction nonzero: 0.007191261252149287
Epoch 1 start
The current lr is: 1.0
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.32; acc: 0.05
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.31; acc: 0.08
Batch: 200; loss: 2.31; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.3; acc: 0.12
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.31; acc: 0.11
Batch: 320; loss: 2.3; acc: 0.05
Batch: 340; loss: 2.32; acc: 0.11
Batch: 360; loss: 2.3; acc: 0.08
Batch: 380; loss: 2.3; acc: 0.11
Batch: 400; loss: 2.28; acc: 0.16
Batch: 420; loss: 2.31; acc: 0.09
Batch: 440; loss: 2.3; acc: 0.12
Batch: 460; loss: 2.3; acc: 0.09
Batch: 480; loss: 2.29; acc: 0.11
Batch: 500; loss: 2.31; acc: 0.08
Batch: 520; loss: 2.31; acc: 0.05
Batch: 540; loss: 2.31; acc: 0.08
Batch: 560; loss: 2.3; acc: 0.11
Batch: 580; loss: 2.3; acc: 0.05
Batch: 600; loss: 2.3; acc: 0.08
Batch: 620; loss: 2.3; acc: 0.09
Batch: 640; loss: 2.3; acc: 0.12
Batch: 660; loss: 2.29; acc: 0.12
Batch: 680; loss: 2.31; acc: 0.08
Batch: 700; loss: 2.3; acc: 0.11
Batch: 720; loss: 2.3; acc: 0.11
Batch: 740; loss: 2.31; acc: 0.05
Batch: 760; loss: 2.3; acc: 0.09
Batch: 780; loss: 2.29; acc: 0.16
Train Epoch over. train_loss: 2.3; train_accuracy: 0.1 

Batch: 0; loss: 2.3; acc: 0.08
Batch: 20; loss: 2.31; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.12
Batch: 60; loss: 2.3; acc: 0.09
Batch: 80; loss: 2.29; acc: 0.08
Batch: 100; loss: 2.3; acc: 0.16
Batch: 120; loss: 2.31; acc: 0.09
Batch: 140; loss: 2.3; acc: 0.12
Val Epoch over. val_loss: 2.2978663338217764; val_accuracy: 0.10877786624203821 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.08
Batch: 40; loss: 2.29; acc: 0.16
Batch: 60; loss: 2.29; acc: 0.14
Batch: 80; loss: 2.29; acc: 0.09
Batch: 100; loss: 2.3; acc: 0.11
Batch: 120; loss: 2.3; acc: 0.09
Batch: 140; loss: 2.31; acc: 0.06
Batch: 160; loss: 2.29; acc: 0.14
Batch: 180; loss: 2.29; acc: 0.17
Batch: 200; loss: 2.31; acc: 0.08
Batch: 220; loss: 2.29; acc: 0.17
Batch: 240; loss: 2.3; acc: 0.12
Batch: 260; loss: 2.29; acc: 0.11
Batch: 280; loss: 2.3; acc: 0.16
Batch: 300; loss: 2.3; acc: 0.16
Batch: 320; loss: 2.31; acc: 0.05
Batch: 340; loss: 2.29; acc: 0.16
Batch: 360; loss: 2.28; acc: 0.22
Batch: 380; loss: 2.31; acc: 0.12
Batch: 400; loss: 2.29; acc: 0.19
Batch: 420; loss: 2.3; acc: 0.17
Batch: 440; loss: 2.29; acc: 0.16
Batch: 460; loss: 2.29; acc: 0.17
Batch: 480; loss: 2.3; acc: 0.12
Batch: 500; loss: 2.3; acc: 0.14
Batch: 520; loss: 2.3; acc: 0.2
Batch: 540; loss: 2.3; acc: 0.14
Batch: 560; loss: 2.31; acc: 0.11
Batch: 580; loss: 2.29; acc: 0.27
Batch: 600; loss: 2.3; acc: 0.17
Batch: 620; loss: 2.3; acc: 0.17
Batch: 640; loss: 2.29; acc: 0.17
Batch: 660; loss: 2.3; acc: 0.19
Batch: 680; loss: 2.3; acc: 0.22
Batch: 700; loss: 2.29; acc: 0.25
Batch: 720; loss: 2.29; acc: 0.2
Batch: 740; loss: 2.3; acc: 0.17
Batch: 760; loss: 2.29; acc: 0.23
Batch: 780; loss: 2.3; acc: 0.14
Train Epoch over. train_loss: 2.3; train_accuracy: 0.16 

Batch: 0; loss: 2.29; acc: 0.17
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.22
Batch: 60; loss: 2.29; acc: 0.2
Batch: 80; loss: 2.29; acc: 0.2
Batch: 100; loss: 2.3; acc: 0.14
Batch: 120; loss: 2.3; acc: 0.14
Batch: 140; loss: 2.29; acc: 0.22
Val Epoch over. val_loss: 2.293655747820617; val_accuracy: 0.17894108280254778 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.16
Batch: 40; loss: 2.29; acc: 0.2
Batch: 60; loss: 2.3; acc: 0.17
Batch: 80; loss: 2.29; acc: 0.2
Batch: 100; loss: 2.29; acc: 0.16
Batch: 120; loss: 2.3; acc: 0.12
Batch: 140; loss: 2.29; acc: 0.19
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.29; acc: 0.17
Batch: 200; loss: 2.29; acc: 0.14
Batch: 220; loss: 2.29; acc: 0.12
Batch: 240; loss: 2.29; acc: 0.14
Batch: 260; loss: 2.29; acc: 0.14
Batch: 280; loss: 2.29; acc: 0.16
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.16
Batch: 340; loss: 2.29; acc: 0.11
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.3; acc: 0.08
Batch: 400; loss: 2.29; acc: 0.19
Batch: 420; loss: 2.29; acc: 0.11
Batch: 440; loss: 2.3; acc: 0.08
Batch: 460; loss: 2.29; acc: 0.09
Batch: 480; loss: 2.29; acc: 0.16
Batch: 500; loss: 2.29; acc: 0.14
Batch: 520; loss: 2.29; acc: 0.12
Batch: 540; loss: 2.29; acc: 0.17
Batch: 560; loss: 2.28; acc: 0.16
Batch: 580; loss: 2.3; acc: 0.08
Batch: 600; loss: 2.29; acc: 0.12
Batch: 620; loss: 2.28; acc: 0.17
Batch: 640; loss: 2.29; acc: 0.16
Batch: 660; loss: 2.28; acc: 0.2
Batch: 680; loss: 2.3; acc: 0.11
Batch: 700; loss: 2.29; acc: 0.16
Batch: 720; loss: 2.28; acc: 0.19
Batch: 740; loss: 2.28; acc: 0.17
Batch: 760; loss: 2.28; acc: 0.14
Batch: 780; loss: 2.29; acc: 0.09
Train Epoch over. train_loss: 2.29; train_accuracy: 0.15 

Batch: 0; loss: 2.29; acc: 0.16
Batch: 20; loss: 2.29; acc: 0.08
Batch: 40; loss: 2.28; acc: 0.14
Batch: 60; loss: 2.28; acc: 0.16
Batch: 80; loss: 2.28; acc: 0.16
Batch: 100; loss: 2.29; acc: 0.08
Batch: 120; loss: 2.29; acc: 0.12
Batch: 140; loss: 2.28; acc: 0.12
Val Epoch over. val_loss: 2.288645395048105; val_accuracy: 0.129578025477707 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.14
Batch: 20; loss: 2.29; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.05
Batch: 60; loss: 2.3; acc: 0.11
Batch: 80; loss: 2.29; acc: 0.16
Batch: 100; loss: 2.28; acc: 0.16
Batch: 120; loss: 2.28; acc: 0.17
Batch: 140; loss: 2.28; acc: 0.14
Batch: 160; loss: 2.29; acc: 0.09
Batch: 180; loss: 2.28; acc: 0.12
Batch: 200; loss: 2.28; acc: 0.12
Batch: 220; loss: 2.27; acc: 0.23
Batch: 240; loss: 2.29; acc: 0.11
Batch: 260; loss: 2.28; acc: 0.17
Batch: 280; loss: 2.29; acc: 0.12
Batch: 300; loss: 2.3; acc: 0.11
Batch: 320; loss: 2.28; acc: 0.2
Batch: 340; loss: 2.31; acc: 0.08
Batch: 360; loss: 2.28; acc: 0.17
Batch: 380; loss: 2.29; acc: 0.22
Batch: 400; loss: 2.27; acc: 0.19
Batch: 420; loss: 2.28; acc: 0.17
Batch: 440; loss: 2.28; acc: 0.14
Batch: 460; loss: 2.27; acc: 0.16
Batch: 480; loss: 2.28; acc: 0.17
Batch: 500; loss: 2.28; acc: 0.19
Batch: 520; loss: 2.29; acc: 0.17
Batch: 540; loss: 2.28; acc: 0.22
Batch: 560; loss: 2.29; acc: 0.14
Batch: 580; loss: 2.29; acc: 0.08
Batch: 600; loss: 2.28; acc: 0.19
Batch: 620; loss: 2.28; acc: 0.16
Batch: 640; loss: 2.26; acc: 0.23
Batch: 660; loss: 2.28; acc: 0.16
Batch: 680; loss: 2.24; acc: 0.33
Batch: 700; loss: 2.29; acc: 0.19
Batch: 720; loss: 2.29; acc: 0.14
Batch: 740; loss: 2.29; acc: 0.16
Batch: 760; loss: 2.29; acc: 0.14
Batch: 780; loss: 2.29; acc: 0.09
Train Epoch over. train_loss: 2.28; train_accuracy: 0.16 

Batch: 0; loss: 2.28; acc: 0.17
Batch: 20; loss: 2.29; acc: 0.14
Batch: 40; loss: 2.27; acc: 0.16
Batch: 60; loss: 2.27; acc: 0.19
Batch: 80; loss: 2.27; acc: 0.2
Batch: 100; loss: 2.29; acc: 0.11
Batch: 120; loss: 2.28; acc: 0.12
Batch: 140; loss: 2.27; acc: 0.23
Val Epoch over. val_loss: 2.2786577932394234; val_accuracy: 0.16291799363057324 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.28; acc: 0.19
Batch: 20; loss: 2.27; acc: 0.22
Batch: 40; loss: 2.27; acc: 0.22
Batch: 60; loss: 2.26; acc: 0.2
Batch: 80; loss: 2.27; acc: 0.22
Batch: 100; loss: 2.29; acc: 0.09
Batch: 120; loss: 2.26; acc: 0.25
Batch: 140; loss: 2.27; acc: 0.25
Batch: 160; loss: 2.27; acc: 0.2
Batch: 180; loss: 2.28; acc: 0.14
Batch: 200; loss: 2.26; acc: 0.22
Batch: 220; loss: 2.28; acc: 0.19
Batch: 240; loss: 2.28; acc: 0.22
Batch: 260; loss: 2.28; acc: 0.16
Batch: 280; loss: 2.27; acc: 0.19
Batch: 300; loss: 2.25; acc: 0.27
Batch: 320; loss: 2.27; acc: 0.17
Batch: 340; loss: 2.24; acc: 0.25
Batch: 360; loss: 2.26; acc: 0.23
Batch: 380; loss: 2.27; acc: 0.2
Batch: 400; loss: 2.29; acc: 0.08
Batch: 420; loss: 2.28; acc: 0.14
Batch: 440; loss: 2.27; acc: 0.12
Batch: 460; loss: 2.25; acc: 0.22
Batch: 480; loss: 2.27; acc: 0.12
Batch: 500; loss: 2.23; acc: 0.23
Batch: 520; loss: 2.28; acc: 0.12
Batch: 540; loss: 2.27; acc: 0.11
Batch: 560; loss: 2.27; acc: 0.09
Batch: 580; loss: 2.27; acc: 0.14
Batch: 600; loss: 2.22; acc: 0.27
Batch: 620; loss: 2.29; acc: 0.08
Batch: 640; loss: 2.27; acc: 0.14
Batch: 660; loss: 2.24; acc: 0.17
Batch: 680; loss: 2.26; acc: 0.09
Batch: 700; loss: 2.27; acc: 0.11
Batch: 720; loss: 2.22; acc: 0.27
Batch: 740; loss: 2.29; acc: 0.06
Batch: 760; loss: 2.23; acc: 0.16
Batch: 780; loss: 2.22; acc: 0.19
Train Epoch over. train_loss: 2.26; train_accuracy: 0.18 

Batch: 0; loss: 2.24; acc: 0.16
Batch: 20; loss: 2.26; acc: 0.14
Batch: 40; loss: 2.22; acc: 0.17
Batch: 60; loss: 2.23; acc: 0.2
Batch: 80; loss: 2.2; acc: 0.2
Batch: 100; loss: 2.27; acc: 0.12
Batch: 120; loss: 2.26; acc: 0.14
Batch: 140; loss: 2.22; acc: 0.27
Val Epoch over. val_loss: 2.2331363866283636; val_accuracy: 0.17973726114649682 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 2.23; acc: 0.16
Batch: 20; loss: 2.27; acc: 0.12
Batch: 40; loss: 2.16; acc: 0.25
Batch: 60; loss: 2.23; acc: 0.17
Batch: 80; loss: 2.24; acc: 0.12
Batch: 100; loss: 2.23; acc: 0.17
Batch: 120; loss: 2.23; acc: 0.14
Batch: 140; loss: 2.21; acc: 0.19
Batch: 160; loss: 2.18; acc: 0.25
Batch: 180; loss: 2.11; acc: 0.3
Batch: 200; loss: 2.18; acc: 0.2
Batch: 220; loss: 2.19; acc: 0.23
Batch: 240; loss: 2.21; acc: 0.17
Batch: 260; loss: 2.15; acc: 0.23
Batch: 280; loss: 2.1; acc: 0.34
Batch: 300; loss: 2.04; acc: 0.33
Batch: 320; loss: 2.1; acc: 0.31
Batch: 340; loss: 2.01; acc: 0.31
Batch: 360; loss: 2.1; acc: 0.27
Batch: 380; loss: 2.09; acc: 0.3
Batch: 400; loss: 1.98; acc: 0.38
Batch: 420; loss: 2.07; acc: 0.22
Batch: 440; loss: 2.09; acc: 0.27
Batch: 460; loss: 1.92; acc: 0.38
Batch: 480; loss: 2.07; acc: 0.22
Batch: 500; loss: 1.98; acc: 0.3
Batch: 520; loss: 1.95; acc: 0.34
Batch: 540; loss: 1.76; acc: 0.38
Batch: 560; loss: 1.9; acc: 0.38
Batch: 580; loss: 1.88; acc: 0.41
Batch: 600; loss: 1.74; acc: 0.44
Batch: 620; loss: 1.88; acc: 0.39
Batch: 640; loss: 1.76; acc: 0.41
Batch: 660; loss: 1.98; acc: 0.27
Batch: 680; loss: 1.87; acc: 0.33
Batch: 700; loss: 1.92; acc: 0.36
Batch: 720; loss: 1.87; acc: 0.34
Batch: 740; loss: 1.82; acc: 0.44
Batch: 760; loss: 1.92; acc: 0.33
Batch: 780; loss: 1.9; acc: 0.3
Train Epoch over. train_loss: 2.05; train_accuracy: 0.27 

Batch: 0; loss: 1.81; acc: 0.44
Batch: 20; loss: 2.03; acc: 0.27
Batch: 40; loss: 1.85; acc: 0.36
Batch: 60; loss: 1.75; acc: 0.47
Batch: 80; loss: 1.66; acc: 0.41
Batch: 100; loss: 1.92; acc: 0.23
Batch: 120; loss: 1.88; acc: 0.39
Batch: 140; loss: 1.87; acc: 0.41
Val Epoch over. val_loss: 1.8944552149742273; val_accuracy: 0.34195859872611467 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.94; acc: 0.36
Batch: 20; loss: 1.91; acc: 0.27
Batch: 40; loss: 1.76; acc: 0.36
Batch: 60; loss: 1.92; acc: 0.3
Batch: 80; loss: 1.97; acc: 0.31
Batch: 100; loss: 2.08; acc: 0.31
Batch: 120; loss: 1.93; acc: 0.31
Batch: 140; loss: 1.97; acc: 0.28
Batch: 160; loss: 1.78; acc: 0.44
Batch: 180; loss: 1.74; acc: 0.44
Batch: 200; loss: 1.63; acc: 0.39
Batch: 220; loss: 1.84; acc: 0.31
Batch: 240; loss: 1.6; acc: 0.44
Batch: 260; loss: 1.72; acc: 0.38
Batch: 280; loss: 1.74; acc: 0.39
Batch: 300; loss: 1.76; acc: 0.36
Batch: 320; loss: 2.12; acc: 0.27
Batch: 340; loss: 1.83; acc: 0.34
Batch: 360; loss: 1.72; acc: 0.41
Batch: 380; loss: 1.84; acc: 0.31
Batch: 400; loss: 1.72; acc: 0.36
Batch: 420; loss: 1.89; acc: 0.34
Batch: 440; loss: 1.61; acc: 0.38
Batch: 460; loss: 1.7; acc: 0.42
Batch: 480; loss: 1.59; acc: 0.47
Batch: 500; loss: 1.81; acc: 0.34
Batch: 520; loss: 1.67; acc: 0.39
Batch: 540; loss: 1.71; acc: 0.38
Batch: 560; loss: 1.97; acc: 0.31
Batch: 580; loss: 1.72; acc: 0.36
Batch: 600; loss: 1.8; acc: 0.44
Batch: 620; loss: 1.67; acc: 0.44
Batch: 640; loss: 1.72; acc: 0.42
Batch: 660; loss: 1.73; acc: 0.38
Batch: 680; loss: 1.81; acc: 0.31
Batch: 700; loss: 1.74; acc: 0.42
Batch: 720; loss: 1.67; acc: 0.44
Batch: 740; loss: 1.69; acc: 0.44
Batch: 760; loss: 1.45; acc: 0.45
Batch: 780; loss: 1.8; acc: 0.33
Train Epoch over. train_loss: 1.76; train_accuracy: 0.38 

Batch: 0; loss: 1.59; acc: 0.48
Batch: 20; loss: 2.04; acc: 0.34
Batch: 40; loss: 1.57; acc: 0.5
Batch: 60; loss: 1.52; acc: 0.48
Batch: 80; loss: 1.48; acc: 0.42
Batch: 100; loss: 1.76; acc: 0.39
Batch: 120; loss: 1.78; acc: 0.44
Batch: 140; loss: 1.88; acc: 0.31
Val Epoch over. val_loss: 1.7782330467442797; val_accuracy: 0.3885350318471338 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.69; acc: 0.39
Batch: 20; loss: 1.63; acc: 0.45
Batch: 40; loss: 1.8; acc: 0.33
Batch: 60; loss: 1.57; acc: 0.41
Batch: 80; loss: 1.55; acc: 0.47
Batch: 100; loss: 1.77; acc: 0.39
Batch: 120; loss: 1.66; acc: 0.41
Batch: 140; loss: 1.65; acc: 0.39
Batch: 160; loss: 1.66; acc: 0.47
Batch: 180; loss: 1.87; acc: 0.33
Batch: 200; loss: 1.55; acc: 0.44
Batch: 220; loss: 1.65; acc: 0.41
Batch: 240; loss: 1.94; acc: 0.33
Batch: 260; loss: 1.8; acc: 0.33
Batch: 280; loss: 1.65; acc: 0.44
Batch: 300; loss: 1.73; acc: 0.44
Batch: 320; loss: 1.84; acc: 0.38
Batch: 340; loss: 1.66; acc: 0.45
Batch: 360; loss: 1.6; acc: 0.42
Batch: 380; loss: 1.58; acc: 0.41
Batch: 400; loss: 1.75; acc: 0.33
Batch: 420; loss: 1.74; acc: 0.44
Batch: 440; loss: 1.69; acc: 0.42
Batch: 460; loss: 1.66; acc: 0.47
Batch: 480; loss: 1.58; acc: 0.44
Batch: 500; loss: 1.57; acc: 0.44
Batch: 520; loss: 1.83; acc: 0.39
Batch: 540; loss: 1.66; acc: 0.41
Batch: 560; loss: 1.66; acc: 0.41
Batch: 580; loss: 1.49; acc: 0.48
Batch: 600; loss: 1.56; acc: 0.47
Batch: 620; loss: 1.33; acc: 0.52
Batch: 640; loss: 1.65; acc: 0.48
Batch: 660; loss: 1.46; acc: 0.47
Batch: 680; loss: 1.68; acc: 0.39
Batch: 700; loss: 1.37; acc: 0.53
Batch: 720; loss: 1.66; acc: 0.39
Batch: 740; loss: 1.44; acc: 0.59
Batch: 760; loss: 1.6; acc: 0.47
Batch: 780; loss: 1.89; acc: 0.34
Train Epoch over. train_loss: 1.67; train_accuracy: 0.42 

Batch: 0; loss: 1.75; acc: 0.42
Batch: 20; loss: 2.29; acc: 0.31
Batch: 40; loss: 1.56; acc: 0.5
Batch: 60; loss: 1.83; acc: 0.41
Batch: 80; loss: 1.81; acc: 0.31
Batch: 100; loss: 1.85; acc: 0.41
Batch: 120; loss: 2.07; acc: 0.39
Batch: 140; loss: 1.78; acc: 0.41
Val Epoch over. val_loss: 1.9000071446607067; val_accuracy: 0.37251194267515925 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.63; acc: 0.47
Batch: 20; loss: 1.68; acc: 0.42
Batch: 40; loss: 1.64; acc: 0.45
Batch: 60; loss: 1.32; acc: 0.58
Batch: 80; loss: 1.52; acc: 0.52
Batch: 100; loss: 1.63; acc: 0.45
Batch: 120; loss: 2.05; acc: 0.28
Batch: 140; loss: 2.1; acc: 0.3
Batch: 160; loss: 1.3; acc: 0.48
Batch: 180; loss: 1.52; acc: 0.45
Batch: 200; loss: 1.46; acc: 0.47
Batch: 220; loss: 1.49; acc: 0.48
Batch: 240; loss: 1.78; acc: 0.39
Batch: 260; loss: 1.83; acc: 0.34
Batch: 280; loss: 1.43; acc: 0.52
Batch: 300; loss: 1.6; acc: 0.5
Batch: 320; loss: 1.83; acc: 0.41
Batch: 340; loss: 1.4; acc: 0.5
Batch: 360; loss: 1.39; acc: 0.59
Batch: 380; loss: 1.85; acc: 0.38
Batch: 400; loss: 1.12; acc: 0.62
Batch: 420; loss: 1.15; acc: 0.58
Batch: 440; loss: 1.63; acc: 0.42
Batch: 460; loss: 1.79; acc: 0.41
Batch: 480; loss: 1.62; acc: 0.45
Batch: 500; loss: 1.52; acc: 0.48
Batch: 520; loss: 1.46; acc: 0.53
Batch: 540; loss: 1.52; acc: 0.5
Batch: 560; loss: 1.45; acc: 0.48
Batch: 580; loss: 1.55; acc: 0.48
Batch: 600; loss: 1.48; acc: 0.47
Batch: 620; loss: 1.43; acc: 0.5
Batch: 640; loss: 1.64; acc: 0.41
Batch: 660; loss: 1.8; acc: 0.41
Batch: 680; loss: 1.55; acc: 0.44
Batch: 700; loss: 1.32; acc: 0.61
Batch: 720; loss: 1.39; acc: 0.48
Batch: 740; loss: 1.85; acc: 0.47
Batch: 760; loss: 1.91; acc: 0.36
Batch: 780; loss: 1.72; acc: 0.47
Train Epoch over. train_loss: 1.55; train_accuracy: 0.47 

Batch: 0; loss: 1.68; acc: 0.38
Batch: 20; loss: 1.77; acc: 0.41
Batch: 40; loss: 1.4; acc: 0.58
Batch: 60; loss: 1.57; acc: 0.42
Batch: 80; loss: 1.63; acc: 0.44
Batch: 100; loss: 1.84; acc: 0.38
Batch: 120; loss: 1.68; acc: 0.39
Batch: 140; loss: 1.22; acc: 0.58
Val Epoch over. val_loss: 1.5632495075274424; val_accuracy: 0.45352308917197454 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.87; acc: 0.36
Batch: 20; loss: 1.37; acc: 0.61
Batch: 40; loss: 1.51; acc: 0.42
Batch: 60; loss: 1.32; acc: 0.56
Batch: 80; loss: 1.53; acc: 0.48
Batch: 100; loss: 1.56; acc: 0.5
Batch: 120; loss: 1.37; acc: 0.56
Batch: 140; loss: 1.44; acc: 0.5
Batch: 160; loss: 1.51; acc: 0.5
Batch: 180; loss: 1.34; acc: 0.52
Batch: 200; loss: 1.15; acc: 0.61
Batch: 220; loss: 1.5; acc: 0.56
Batch: 240; loss: 1.27; acc: 0.52
Batch: 260; loss: 2.01; acc: 0.41
Batch: 280; loss: 1.63; acc: 0.45
Batch: 300; loss: 1.72; acc: 0.38
Batch: 320; loss: 1.78; acc: 0.42
Batch: 340; loss: 1.68; acc: 0.47
Batch: 360; loss: 1.45; acc: 0.48
Batch: 380; loss: 1.74; acc: 0.53
Batch: 400; loss: 1.87; acc: 0.41
Batch: 420; loss: 1.77; acc: 0.38
Batch: 440; loss: 1.46; acc: 0.45
Batch: 460; loss: 1.77; acc: 0.38
Batch: 480; loss: 1.63; acc: 0.5
Batch: 500; loss: 1.58; acc: 0.48
Batch: 520; loss: 1.58; acc: 0.47
Batch: 540; loss: 1.61; acc: 0.47
Batch: 560; loss: 1.37; acc: 0.53
Batch: 580; loss: 1.46; acc: 0.52
Batch: 600; loss: 1.44; acc: 0.52
Batch: 620; loss: 1.62; acc: 0.5
Batch: 640; loss: 1.56; acc: 0.48
Batch: 660; loss: 1.48; acc: 0.47
Batch: 680; loss: 1.72; acc: 0.36
Batch: 700; loss: 1.38; acc: 0.52
Batch: 720; loss: 1.64; acc: 0.39
Batch: 740; loss: 1.74; acc: 0.39
Batch: 760; loss: 1.63; acc: 0.39
Batch: 780; loss: 1.32; acc: 0.58
Train Epoch over. train_loss: 1.51; train_accuracy: 0.49 

Batch: 0; loss: 1.51; acc: 0.52
Batch: 20; loss: 1.98; acc: 0.41
Batch: 40; loss: 1.25; acc: 0.58
Batch: 60; loss: 1.54; acc: 0.52
Batch: 80; loss: 1.7; acc: 0.5
Batch: 100; loss: 1.44; acc: 0.5
Batch: 120; loss: 1.7; acc: 0.45
Batch: 140; loss: 1.52; acc: 0.56
Val Epoch over. val_loss: 1.6225029108630624; val_accuracy: 0.4758160828025478 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.78; acc: 0.42
Batch: 20; loss: 1.43; acc: 0.53
Batch: 40; loss: 1.42; acc: 0.61
Batch: 60; loss: 1.19; acc: 0.69
Batch: 80; loss: 1.32; acc: 0.53
Batch: 100; loss: 1.35; acc: 0.48
Batch: 120; loss: 1.29; acc: 0.58
Batch: 140; loss: 1.4; acc: 0.55
Batch: 160; loss: 1.38; acc: 0.61
Batch: 180; loss: 1.25; acc: 0.52
Batch: 200; loss: 1.33; acc: 0.56
Batch: 220; loss: 1.35; acc: 0.52
Batch: 240; loss: 1.34; acc: 0.59
Batch: 260; loss: 1.27; acc: 0.53
Batch: 280; loss: 1.43; acc: 0.52
Batch: 300; loss: 1.17; acc: 0.61
Batch: 320; loss: 1.09; acc: 0.61
Batch: 340; loss: 1.21; acc: 0.61
Batch: 360; loss: 1.36; acc: 0.48
Batch: 380; loss: 1.33; acc: 0.52
Batch: 400; loss: 1.38; acc: 0.56
Batch: 420; loss: 1.04; acc: 0.69
Batch: 440; loss: 1.3; acc: 0.53
Batch: 460; loss: 1.34; acc: 0.58
Batch: 480; loss: 1.42; acc: 0.53
Batch: 500; loss: 1.35; acc: 0.5
Batch: 520; loss: 1.37; acc: 0.48
Batch: 540; loss: 1.28; acc: 0.55
Batch: 560; loss: 1.37; acc: 0.53
Batch: 580; loss: 1.44; acc: 0.58
Batch: 600; loss: 1.26; acc: 0.55
Batch: 620; loss: 1.2; acc: 0.61
Batch: 640; loss: 1.36; acc: 0.45
Batch: 660; loss: 1.29; acc: 0.59
Batch: 680; loss: 1.39; acc: 0.62
Batch: 700; loss: 1.19; acc: 0.62
Batch: 720; loss: 1.51; acc: 0.45
Batch: 740; loss: 1.04; acc: 0.66
Batch: 760; loss: 1.33; acc: 0.55
Batch: 780; loss: 1.49; acc: 0.5
Train Epoch over. train_loss: 1.34; train_accuracy: 0.55 

Batch: 0; loss: 1.54; acc: 0.55
Batch: 20; loss: 1.65; acc: 0.36
Batch: 40; loss: 1.48; acc: 0.5
Batch: 60; loss: 1.54; acc: 0.42
Batch: 80; loss: 1.39; acc: 0.64
Batch: 100; loss: 1.63; acc: 0.45
Batch: 120; loss: 1.87; acc: 0.39
Batch: 140; loss: 1.19; acc: 0.58
Val Epoch over. val_loss: 1.528884290130275; val_accuracy: 0.5016918789808917 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.65; acc: 0.55
Batch: 20; loss: 1.28; acc: 0.61
Batch: 40; loss: 1.37; acc: 0.56
Batch: 60; loss: 1.3; acc: 0.61
Batch: 80; loss: 1.34; acc: 0.56
Batch: 100; loss: 1.55; acc: 0.47
Batch: 120; loss: 1.31; acc: 0.48
Batch: 140; loss: 1.08; acc: 0.64
Batch: 160; loss: 1.18; acc: 0.59
Batch: 180; loss: 1.33; acc: 0.62
Batch: 200; loss: 1.36; acc: 0.53
Batch: 220; loss: 1.06; acc: 0.62
Batch: 240; loss: 1.17; acc: 0.59
Batch: 260; loss: 1.0; acc: 0.72
Batch: 280; loss: 1.23; acc: 0.62
Batch: 300; loss: 1.66; acc: 0.39
Batch: 320; loss: 1.25; acc: 0.59
Batch: 340; loss: 1.13; acc: 0.69
Batch: 360; loss: 1.37; acc: 0.47
Batch: 380; loss: 1.65; acc: 0.56
Batch: 400; loss: 1.35; acc: 0.61
Batch: 420; loss: 1.12; acc: 0.69
Batch: 440; loss: 1.26; acc: 0.61
Batch: 460; loss: 1.44; acc: 0.5
Batch: 480; loss: 1.28; acc: 0.56
Batch: 500; loss: 1.32; acc: 0.61
Batch: 520; loss: 1.24; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.56
Batch: 560; loss: 1.37; acc: 0.56
Batch: 580; loss: 1.52; acc: 0.5
Batch: 600; loss: 1.1; acc: 0.59
Batch: 620; loss: 1.58; acc: 0.45
Batch: 640; loss: 1.26; acc: 0.59
Batch: 660; loss: 1.31; acc: 0.53
Batch: 680; loss: 1.46; acc: 0.53
Batch: 700; loss: 1.18; acc: 0.58
Batch: 720; loss: 1.2; acc: 0.59
Batch: 740; loss: 1.52; acc: 0.45
Batch: 760; loss: 1.35; acc: 0.56
Batch: 780; loss: 1.34; acc: 0.52
Train Epoch over. train_loss: 1.31; train_accuracy: 0.56 

Batch: 0; loss: 1.25; acc: 0.59
Batch: 20; loss: 1.61; acc: 0.52
Batch: 40; loss: 1.16; acc: 0.58
Batch: 60; loss: 1.25; acc: 0.55
Batch: 80; loss: 1.2; acc: 0.64
Batch: 100; loss: 1.16; acc: 0.53
Batch: 120; loss: 1.55; acc: 0.45
Batch: 140; loss: 1.14; acc: 0.59
Val Epoch over. val_loss: 1.2753568458708988; val_accuracy: 0.5600119426751592 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.45; acc: 0.58
Batch: 20; loss: 1.1; acc: 0.64
Batch: 40; loss: 1.43; acc: 0.58
Batch: 60; loss: 1.32; acc: 0.5
Batch: 80; loss: 1.2; acc: 0.58
Batch: 100; loss: 1.13; acc: 0.61
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 1.37; acc: 0.47
Batch: 160; loss: 1.52; acc: 0.56
Batch: 180; loss: 1.27; acc: 0.58
Batch: 200; loss: 1.27; acc: 0.56
Batch: 220; loss: 1.4; acc: 0.45
Batch: 240; loss: 1.1; acc: 0.59
Batch: 260; loss: 1.24; acc: 0.55
Batch: 280; loss: 1.47; acc: 0.5
Batch: 300; loss: 1.48; acc: 0.5
Batch: 320; loss: 1.17; acc: 0.62
Batch: 340; loss: 1.14; acc: 0.66
Batch: 360; loss: 1.13; acc: 0.62
Batch: 380; loss: 1.37; acc: 0.58
Batch: 400; loss: 1.38; acc: 0.56
Batch: 420; loss: 1.29; acc: 0.56
Batch: 440; loss: 1.49; acc: 0.45
Batch: 460; loss: 1.12; acc: 0.64
Batch: 480; loss: 1.2; acc: 0.62
Batch: 500; loss: 1.09; acc: 0.66
Batch: 520; loss: 1.26; acc: 0.62
Batch: 540; loss: 1.21; acc: 0.59
Batch: 560; loss: 1.41; acc: 0.5
Batch: 580; loss: 1.42; acc: 0.52
Batch: 600; loss: 1.39; acc: 0.55
Batch: 620; loss: 1.43; acc: 0.5
Batch: 640; loss: 1.26; acc: 0.55
Batch: 660; loss: 1.43; acc: 0.52
Batch: 680; loss: 1.28; acc: 0.61
Batch: 700; loss: 1.28; acc: 0.58
Batch: 720; loss: 1.27; acc: 0.55
Batch: 740; loss: 1.25; acc: 0.64
Batch: 760; loss: 1.2; acc: 0.61
Batch: 780; loss: 1.33; acc: 0.48
Train Epoch over. train_loss: 1.31; train_accuracy: 0.56 

Batch: 0; loss: 1.19; acc: 0.64
Batch: 20; loss: 1.47; acc: 0.48
Batch: 40; loss: 1.18; acc: 0.61
Batch: 60; loss: 1.24; acc: 0.55
Batch: 80; loss: 1.14; acc: 0.67
Batch: 100; loss: 1.18; acc: 0.56
Batch: 120; loss: 1.44; acc: 0.5
Batch: 140; loss: 1.03; acc: 0.62
Val Epoch over. val_loss: 1.2617401040283738; val_accuracy: 0.5789211783439491 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.29; acc: 0.55
Batch: 20; loss: 1.38; acc: 0.62
Batch: 40; loss: 1.43; acc: 0.52
Batch: 60; loss: 1.24; acc: 0.64
Batch: 80; loss: 1.35; acc: 0.58
Batch: 100; loss: 1.17; acc: 0.59
Batch: 120; loss: 1.24; acc: 0.56
Batch: 140; loss: 1.21; acc: 0.59
Batch: 160; loss: 1.18; acc: 0.59
Batch: 180; loss: 1.12; acc: 0.61
Batch: 200; loss: 1.22; acc: 0.61
Batch: 220; loss: 0.94; acc: 0.7
Batch: 240; loss: 1.49; acc: 0.5
Batch: 260; loss: 1.59; acc: 0.39
Batch: 280; loss: 1.44; acc: 0.55
Batch: 300; loss: 0.92; acc: 0.72
Batch: 320; loss: 1.52; acc: 0.52
Batch: 340; loss: 1.2; acc: 0.62
Batch: 360; loss: 1.18; acc: 0.66
Batch: 380; loss: 1.33; acc: 0.52
Batch: 400; loss: 1.13; acc: 0.62
Batch: 420; loss: 1.26; acc: 0.62
Batch: 440; loss: 1.52; acc: 0.48
Batch: 460; loss: 1.32; acc: 0.61
Batch: 480; loss: 1.26; acc: 0.62
Batch: 500; loss: 1.33; acc: 0.58
Batch: 520; loss: 1.26; acc: 0.56
Batch: 540; loss: 1.22; acc: 0.59
Batch: 560; loss: 1.22; acc: 0.58
Batch: 580; loss: 1.31; acc: 0.53
Batch: 600; loss: 1.58; acc: 0.45
Batch: 620; loss: 1.0; acc: 0.61
Batch: 640; loss: 1.43; acc: 0.48
Batch: 660; loss: 1.26; acc: 0.56
Batch: 680; loss: 1.39; acc: 0.5
Batch: 700; loss: 1.21; acc: 0.58
Batch: 720; loss: 1.45; acc: 0.39
Batch: 740; loss: 1.07; acc: 0.64
Batch: 760; loss: 1.34; acc: 0.52
Batch: 780; loss: 1.17; acc: 0.62
Train Epoch over. train_loss: 1.3; train_accuracy: 0.56 

Batch: 0; loss: 1.2; acc: 0.59
Batch: 20; loss: 1.76; acc: 0.38
Batch: 40; loss: 1.12; acc: 0.59
Batch: 60; loss: 1.32; acc: 0.53
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.14; acc: 0.61
Batch: 120; loss: 1.57; acc: 0.48
Batch: 140; loss: 1.28; acc: 0.52
Val Epoch over. val_loss: 1.3837679024714573; val_accuracy: 0.5272691082802548 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 1.19; acc: 0.59
Batch: 20; loss: 1.26; acc: 0.62
Batch: 40; loss: 1.48; acc: 0.52
Batch: 60; loss: 1.56; acc: 0.56
Batch: 80; loss: 1.04; acc: 0.72
Batch: 100; loss: 1.3; acc: 0.55
Batch: 120; loss: 1.47; acc: 0.5
Batch: 140; loss: 1.41; acc: 0.48
Batch: 160; loss: 1.48; acc: 0.61
Batch: 180; loss: 1.34; acc: 0.62
Batch: 200; loss: 1.24; acc: 0.64
Batch: 220; loss: 1.19; acc: 0.59
Batch: 240; loss: 1.23; acc: 0.59
Batch: 260; loss: 1.37; acc: 0.52
Batch: 280; loss: 1.3; acc: 0.48
Batch: 300; loss: 1.14; acc: 0.67
Batch: 320; loss: 1.52; acc: 0.47
Batch: 340; loss: 1.35; acc: 0.5
Batch: 360; loss: 1.3; acc: 0.59
Batch: 380; loss: 1.19; acc: 0.59
Batch: 400; loss: 1.2; acc: 0.53
Batch: 420; loss: 1.58; acc: 0.44
Batch: 440; loss: 1.33; acc: 0.58
Batch: 460; loss: 1.37; acc: 0.5
Batch: 480; loss: 1.17; acc: 0.56
Batch: 500; loss: 1.55; acc: 0.5
Batch: 520; loss: 1.37; acc: 0.55
Batch: 540; loss: 1.25; acc: 0.59
Batch: 560; loss: 1.35; acc: 0.53
Batch: 580; loss: 1.35; acc: 0.59
Batch: 600; loss: 1.6; acc: 0.47
Batch: 620; loss: 1.6; acc: 0.45
Batch: 640; loss: 1.47; acc: 0.44
Batch: 660; loss: 1.33; acc: 0.58
Batch: 680; loss: 1.24; acc: 0.61
Batch: 700; loss: 1.39; acc: 0.5
Batch: 720; loss: 1.52; acc: 0.53
Batch: 740; loss: 1.38; acc: 0.47
Batch: 760; loss: 1.4; acc: 0.58
Batch: 780; loss: 1.23; acc: 0.56
Train Epoch over. train_loss: 1.3; train_accuracy: 0.56 

Batch: 0; loss: 1.13; acc: 0.66
Batch: 20; loss: 1.57; acc: 0.47
Batch: 40; loss: 1.09; acc: 0.59
Batch: 60; loss: 1.15; acc: 0.56
Batch: 80; loss: 1.09; acc: 0.69
Batch: 100; loss: 1.03; acc: 0.61
Batch: 120; loss: 1.36; acc: 0.52
Batch: 140; loss: 1.14; acc: 0.56
Val Epoch over. val_loss: 1.263110951253563; val_accuracy: 0.580812101910828 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.38; acc: 0.56
Batch: 20; loss: 1.25; acc: 0.55
Batch: 40; loss: 1.62; acc: 0.48
Batch: 60; loss: 1.48; acc: 0.47
Batch: 80; loss: 1.31; acc: 0.55
Batch: 100; loss: 1.13; acc: 0.61
Batch: 120; loss: 1.35; acc: 0.47
Batch: 140; loss: 1.2; acc: 0.53
Batch: 160; loss: 1.36; acc: 0.61
Batch: 180; loss: 1.31; acc: 0.61
Batch: 200; loss: 1.09; acc: 0.64
Batch: 220; loss: 1.31; acc: 0.5
Batch: 240; loss: 1.02; acc: 0.61
Batch: 260; loss: 1.35; acc: 0.53
Batch: 280; loss: 1.31; acc: 0.53
Batch: 300; loss: 1.31; acc: 0.58
Batch: 320; loss: 1.4; acc: 0.59
Batch: 340; loss: 1.5; acc: 0.56
Batch: 360; loss: 1.26; acc: 0.58
Batch: 380; loss: 1.25; acc: 0.53
Batch: 400; loss: 1.28; acc: 0.59
Batch: 420; loss: 1.32; acc: 0.59
Batch: 440; loss: 1.1; acc: 0.69
Batch: 460; loss: 1.43; acc: 0.48
Batch: 480; loss: 1.43; acc: 0.59
Batch: 500; loss: 1.37; acc: 0.5
Batch: 520; loss: 1.33; acc: 0.5
Batch: 540; loss: 1.41; acc: 0.55
Batch: 560; loss: 1.27; acc: 0.69
Batch: 580; loss: 1.2; acc: 0.64
Batch: 600; loss: 1.08; acc: 0.61
Batch: 620; loss: 1.41; acc: 0.53
Batch: 640; loss: 1.49; acc: 0.52
Batch: 660; loss: 1.11; acc: 0.64
Batch: 680; loss: 1.38; acc: 0.53
Batch: 700; loss: 1.27; acc: 0.58
Batch: 720; loss: 1.27; acc: 0.56
Batch: 740; loss: 1.3; acc: 0.52
Batch: 760; loss: 1.29; acc: 0.59
Batch: 780; loss: 1.16; acc: 0.67
Train Epoch over. train_loss: 1.3; train_accuracy: 0.56 

Batch: 0; loss: 1.18; acc: 0.53
Batch: 20; loss: 1.66; acc: 0.48
Batch: 40; loss: 1.09; acc: 0.59
Batch: 60; loss: 1.2; acc: 0.59
Batch: 80; loss: 1.17; acc: 0.66
Batch: 100; loss: 1.11; acc: 0.59
Batch: 120; loss: 1.46; acc: 0.47
Batch: 140; loss: 1.12; acc: 0.58
Val Epoch over. val_loss: 1.279054568451681; val_accuracy: 0.5652866242038217 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 1.18; acc: 0.58
Batch: 20; loss: 1.34; acc: 0.5
Batch: 40; loss: 1.45; acc: 0.52
Batch: 60; loss: 1.35; acc: 0.52
Batch: 80; loss: 1.52; acc: 0.45
Batch: 100; loss: 1.34; acc: 0.56
Batch: 120; loss: 1.67; acc: 0.44
Batch: 140; loss: 1.33; acc: 0.52
Batch: 160; loss: 1.15; acc: 0.59
Batch: 180; loss: 1.18; acc: 0.59
Batch: 200; loss: 1.28; acc: 0.52
Batch: 220; loss: 1.28; acc: 0.56
Batch: 240; loss: 1.5; acc: 0.55
Batch: 260; loss: 1.29; acc: 0.59
Batch: 280; loss: 1.33; acc: 0.64
Batch: 300; loss: 1.24; acc: 0.61
Batch: 320; loss: 1.26; acc: 0.64
Batch: 340; loss: 1.14; acc: 0.64
Batch: 360; loss: 1.25; acc: 0.55
Batch: 380; loss: 1.61; acc: 0.5
Batch: 400; loss: 1.38; acc: 0.59
Batch: 420; loss: 1.56; acc: 0.39
Batch: 440; loss: 1.19; acc: 0.62
Batch: 460; loss: 1.42; acc: 0.53
Batch: 480; loss: 1.48; acc: 0.44
Batch: 500; loss: 1.27; acc: 0.55
Batch: 520; loss: 1.29; acc: 0.53
Batch: 540; loss: 1.14; acc: 0.62
Batch: 560; loss: 1.46; acc: 0.55
Batch: 580; loss: 1.2; acc: 0.53
Batch: 600; loss: 1.56; acc: 0.45
Batch: 620; loss: 1.4; acc: 0.52
Batch: 640; loss: 1.21; acc: 0.62
Batch: 660; loss: 1.49; acc: 0.53
Batch: 680; loss: 1.38; acc: 0.55
Batch: 700; loss: 1.2; acc: 0.56
Batch: 720; loss: 1.3; acc: 0.59
Batch: 740; loss: 1.61; acc: 0.41
Batch: 760; loss: 1.2; acc: 0.59
Batch: 780; loss: 1.29; acc: 0.59
Train Epoch over. train_loss: 1.31; train_accuracy: 0.56 

Batch: 0; loss: 1.4; acc: 0.5
Batch: 20; loss: 2.01; acc: 0.41
Batch: 40; loss: 1.31; acc: 0.59
Batch: 60; loss: 1.44; acc: 0.55
Batch: 80; loss: 1.43; acc: 0.5
Batch: 100; loss: 1.27; acc: 0.59
Batch: 120; loss: 1.72; acc: 0.39
Batch: 140; loss: 1.44; acc: 0.48
Val Epoch over. val_loss: 1.5693337704725325; val_accuracy: 0.4661624203821656 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 1.62; acc: 0.42
Batch: 20; loss: 1.13; acc: 0.62
Batch: 40; loss: 1.31; acc: 0.47
Batch: 60; loss: 1.12; acc: 0.59
Batch: 80; loss: 1.43; acc: 0.58
Batch: 100; loss: 1.15; acc: 0.67
Batch: 120; loss: 1.44; acc: 0.44
Batch: 140; loss: 1.28; acc: 0.53
Batch: 160; loss: 1.29; acc: 0.55
Batch: 180; loss: 1.52; acc: 0.5
Batch: 200; loss: 1.64; acc: 0.47
Batch: 220; loss: 1.3; acc: 0.56
Batch: 240; loss: 1.07; acc: 0.61
Batch: 260; loss: 1.08; acc: 0.56
Batch: 280; loss: 1.28; acc: 0.55
Batch: 300; loss: 1.35; acc: 0.56
Batch: 320; loss: 1.48; acc: 0.44
Batch: 340; loss: 1.19; acc: 0.59
Batch: 360; loss: 1.39; acc: 0.58
Batch: 380; loss: 1.22; acc: 0.61
Batch: 400; loss: 1.35; acc: 0.48
Batch: 420; loss: 1.31; acc: 0.66
Batch: 440; loss: 1.26; acc: 0.53
Batch: 460; loss: 1.12; acc: 0.69
Batch: 480; loss: 1.3; acc: 0.56
Batch: 500; loss: 1.3; acc: 0.58
Batch: 520; loss: 1.05; acc: 0.59
Batch: 540; loss: 0.85; acc: 0.72
Batch: 560; loss: 1.17; acc: 0.56
Batch: 580; loss: 1.48; acc: 0.52
Batch: 600; loss: 1.04; acc: 0.69
Batch: 620; loss: 1.62; acc: 0.42
Batch: 640; loss: 1.1; acc: 0.62
Batch: 660; loss: 1.32; acc: 0.53
Batch: 680; loss: 1.38; acc: 0.59
Batch: 700; loss: 1.39; acc: 0.47
Batch: 720; loss: 1.39; acc: 0.56
Batch: 740; loss: 1.17; acc: 0.62
Batch: 760; loss: 1.36; acc: 0.61
Batch: 780; loss: 1.4; acc: 0.52
Train Epoch over. train_loss: 1.3; train_accuracy: 0.56 

Batch: 0; loss: 1.2; acc: 0.64
Batch: 20; loss: 1.5; acc: 0.5
Batch: 40; loss: 1.2; acc: 0.59
Batch: 60; loss: 1.22; acc: 0.56
Batch: 80; loss: 1.17; acc: 0.64
Batch: 100; loss: 1.18; acc: 0.52
Batch: 120; loss: 1.46; acc: 0.48
Batch: 140; loss: 1.03; acc: 0.59
Val Epoch over. val_loss: 1.2617962554002264; val_accuracy: 0.5789211783439491 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 1.36; acc: 0.58
Batch: 20; loss: 1.54; acc: 0.48
Batch: 40; loss: 1.43; acc: 0.45
Batch: 60; loss: 1.22; acc: 0.59
Batch: 80; loss: 1.31; acc: 0.55
Batch: 100; loss: 1.51; acc: 0.53
Batch: 120; loss: 1.57; acc: 0.44
Batch: 140; loss: 1.08; acc: 0.62
Batch: 160; loss: 1.38; acc: 0.5
Batch: 180; loss: 1.25; acc: 0.59
Batch: 200; loss: 1.13; acc: 0.64
Batch: 220; loss: 1.6; acc: 0.45
Batch: 240; loss: 1.48; acc: 0.48
Batch: 260; loss: 1.39; acc: 0.53
Batch: 280; loss: 1.31; acc: 0.52
Batch: 300; loss: 1.34; acc: 0.59
Batch: 320; loss: 1.07; acc: 0.67
Batch: 340; loss: 1.25; acc: 0.55
Batch: 360; loss: 1.14; acc: 0.67
Batch: 380; loss: 1.12; acc: 0.59
Batch: 400; loss: 1.09; acc: 0.64
Batch: 420; loss: 1.32; acc: 0.59
Batch: 440; loss: 1.35; acc: 0.59
Batch: 460; loss: 1.59; acc: 0.45
Batch: 480; loss: 1.45; acc: 0.52
Batch: 500; loss: 1.43; acc: 0.53
Batch: 520; loss: 1.56; acc: 0.5
Batch: 540; loss: 1.71; acc: 0.45
Batch: 560; loss: 1.56; acc: 0.48
Batch: 580; loss: 1.25; acc: 0.58
Batch: 600; loss: 1.36; acc: 0.55
Batch: 620; loss: 1.18; acc: 0.62
Batch: 640; loss: 1.14; acc: 0.61
Batch: 660; loss: 1.34; acc: 0.56
Batch: 680; loss: 1.48; acc: 0.52
Batch: 700; loss: 1.42; acc: 0.55
Batch: 720; loss: 1.28; acc: 0.61
Batch: 740; loss: 1.46; acc: 0.56
Batch: 760; loss: 1.15; acc: 0.62
Batch: 780; loss: 1.52; acc: 0.52
Train Epoch over. train_loss: 1.3; train_accuracy: 0.56 

Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.57; acc: 0.52
Batch: 40; loss: 1.08; acc: 0.59
Batch: 60; loss: 1.17; acc: 0.55
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.12; acc: 0.56
Batch: 120; loss: 1.48; acc: 0.5
Batch: 140; loss: 1.13; acc: 0.61
Val Epoch over. val_loss: 1.2487618930780204; val_accuracy: 0.5769307324840764 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 1.2; acc: 0.59
Batch: 20; loss: 1.37; acc: 0.55
Batch: 40; loss: 1.49; acc: 0.52
Batch: 60; loss: 1.21; acc: 0.67
Batch: 80; loss: 1.27; acc: 0.56
Batch: 100; loss: 1.41; acc: 0.52
Batch: 120; loss: 1.68; acc: 0.42
Batch: 140; loss: 1.38; acc: 0.61
Batch: 160; loss: 1.25; acc: 0.59
Batch: 180; loss: 1.19; acc: 0.61
Batch: 200; loss: 1.22; acc: 0.58
Batch: 220; loss: 1.16; acc: 0.53
Batch: 240; loss: 1.03; acc: 0.69
Batch: 260; loss: 1.42; acc: 0.55
Batch: 280; loss: 1.28; acc: 0.55
Batch: 300; loss: 1.39; acc: 0.59
Batch: 320; loss: 1.43; acc: 0.5
Batch: 340; loss: 1.44; acc: 0.58
Batch: 360; loss: 1.46; acc: 0.55
Batch: 380; loss: 1.24; acc: 0.58
Batch: 400; loss: 1.23; acc: 0.59
Batch: 420; loss: 1.37; acc: 0.53
Batch: 440; loss: 1.19; acc: 0.64
Batch: 460; loss: 1.42; acc: 0.48
Batch: 480; loss: 1.44; acc: 0.47
Batch: 500; loss: 1.19; acc: 0.56
Batch: 520; loss: 1.24; acc: 0.69
Batch: 540; loss: 1.35; acc: 0.56
Batch: 560; loss: 1.21; acc: 0.62
Batch: 580; loss: 1.44; acc: 0.44
Batch: 600; loss: 1.46; acc: 0.55
Batch: 620; loss: 1.26; acc: 0.59
Batch: 640; loss: 1.31; acc: 0.5
Batch: 660; loss: 1.45; acc: 0.53
Batch: 680; loss: 1.29; acc: 0.58
Batch: 700; loss: 1.19; acc: 0.58
Batch: 720; loss: 1.23; acc: 0.61
Batch: 740; loss: 1.16; acc: 0.58
Batch: 760; loss: 1.17; acc: 0.61
Batch: 780; loss: 1.48; acc: 0.58
Train Epoch over. train_loss: 1.3; train_accuracy: 0.56 

Batch: 0; loss: 1.21; acc: 0.55
Batch: 20; loss: 1.59; acc: 0.42
Batch: 40; loss: 1.14; acc: 0.59
Batch: 60; loss: 1.24; acc: 0.53
Batch: 80; loss: 1.16; acc: 0.66
Batch: 100; loss: 1.19; acc: 0.52
Batch: 120; loss: 1.5; acc: 0.45
Batch: 140; loss: 1.13; acc: 0.59
Val Epoch over. val_loss: 1.3035648730909748; val_accuracy: 0.5531449044585988 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.31; acc: 0.56
Batch: 20; loss: 1.19; acc: 0.58
Batch: 40; loss: 1.45; acc: 0.47
Batch: 60; loss: 1.43; acc: 0.45
Batch: 80; loss: 1.13; acc: 0.61
Batch: 100; loss: 1.46; acc: 0.47
Batch: 120; loss: 1.08; acc: 0.64
Batch: 140; loss: 1.19; acc: 0.58
Batch: 160; loss: 1.15; acc: 0.61
Batch: 180; loss: 1.41; acc: 0.55
Batch: 200; loss: 1.36; acc: 0.53
Batch: 220; loss: 1.2; acc: 0.56
Batch: 240; loss: 1.33; acc: 0.53
Batch: 260; loss: 1.31; acc: 0.56
Batch: 280; loss: 1.07; acc: 0.69
Batch: 300; loss: 1.35; acc: 0.56
Batch: 320; loss: 1.5; acc: 0.58
Batch: 340; loss: 1.33; acc: 0.58
Batch: 360; loss: 1.21; acc: 0.59
Batch: 380; loss: 1.27; acc: 0.62
Batch: 400; loss: 1.34; acc: 0.59
Batch: 420; loss: 1.38; acc: 0.58
Batch: 440; loss: 1.33; acc: 0.59
Batch: 460; loss: 1.21; acc: 0.58
Batch: 480; loss: 1.43; acc: 0.48
Batch: 500; loss: 1.27; acc: 0.55
Batch: 520; loss: 1.45; acc: 0.59
Batch: 540; loss: 1.13; acc: 0.61
Batch: 560; loss: 1.0; acc: 0.66
Batch: 580; loss: 1.09; acc: 0.59
Batch: 600; loss: 1.36; acc: 0.59
Batch: 620; loss: 1.14; acc: 0.58
Batch: 640; loss: 1.58; acc: 0.45
Batch: 660; loss: 1.38; acc: 0.55
Batch: 680; loss: 1.24; acc: 0.61
Batch: 700; loss: 1.37; acc: 0.55
Batch: 720; loss: 1.36; acc: 0.67
Batch: 740; loss: 1.31; acc: 0.64
Batch: 760; loss: 1.38; acc: 0.58
Batch: 780; loss: 1.39; acc: 0.53
Train Epoch over. train_loss: 1.27; train_accuracy: 0.57 

Batch: 0; loss: 1.14; acc: 0.64
Batch: 20; loss: 1.69; acc: 0.48
Batch: 40; loss: 1.09; acc: 0.59
Batch: 60; loss: 1.22; acc: 0.52
Batch: 80; loss: 1.21; acc: 0.61
Batch: 100; loss: 1.05; acc: 0.59
Batch: 120; loss: 1.47; acc: 0.44
Batch: 140; loss: 1.17; acc: 0.53
Val Epoch over. val_loss: 1.312988592181236; val_accuracy: 0.5519506369426752 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.57; acc: 0.48
Batch: 20; loss: 1.17; acc: 0.59
Batch: 40; loss: 1.11; acc: 0.66
Batch: 60; loss: 1.19; acc: 0.59
Batch: 80; loss: 1.12; acc: 0.67
Batch: 100; loss: 1.42; acc: 0.53
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 1.4; acc: 0.53
Batch: 160; loss: 1.52; acc: 0.52
Batch: 180; loss: 1.3; acc: 0.53
Batch: 200; loss: 1.05; acc: 0.64
Batch: 220; loss: 1.51; acc: 0.47
Batch: 240; loss: 1.19; acc: 0.61
Batch: 260; loss: 1.34; acc: 0.52
Batch: 280; loss: 1.3; acc: 0.62
Batch: 300; loss: 1.33; acc: 0.56
Batch: 320; loss: 1.23; acc: 0.53
Batch: 340; loss: 1.35; acc: 0.56
Batch: 360; loss: 1.43; acc: 0.55
Batch: 380; loss: 1.19; acc: 0.56
Batch: 400; loss: 1.19; acc: 0.56
Batch: 420; loss: 1.24; acc: 0.56
Batch: 440; loss: 1.07; acc: 0.66
Batch: 460; loss: 1.19; acc: 0.67
Batch: 480; loss: 1.16; acc: 0.67
Batch: 500; loss: 1.09; acc: 0.69
Batch: 520; loss: 1.2; acc: 0.58
Batch: 540; loss: 1.43; acc: 0.53
Batch: 560; loss: 1.41; acc: 0.53
Batch: 580; loss: 1.38; acc: 0.53
Batch: 600; loss: 1.21; acc: 0.59
Batch: 620; loss: 1.19; acc: 0.64
Batch: 640; loss: 1.38; acc: 0.53
Batch: 660; loss: 1.16; acc: 0.59
Batch: 680; loss: 1.09; acc: 0.7
Batch: 700; loss: 1.14; acc: 0.67
Batch: 720; loss: 1.41; acc: 0.53
Batch: 740; loss: 1.26; acc: 0.55
Batch: 760; loss: 1.22; acc: 0.55
Batch: 780; loss: 1.25; acc: 0.53
Train Epoch over. train_loss: 1.27; train_accuracy: 0.57 

Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 1.55; acc: 0.5
Batch: 40; loss: 1.09; acc: 0.59
Batch: 60; loss: 1.21; acc: 0.55
Batch: 80; loss: 1.1; acc: 0.66
Batch: 100; loss: 1.1; acc: 0.56
Batch: 120; loss: 1.49; acc: 0.47
Batch: 140; loss: 1.04; acc: 0.56
Val Epoch over. val_loss: 1.233821643765565; val_accuracy: 0.5823049363057324 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.34; acc: 0.56
Batch: 20; loss: 1.11; acc: 0.59
Batch: 40; loss: 1.2; acc: 0.67
Batch: 60; loss: 1.2; acc: 0.59
Batch: 80; loss: 1.36; acc: 0.5
Batch: 100; loss: 1.23; acc: 0.58
Batch: 120; loss: 1.28; acc: 0.62
Batch: 140; loss: 1.42; acc: 0.55
Batch: 160; loss: 1.28; acc: 0.58
Batch: 180; loss: 1.14; acc: 0.66
Batch: 200; loss: 1.33; acc: 0.52
Batch: 220; loss: 1.18; acc: 0.55
Batch: 240; loss: 1.22; acc: 0.53
Batch: 260; loss: 1.32; acc: 0.58
Batch: 280; loss: 1.29; acc: 0.58
Batch: 300; loss: 1.23; acc: 0.58
Batch: 320; loss: 1.14; acc: 0.62
Batch: 340; loss: 1.21; acc: 0.56
Batch: 360; loss: 1.15; acc: 0.59
Batch: 380; loss: 1.21; acc: 0.61
Batch: 400; loss: 1.28; acc: 0.55
Batch: 420; loss: 1.15; acc: 0.64
Batch: 440; loss: 1.17; acc: 0.67
Batch: 460; loss: 1.43; acc: 0.53
Batch: 480; loss: 1.3; acc: 0.53
Batch: 500; loss: 1.24; acc: 0.52
Batch: 520; loss: 1.57; acc: 0.53
Batch: 540; loss: 1.46; acc: 0.52
Batch: 560; loss: 1.21; acc: 0.62
Batch: 580; loss: 1.34; acc: 0.52
Batch: 600; loss: 1.28; acc: 0.55
Batch: 620; loss: 1.05; acc: 0.67
Batch: 640; loss: 1.47; acc: 0.55
Batch: 660; loss: 1.23; acc: 0.62
Batch: 680; loss: 1.24; acc: 0.59
Batch: 700; loss: 1.28; acc: 0.59
Batch: 720; loss: 1.53; acc: 0.52
Batch: 740; loss: 1.36; acc: 0.58
Batch: 760; loss: 1.12; acc: 0.69
Batch: 780; loss: 1.35; acc: 0.53
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.17; acc: 0.64
Batch: 20; loss: 1.56; acc: 0.48
Batch: 40; loss: 1.1; acc: 0.58
Batch: 60; loss: 1.2; acc: 0.5
Batch: 80; loss: 1.09; acc: 0.69
Batch: 100; loss: 1.14; acc: 0.53
Batch: 120; loss: 1.48; acc: 0.45
Batch: 140; loss: 1.07; acc: 0.55
Val Epoch over. val_loss: 1.2485076295342414; val_accuracy: 0.5728503184713376 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.45; acc: 0.48
Batch: 20; loss: 1.08; acc: 0.67
Batch: 40; loss: 1.32; acc: 0.5
Batch: 60; loss: 1.29; acc: 0.61
Batch: 80; loss: 1.04; acc: 0.62
Batch: 100; loss: 1.2; acc: 0.59
Batch: 120; loss: 1.17; acc: 0.66
Batch: 140; loss: 1.31; acc: 0.56
Batch: 160; loss: 1.24; acc: 0.59
Batch: 180; loss: 1.22; acc: 0.61
Batch: 200; loss: 1.21; acc: 0.56
Batch: 220; loss: 1.19; acc: 0.55
Batch: 240; loss: 1.21; acc: 0.55
Batch: 260; loss: 1.5; acc: 0.47
Batch: 280; loss: 0.87; acc: 0.69
Batch: 300; loss: 1.34; acc: 0.53
Batch: 320; loss: 1.21; acc: 0.58
Batch: 340; loss: 1.29; acc: 0.66
Batch: 360; loss: 1.14; acc: 0.62
Batch: 380; loss: 1.14; acc: 0.62
Batch: 400; loss: 1.31; acc: 0.55
Batch: 420; loss: 1.03; acc: 0.67
Batch: 440; loss: 1.58; acc: 0.55
Batch: 460; loss: 1.48; acc: 0.52
Batch: 480; loss: 1.16; acc: 0.61
Batch: 500; loss: 1.43; acc: 0.52
Batch: 520; loss: 0.99; acc: 0.69
Batch: 540; loss: 1.12; acc: 0.58
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.26; acc: 0.58
Batch: 600; loss: 1.26; acc: 0.52
Batch: 620; loss: 1.3; acc: 0.56
Batch: 640; loss: 1.23; acc: 0.58
Batch: 660; loss: 1.28; acc: 0.53
Batch: 680; loss: 1.31; acc: 0.56
Batch: 700; loss: 1.29; acc: 0.5
Batch: 720; loss: 1.31; acc: 0.59
Batch: 740; loss: 1.18; acc: 0.61
Batch: 760; loss: 1.45; acc: 0.55
Batch: 780; loss: 1.12; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.57 

Batch: 0; loss: 1.13; acc: 0.62
Batch: 20; loss: 1.53; acc: 0.48
Batch: 40; loss: 1.08; acc: 0.56
Batch: 60; loss: 1.17; acc: 0.55
Batch: 80; loss: 1.08; acc: 0.7
Batch: 100; loss: 1.07; acc: 0.58
Batch: 120; loss: 1.43; acc: 0.5
Batch: 140; loss: 1.03; acc: 0.59
Val Epoch over. val_loss: 1.2255006664118189; val_accuracy: 0.5925557324840764 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.43; acc: 0.55
Batch: 20; loss: 1.21; acc: 0.61
Batch: 40; loss: 1.24; acc: 0.56
Batch: 60; loss: 1.29; acc: 0.52
Batch: 80; loss: 1.2; acc: 0.53
Batch: 100; loss: 1.41; acc: 0.59
Batch: 120; loss: 1.31; acc: 0.56
Batch: 140; loss: 1.41; acc: 0.53
Batch: 160; loss: 1.27; acc: 0.62
Batch: 180; loss: 1.16; acc: 0.66
Batch: 200; loss: 1.28; acc: 0.55
Batch: 220; loss: 1.22; acc: 0.61
Batch: 240; loss: 1.27; acc: 0.55
Batch: 260; loss: 1.09; acc: 0.61
Batch: 280; loss: 1.49; acc: 0.5
Batch: 300; loss: 1.14; acc: 0.69
Batch: 320; loss: 0.98; acc: 0.64
Batch: 340; loss: 1.3; acc: 0.56
Batch: 360; loss: 1.18; acc: 0.56
Batch: 380; loss: 1.12; acc: 0.58
Batch: 400; loss: 1.19; acc: 0.59
Batch: 420; loss: 1.22; acc: 0.56
Batch: 440; loss: 1.31; acc: 0.62
Batch: 460; loss: 1.38; acc: 0.56
Batch: 480; loss: 1.27; acc: 0.55
Batch: 500; loss: 1.13; acc: 0.66
Batch: 520; loss: 1.11; acc: 0.64
Batch: 540; loss: 1.35; acc: 0.55
Batch: 560; loss: 1.4; acc: 0.55
Batch: 580; loss: 1.11; acc: 0.69
Batch: 600; loss: 1.36; acc: 0.48
Batch: 620; loss: 1.4; acc: 0.58
Batch: 640; loss: 1.37; acc: 0.58
Batch: 660; loss: 1.4; acc: 0.47
Batch: 680; loss: 1.17; acc: 0.62
Batch: 700; loss: 1.09; acc: 0.64
Batch: 720; loss: 1.13; acc: 0.62
Batch: 740; loss: 1.14; acc: 0.69
Batch: 760; loss: 1.37; acc: 0.56
Batch: 780; loss: 0.97; acc: 0.69
Train Epoch over. train_loss: 1.27; train_accuracy: 0.57 

Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.51; acc: 0.48
Batch: 40; loss: 1.11; acc: 0.61
Batch: 60; loss: 1.17; acc: 0.53
Batch: 80; loss: 1.08; acc: 0.66
Batch: 100; loss: 1.06; acc: 0.61
Batch: 120; loss: 1.42; acc: 0.53
Batch: 140; loss: 1.05; acc: 0.69
Val Epoch over. val_loss: 1.2361005271316334; val_accuracy: 0.596437101910828 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.44; acc: 0.53
Batch: 20; loss: 1.23; acc: 0.56
Batch: 40; loss: 1.11; acc: 0.64
Batch: 60; loss: 1.45; acc: 0.52
Batch: 80; loss: 1.22; acc: 0.62
Batch: 100; loss: 1.02; acc: 0.64
Batch: 120; loss: 1.28; acc: 0.58
Batch: 140; loss: 1.08; acc: 0.64
Batch: 160; loss: 1.11; acc: 0.55
Batch: 180; loss: 1.39; acc: 0.5
Batch: 200; loss: 1.36; acc: 0.62
Batch: 220; loss: 1.33; acc: 0.55
Batch: 240; loss: 1.35; acc: 0.53
Batch: 260; loss: 1.42; acc: 0.48
Batch: 280; loss: 1.39; acc: 0.58
Batch: 300; loss: 1.29; acc: 0.55
Batch: 320; loss: 1.45; acc: 0.5
Batch: 340; loss: 1.39; acc: 0.47
Batch: 360; loss: 1.2; acc: 0.59
Batch: 380; loss: 1.33; acc: 0.59
Batch: 400; loss: 1.33; acc: 0.66
Batch: 420; loss: 1.21; acc: 0.64
Batch: 440; loss: 1.26; acc: 0.61
Batch: 460; loss: 1.15; acc: 0.59
Batch: 480; loss: 1.16; acc: 0.61
Batch: 500; loss: 1.42; acc: 0.56
Batch: 520; loss: 1.37; acc: 0.53
Batch: 540; loss: 1.27; acc: 0.58
Batch: 560; loss: 1.21; acc: 0.59
Batch: 580; loss: 1.28; acc: 0.62
Batch: 600; loss: 1.1; acc: 0.58
Batch: 620; loss: 1.42; acc: 0.56
Batch: 640; loss: 1.36; acc: 0.52
Batch: 660; loss: 1.38; acc: 0.53
Batch: 680; loss: 1.05; acc: 0.67
Batch: 700; loss: 1.34; acc: 0.58
Batch: 720; loss: 1.37; acc: 0.53
Batch: 740; loss: 1.21; acc: 0.55
Batch: 760; loss: 1.39; acc: 0.55
Batch: 780; loss: 1.25; acc: 0.64
Train Epoch over. train_loss: 1.27; train_accuracy: 0.57 

Batch: 0; loss: 1.13; acc: 0.61
Batch: 20; loss: 1.53; acc: 0.44
Batch: 40; loss: 1.13; acc: 0.61
Batch: 60; loss: 1.18; acc: 0.53
Batch: 80; loss: 1.1; acc: 0.67
Batch: 100; loss: 1.09; acc: 0.58
Batch: 120; loss: 1.48; acc: 0.53
Batch: 140; loss: 1.0; acc: 0.61
Val Epoch over. val_loss: 1.239268869731077; val_accuracy: 0.5861863057324841 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.32; acc: 0.59
Batch: 20; loss: 1.26; acc: 0.55
Batch: 40; loss: 1.46; acc: 0.55
Batch: 60; loss: 1.13; acc: 0.62
Batch: 80; loss: 1.33; acc: 0.59
Batch: 100; loss: 1.28; acc: 0.58
Batch: 120; loss: 1.41; acc: 0.45
Batch: 140; loss: 1.32; acc: 0.52
Batch: 160; loss: 1.46; acc: 0.53
Batch: 180; loss: 1.11; acc: 0.56
Batch: 200; loss: 1.19; acc: 0.58
Batch: 220; loss: 1.29; acc: 0.5
Batch: 240; loss: 1.12; acc: 0.62
Batch: 260; loss: 1.28; acc: 0.53
Batch: 280; loss: 1.01; acc: 0.67
Batch: 300; loss: 1.37; acc: 0.48
Batch: 320; loss: 1.21; acc: 0.66
Batch: 340; loss: 1.51; acc: 0.44
Batch: 360; loss: 1.29; acc: 0.48
Batch: 380; loss: 1.26; acc: 0.62
Batch: 400; loss: 1.22; acc: 0.58
Batch: 420; loss: 1.03; acc: 0.67
Batch: 440; loss: 1.17; acc: 0.66
Batch: 460; loss: 1.21; acc: 0.69
Batch: 480; loss: 1.41; acc: 0.58
Batch: 500; loss: 1.23; acc: 0.55
Batch: 520; loss: 1.11; acc: 0.61
Batch: 540; loss: 1.32; acc: 0.58
Batch: 560; loss: 1.22; acc: 0.56
Batch: 580; loss: 1.5; acc: 0.48
Batch: 600; loss: 1.34; acc: 0.56
Batch: 620; loss: 1.14; acc: 0.62
Batch: 640; loss: 1.5; acc: 0.5
Batch: 660; loss: 1.14; acc: 0.62
Batch: 680; loss: 1.23; acc: 0.66
Batch: 700; loss: 1.14; acc: 0.56
Batch: 720; loss: 1.36; acc: 0.52
Batch: 740; loss: 1.49; acc: 0.53
Batch: 760; loss: 1.09; acc: 0.58
Batch: 780; loss: 1.34; acc: 0.55
Train Epoch over. train_loss: 1.27; train_accuracy: 0.57 

Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 1.56; acc: 0.44
Batch: 40; loss: 1.05; acc: 0.58
Batch: 60; loss: 1.17; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.66
Batch: 100; loss: 1.11; acc: 0.53
Batch: 120; loss: 1.45; acc: 0.48
Batch: 140; loss: 1.06; acc: 0.58
Val Epoch over. val_loss: 1.2263438287813953; val_accuracy: 0.5866839171974523 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.62
Batch: 40; loss: 1.29; acc: 0.48
Batch: 60; loss: 1.32; acc: 0.61
Batch: 80; loss: 1.4; acc: 0.5
Batch: 100; loss: 1.31; acc: 0.53
Batch: 120; loss: 1.43; acc: 0.58
Batch: 140; loss: 1.17; acc: 0.59
Batch: 160; loss: 1.26; acc: 0.52
Batch: 180; loss: 1.55; acc: 0.41
Batch: 200; loss: 1.48; acc: 0.56
Batch: 220; loss: 1.13; acc: 0.69
Batch: 240; loss: 1.19; acc: 0.64
Batch: 260; loss: 1.51; acc: 0.59
Batch: 280; loss: 1.23; acc: 0.61
Batch: 300; loss: 1.28; acc: 0.59
Batch: 320; loss: 1.26; acc: 0.52
Batch: 340; loss: 1.53; acc: 0.53
Batch: 360; loss: 1.31; acc: 0.56
Batch: 380; loss: 1.3; acc: 0.61
Batch: 400; loss: 1.26; acc: 0.56
Batch: 420; loss: 1.51; acc: 0.48
Batch: 440; loss: 1.19; acc: 0.52
Batch: 460; loss: 0.99; acc: 0.62
Batch: 480; loss: 1.24; acc: 0.59
Batch: 500; loss: 1.28; acc: 0.61
Batch: 520; loss: 1.24; acc: 0.5
Batch: 540; loss: 1.24; acc: 0.61
Batch: 560; loss: 1.34; acc: 0.56
Batch: 580; loss: 1.35; acc: 0.48
Batch: 600; loss: 1.26; acc: 0.64
Batch: 620; loss: 1.44; acc: 0.48
Batch: 640; loss: 1.47; acc: 0.53
Batch: 660; loss: 1.54; acc: 0.52
Batch: 680; loss: 1.15; acc: 0.61
Batch: 700; loss: 1.3; acc: 0.55
Batch: 720; loss: 1.25; acc: 0.59
Batch: 740; loss: 1.34; acc: 0.59
Batch: 760; loss: 1.31; acc: 0.53
Batch: 780; loss: 1.11; acc: 0.66
Train Epoch over. train_loss: 1.27; train_accuracy: 0.57 

Batch: 0; loss: 1.14; acc: 0.61
Batch: 20; loss: 1.53; acc: 0.48
Batch: 40; loss: 1.11; acc: 0.59
Batch: 60; loss: 1.17; acc: 0.52
Batch: 80; loss: 1.09; acc: 0.64
Batch: 100; loss: 1.1; acc: 0.55
Batch: 120; loss: 1.4; acc: 0.5
Batch: 140; loss: 1.02; acc: 0.55
Val Epoch over. val_loss: 1.2303613257256283; val_accuracy: 0.5920581210191083 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.49; acc: 0.55
Batch: 20; loss: 1.27; acc: 0.55
Batch: 40; loss: 1.09; acc: 0.64
Batch: 60; loss: 0.95; acc: 0.73
Batch: 80; loss: 1.08; acc: 0.61
Batch: 100; loss: 1.36; acc: 0.47
Batch: 120; loss: 1.12; acc: 0.62
Batch: 140; loss: 1.34; acc: 0.55
Batch: 160; loss: 1.26; acc: 0.59
Batch: 180; loss: 1.37; acc: 0.55
Batch: 200; loss: 1.32; acc: 0.53
Batch: 220; loss: 1.05; acc: 0.62
Batch: 240; loss: 1.57; acc: 0.61
Batch: 260; loss: 1.11; acc: 0.64
Batch: 280; loss: 1.45; acc: 0.5
Batch: 300; loss: 1.25; acc: 0.58
Batch: 320; loss: 1.39; acc: 0.53
Batch: 340; loss: 1.09; acc: 0.62
Batch: 360; loss: 1.43; acc: 0.5
Batch: 380; loss: 1.26; acc: 0.58
Batch: 400; loss: 0.98; acc: 0.64
Batch: 420; loss: 1.35; acc: 0.52
Batch: 440; loss: 1.36; acc: 0.55
Batch: 460; loss: 1.11; acc: 0.62
Batch: 480; loss: 1.32; acc: 0.59
Batch: 500; loss: 1.06; acc: 0.67
Batch: 520; loss: 1.18; acc: 0.64
Batch: 540; loss: 1.48; acc: 0.55
Batch: 560; loss: 1.44; acc: 0.56
Batch: 580; loss: 1.17; acc: 0.58
Batch: 600; loss: 1.4; acc: 0.55
Batch: 620; loss: 1.74; acc: 0.47
Batch: 640; loss: 1.15; acc: 0.52
Batch: 660; loss: 1.39; acc: 0.56
Batch: 680; loss: 1.13; acc: 0.61
Batch: 700; loss: 1.36; acc: 0.55
Batch: 720; loss: 1.19; acc: 0.62
Batch: 740; loss: 1.04; acc: 0.69
Batch: 760; loss: 1.26; acc: 0.59
Batch: 780; loss: 0.96; acc: 0.7
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.53; acc: 0.48
Batch: 40; loss: 1.11; acc: 0.61
Batch: 60; loss: 1.17; acc: 0.58
Batch: 80; loss: 1.03; acc: 0.75
Batch: 100; loss: 1.15; acc: 0.55
Batch: 120; loss: 1.44; acc: 0.47
Batch: 140; loss: 1.03; acc: 0.64
Val Epoch over. val_loss: 1.2346437759460158; val_accuracy: 0.5922571656050956 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.46; acc: 0.58
Batch: 20; loss: 1.24; acc: 0.62
Batch: 40; loss: 1.29; acc: 0.55
Batch: 60; loss: 1.24; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.52
Batch: 100; loss: 1.2; acc: 0.61
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.4; acc: 0.5
Batch: 160; loss: 1.22; acc: 0.58
Batch: 180; loss: 1.41; acc: 0.53
Batch: 200; loss: 1.47; acc: 0.5
Batch: 220; loss: 1.13; acc: 0.61
Batch: 240; loss: 1.37; acc: 0.5
Batch: 260; loss: 1.33; acc: 0.67
Batch: 280; loss: 1.3; acc: 0.64
Batch: 300; loss: 1.14; acc: 0.62
Batch: 320; loss: 1.08; acc: 0.58
Batch: 340; loss: 1.37; acc: 0.53
Batch: 360; loss: 1.06; acc: 0.67
Batch: 380; loss: 1.34; acc: 0.47
Batch: 400; loss: 1.4; acc: 0.59
Batch: 420; loss: 1.13; acc: 0.61
Batch: 440; loss: 1.3; acc: 0.58
Batch: 460; loss: 1.22; acc: 0.58
Batch: 480; loss: 1.42; acc: 0.52
Batch: 500; loss: 1.24; acc: 0.58
Batch: 520; loss: 1.3; acc: 0.55
Batch: 540; loss: 1.24; acc: 0.62
Batch: 560; loss: 1.04; acc: 0.62
Batch: 580; loss: 1.14; acc: 0.55
Batch: 600; loss: 1.19; acc: 0.55
Batch: 620; loss: 1.24; acc: 0.61
Batch: 640; loss: 1.17; acc: 0.58
Batch: 660; loss: 1.3; acc: 0.59
Batch: 680; loss: 1.23; acc: 0.55
Batch: 700; loss: 1.47; acc: 0.48
Batch: 720; loss: 1.45; acc: 0.47
Batch: 740; loss: 1.17; acc: 0.59
Batch: 760; loss: 1.43; acc: 0.45
Batch: 780; loss: 1.24; acc: 0.62
Train Epoch over. train_loss: 1.27; train_accuracy: 0.58 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.51; acc: 0.45
Batch: 40; loss: 1.11; acc: 0.62
Batch: 60; loss: 1.16; acc: 0.53
Batch: 80; loss: 1.05; acc: 0.73
Batch: 100; loss: 1.12; acc: 0.53
Batch: 120; loss: 1.39; acc: 0.56
Batch: 140; loss: 1.02; acc: 0.64
Val Epoch over. val_loss: 1.2305583171783738; val_accuracy: 0.5914609872611465 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.04; acc: 0.62
Batch: 20; loss: 1.23; acc: 0.58
Batch: 40; loss: 1.53; acc: 0.42
Batch: 60; loss: 1.28; acc: 0.61
Batch: 80; loss: 1.27; acc: 0.59
Batch: 100; loss: 1.58; acc: 0.42
Batch: 120; loss: 1.35; acc: 0.56
Batch: 140; loss: 1.26; acc: 0.58
Batch: 160; loss: 1.36; acc: 0.52
Batch: 180; loss: 1.25; acc: 0.59
Batch: 200; loss: 1.02; acc: 0.69
Batch: 220; loss: 1.09; acc: 0.62
Batch: 240; loss: 1.04; acc: 0.66
Batch: 260; loss: 1.37; acc: 0.55
Batch: 280; loss: 1.31; acc: 0.58
Batch: 300; loss: 1.56; acc: 0.48
Batch: 320; loss: 1.14; acc: 0.62
Batch: 340; loss: 1.1; acc: 0.59
Batch: 360; loss: 1.39; acc: 0.59
Batch: 380; loss: 1.52; acc: 0.52
Batch: 400; loss: 1.45; acc: 0.56
Batch: 420; loss: 1.39; acc: 0.52
Batch: 440; loss: 1.45; acc: 0.5
Batch: 460; loss: 1.54; acc: 0.53
Batch: 480; loss: 1.28; acc: 0.48
Batch: 500; loss: 1.33; acc: 0.55
Batch: 520; loss: 1.29; acc: 0.59
Batch: 540; loss: 1.37; acc: 0.53
Batch: 560; loss: 1.09; acc: 0.58
Batch: 580; loss: 1.45; acc: 0.55
Batch: 600; loss: 0.99; acc: 0.7
Batch: 620; loss: 1.4; acc: 0.55
Batch: 640; loss: 1.33; acc: 0.59
Batch: 660; loss: 1.08; acc: 0.67
Batch: 680; loss: 1.23; acc: 0.56
Batch: 700; loss: 1.21; acc: 0.56
Batch: 720; loss: 1.12; acc: 0.64
Batch: 740; loss: 1.39; acc: 0.59
Batch: 760; loss: 1.27; acc: 0.62
Batch: 780; loss: 1.13; acc: 0.59
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.54; acc: 0.45
Batch: 40; loss: 1.07; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.7
Batch: 100; loss: 1.08; acc: 0.58
Batch: 120; loss: 1.42; acc: 0.52
Batch: 140; loss: 1.04; acc: 0.55
Val Epoch over. val_loss: 1.2169194149363571; val_accuracy: 0.5938495222929936 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.49; acc: 0.53
Batch: 20; loss: 1.33; acc: 0.58
Batch: 40; loss: 1.49; acc: 0.48
Batch: 60; loss: 1.11; acc: 0.64
Batch: 80; loss: 0.98; acc: 0.66
Batch: 100; loss: 1.36; acc: 0.55
Batch: 120; loss: 1.23; acc: 0.55
Batch: 140; loss: 1.43; acc: 0.48
Batch: 160; loss: 1.53; acc: 0.47
Batch: 180; loss: 1.36; acc: 0.56
Batch: 200; loss: 1.4; acc: 0.55
Batch: 220; loss: 1.49; acc: 0.55
Batch: 240; loss: 1.26; acc: 0.55
Batch: 260; loss: 1.12; acc: 0.66
Batch: 280; loss: 1.47; acc: 0.52
Batch: 300; loss: 1.36; acc: 0.52
Batch: 320; loss: 1.24; acc: 0.62
Batch: 340; loss: 1.23; acc: 0.55
Batch: 360; loss: 1.07; acc: 0.62
Batch: 380; loss: 1.28; acc: 0.59
Batch: 400; loss: 0.98; acc: 0.66
Batch: 420; loss: 1.16; acc: 0.7
Batch: 440; loss: 1.6; acc: 0.47
Batch: 460; loss: 1.41; acc: 0.5
Batch: 480; loss: 1.32; acc: 0.56
Batch: 500; loss: 1.33; acc: 0.55
Batch: 520; loss: 1.5; acc: 0.5
Batch: 540; loss: 1.13; acc: 0.64
Batch: 560; loss: 1.26; acc: 0.56
Batch: 580; loss: 1.09; acc: 0.58
Batch: 600; loss: 1.36; acc: 0.5
Batch: 620; loss: 1.46; acc: 0.53
Batch: 640; loss: 1.29; acc: 0.59
Batch: 660; loss: 1.36; acc: 0.52
Batch: 680; loss: 1.48; acc: 0.52
Batch: 700; loss: 1.43; acc: 0.58
Batch: 720; loss: 1.24; acc: 0.55
Batch: 740; loss: 1.0; acc: 0.67
Batch: 760; loss: 1.36; acc: 0.52
Batch: 780; loss: 1.41; acc: 0.55
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 1.52; acc: 0.47
Batch: 40; loss: 1.08; acc: 0.59
Batch: 60; loss: 1.17; acc: 0.55
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.1; acc: 0.55
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 1.02; acc: 0.61
Val Epoch over. val_loss: 1.2160383090851412; val_accuracy: 0.596437101910828 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.21; acc: 0.56
Batch: 20; loss: 1.16; acc: 0.61
Batch: 40; loss: 1.56; acc: 0.52
Batch: 60; loss: 1.06; acc: 0.64
Batch: 80; loss: 1.26; acc: 0.56
Batch: 100; loss: 1.22; acc: 0.55
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.43; acc: 0.53
Batch: 160; loss: 1.39; acc: 0.59
Batch: 180; loss: 1.23; acc: 0.59
Batch: 200; loss: 1.49; acc: 0.52
Batch: 220; loss: 1.35; acc: 0.52
Batch: 240; loss: 1.35; acc: 0.56
Batch: 260; loss: 1.31; acc: 0.56
Batch: 280; loss: 1.13; acc: 0.61
Batch: 300; loss: 1.19; acc: 0.62
Batch: 320; loss: 1.34; acc: 0.56
Batch: 340; loss: 1.19; acc: 0.58
Batch: 360; loss: 1.29; acc: 0.55
Batch: 380; loss: 1.03; acc: 0.69
Batch: 400; loss: 1.31; acc: 0.55
Batch: 420; loss: 1.27; acc: 0.59
Batch: 440; loss: 1.37; acc: 0.64
Batch: 460; loss: 1.24; acc: 0.56
Batch: 480; loss: 1.43; acc: 0.52
Batch: 500; loss: 1.1; acc: 0.59
Batch: 520; loss: 1.68; acc: 0.39
Batch: 540; loss: 1.16; acc: 0.59
Batch: 560; loss: 1.06; acc: 0.72
Batch: 580; loss: 1.45; acc: 0.48
Batch: 600; loss: 1.27; acc: 0.52
Batch: 620; loss: 1.22; acc: 0.55
Batch: 640; loss: 1.25; acc: 0.67
Batch: 660; loss: 1.31; acc: 0.56
Batch: 680; loss: 1.34; acc: 0.48
Batch: 700; loss: 1.32; acc: 0.55
Batch: 720; loss: 1.22; acc: 0.66
Batch: 740; loss: 1.34; acc: 0.48
Batch: 760; loss: 1.21; acc: 0.66
Batch: 780; loss: 1.44; acc: 0.55
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.53; acc: 0.44
Batch: 40; loss: 1.08; acc: 0.61
Batch: 60; loss: 1.16; acc: 0.55
Batch: 80; loss: 1.03; acc: 0.69
Batch: 100; loss: 1.12; acc: 0.55
Batch: 120; loss: 1.44; acc: 0.5
Batch: 140; loss: 1.03; acc: 0.64
Val Epoch over. val_loss: 1.2230215505429893; val_accuracy: 0.5921576433121019 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.24; acc: 0.61
Batch: 20; loss: 1.35; acc: 0.5
Batch: 40; loss: 1.38; acc: 0.56
Batch: 60; loss: 1.34; acc: 0.48
Batch: 80; loss: 1.48; acc: 0.48
Batch: 100; loss: 1.11; acc: 0.64
Batch: 120; loss: 1.26; acc: 0.61
Batch: 140; loss: 1.14; acc: 0.61
Batch: 160; loss: 1.48; acc: 0.5
Batch: 180; loss: 1.32; acc: 0.56
Batch: 200; loss: 1.23; acc: 0.55
Batch: 220; loss: 1.2; acc: 0.61
Batch: 240; loss: 1.31; acc: 0.53
Batch: 260; loss: 1.5; acc: 0.5
Batch: 280; loss: 1.08; acc: 0.67
Batch: 300; loss: 1.25; acc: 0.64
Batch: 320; loss: 1.03; acc: 0.64
Batch: 340; loss: 1.28; acc: 0.59
Batch: 360; loss: 1.34; acc: 0.58
Batch: 380; loss: 1.18; acc: 0.61
Batch: 400; loss: 1.3; acc: 0.56
Batch: 420; loss: 1.03; acc: 0.62
Batch: 440; loss: 1.2; acc: 0.55
Batch: 460; loss: 1.2; acc: 0.58
Batch: 480; loss: 1.11; acc: 0.58
Batch: 500; loss: 1.43; acc: 0.55
Batch: 520; loss: 1.15; acc: 0.66
Batch: 540; loss: 1.21; acc: 0.56
Batch: 560; loss: 1.14; acc: 0.59
Batch: 580; loss: 1.13; acc: 0.56
Batch: 600; loss: 1.24; acc: 0.56
Batch: 620; loss: 1.43; acc: 0.58
Batch: 640; loss: 1.16; acc: 0.56
Batch: 660; loss: 1.36; acc: 0.52
Batch: 680; loss: 1.24; acc: 0.62
Batch: 700; loss: 1.41; acc: 0.48
Batch: 720; loss: 1.29; acc: 0.52
Batch: 740; loss: 0.92; acc: 0.73
Batch: 760; loss: 0.98; acc: 0.66
Batch: 780; loss: 1.38; acc: 0.47
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.62
Batch: 20; loss: 1.53; acc: 0.47
Batch: 40; loss: 1.08; acc: 0.56
Batch: 60; loss: 1.15; acc: 0.58
Batch: 80; loss: 1.05; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.58
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 1.02; acc: 0.58
Val Epoch over. val_loss: 1.2174853636960314; val_accuracy: 0.5936504777070064 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.16; acc: 0.69
Batch: 20; loss: 1.27; acc: 0.66
Batch: 40; loss: 1.06; acc: 0.7
Batch: 60; loss: 1.38; acc: 0.53
Batch: 80; loss: 1.41; acc: 0.53
Batch: 100; loss: 1.17; acc: 0.66
Batch: 120; loss: 1.08; acc: 0.66
Batch: 140; loss: 1.22; acc: 0.61
Batch: 160; loss: 1.47; acc: 0.44
Batch: 180; loss: 1.31; acc: 0.62
Batch: 200; loss: 1.18; acc: 0.64
Batch: 220; loss: 1.52; acc: 0.48
Batch: 240; loss: 1.22; acc: 0.67
Batch: 260; loss: 1.01; acc: 0.64
Batch: 280; loss: 1.21; acc: 0.52
Batch: 300; loss: 1.28; acc: 0.62
Batch: 320; loss: 1.12; acc: 0.66
Batch: 340; loss: 1.08; acc: 0.62
Batch: 360; loss: 1.25; acc: 0.55
Batch: 380; loss: 1.04; acc: 0.69
Batch: 400; loss: 1.28; acc: 0.55
Batch: 420; loss: 1.3; acc: 0.62
Batch: 440; loss: 1.32; acc: 0.53
Batch: 460; loss: 1.28; acc: 0.55
Batch: 480; loss: 1.11; acc: 0.61
Batch: 500; loss: 1.02; acc: 0.64
Batch: 520; loss: 1.35; acc: 0.62
Batch: 540; loss: 1.23; acc: 0.58
Batch: 560; loss: 1.22; acc: 0.56
Batch: 580; loss: 1.06; acc: 0.59
Batch: 600; loss: 1.32; acc: 0.58
Batch: 620; loss: 1.24; acc: 0.52
Batch: 640; loss: 1.2; acc: 0.56
Batch: 660; loss: 1.38; acc: 0.53
Batch: 680; loss: 1.15; acc: 0.59
Batch: 700; loss: 1.08; acc: 0.59
Batch: 720; loss: 0.99; acc: 0.66
Batch: 740; loss: 1.57; acc: 0.58
Batch: 760; loss: 1.19; acc: 0.62
Batch: 780; loss: 1.22; acc: 0.61
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.11; acc: 0.66
Batch: 20; loss: 1.55; acc: 0.45
Batch: 40; loss: 1.07; acc: 0.59
Batch: 60; loss: 1.17; acc: 0.55
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.1; acc: 0.53
Batch: 120; loss: 1.45; acc: 0.48
Batch: 140; loss: 1.05; acc: 0.58
Val Epoch over. val_loss: 1.223973103009971; val_accuracy: 0.5889729299363057 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.27; acc: 0.61
Batch: 20; loss: 1.32; acc: 0.48
Batch: 40; loss: 1.57; acc: 0.42
Batch: 60; loss: 1.23; acc: 0.61
Batch: 80; loss: 1.01; acc: 0.62
Batch: 100; loss: 1.32; acc: 0.55
Batch: 120; loss: 1.28; acc: 0.66
Batch: 140; loss: 1.37; acc: 0.58
Batch: 160; loss: 1.55; acc: 0.52
Batch: 180; loss: 1.23; acc: 0.58
Batch: 200; loss: 1.43; acc: 0.58
Batch: 220; loss: 1.06; acc: 0.66
Batch: 240; loss: 1.17; acc: 0.62
Batch: 260; loss: 1.12; acc: 0.64
Batch: 280; loss: 1.14; acc: 0.67
Batch: 300; loss: 1.35; acc: 0.59
Batch: 320; loss: 1.23; acc: 0.64
Batch: 340; loss: 1.11; acc: 0.67
Batch: 360; loss: 1.2; acc: 0.58
Batch: 380; loss: 1.29; acc: 0.58
Batch: 400; loss: 1.27; acc: 0.61
Batch: 420; loss: 1.14; acc: 0.61
Batch: 440; loss: 1.0; acc: 0.67
Batch: 460; loss: 1.09; acc: 0.66
Batch: 480; loss: 1.38; acc: 0.5
Batch: 500; loss: 1.04; acc: 0.64
Batch: 520; loss: 1.54; acc: 0.53
Batch: 540; loss: 1.15; acc: 0.62
Batch: 560; loss: 1.17; acc: 0.59
Batch: 580; loss: 1.31; acc: 0.52
Batch: 600; loss: 1.46; acc: 0.47
Batch: 620; loss: 1.11; acc: 0.64
Batch: 640; loss: 1.35; acc: 0.55
Batch: 660; loss: 1.17; acc: 0.61
Batch: 680; loss: 1.27; acc: 0.58
Batch: 700; loss: 1.33; acc: 0.53
Batch: 720; loss: 1.4; acc: 0.55
Batch: 740; loss: 1.11; acc: 0.61
Batch: 760; loss: 0.88; acc: 0.72
Batch: 780; loss: 1.01; acc: 0.62
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.11; acc: 0.61
Batch: 20; loss: 1.53; acc: 0.48
Batch: 40; loss: 1.1; acc: 0.59
Batch: 60; loss: 1.18; acc: 0.48
Batch: 80; loss: 1.05; acc: 0.7
Batch: 100; loss: 1.13; acc: 0.52
Batch: 120; loss: 1.42; acc: 0.52
Batch: 140; loss: 1.04; acc: 0.61
Val Epoch over. val_loss: 1.2267969618937014; val_accuracy: 0.5887738853503185 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.34; acc: 0.55
Batch: 20; loss: 1.24; acc: 0.59
Batch: 40; loss: 1.36; acc: 0.55
Batch: 60; loss: 1.08; acc: 0.66
Batch: 80; loss: 1.19; acc: 0.62
Batch: 100; loss: 1.4; acc: 0.55
Batch: 120; loss: 1.19; acc: 0.55
Batch: 140; loss: 1.1; acc: 0.67
Batch: 160; loss: 1.44; acc: 0.47
Batch: 180; loss: 1.42; acc: 0.58
Batch: 200; loss: 0.89; acc: 0.73
Batch: 220; loss: 1.33; acc: 0.58
Batch: 240; loss: 1.18; acc: 0.56
Batch: 260; loss: 0.97; acc: 0.72
Batch: 280; loss: 1.27; acc: 0.56
Batch: 300; loss: 1.15; acc: 0.64
Batch: 320; loss: 1.24; acc: 0.59
Batch: 340; loss: 1.15; acc: 0.64
Batch: 360; loss: 1.12; acc: 0.67
Batch: 380; loss: 1.39; acc: 0.53
Batch: 400; loss: 1.11; acc: 0.72
Batch: 420; loss: 1.44; acc: 0.53
Batch: 440; loss: 1.42; acc: 0.5
Batch: 460; loss: 1.32; acc: 0.56
Batch: 480; loss: 1.22; acc: 0.52
Batch: 500; loss: 1.13; acc: 0.56
Batch: 520; loss: 1.47; acc: 0.48
Batch: 540; loss: 1.24; acc: 0.58
Batch: 560; loss: 1.53; acc: 0.53
Batch: 580; loss: 1.35; acc: 0.59
Batch: 600; loss: 1.24; acc: 0.55
Batch: 620; loss: 1.34; acc: 0.5
Batch: 640; loss: 1.05; acc: 0.73
Batch: 660; loss: 1.26; acc: 0.61
Batch: 680; loss: 1.37; acc: 0.48
Batch: 700; loss: 1.26; acc: 0.53
Batch: 720; loss: 1.37; acc: 0.52
Batch: 740; loss: 1.19; acc: 0.61
Batch: 760; loss: 1.16; acc: 0.62
Batch: 780; loss: 1.3; acc: 0.53
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.58; acc: 0.45
Batch: 40; loss: 1.06; acc: 0.61
Batch: 60; loss: 1.16; acc: 0.55
Batch: 80; loss: 1.08; acc: 0.7
Batch: 100; loss: 1.05; acc: 0.59
Batch: 120; loss: 1.42; acc: 0.53
Batch: 140; loss: 1.05; acc: 0.56
Val Epoch over. val_loss: 1.2323203576598198; val_accuracy: 0.5868829617834395 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.31; acc: 0.59
Batch: 20; loss: 1.23; acc: 0.66
Batch: 40; loss: 1.31; acc: 0.59
Batch: 60; loss: 1.53; acc: 0.55
Batch: 80; loss: 1.3; acc: 0.64
Batch: 100; loss: 1.37; acc: 0.48
Batch: 120; loss: 1.46; acc: 0.52
Batch: 140; loss: 1.05; acc: 0.7
Batch: 160; loss: 1.33; acc: 0.52
Batch: 180; loss: 1.1; acc: 0.69
Batch: 200; loss: 1.17; acc: 0.62
Batch: 220; loss: 1.16; acc: 0.67
Batch: 240; loss: 1.26; acc: 0.55
Batch: 260; loss: 1.37; acc: 0.5
Batch: 280; loss: 1.15; acc: 0.55
Batch: 300; loss: 1.32; acc: 0.61
Batch: 320; loss: 1.3; acc: 0.48
Batch: 340; loss: 1.23; acc: 0.5
Batch: 360; loss: 1.0; acc: 0.67
Batch: 380; loss: 1.14; acc: 0.66
Batch: 400; loss: 1.1; acc: 0.66
Batch: 420; loss: 1.03; acc: 0.73
Batch: 440; loss: 1.14; acc: 0.66
Batch: 460; loss: 1.4; acc: 0.52
Batch: 480; loss: 1.22; acc: 0.52
Batch: 500; loss: 1.43; acc: 0.55
Batch: 520; loss: 1.42; acc: 0.53
Batch: 540; loss: 1.01; acc: 0.67
Batch: 560; loss: 1.33; acc: 0.61
Batch: 580; loss: 1.05; acc: 0.69
Batch: 600; loss: 1.0; acc: 0.59
Batch: 620; loss: 1.23; acc: 0.62
Batch: 640; loss: 1.12; acc: 0.69
Batch: 660; loss: 1.52; acc: 0.53
Batch: 680; loss: 1.04; acc: 0.66
Batch: 700; loss: 1.45; acc: 0.53
Batch: 720; loss: 1.3; acc: 0.59
Batch: 740; loss: 1.14; acc: 0.59
Batch: 760; loss: 1.16; acc: 0.66
Batch: 780; loss: 1.08; acc: 0.62
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 1.53; acc: 0.47
Batch: 40; loss: 1.1; acc: 0.61
Batch: 60; loss: 1.17; acc: 0.55
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.13; acc: 0.52
Batch: 120; loss: 1.45; acc: 0.52
Batch: 140; loss: 1.02; acc: 0.62
Val Epoch over. val_loss: 1.2261208561575336; val_accuracy: 0.5896695859872612 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.29; acc: 0.56
Batch: 20; loss: 1.54; acc: 0.47
Batch: 40; loss: 1.32; acc: 0.58
Batch: 60; loss: 1.12; acc: 0.62
Batch: 80; loss: 1.36; acc: 0.56
Batch: 100; loss: 1.23; acc: 0.53
Batch: 120; loss: 1.44; acc: 0.45
Batch: 140; loss: 1.26; acc: 0.55
Batch: 160; loss: 1.01; acc: 0.69
Batch: 180; loss: 1.52; acc: 0.5
Batch: 200; loss: 1.18; acc: 0.61
Batch: 220; loss: 1.18; acc: 0.59
Batch: 240; loss: 1.07; acc: 0.61
Batch: 260; loss: 1.21; acc: 0.7
Batch: 280; loss: 1.5; acc: 0.42
Batch: 300; loss: 1.16; acc: 0.59
Batch: 320; loss: 1.37; acc: 0.48
Batch: 340; loss: 1.33; acc: 0.56
Batch: 360; loss: 1.28; acc: 0.61
Batch: 380; loss: 1.32; acc: 0.48
Batch: 400; loss: 1.22; acc: 0.52
Batch: 420; loss: 1.23; acc: 0.62
Batch: 440; loss: 1.28; acc: 0.53
Batch: 460; loss: 1.61; acc: 0.5
Batch: 480; loss: 1.32; acc: 0.64
Batch: 500; loss: 1.21; acc: 0.61
Batch: 520; loss: 1.63; acc: 0.47
Batch: 540; loss: 1.41; acc: 0.52
Batch: 560; loss: 1.31; acc: 0.55
Batch: 580; loss: 1.27; acc: 0.59
Batch: 600; loss: 1.07; acc: 0.59
Batch: 620; loss: 1.38; acc: 0.53
Batch: 640; loss: 1.48; acc: 0.45
Batch: 660; loss: 1.19; acc: 0.62
Batch: 680; loss: 1.57; acc: 0.52
Batch: 700; loss: 1.15; acc: 0.62
Batch: 720; loss: 1.47; acc: 0.55
Batch: 740; loss: 1.23; acc: 0.52
Batch: 760; loss: 1.12; acc: 0.64
Batch: 780; loss: 1.21; acc: 0.62
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.52; acc: 0.48
Batch: 40; loss: 1.08; acc: 0.59
Batch: 60; loss: 1.15; acc: 0.56
Batch: 80; loss: 1.05; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.58
Batch: 120; loss: 1.4; acc: 0.5
Batch: 140; loss: 1.02; acc: 0.59
Val Epoch over. val_loss: 1.2189318492154406; val_accuracy: 0.5963375796178344 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.96; acc: 0.7
Batch: 20; loss: 1.15; acc: 0.62
Batch: 40; loss: 1.42; acc: 0.55
Batch: 60; loss: 1.62; acc: 0.48
Batch: 80; loss: 1.25; acc: 0.62
Batch: 100; loss: 1.15; acc: 0.62
Batch: 120; loss: 1.41; acc: 0.55
Batch: 140; loss: 0.99; acc: 0.67
Batch: 160; loss: 1.33; acc: 0.5
Batch: 180; loss: 1.08; acc: 0.7
Batch: 200; loss: 1.49; acc: 0.56
Batch: 220; loss: 1.52; acc: 0.44
Batch: 240; loss: 1.06; acc: 0.61
Batch: 260; loss: 1.25; acc: 0.53
Batch: 280; loss: 1.33; acc: 0.5
Batch: 300; loss: 1.32; acc: 0.55
Batch: 320; loss: 1.53; acc: 0.52
Batch: 340; loss: 1.25; acc: 0.62
Batch: 360; loss: 1.07; acc: 0.62
Batch: 380; loss: 1.27; acc: 0.55
Batch: 400; loss: 1.13; acc: 0.62
Batch: 420; loss: 1.35; acc: 0.58
Batch: 440; loss: 1.32; acc: 0.52
Batch: 460; loss: 1.32; acc: 0.55
Batch: 480; loss: 1.17; acc: 0.55
Batch: 500; loss: 1.21; acc: 0.56
Batch: 520; loss: 1.22; acc: 0.55
Batch: 540; loss: 1.44; acc: 0.44
Batch: 560; loss: 1.15; acc: 0.67
Batch: 580; loss: 1.1; acc: 0.62
Batch: 600; loss: 1.47; acc: 0.48
Batch: 620; loss: 1.18; acc: 0.53
Batch: 640; loss: 1.3; acc: 0.58
Batch: 660; loss: 1.37; acc: 0.61
Batch: 680; loss: 1.44; acc: 0.56
Batch: 700; loss: 1.23; acc: 0.59
Batch: 720; loss: 1.0; acc: 0.69
Batch: 740; loss: 1.26; acc: 0.56
Batch: 760; loss: 1.55; acc: 0.45
Batch: 780; loss: 1.39; acc: 0.62
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.11; acc: 0.66
Batch: 20; loss: 1.53; acc: 0.5
Batch: 40; loss: 1.07; acc: 0.56
Batch: 60; loss: 1.17; acc: 0.55
Batch: 80; loss: 1.05; acc: 0.67
Batch: 100; loss: 1.1; acc: 0.55
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 1.01; acc: 0.59
Val Epoch over. val_loss: 1.2175054090797521; val_accuracy: 0.5925557324840764 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.2; acc: 0.59
Batch: 20; loss: 1.53; acc: 0.45
Batch: 40; loss: 1.46; acc: 0.59
Batch: 60; loss: 1.27; acc: 0.56
Batch: 80; loss: 1.38; acc: 0.53
Batch: 100; loss: 1.46; acc: 0.56
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 1.23; acc: 0.5
Batch: 160; loss: 1.06; acc: 0.66
Batch: 180; loss: 1.04; acc: 0.64
Batch: 200; loss: 1.56; acc: 0.38
Batch: 220; loss: 1.27; acc: 0.61
Batch: 240; loss: 1.03; acc: 0.67
Batch: 260; loss: 1.3; acc: 0.5
Batch: 280; loss: 1.35; acc: 0.55
Batch: 300; loss: 1.13; acc: 0.61
Batch: 320; loss: 1.25; acc: 0.61
Batch: 340; loss: 1.28; acc: 0.55
Batch: 360; loss: 1.03; acc: 0.66
Batch: 380; loss: 0.98; acc: 0.64
Batch: 400; loss: 1.09; acc: 0.62
Batch: 420; loss: 1.03; acc: 0.66
Batch: 440; loss: 1.26; acc: 0.56
Batch: 460; loss: 1.33; acc: 0.52
Batch: 480; loss: 1.48; acc: 0.53
Batch: 500; loss: 1.26; acc: 0.58
Batch: 520; loss: 1.15; acc: 0.55
Batch: 540; loss: 1.18; acc: 0.62
Batch: 560; loss: 1.22; acc: 0.64
Batch: 580; loss: 1.26; acc: 0.59
Batch: 600; loss: 1.39; acc: 0.52
Batch: 620; loss: 1.3; acc: 0.52
Batch: 640; loss: 1.15; acc: 0.62
Batch: 660; loss: 1.24; acc: 0.56
Batch: 680; loss: 1.32; acc: 0.5
Batch: 700; loss: 1.36; acc: 0.56
Batch: 720; loss: 1.11; acc: 0.61
Batch: 740; loss: 1.29; acc: 0.58
Batch: 760; loss: 1.38; acc: 0.55
Batch: 780; loss: 1.22; acc: 0.55
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.67
Batch: 20; loss: 1.54; acc: 0.47
Batch: 40; loss: 1.06; acc: 0.56
Batch: 60; loss: 1.16; acc: 0.56
Batch: 80; loss: 1.04; acc: 0.7
Batch: 100; loss: 1.09; acc: 0.56
Batch: 120; loss: 1.43; acc: 0.5
Batch: 140; loss: 1.03; acc: 0.56
Val Epoch over. val_loss: 1.2168204260479873; val_accuracy: 0.5940485668789809 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.15; acc: 0.61
Batch: 20; loss: 1.09; acc: 0.67
Batch: 40; loss: 1.36; acc: 0.52
Batch: 60; loss: 1.11; acc: 0.7
Batch: 80; loss: 1.46; acc: 0.48
Batch: 100; loss: 1.25; acc: 0.61
Batch: 120; loss: 1.09; acc: 0.59
Batch: 140; loss: 1.21; acc: 0.56
Batch: 160; loss: 1.23; acc: 0.62
Batch: 180; loss: 1.22; acc: 0.58
Batch: 200; loss: 1.35; acc: 0.62
Batch: 220; loss: 1.41; acc: 0.5
Batch: 240; loss: 1.02; acc: 0.67
Batch: 260; loss: 1.32; acc: 0.53
Batch: 280; loss: 1.45; acc: 0.44
Batch: 300; loss: 1.19; acc: 0.64
Batch: 320; loss: 1.09; acc: 0.62
Batch: 340; loss: 1.28; acc: 0.53
Batch: 360; loss: 1.04; acc: 0.66
Batch: 380; loss: 1.12; acc: 0.64
Batch: 400; loss: 1.37; acc: 0.52
Batch: 420; loss: 1.35; acc: 0.53
Batch: 440; loss: 1.3; acc: 0.53
Batch: 460; loss: 1.23; acc: 0.58
Batch: 480; loss: 1.18; acc: 0.55
Batch: 500; loss: 1.08; acc: 0.59
Batch: 520; loss: 1.15; acc: 0.56
Batch: 540; loss: 1.41; acc: 0.52
Batch: 560; loss: 1.16; acc: 0.61
Batch: 580; loss: 1.44; acc: 0.48
Batch: 600; loss: 1.33; acc: 0.52
Batch: 620; loss: 1.4; acc: 0.61
Batch: 640; loss: 1.46; acc: 0.59
Batch: 660; loss: 1.0; acc: 0.69
Batch: 680; loss: 1.35; acc: 0.58
Batch: 700; loss: 1.37; acc: 0.56
Batch: 720; loss: 1.06; acc: 0.64
Batch: 740; loss: 1.4; acc: 0.56
Batch: 760; loss: 1.22; acc: 0.62
Batch: 780; loss: 1.37; acc: 0.55
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.52; acc: 0.48
Batch: 40; loss: 1.07; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.56
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.58
Batch: 120; loss: 1.41; acc: 0.56
Batch: 140; loss: 1.02; acc: 0.59
Val Epoch over. val_loss: 1.2157002748197812; val_accuracy: 0.5982285031847133 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.45; acc: 0.53
Batch: 20; loss: 1.44; acc: 0.44
Batch: 40; loss: 1.11; acc: 0.62
Batch: 60; loss: 1.46; acc: 0.47
Batch: 80; loss: 1.17; acc: 0.64
Batch: 100; loss: 1.44; acc: 0.5
Batch: 120; loss: 1.25; acc: 0.58
Batch: 140; loss: 1.35; acc: 0.52
Batch: 160; loss: 1.19; acc: 0.55
Batch: 180; loss: 1.16; acc: 0.58
Batch: 200; loss: 1.07; acc: 0.69
Batch: 220; loss: 1.05; acc: 0.66
Batch: 240; loss: 1.18; acc: 0.55
Batch: 260; loss: 1.21; acc: 0.61
Batch: 280; loss: 1.27; acc: 0.53
Batch: 300; loss: 1.41; acc: 0.55
Batch: 320; loss: 1.22; acc: 0.61
Batch: 340; loss: 1.43; acc: 0.53
Batch: 360; loss: 1.41; acc: 0.5
Batch: 380; loss: 1.19; acc: 0.59
Batch: 400; loss: 1.13; acc: 0.64
Batch: 420; loss: 1.34; acc: 0.53
Batch: 440; loss: 1.27; acc: 0.61
Batch: 460; loss: 1.17; acc: 0.59
Batch: 480; loss: 1.3; acc: 0.56
Batch: 500; loss: 1.01; acc: 0.7
Batch: 520; loss: 1.19; acc: 0.62
Batch: 540; loss: 1.63; acc: 0.52
Batch: 560; loss: 1.29; acc: 0.59
Batch: 580; loss: 1.34; acc: 0.55
Batch: 600; loss: 1.38; acc: 0.56
Batch: 620; loss: 1.18; acc: 0.55
Batch: 640; loss: 1.24; acc: 0.53
Batch: 660; loss: 1.17; acc: 0.59
Batch: 680; loss: 1.34; acc: 0.59
Batch: 700; loss: 1.33; acc: 0.61
Batch: 720; loss: 1.39; acc: 0.52
Batch: 740; loss: 0.93; acc: 0.66
Batch: 760; loss: 1.01; acc: 0.62
Batch: 780; loss: 1.06; acc: 0.64
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.52; acc: 0.48
Batch: 40; loss: 1.07; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.56
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.56
Batch: 120; loss: 1.41; acc: 0.53
Batch: 140; loss: 1.02; acc: 0.58
Val Epoch over. val_loss: 1.2148074131862374; val_accuracy: 0.5980294585987261 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.06; acc: 0.64
Batch: 20; loss: 1.39; acc: 0.53
Batch: 40; loss: 1.42; acc: 0.45
Batch: 60; loss: 1.38; acc: 0.55
Batch: 80; loss: 1.24; acc: 0.55
Batch: 100; loss: 1.3; acc: 0.59
Batch: 120; loss: 1.29; acc: 0.53
Batch: 140; loss: 1.54; acc: 0.53
Batch: 160; loss: 1.4; acc: 0.55
Batch: 180; loss: 0.94; acc: 0.67
Batch: 200; loss: 1.22; acc: 0.61
Batch: 220; loss: 1.38; acc: 0.56
Batch: 240; loss: 1.12; acc: 0.61
Batch: 260; loss: 1.1; acc: 0.61
Batch: 280; loss: 1.14; acc: 0.56
Batch: 300; loss: 1.38; acc: 0.52
Batch: 320; loss: 1.35; acc: 0.56
Batch: 340; loss: 1.44; acc: 0.47
Batch: 360; loss: 1.51; acc: 0.55
Batch: 380; loss: 1.24; acc: 0.61
Batch: 400; loss: 1.34; acc: 0.55
Batch: 420; loss: 1.23; acc: 0.64
Batch: 440; loss: 1.24; acc: 0.55
Batch: 460; loss: 1.17; acc: 0.56
Batch: 480; loss: 1.37; acc: 0.61
Batch: 500; loss: 1.21; acc: 0.58
Batch: 520; loss: 1.19; acc: 0.56
Batch: 540; loss: 1.3; acc: 0.5
Batch: 560; loss: 1.24; acc: 0.62
Batch: 580; loss: 1.22; acc: 0.61
Batch: 600; loss: 1.34; acc: 0.52
Batch: 620; loss: 1.31; acc: 0.58
Batch: 640; loss: 1.38; acc: 0.48
Batch: 660; loss: 1.39; acc: 0.56
Batch: 680; loss: 1.36; acc: 0.59
Batch: 700; loss: 1.38; acc: 0.5
Batch: 720; loss: 1.17; acc: 0.61
Batch: 740; loss: 1.34; acc: 0.53
Batch: 760; loss: 1.06; acc: 0.59
Batch: 780; loss: 1.08; acc: 0.67
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 1.52; acc: 0.47
Batch: 40; loss: 1.07; acc: 0.58
Batch: 60; loss: 1.16; acc: 0.53
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 1.1; acc: 0.56
Batch: 120; loss: 1.43; acc: 0.52
Batch: 140; loss: 1.02; acc: 0.59
Val Epoch over. val_loss: 1.2154153444964415; val_accuracy: 0.5958399681528662 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.48; acc: 0.52
Batch: 20; loss: 1.52; acc: 0.52
Batch: 40; loss: 1.29; acc: 0.48
Batch: 60; loss: 1.16; acc: 0.67
Batch: 80; loss: 1.09; acc: 0.59
Batch: 100; loss: 1.16; acc: 0.64
Batch: 120; loss: 1.12; acc: 0.61
Batch: 140; loss: 1.47; acc: 0.48
Batch: 160; loss: 1.4; acc: 0.52
Batch: 180; loss: 1.49; acc: 0.5
Batch: 200; loss: 1.4; acc: 0.56
Batch: 220; loss: 1.22; acc: 0.62
Batch: 240; loss: 1.22; acc: 0.59
Batch: 260; loss: 1.18; acc: 0.7
Batch: 280; loss: 1.22; acc: 0.58
Batch: 300; loss: 1.19; acc: 0.61
Batch: 320; loss: 1.25; acc: 0.58
Batch: 340; loss: 1.4; acc: 0.47
Batch: 360; loss: 1.2; acc: 0.53
Batch: 380; loss: 1.34; acc: 0.58
Batch: 400; loss: 1.46; acc: 0.47
Batch: 420; loss: 1.41; acc: 0.52
Batch: 440; loss: 1.5; acc: 0.52
Batch: 460; loss: 1.34; acc: 0.53
Batch: 480; loss: 1.37; acc: 0.52
Batch: 500; loss: 1.36; acc: 0.47
Batch: 520; loss: 1.16; acc: 0.59
Batch: 540; loss: 1.29; acc: 0.58
Batch: 560; loss: 1.0; acc: 0.67
Batch: 580; loss: 1.45; acc: 0.48
Batch: 600; loss: 1.38; acc: 0.58
Batch: 620; loss: 1.19; acc: 0.66
Batch: 640; loss: 1.19; acc: 0.59
Batch: 660; loss: 1.34; acc: 0.56
Batch: 680; loss: 1.19; acc: 0.61
Batch: 700; loss: 1.49; acc: 0.55
Batch: 720; loss: 1.18; acc: 0.62
Batch: 740; loss: 1.13; acc: 0.61
Batch: 760; loss: 1.15; acc: 0.55
Batch: 780; loss: 1.33; acc: 0.61
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.54; acc: 0.47
Batch: 40; loss: 1.07; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.56
Batch: 80; loss: 1.05; acc: 0.7
Batch: 100; loss: 1.07; acc: 0.58
Batch: 120; loss: 1.42; acc: 0.55
Batch: 140; loss: 1.02; acc: 0.61
Val Epoch over. val_loss: 1.2173765897750854; val_accuracy: 0.5965366242038217 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.39; acc: 0.61
Batch: 20; loss: 1.28; acc: 0.59
Batch: 40; loss: 1.3; acc: 0.56
Batch: 60; loss: 1.26; acc: 0.52
Batch: 80; loss: 1.26; acc: 0.59
Batch: 100; loss: 1.51; acc: 0.47
Batch: 120; loss: 1.41; acc: 0.55
Batch: 140; loss: 1.3; acc: 0.56
Batch: 160; loss: 0.99; acc: 0.64
Batch: 180; loss: 1.28; acc: 0.62
Batch: 200; loss: 1.08; acc: 0.7
Batch: 220; loss: 0.95; acc: 0.7
Batch: 240; loss: 1.03; acc: 0.62
Batch: 260; loss: 1.38; acc: 0.56
Batch: 280; loss: 1.05; acc: 0.7
Batch: 300; loss: 1.17; acc: 0.61
Batch: 320; loss: 1.34; acc: 0.58
Batch: 340; loss: 1.32; acc: 0.62
Batch: 360; loss: 1.09; acc: 0.66
Batch: 380; loss: 1.25; acc: 0.55
Batch: 400; loss: 1.26; acc: 0.59
Batch: 420; loss: 1.19; acc: 0.66
Batch: 440; loss: 1.15; acc: 0.58
Batch: 460; loss: 1.05; acc: 0.67
Batch: 480; loss: 1.37; acc: 0.53
Batch: 500; loss: 1.19; acc: 0.61
Batch: 520; loss: 1.26; acc: 0.66
Batch: 540; loss: 1.23; acc: 0.53
Batch: 560; loss: 1.25; acc: 0.61
Batch: 580; loss: 1.15; acc: 0.64
Batch: 600; loss: 1.11; acc: 0.59
Batch: 620; loss: 1.24; acc: 0.61
Batch: 640; loss: 0.97; acc: 0.7
Batch: 660; loss: 1.23; acc: 0.61
Batch: 680; loss: 1.21; acc: 0.61
Batch: 700; loss: 1.5; acc: 0.52
Batch: 720; loss: 1.47; acc: 0.47
Batch: 740; loss: 1.37; acc: 0.48
Batch: 760; loss: 1.45; acc: 0.53
Batch: 780; loss: 1.24; acc: 0.53
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.1; acc: 0.66
Batch: 20; loss: 1.52; acc: 0.45
Batch: 40; loss: 1.08; acc: 0.61
Batch: 60; loss: 1.16; acc: 0.56
Batch: 80; loss: 1.05; acc: 0.66
Batch: 100; loss: 1.1; acc: 0.55
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 1.01; acc: 0.59
Val Epoch over. val_loss: 1.2165191826547028; val_accuracy: 0.5941480891719745 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.15; acc: 0.66
Batch: 20; loss: 1.19; acc: 0.59
Batch: 40; loss: 1.4; acc: 0.55
Batch: 60; loss: 1.24; acc: 0.55
Batch: 80; loss: 1.52; acc: 0.48
Batch: 100; loss: 1.13; acc: 0.58
Batch: 120; loss: 1.42; acc: 0.55
Batch: 140; loss: 1.28; acc: 0.62
Batch: 160; loss: 1.36; acc: 0.55
Batch: 180; loss: 1.34; acc: 0.56
Batch: 200; loss: 1.14; acc: 0.64
Batch: 220; loss: 1.1; acc: 0.59
Batch: 240; loss: 1.15; acc: 0.62
Batch: 260; loss: 1.24; acc: 0.59
Batch: 280; loss: 1.47; acc: 0.5
Batch: 300; loss: 1.07; acc: 0.58
Batch: 320; loss: 1.16; acc: 0.62
Batch: 340; loss: 1.09; acc: 0.67
Batch: 360; loss: 1.25; acc: 0.59
Batch: 380; loss: 1.24; acc: 0.55
Batch: 400; loss: 1.49; acc: 0.55
Batch: 420; loss: 1.35; acc: 0.58
Batch: 440; loss: 1.5; acc: 0.55
Batch: 460; loss: 1.28; acc: 0.59
Batch: 480; loss: 1.24; acc: 0.58
Batch: 500; loss: 1.35; acc: 0.53
Batch: 520; loss: 1.36; acc: 0.52
Batch: 540; loss: 1.43; acc: 0.53
Batch: 560; loss: 1.36; acc: 0.56
Batch: 580; loss: 1.18; acc: 0.56
Batch: 600; loss: 1.18; acc: 0.5
Batch: 620; loss: 1.24; acc: 0.61
Batch: 640; loss: 1.39; acc: 0.55
Batch: 660; loss: 1.33; acc: 0.59
Batch: 680; loss: 1.38; acc: 0.55
Batch: 700; loss: 1.16; acc: 0.58
Batch: 720; loss: 1.1; acc: 0.66
Batch: 740; loss: 1.23; acc: 0.61
Batch: 760; loss: 1.44; acc: 0.41
Batch: 780; loss: 1.34; acc: 0.52
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.53; acc: 0.48
Batch: 40; loss: 1.06; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.56
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.08; acc: 0.56
Batch: 120; loss: 1.42; acc: 0.53
Batch: 140; loss: 1.02; acc: 0.61
Val Epoch over. val_loss: 1.2147076889208168; val_accuracy: 0.5972332802547771 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.27; acc: 0.52
Batch: 20; loss: 1.23; acc: 0.67
Batch: 40; loss: 1.3; acc: 0.56
Batch: 60; loss: 1.18; acc: 0.67
Batch: 80; loss: 1.13; acc: 0.56
Batch: 100; loss: 1.19; acc: 0.61
Batch: 120; loss: 1.54; acc: 0.45
Batch: 140; loss: 1.37; acc: 0.55
Batch: 160; loss: 1.24; acc: 0.58
Batch: 180; loss: 1.19; acc: 0.58
Batch: 200; loss: 1.19; acc: 0.58
Batch: 220; loss: 1.38; acc: 0.55
Batch: 240; loss: 1.23; acc: 0.53
Batch: 260; loss: 1.29; acc: 0.56
Batch: 280; loss: 1.28; acc: 0.55
Batch: 300; loss: 1.17; acc: 0.61
Batch: 320; loss: 0.99; acc: 0.69
Batch: 340; loss: 1.25; acc: 0.61
Batch: 360; loss: 1.04; acc: 0.67
Batch: 380; loss: 0.92; acc: 0.8
Batch: 400; loss: 1.28; acc: 0.59
Batch: 420; loss: 1.3; acc: 0.61
Batch: 440; loss: 1.38; acc: 0.5
Batch: 460; loss: 1.16; acc: 0.59
Batch: 480; loss: 1.27; acc: 0.61
Batch: 500; loss: 1.32; acc: 0.53
Batch: 520; loss: 1.17; acc: 0.66
Batch: 540; loss: 1.28; acc: 0.59
Batch: 560; loss: 1.39; acc: 0.56
Batch: 580; loss: 1.04; acc: 0.7
Batch: 600; loss: 1.16; acc: 0.55
Batch: 620; loss: 1.12; acc: 0.62
Batch: 640; loss: 1.23; acc: 0.66
Batch: 660; loss: 1.38; acc: 0.55
Batch: 680; loss: 1.0; acc: 0.66
Batch: 700; loss: 1.26; acc: 0.56
Batch: 720; loss: 1.4; acc: 0.56
Batch: 740; loss: 1.47; acc: 0.59
Batch: 760; loss: 1.42; acc: 0.42
Batch: 780; loss: 1.23; acc: 0.56
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.53; acc: 0.47
Batch: 40; loss: 1.07; acc: 0.59
Batch: 60; loss: 1.15; acc: 0.55
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 1.08; acc: 0.56
Batch: 120; loss: 1.41; acc: 0.53
Batch: 140; loss: 1.02; acc: 0.59
Val Epoch over. val_loss: 1.2151119450854648; val_accuracy: 0.5954418789808917 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.1; acc: 0.62
Batch: 20; loss: 1.11; acc: 0.56
Batch: 40; loss: 1.51; acc: 0.55
Batch: 60; loss: 1.44; acc: 0.48
Batch: 80; loss: 1.41; acc: 0.55
Batch: 100; loss: 1.65; acc: 0.5
Batch: 120; loss: 1.3; acc: 0.59
Batch: 140; loss: 1.47; acc: 0.5
Batch: 160; loss: 1.06; acc: 0.59
Batch: 180; loss: 1.49; acc: 0.53
Batch: 200; loss: 1.2; acc: 0.62
Batch: 220; loss: 1.11; acc: 0.72
Batch: 240; loss: 1.29; acc: 0.61
Batch: 260; loss: 1.2; acc: 0.53
Batch: 280; loss: 1.27; acc: 0.56
Batch: 300; loss: 1.27; acc: 0.62
Batch: 320; loss: 1.3; acc: 0.56
Batch: 340; loss: 1.3; acc: 0.59
Batch: 360; loss: 1.37; acc: 0.56
Batch: 380; loss: 1.24; acc: 0.64
Batch: 400; loss: 1.23; acc: 0.58
Batch: 420; loss: 1.11; acc: 0.61
Batch: 440; loss: 1.39; acc: 0.5
Batch: 460; loss: 1.3; acc: 0.58
Batch: 480; loss: 1.3; acc: 0.55
Batch: 500; loss: 1.06; acc: 0.58
Batch: 520; loss: 1.26; acc: 0.55
Batch: 540; loss: 1.24; acc: 0.66
Batch: 560; loss: 1.3; acc: 0.56
Batch: 580; loss: 1.21; acc: 0.59
Batch: 600; loss: 1.14; acc: 0.61
Batch: 620; loss: 1.53; acc: 0.47
Batch: 640; loss: 0.93; acc: 0.73
Batch: 660; loss: 1.16; acc: 0.62
Batch: 680; loss: 1.55; acc: 0.45
Batch: 700; loss: 1.54; acc: 0.52
Batch: 720; loss: 1.38; acc: 0.53
Batch: 740; loss: 1.41; acc: 0.48
Batch: 760; loss: 1.22; acc: 0.58
Batch: 780; loss: 1.41; acc: 0.55
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.67
Batch: 20; loss: 1.53; acc: 0.47
Batch: 40; loss: 1.06; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.58
Batch: 80; loss: 1.04; acc: 0.67
Batch: 100; loss: 1.08; acc: 0.56
Batch: 120; loss: 1.42; acc: 0.52
Batch: 140; loss: 1.02; acc: 0.59
Val Epoch over. val_loss: 1.2151352482236875; val_accuracy: 0.5960390127388535 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 1.0; acc: 0.67
Batch: 40; loss: 1.2; acc: 0.64
Batch: 60; loss: 1.15; acc: 0.64
Batch: 80; loss: 1.21; acc: 0.61
Batch: 100; loss: 1.33; acc: 0.55
Batch: 120; loss: 1.4; acc: 0.48
Batch: 140; loss: 1.13; acc: 0.67
Batch: 160; loss: 1.35; acc: 0.48
Batch: 180; loss: 1.34; acc: 0.58
Batch: 200; loss: 1.26; acc: 0.53
Batch: 220; loss: 1.45; acc: 0.53
Batch: 240; loss: 1.19; acc: 0.58
Batch: 260; loss: 1.25; acc: 0.58
Batch: 280; loss: 1.27; acc: 0.53
Batch: 300; loss: 1.13; acc: 0.61
Batch: 320; loss: 1.24; acc: 0.55
Batch: 340; loss: 1.31; acc: 0.58
Batch: 360; loss: 1.35; acc: 0.61
Batch: 380; loss: 1.6; acc: 0.52
Batch: 400; loss: 1.15; acc: 0.64
Batch: 420; loss: 1.32; acc: 0.56
Batch: 440; loss: 1.25; acc: 0.56
Batch: 460; loss: 1.07; acc: 0.66
Batch: 480; loss: 1.18; acc: 0.58
Batch: 500; loss: 1.35; acc: 0.53
Batch: 520; loss: 1.29; acc: 0.55
Batch: 540; loss: 1.43; acc: 0.55
Batch: 560; loss: 0.91; acc: 0.69
Batch: 580; loss: 1.2; acc: 0.58
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.25; acc: 0.58
Batch: 640; loss: 1.56; acc: 0.44
Batch: 660; loss: 1.34; acc: 0.61
Batch: 680; loss: 1.39; acc: 0.53
Batch: 700; loss: 1.32; acc: 0.58
Batch: 720; loss: 1.15; acc: 0.64
Batch: 740; loss: 1.37; acc: 0.56
Batch: 760; loss: 1.39; acc: 0.58
Batch: 780; loss: 1.52; acc: 0.47
Train Epoch over. train_loss: 1.26; train_accuracy: 0.58 

Batch: 0; loss: 1.09; acc: 0.66
Batch: 20; loss: 1.53; acc: 0.47
Batch: 40; loss: 1.07; acc: 0.58
Batch: 60; loss: 1.15; acc: 0.56
Batch: 80; loss: 1.05; acc: 0.69
Batch: 100; loss: 1.07; acc: 0.56
Batch: 120; loss: 1.41; acc: 0.53
Batch: 140; loss: 1.03; acc: 0.62
Val Epoch over. val_loss: 1.2164072439928724; val_accuracy: 0.5946457006369427 

plots/subspace_training/reg_lenet_2/2020-01-19 19:11:48/d_dim_50_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 13941
elements in E: 1977400
fraction nonzero: 0.007050166885809649
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.31; acc: 0.05
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.31; acc: 0.08
Batch: 200; loss: 2.31; acc: 0.11
Batch: 220; loss: 2.31; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.09
Batch: 260; loss: 2.29; acc: 0.12
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.31; acc: 0.11
Batch: 320; loss: 2.29; acc: 0.05
Batch: 340; loss: 2.31; acc: 0.16
Batch: 360; loss: 2.3; acc: 0.17
Batch: 380; loss: 2.3; acc: 0.25
Batch: 400; loss: 2.29; acc: 0.22
Batch: 420; loss: 2.31; acc: 0.19
Batch: 440; loss: 2.31; acc: 0.17
Batch: 460; loss: 2.29; acc: 0.28
Batch: 480; loss: 2.29; acc: 0.17
Batch: 500; loss: 2.3; acc: 0.17
Batch: 520; loss: 2.31; acc: 0.05
Batch: 540; loss: 2.3; acc: 0.12
Batch: 560; loss: 2.3; acc: 0.08
Batch: 580; loss: 2.3; acc: 0.12
Batch: 600; loss: 2.3; acc: 0.11
Batch: 620; loss: 2.3; acc: 0.12
Batch: 640; loss: 2.3; acc: 0.16
Batch: 660; loss: 2.28; acc: 0.16
Batch: 680; loss: 2.3; acc: 0.2
Batch: 700; loss: 2.3; acc: 0.22
Batch: 720; loss: 2.28; acc: 0.28
Batch: 740; loss: 2.3; acc: 0.19
Batch: 760; loss: 2.29; acc: 0.22
Batch: 780; loss: 2.27; acc: 0.34
Train Epoch over. train_loss: 2.3; train_accuracy: 0.14 

Batch: 0; loss: 2.3; acc: 0.19
Batch: 20; loss: 2.31; acc: 0.14
Batch: 40; loss: 2.29; acc: 0.27
Batch: 60; loss: 2.3; acc: 0.2
Batch: 80; loss: 2.28; acc: 0.22
Batch: 100; loss: 2.3; acc: 0.19
Batch: 120; loss: 2.31; acc: 0.19
Batch: 140; loss: 2.29; acc: 0.22
Val Epoch over. val_loss: 2.2948870021066847; val_accuracy: 0.20889729299363058 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.29; acc: 0.27
Batch: 20; loss: 2.29; acc: 0.17
Batch: 40; loss: 2.29; acc: 0.25
Batch: 60; loss: 2.29; acc: 0.22
Batch: 80; loss: 2.28; acc: 0.23
Batch: 100; loss: 2.3; acc: 0.16
Batch: 120; loss: 2.3; acc: 0.16
Batch: 140; loss: 2.31; acc: 0.19
Batch: 160; loss: 2.28; acc: 0.23
Batch: 180; loss: 2.28; acc: 0.3
Batch: 200; loss: 2.31; acc: 0.16
Batch: 220; loss: 2.28; acc: 0.25
Batch: 240; loss: 2.29; acc: 0.19
Batch: 260; loss: 2.27; acc: 0.27
Batch: 280; loss: 2.29; acc: 0.23
Batch: 300; loss: 2.29; acc: 0.22
Batch: 320; loss: 2.29; acc: 0.17
Batch: 340; loss: 2.28; acc: 0.23
Batch: 360; loss: 2.26; acc: 0.3
Batch: 380; loss: 2.31; acc: 0.16
Batch: 400; loss: 2.26; acc: 0.27
Batch: 420; loss: 2.29; acc: 0.23
Batch: 440; loss: 2.27; acc: 0.23
Batch: 460; loss: 2.27; acc: 0.19
Batch: 480; loss: 2.28; acc: 0.16
Batch: 500; loss: 2.28; acc: 0.14
Batch: 520; loss: 2.28; acc: 0.19
Batch: 540; loss: 2.28; acc: 0.14
Batch: 560; loss: 2.29; acc: 0.11
Batch: 580; loss: 2.26; acc: 0.27
Batch: 600; loss: 2.28; acc: 0.17
Batch: 620; loss: 2.28; acc: 0.17
Batch: 640; loss: 2.26; acc: 0.19
Batch: 660; loss: 2.26; acc: 0.19
Batch: 680; loss: 2.25; acc: 0.22
Batch: 700; loss: 2.23; acc: 0.23
Batch: 720; loss: 2.23; acc: 0.2
Batch: 740; loss: 2.24; acc: 0.28
Batch: 760; loss: 2.18; acc: 0.27
Batch: 780; loss: 2.26; acc: 0.23
Train Epoch over. train_loss: 2.28; train_accuracy: 0.21 

Batch: 0; loss: 2.2; acc: 0.25
Batch: 20; loss: 2.26; acc: 0.2
Batch: 40; loss: 2.16; acc: 0.33
Batch: 60; loss: 2.18; acc: 0.27
Batch: 80; loss: 2.16; acc: 0.27
Batch: 100; loss: 2.23; acc: 0.23
Batch: 120; loss: 2.21; acc: 0.3
Batch: 140; loss: 2.2; acc: 0.31
Val Epoch over. val_loss: 2.208598434545432; val_accuracy: 0.25398089171974525 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 2.26; acc: 0.17
Batch: 20; loss: 2.25; acc: 0.2
Batch: 40; loss: 2.16; acc: 0.25
Batch: 60; loss: 2.19; acc: 0.28
Batch: 80; loss: 2.08; acc: 0.33
Batch: 100; loss: 2.09; acc: 0.31
Batch: 120; loss: 2.07; acc: 0.22
Batch: 140; loss: 1.97; acc: 0.25
Batch: 160; loss: 2.1; acc: 0.3
Batch: 180; loss: 1.87; acc: 0.28
Batch: 200; loss: 1.88; acc: 0.33
Batch: 220; loss: 1.88; acc: 0.3
Batch: 240; loss: 1.75; acc: 0.39
Batch: 260; loss: 1.95; acc: 0.3
Batch: 280; loss: 1.68; acc: 0.39
Batch: 300; loss: 1.77; acc: 0.38
Batch: 320; loss: 1.7; acc: 0.33
Batch: 340; loss: 1.73; acc: 0.33
Batch: 360; loss: 1.64; acc: 0.39
Batch: 380; loss: 1.52; acc: 0.5
Batch: 400; loss: 1.63; acc: 0.39
Batch: 420; loss: 1.59; acc: 0.34
Batch: 440; loss: 1.96; acc: 0.27
Batch: 460; loss: 1.73; acc: 0.33
Batch: 480; loss: 1.61; acc: 0.5
Batch: 500; loss: 1.6; acc: 0.47
Batch: 520; loss: 1.74; acc: 0.39
Batch: 540; loss: 1.51; acc: 0.5
Batch: 560; loss: 1.57; acc: 0.45
Batch: 580; loss: 2.26; acc: 0.23
Batch: 600; loss: 1.55; acc: 0.44
Batch: 620; loss: 1.33; acc: 0.61
Batch: 640; loss: 1.51; acc: 0.39
Batch: 660; loss: 1.48; acc: 0.62
Batch: 680; loss: 1.64; acc: 0.41
Batch: 700; loss: 1.48; acc: 0.47
Batch: 720; loss: 1.41; acc: 0.47
Batch: 740; loss: 1.56; acc: 0.42
Batch: 760; loss: 1.49; acc: 0.52
Batch: 780; loss: 1.42; acc: 0.47
Train Epoch over. train_loss: 1.72; train_accuracy: 0.4 

Batch: 0; loss: 1.55; acc: 0.55
Batch: 20; loss: 1.95; acc: 0.41
Batch: 40; loss: 1.32; acc: 0.56
Batch: 60; loss: 1.34; acc: 0.5
Batch: 80; loss: 1.53; acc: 0.39
Batch: 100; loss: 1.51; acc: 0.5
Batch: 120; loss: 1.77; acc: 0.42
Batch: 140; loss: 1.46; acc: 0.45
Val Epoch over. val_loss: 1.6025552909085705; val_accuracy: 0.4498407643312102 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.53; acc: 0.53
Batch: 20; loss: 1.6; acc: 0.36
Batch: 40; loss: 1.86; acc: 0.34
Batch: 60; loss: 1.34; acc: 0.53
Batch: 80; loss: 1.36; acc: 0.55
Batch: 100; loss: 1.54; acc: 0.5
Batch: 120; loss: 1.61; acc: 0.45
Batch: 140; loss: 1.52; acc: 0.56
Batch: 160; loss: 1.55; acc: 0.41
Batch: 180; loss: 1.18; acc: 0.58
Batch: 200; loss: 1.61; acc: 0.53
Batch: 220; loss: 1.64; acc: 0.52
Batch: 240; loss: 1.45; acc: 0.48
Batch: 260; loss: 1.14; acc: 0.62
Batch: 280; loss: 1.45; acc: 0.48
Batch: 300; loss: 1.3; acc: 0.56
Batch: 320; loss: 1.3; acc: 0.61
Batch: 340; loss: 1.44; acc: 0.52
Batch: 360; loss: 1.24; acc: 0.52
Batch: 380; loss: 1.81; acc: 0.47
Batch: 400; loss: 1.1; acc: 0.64
Batch: 420; loss: 1.09; acc: 0.61
Batch: 440; loss: 1.5; acc: 0.39
Batch: 460; loss: 1.19; acc: 0.55
Batch: 480; loss: 1.1; acc: 0.66
Batch: 500; loss: 1.06; acc: 0.66
Batch: 520; loss: 1.45; acc: 0.52
Batch: 540; loss: 1.11; acc: 0.62
Batch: 560; loss: 1.36; acc: 0.52
Batch: 580; loss: 1.36; acc: 0.58
Batch: 600; loss: 1.09; acc: 0.59
Batch: 620; loss: 1.17; acc: 0.59
Batch: 640; loss: 1.07; acc: 0.61
Batch: 660; loss: 1.29; acc: 0.59
Batch: 680; loss: 0.91; acc: 0.72
Batch: 700; loss: 1.32; acc: 0.58
Batch: 720; loss: 1.11; acc: 0.56
Batch: 740; loss: 1.14; acc: 0.64
Batch: 760; loss: 1.13; acc: 0.61
Batch: 780; loss: 1.25; acc: 0.62
Train Epoch over. train_loss: 1.36; train_accuracy: 0.54 

Batch: 0; loss: 1.48; acc: 0.52
Batch: 20; loss: 2.13; acc: 0.38
Batch: 40; loss: 1.27; acc: 0.52
Batch: 60; loss: 1.54; acc: 0.53
Batch: 80; loss: 1.58; acc: 0.53
Batch: 100; loss: 1.56; acc: 0.48
Batch: 120; loss: 1.86; acc: 0.45
Batch: 140; loss: 1.46; acc: 0.55
Val Epoch over. val_loss: 1.5050346042699874; val_accuracy: 0.5142316878980892 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.83; acc: 0.48
Batch: 20; loss: 1.42; acc: 0.52
Batch: 40; loss: 1.13; acc: 0.56
Batch: 60; loss: 1.28; acc: 0.62
Batch: 80; loss: 1.06; acc: 0.66
Batch: 100; loss: 1.42; acc: 0.58
Batch: 120; loss: 1.33; acc: 0.62
Batch: 140; loss: 1.23; acc: 0.56
Batch: 160; loss: 1.1; acc: 0.64
Batch: 180; loss: 1.33; acc: 0.62
Batch: 200; loss: 1.16; acc: 0.64
Batch: 220; loss: 1.18; acc: 0.59
Batch: 240; loss: 1.29; acc: 0.56
Batch: 260; loss: 1.25; acc: 0.59
Batch: 280; loss: 1.0; acc: 0.64
Batch: 300; loss: 1.22; acc: 0.5
Batch: 320; loss: 1.26; acc: 0.59
Batch: 340; loss: 1.27; acc: 0.55
Batch: 360; loss: 1.26; acc: 0.67
Batch: 380; loss: 1.61; acc: 0.55
Batch: 400; loss: 1.64; acc: 0.5
Batch: 420; loss: 1.36; acc: 0.56
Batch: 440; loss: 1.16; acc: 0.56
Batch: 460; loss: 1.14; acc: 0.64
Batch: 480; loss: 1.13; acc: 0.62
Batch: 500; loss: 1.09; acc: 0.64
Batch: 520; loss: 1.47; acc: 0.55
Batch: 540; loss: 1.16; acc: 0.64
Batch: 560; loss: 1.45; acc: 0.55
Batch: 580; loss: 1.27; acc: 0.58
Batch: 600; loss: 0.92; acc: 0.73
Batch: 620; loss: 1.34; acc: 0.52
Batch: 640; loss: 1.06; acc: 0.62
Batch: 660; loss: 1.47; acc: 0.48
Batch: 680; loss: 1.51; acc: 0.48
Batch: 700; loss: 1.32; acc: 0.59
Batch: 720; loss: 1.78; acc: 0.48
Batch: 740; loss: 1.49; acc: 0.52
Batch: 760; loss: 1.35; acc: 0.53
Batch: 780; loss: 1.1; acc: 0.64
Train Epoch over. train_loss: 1.25; train_accuracy: 0.59 

Batch: 0; loss: 1.04; acc: 0.66
Batch: 20; loss: 1.21; acc: 0.7
Batch: 40; loss: 1.03; acc: 0.73
Batch: 60; loss: 1.14; acc: 0.66
Batch: 80; loss: 0.92; acc: 0.69
Batch: 100; loss: 1.19; acc: 0.56
Batch: 120; loss: 1.23; acc: 0.59
Batch: 140; loss: 0.98; acc: 0.62
Val Epoch over. val_loss: 1.212359096973565; val_accuracy: 0.6015127388535032 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.97; acc: 0.67
Batch: 20; loss: 1.46; acc: 0.52
Batch: 40; loss: 1.05; acc: 0.62
Batch: 60; loss: 1.4; acc: 0.52
Batch: 80; loss: 1.03; acc: 0.7
Batch: 100; loss: 1.22; acc: 0.52
Batch: 120; loss: 1.45; acc: 0.58
Batch: 140; loss: 1.39; acc: 0.52
Batch: 160; loss: 1.1; acc: 0.69
Batch: 180; loss: 1.14; acc: 0.62
Batch: 200; loss: 1.18; acc: 0.64
Batch: 220; loss: 1.55; acc: 0.53
Batch: 240; loss: 1.2; acc: 0.61
Batch: 260; loss: 1.04; acc: 0.66
Batch: 280; loss: 1.09; acc: 0.66
Batch: 300; loss: 1.32; acc: 0.53
Batch: 320; loss: 1.13; acc: 0.69
Batch: 340; loss: 0.87; acc: 0.73
Batch: 360; loss: 1.28; acc: 0.56
Batch: 380; loss: 1.22; acc: 0.58
Batch: 400; loss: 0.92; acc: 0.7
Batch: 420; loss: 1.07; acc: 0.73
Batch: 440; loss: 1.2; acc: 0.61
Batch: 460; loss: 1.25; acc: 0.66
Batch: 480; loss: 1.06; acc: 0.67
Batch: 500; loss: 1.18; acc: 0.59
Batch: 520; loss: 1.22; acc: 0.64
Batch: 540; loss: 1.1; acc: 0.59
Batch: 560; loss: 0.88; acc: 0.73
Batch: 580; loss: 1.23; acc: 0.58
Batch: 600; loss: 1.1; acc: 0.69
Batch: 620; loss: 1.25; acc: 0.55
Batch: 640; loss: 1.73; acc: 0.55
Batch: 660; loss: 1.09; acc: 0.59
Batch: 680; loss: 1.17; acc: 0.61
Batch: 700; loss: 1.19; acc: 0.59
Batch: 720; loss: 1.44; acc: 0.52
Batch: 740; loss: 0.93; acc: 0.66
Batch: 760; loss: 1.2; acc: 0.62
Batch: 780; loss: 0.92; acc: 0.73
Train Epoch over. train_loss: 1.21; train_accuracy: 0.61 

Batch: 0; loss: 1.03; acc: 0.72
Batch: 20; loss: 1.36; acc: 0.58
Batch: 40; loss: 0.99; acc: 0.66
Batch: 60; loss: 1.09; acc: 0.64
Batch: 80; loss: 0.96; acc: 0.72
Batch: 100; loss: 1.09; acc: 0.64
Batch: 120; loss: 1.12; acc: 0.64
Batch: 140; loss: 1.01; acc: 0.69
Val Epoch over. val_loss: 1.2786402208789898; val_accuracy: 0.5973328025477707 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 1.51; acc: 0.53
Batch: 20; loss: 0.95; acc: 0.67
Batch: 40; loss: 1.31; acc: 0.61
Batch: 60; loss: 1.08; acc: 0.62
Batch: 80; loss: 1.11; acc: 0.61
Batch: 100; loss: 1.06; acc: 0.64
Batch: 120; loss: 1.31; acc: 0.48
Batch: 140; loss: 1.09; acc: 0.64
Batch: 160; loss: 1.03; acc: 0.66
Batch: 180; loss: 1.32; acc: 0.56
Batch: 200; loss: 1.53; acc: 0.52
Batch: 220; loss: 1.39; acc: 0.47
Batch: 240; loss: 0.93; acc: 0.66
Batch: 260; loss: 1.01; acc: 0.72
Batch: 280; loss: 1.55; acc: 0.45
Batch: 300; loss: 1.05; acc: 0.69
Batch: 320; loss: 1.51; acc: 0.47
Batch: 340; loss: 1.0; acc: 0.59
Batch: 360; loss: 1.2; acc: 0.55
Batch: 380; loss: 1.35; acc: 0.61
Batch: 400; loss: 1.32; acc: 0.48
Batch: 420; loss: 1.17; acc: 0.62
Batch: 440; loss: 1.46; acc: 0.62
Batch: 460; loss: 1.34; acc: 0.58
Batch: 480; loss: 0.93; acc: 0.78
Batch: 500; loss: 1.66; acc: 0.52
Batch: 520; loss: 1.27; acc: 0.67
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 1.36; acc: 0.64
Batch: 580; loss: 1.21; acc: 0.58
Batch: 600; loss: 1.17; acc: 0.66
Batch: 620; loss: 1.11; acc: 0.69
Batch: 640; loss: 1.33; acc: 0.67
Batch: 660; loss: 1.26; acc: 0.58
Batch: 680; loss: 1.08; acc: 0.67
Batch: 700; loss: 1.36; acc: 0.64
Batch: 720; loss: 0.94; acc: 0.66
Batch: 740; loss: 1.08; acc: 0.66
Batch: 760; loss: 0.84; acc: 0.72
Batch: 780; loss: 0.95; acc: 0.66
Train Epoch over. train_loss: 1.16; train_accuracy: 0.62 

Batch: 0; loss: 1.17; acc: 0.67
Batch: 20; loss: 1.26; acc: 0.58
Batch: 40; loss: 0.98; acc: 0.69
Batch: 60; loss: 1.11; acc: 0.64
Batch: 80; loss: 0.86; acc: 0.77
Batch: 100; loss: 1.25; acc: 0.58
Batch: 120; loss: 1.38; acc: 0.55
Batch: 140; loss: 0.89; acc: 0.67
Val Epoch over. val_loss: 1.263326454694104; val_accuracy: 0.587281050955414 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.25; acc: 0.59
Batch: 20; loss: 1.24; acc: 0.66
Batch: 40; loss: 0.93; acc: 0.67
Batch: 60; loss: 1.4; acc: 0.55
Batch: 80; loss: 0.96; acc: 0.67
Batch: 100; loss: 1.38; acc: 0.56
Batch: 120; loss: 0.94; acc: 0.66
Batch: 140; loss: 0.99; acc: 0.69
Batch: 160; loss: 0.78; acc: 0.78
Batch: 180; loss: 1.41; acc: 0.52
Batch: 200; loss: 1.12; acc: 0.64
Batch: 220; loss: 1.21; acc: 0.55
Batch: 240; loss: 1.13; acc: 0.66
Batch: 260; loss: 1.05; acc: 0.66
Batch: 280; loss: 1.21; acc: 0.61
Batch: 300; loss: 1.08; acc: 0.67
Batch: 320; loss: 1.08; acc: 0.64
Batch: 340; loss: 0.99; acc: 0.67
Batch: 360; loss: 1.17; acc: 0.58
Batch: 380; loss: 1.04; acc: 0.69
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 0.92; acc: 0.66
Batch: 440; loss: 0.99; acc: 0.64
Batch: 460; loss: 1.21; acc: 0.62
Batch: 480; loss: 1.04; acc: 0.67
Batch: 500; loss: 1.12; acc: 0.66
Batch: 520; loss: 1.25; acc: 0.67
Batch: 540; loss: 1.25; acc: 0.69
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 1.05; acc: 0.66
Batch: 600; loss: 0.97; acc: 0.67
Batch: 620; loss: 1.11; acc: 0.67
Batch: 640; loss: 1.14; acc: 0.7
Batch: 660; loss: 1.09; acc: 0.66
Batch: 680; loss: 1.06; acc: 0.69
Batch: 700; loss: 1.14; acc: 0.67
Batch: 720; loss: 1.06; acc: 0.67
Batch: 740; loss: 1.1; acc: 0.72
Batch: 760; loss: 1.05; acc: 0.66
Batch: 780; loss: 1.24; acc: 0.59
Train Epoch over. train_loss: 1.14; train_accuracy: 0.63 

Batch: 0; loss: 1.73; acc: 0.48
Batch: 20; loss: 1.67; acc: 0.48
Batch: 40; loss: 1.44; acc: 0.52
Batch: 60; loss: 1.74; acc: 0.53
Batch: 80; loss: 1.7; acc: 0.45
Batch: 100; loss: 1.81; acc: 0.47
Batch: 120; loss: 1.86; acc: 0.42
Batch: 140; loss: 1.28; acc: 0.56
Val Epoch over. val_loss: 1.698355512254557; val_accuracy: 0.4596934713375796 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 1.49; acc: 0.47
Batch: 20; loss: 1.12; acc: 0.64
Batch: 40; loss: 1.35; acc: 0.53
Batch: 60; loss: 1.05; acc: 0.69
Batch: 80; loss: 1.1; acc: 0.62
Batch: 100; loss: 1.23; acc: 0.52
Batch: 120; loss: 1.35; acc: 0.55
Batch: 140; loss: 1.18; acc: 0.52
Batch: 160; loss: 0.78; acc: 0.7
Batch: 180; loss: 1.02; acc: 0.72
Batch: 200; loss: 0.95; acc: 0.66
Batch: 220; loss: 1.26; acc: 0.53
Batch: 240; loss: 1.16; acc: 0.52
Batch: 260; loss: 0.93; acc: 0.75
Batch: 280; loss: 0.88; acc: 0.66
Batch: 300; loss: 1.13; acc: 0.58
Batch: 320; loss: 1.44; acc: 0.56
Batch: 340; loss: 0.72; acc: 0.81
Batch: 360; loss: 1.11; acc: 0.67
Batch: 380; loss: 1.12; acc: 0.61
Batch: 400; loss: 1.41; acc: 0.55
Batch: 420; loss: 1.01; acc: 0.64
Batch: 440; loss: 1.27; acc: 0.56
Batch: 460; loss: 1.16; acc: 0.52
Batch: 480; loss: 1.35; acc: 0.58
Batch: 500; loss: 1.57; acc: 0.55
Batch: 520; loss: 1.3; acc: 0.55
Batch: 540; loss: 0.9; acc: 0.69
Batch: 560; loss: 1.19; acc: 0.62
Batch: 580; loss: 1.05; acc: 0.64
Batch: 600; loss: 1.33; acc: 0.55
Batch: 620; loss: 1.35; acc: 0.62
Batch: 640; loss: 1.97; acc: 0.42
Batch: 660; loss: 1.22; acc: 0.61
Batch: 680; loss: 1.01; acc: 0.64
Batch: 700; loss: 1.0; acc: 0.69
Batch: 720; loss: 1.09; acc: 0.62
Batch: 740; loss: 0.95; acc: 0.66
Batch: 760; loss: 1.11; acc: 0.59
Batch: 780; loss: 1.06; acc: 0.64
Train Epoch over. train_loss: 1.13; train_accuracy: 0.63 

Batch: 0; loss: 1.31; acc: 0.52
Batch: 20; loss: 1.45; acc: 0.62
Batch: 40; loss: 0.98; acc: 0.59
Batch: 60; loss: 1.26; acc: 0.53
Batch: 80; loss: 1.02; acc: 0.72
Batch: 100; loss: 1.36; acc: 0.52
Batch: 120; loss: 1.77; acc: 0.47
Batch: 140; loss: 0.88; acc: 0.72
Val Epoch over. val_loss: 1.2411134596083575; val_accuracy: 0.5832006369426752 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 1.41; acc: 0.47
Batch: 20; loss: 1.09; acc: 0.64
Batch: 40; loss: 1.12; acc: 0.66
Batch: 60; loss: 0.77; acc: 0.75
Batch: 80; loss: 0.81; acc: 0.75
Batch: 100; loss: 1.1; acc: 0.61
Batch: 120; loss: 1.19; acc: 0.59
Batch: 140; loss: 0.97; acc: 0.69
Batch: 160; loss: 1.22; acc: 0.53
Batch: 180; loss: 0.83; acc: 0.69
Batch: 200; loss: 0.85; acc: 0.67
Batch: 220; loss: 1.56; acc: 0.48
Batch: 240; loss: 0.9; acc: 0.7
Batch: 260; loss: 1.04; acc: 0.69
Batch: 280; loss: 0.91; acc: 0.67
Batch: 300; loss: 1.05; acc: 0.66
Batch: 320; loss: 0.98; acc: 0.73
Batch: 340; loss: 1.27; acc: 0.61
Batch: 360; loss: 1.09; acc: 0.59
Batch: 380; loss: 1.65; acc: 0.48
Batch: 400; loss: 1.29; acc: 0.66
Batch: 420; loss: 1.19; acc: 0.64
Batch: 440; loss: 2.17; acc: 0.48
Batch: 460; loss: 1.36; acc: 0.59
Batch: 480; loss: 1.56; acc: 0.44
Batch: 500; loss: 1.16; acc: 0.58
Batch: 520; loss: 1.07; acc: 0.62
Batch: 540; loss: 0.93; acc: 0.72
Batch: 560; loss: 1.54; acc: 0.5
Batch: 580; loss: 1.08; acc: 0.62
Batch: 600; loss: 0.99; acc: 0.64
Batch: 620; loss: 1.08; acc: 0.7
Batch: 640; loss: 0.97; acc: 0.67
Batch: 660; loss: 1.14; acc: 0.64
Batch: 680; loss: 1.04; acc: 0.56
Batch: 700; loss: 1.09; acc: 0.69
Batch: 720; loss: 1.31; acc: 0.53
Batch: 740; loss: 1.26; acc: 0.55
Batch: 760; loss: 1.23; acc: 0.45
Batch: 780; loss: 1.04; acc: 0.58
Train Epoch over. train_loss: 1.12; train_accuracy: 0.63 

Batch: 0; loss: 1.19; acc: 0.59
Batch: 20; loss: 1.19; acc: 0.61
Batch: 40; loss: 0.76; acc: 0.72
Batch: 60; loss: 1.05; acc: 0.61
Batch: 80; loss: 0.95; acc: 0.66
Batch: 100; loss: 1.08; acc: 0.59
Batch: 120; loss: 1.3; acc: 0.52
Batch: 140; loss: 0.83; acc: 0.77
Val Epoch over. val_loss: 1.08429631989473; val_accuracy: 0.6188296178343949 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.08; acc: 0.66
Batch: 20; loss: 0.92; acc: 0.73
Batch: 40; loss: 1.11; acc: 0.64
Batch: 60; loss: 0.89; acc: 0.72
Batch: 80; loss: 1.2; acc: 0.58
Batch: 100; loss: 0.86; acc: 0.69
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.96; acc: 0.72
Batch: 160; loss: 0.95; acc: 0.7
Batch: 180; loss: 0.93; acc: 0.66
Batch: 200; loss: 1.13; acc: 0.61
Batch: 220; loss: 0.94; acc: 0.72
Batch: 240; loss: 1.26; acc: 0.64
Batch: 260; loss: 0.83; acc: 0.75
Batch: 280; loss: 0.84; acc: 0.72
Batch: 300; loss: 0.81; acc: 0.66
Batch: 320; loss: 0.7; acc: 0.75
Batch: 340; loss: 0.81; acc: 0.8
Batch: 360; loss: 0.78; acc: 0.72
Batch: 380; loss: 0.89; acc: 0.72
Batch: 400; loss: 1.05; acc: 0.69
Batch: 420; loss: 1.21; acc: 0.62
Batch: 440; loss: 1.23; acc: 0.59
Batch: 460; loss: 0.83; acc: 0.7
Batch: 480; loss: 0.78; acc: 0.81
Batch: 500; loss: 1.11; acc: 0.69
Batch: 520; loss: 0.92; acc: 0.61
Batch: 540; loss: 0.91; acc: 0.73
Batch: 560; loss: 0.99; acc: 0.69
Batch: 580; loss: 1.09; acc: 0.67
Batch: 600; loss: 0.99; acc: 0.7
Batch: 620; loss: 0.89; acc: 0.75
Batch: 640; loss: 0.82; acc: 0.7
Batch: 660; loss: 1.02; acc: 0.72
Batch: 680; loss: 1.02; acc: 0.67
Batch: 700; loss: 1.01; acc: 0.69
Batch: 720; loss: 1.06; acc: 0.69
Batch: 740; loss: 0.97; acc: 0.7
Batch: 760; loss: 0.94; acc: 0.75
Batch: 780; loss: 1.25; acc: 0.61
Train Epoch over. train_loss: 0.95; train_accuracy: 0.69 

Batch: 0; loss: 1.08; acc: 0.64
Batch: 20; loss: 1.15; acc: 0.61
Batch: 40; loss: 0.72; acc: 0.83
Batch: 60; loss: 1.09; acc: 0.59
Batch: 80; loss: 0.81; acc: 0.75
Batch: 100; loss: 1.05; acc: 0.62
Batch: 120; loss: 1.42; acc: 0.61
Batch: 140; loss: 0.58; acc: 0.83
Val Epoch over. val_loss: 1.003167413792033; val_accuracy: 0.6779458598726115 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 1.29; acc: 0.59
Batch: 20; loss: 1.09; acc: 0.61
Batch: 40; loss: 1.01; acc: 0.67
Batch: 60; loss: 1.03; acc: 0.7
Batch: 80; loss: 0.91; acc: 0.67
Batch: 100; loss: 0.96; acc: 0.64
Batch: 120; loss: 1.13; acc: 0.64
Batch: 140; loss: 0.52; acc: 0.84
Batch: 160; loss: 1.15; acc: 0.64
Batch: 180; loss: 0.87; acc: 0.75
Batch: 200; loss: 0.71; acc: 0.72
Batch: 220; loss: 0.65; acc: 0.83
Batch: 240; loss: 1.0; acc: 0.59
Batch: 260; loss: 1.04; acc: 0.73
Batch: 280; loss: 0.83; acc: 0.7
Batch: 300; loss: 1.01; acc: 0.66
Batch: 320; loss: 1.02; acc: 0.67
Batch: 340; loss: 0.64; acc: 0.77
Batch: 360; loss: 1.11; acc: 0.64
Batch: 380; loss: 1.12; acc: 0.66
Batch: 400; loss: 0.98; acc: 0.67
Batch: 420; loss: 1.05; acc: 0.78
Batch: 440; loss: 0.86; acc: 0.73
Batch: 460; loss: 0.74; acc: 0.7
Batch: 480; loss: 0.86; acc: 0.69
Batch: 500; loss: 0.87; acc: 0.72
Batch: 520; loss: 0.88; acc: 0.69
Batch: 540; loss: 0.88; acc: 0.7
Batch: 560; loss: 1.13; acc: 0.67
Batch: 580; loss: 1.11; acc: 0.69
Batch: 600; loss: 0.66; acc: 0.78
Batch: 620; loss: 1.05; acc: 0.67
Batch: 640; loss: 0.93; acc: 0.67
Batch: 660; loss: 1.34; acc: 0.56
Batch: 680; loss: 1.02; acc: 0.69
Batch: 700; loss: 0.95; acc: 0.7
Batch: 720; loss: 0.93; acc: 0.64
Batch: 740; loss: 1.19; acc: 0.62
Batch: 760; loss: 1.15; acc: 0.55
Batch: 780; loss: 1.24; acc: 0.69
Train Epoch over. train_loss: 0.94; train_accuracy: 0.7 

Batch: 0; loss: 1.04; acc: 0.66
Batch: 20; loss: 1.23; acc: 0.64
Batch: 40; loss: 0.7; acc: 0.75
Batch: 60; loss: 1.06; acc: 0.58
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 1.16; acc: 0.58
Batch: 120; loss: 1.37; acc: 0.55
Batch: 140; loss: 0.68; acc: 0.75
Val Epoch over. val_loss: 1.000863459839183; val_accuracy: 0.6713773885350318 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 1.13; acc: 0.59
Batch: 20; loss: 0.91; acc: 0.72
Batch: 40; loss: 1.01; acc: 0.67
Batch: 60; loss: 0.8; acc: 0.78
Batch: 80; loss: 0.87; acc: 0.7
Batch: 100; loss: 0.91; acc: 0.7
Batch: 120; loss: 0.82; acc: 0.7
Batch: 140; loss: 0.72; acc: 0.78
Batch: 160; loss: 0.97; acc: 0.67
Batch: 180; loss: 1.13; acc: 0.55
Batch: 200; loss: 0.84; acc: 0.69
Batch: 220; loss: 1.08; acc: 0.67
Batch: 240; loss: 0.75; acc: 0.78
Batch: 260; loss: 0.86; acc: 0.81
Batch: 280; loss: 1.13; acc: 0.56
Batch: 300; loss: 0.71; acc: 0.81
Batch: 320; loss: 0.79; acc: 0.75
Batch: 340; loss: 1.0; acc: 0.66
Batch: 360; loss: 0.89; acc: 0.73
Batch: 380; loss: 1.05; acc: 0.64
Batch: 400; loss: 0.78; acc: 0.75
Batch: 420; loss: 0.92; acc: 0.75
Batch: 440; loss: 0.91; acc: 0.66
Batch: 460; loss: 0.71; acc: 0.8
Batch: 480; loss: 1.01; acc: 0.73
Batch: 500; loss: 1.02; acc: 0.61
Batch: 520; loss: 1.12; acc: 0.69
Batch: 540; loss: 0.95; acc: 0.7
Batch: 560; loss: 1.06; acc: 0.61
Batch: 580; loss: 0.86; acc: 0.7
Batch: 600; loss: 1.06; acc: 0.64
Batch: 620; loss: 1.05; acc: 0.61
Batch: 640; loss: 1.05; acc: 0.66
Batch: 660; loss: 1.2; acc: 0.66
Batch: 680; loss: 0.78; acc: 0.73
Batch: 700; loss: 0.79; acc: 0.78
Batch: 720; loss: 0.92; acc: 0.72
Batch: 740; loss: 0.76; acc: 0.75
Batch: 760; loss: 1.14; acc: 0.67
Batch: 780; loss: 1.0; acc: 0.72
Train Epoch over. train_loss: 0.94; train_accuracy: 0.7 

Batch: 0; loss: 1.07; acc: 0.59
Batch: 20; loss: 1.04; acc: 0.66
Batch: 40; loss: 0.66; acc: 0.78
Batch: 60; loss: 1.04; acc: 0.59
Batch: 80; loss: 0.85; acc: 0.69
Batch: 100; loss: 1.05; acc: 0.59
Batch: 120; loss: 1.36; acc: 0.53
Batch: 140; loss: 0.53; acc: 0.86
Val Epoch over. val_loss: 0.9781275590893569; val_accuracy: 0.6597332802547771 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 1.06; acc: 0.66
Batch: 20; loss: 1.03; acc: 0.67
Batch: 40; loss: 1.05; acc: 0.72
Batch: 60; loss: 0.8; acc: 0.72
Batch: 80; loss: 0.92; acc: 0.7
Batch: 100; loss: 1.01; acc: 0.7
Batch: 120; loss: 0.88; acc: 0.69
Batch: 140; loss: 0.92; acc: 0.67
Batch: 160; loss: 1.12; acc: 0.67
Batch: 180; loss: 0.88; acc: 0.72
Batch: 200; loss: 1.0; acc: 0.66
Batch: 220; loss: 0.76; acc: 0.8
Batch: 240; loss: 0.81; acc: 0.77
Batch: 260; loss: 1.01; acc: 0.7
Batch: 280; loss: 0.97; acc: 0.62
Batch: 300; loss: 0.68; acc: 0.8
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.86; acc: 0.69
Batch: 360; loss: 0.92; acc: 0.7
Batch: 380; loss: 0.9; acc: 0.77
Batch: 400; loss: 1.21; acc: 0.69
Batch: 420; loss: 0.79; acc: 0.75
Batch: 440; loss: 1.04; acc: 0.66
Batch: 460; loss: 0.92; acc: 0.7
Batch: 480; loss: 0.92; acc: 0.73
Batch: 500; loss: 0.89; acc: 0.73
Batch: 520; loss: 1.08; acc: 0.75
Batch: 540; loss: 1.0; acc: 0.66
Batch: 560; loss: 0.67; acc: 0.73
Batch: 580; loss: 1.15; acc: 0.66
Batch: 600; loss: 1.14; acc: 0.61
Batch: 620; loss: 0.69; acc: 0.78
Batch: 640; loss: 1.07; acc: 0.56
Batch: 660; loss: 1.02; acc: 0.67
Batch: 680; loss: 1.04; acc: 0.67
Batch: 700; loss: 0.91; acc: 0.72
Batch: 720; loss: 0.73; acc: 0.75
Batch: 740; loss: 0.77; acc: 0.72
Batch: 760; loss: 0.97; acc: 0.58
Batch: 780; loss: 0.95; acc: 0.64
Train Epoch over. train_loss: 0.94; train_accuracy: 0.7 

Batch: 0; loss: 1.25; acc: 0.66
Batch: 20; loss: 1.29; acc: 0.58
Batch: 40; loss: 0.82; acc: 0.73
Batch: 60; loss: 1.22; acc: 0.64
Batch: 80; loss: 1.31; acc: 0.62
Batch: 100; loss: 1.12; acc: 0.62
Batch: 120; loss: 1.8; acc: 0.5
Batch: 140; loss: 0.84; acc: 0.64
Val Epoch over. val_loss: 1.18941812188762; val_accuracy: 0.6178343949044586 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.73; acc: 0.73
Batch: 20; loss: 0.91; acc: 0.77
Batch: 40; loss: 0.9; acc: 0.66
Batch: 60; loss: 1.23; acc: 0.64
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.93; acc: 0.67
Batch: 120; loss: 1.01; acc: 0.69
Batch: 140; loss: 1.23; acc: 0.62
Batch: 160; loss: 1.15; acc: 0.61
Batch: 180; loss: 1.15; acc: 0.62
Batch: 200; loss: 0.96; acc: 0.73
Batch: 220; loss: 1.0; acc: 0.61
Batch: 240; loss: 0.87; acc: 0.75
Batch: 260; loss: 0.87; acc: 0.78
Batch: 280; loss: 0.99; acc: 0.67
Batch: 300; loss: 0.81; acc: 0.77
Batch: 320; loss: 1.05; acc: 0.61
Batch: 340; loss: 1.14; acc: 0.62
Batch: 360; loss: 1.15; acc: 0.58
Batch: 380; loss: 0.86; acc: 0.72
Batch: 400; loss: 0.96; acc: 0.78
Batch: 420; loss: 0.82; acc: 0.62
Batch: 440; loss: 1.09; acc: 0.69
Batch: 460; loss: 1.18; acc: 0.7
Batch: 480; loss: 0.83; acc: 0.7
Batch: 500; loss: 1.11; acc: 0.62
Batch: 520; loss: 0.97; acc: 0.59
Batch: 540; loss: 1.02; acc: 0.67
Batch: 560; loss: 1.17; acc: 0.62
Batch: 580; loss: 0.93; acc: 0.72
Batch: 600; loss: 1.16; acc: 0.64
Batch: 620; loss: 0.99; acc: 0.72
Batch: 640; loss: 1.32; acc: 0.61
Batch: 660; loss: 1.07; acc: 0.69
Batch: 680; loss: 0.75; acc: 0.75
Batch: 700; loss: 0.95; acc: 0.7
Batch: 720; loss: 0.92; acc: 0.72
Batch: 740; loss: 0.86; acc: 0.7
Batch: 760; loss: 0.88; acc: 0.7
Batch: 780; loss: 0.74; acc: 0.72
Train Epoch over. train_loss: 0.94; train_accuracy: 0.7 

Batch: 0; loss: 1.06; acc: 0.67
Batch: 20; loss: 0.99; acc: 0.64
Batch: 40; loss: 0.76; acc: 0.7
Batch: 60; loss: 1.13; acc: 0.64
Batch: 80; loss: 1.08; acc: 0.59
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.49; acc: 0.59
Batch: 140; loss: 0.82; acc: 0.78
Val Epoch over. val_loss: 1.0997284833033374; val_accuracy: 0.6365445859872612 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 1.18; acc: 0.66
Batch: 20; loss: 0.96; acc: 0.72
Batch: 40; loss: 0.96; acc: 0.66
Batch: 60; loss: 1.15; acc: 0.64
Batch: 80; loss: 1.01; acc: 0.7
Batch: 100; loss: 0.87; acc: 0.69
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.9; acc: 0.7
Batch: 160; loss: 0.97; acc: 0.73
Batch: 180; loss: 0.96; acc: 0.69
Batch: 200; loss: 0.8; acc: 0.7
Batch: 220; loss: 1.02; acc: 0.72
Batch: 240; loss: 0.84; acc: 0.75
Batch: 260; loss: 0.99; acc: 0.64
Batch: 280; loss: 0.99; acc: 0.67
Batch: 300; loss: 0.81; acc: 0.72
Batch: 320; loss: 0.9; acc: 0.67
Batch: 340; loss: 0.91; acc: 0.69
Batch: 360; loss: 0.81; acc: 0.73
Batch: 380; loss: 0.89; acc: 0.69
Batch: 400; loss: 1.17; acc: 0.59
Batch: 420; loss: 0.96; acc: 0.69
Batch: 440; loss: 0.75; acc: 0.8
Batch: 460; loss: 1.25; acc: 0.62
Batch: 480; loss: 0.93; acc: 0.64
Batch: 500; loss: 1.0; acc: 0.64
Batch: 520; loss: 0.99; acc: 0.67
Batch: 540; loss: 1.07; acc: 0.66
Batch: 560; loss: 0.91; acc: 0.75
Batch: 580; loss: 1.02; acc: 0.66
Batch: 600; loss: 0.73; acc: 0.75
Batch: 620; loss: 1.23; acc: 0.64
Batch: 640; loss: 1.2; acc: 0.62
Batch: 660; loss: 0.9; acc: 0.72
Batch: 680; loss: 0.98; acc: 0.66
Batch: 700; loss: 0.97; acc: 0.69
Batch: 720; loss: 0.78; acc: 0.73
Batch: 740; loss: 0.91; acc: 0.67
Batch: 760; loss: 0.81; acc: 0.73
Batch: 780; loss: 0.76; acc: 0.78
Train Epoch over. train_loss: 0.94; train_accuracy: 0.7 

Batch: 0; loss: 0.86; acc: 0.78
Batch: 20; loss: 1.05; acc: 0.67
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.88; acc: 0.67
Batch: 80; loss: 0.67; acc: 0.78
Batch: 100; loss: 0.93; acc: 0.72
Batch: 120; loss: 1.3; acc: 0.58
Batch: 140; loss: 0.65; acc: 0.78
Val Epoch over. val_loss: 0.8851082305999318; val_accuracy: 0.7208399681528662 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.84; acc: 0.8
Batch: 20; loss: 0.84; acc: 0.75
Batch: 40; loss: 0.93; acc: 0.75
Batch: 60; loss: 0.85; acc: 0.72
Batch: 80; loss: 1.17; acc: 0.7
Batch: 100; loss: 0.9; acc: 0.67
Batch: 120; loss: 1.0; acc: 0.66
Batch: 140; loss: 1.02; acc: 0.67
Batch: 160; loss: 0.9; acc: 0.67
Batch: 180; loss: 0.81; acc: 0.75
Batch: 200; loss: 1.15; acc: 0.66
Batch: 220; loss: 1.06; acc: 0.66
Batch: 240; loss: 0.98; acc: 0.66
Batch: 260; loss: 1.06; acc: 0.66
Batch: 280; loss: 1.09; acc: 0.7
Batch: 300; loss: 0.96; acc: 0.67
Batch: 320; loss: 1.03; acc: 0.58
Batch: 340; loss: 0.68; acc: 0.75
Batch: 360; loss: 1.1; acc: 0.59
Batch: 380; loss: 1.15; acc: 0.59
Batch: 400; loss: 1.11; acc: 0.66
Batch: 420; loss: 0.87; acc: 0.69
Batch: 440; loss: 0.86; acc: 0.75
Batch: 460; loss: 0.93; acc: 0.7
Batch: 480; loss: 1.06; acc: 0.69
Batch: 500; loss: 0.79; acc: 0.73
Batch: 520; loss: 1.23; acc: 0.64
Batch: 540; loss: 0.84; acc: 0.73
Batch: 560; loss: 0.84; acc: 0.69
Batch: 580; loss: 0.64; acc: 0.8
Batch: 600; loss: 0.99; acc: 0.7
Batch: 620; loss: 1.09; acc: 0.62
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 1.01; acc: 0.7
Batch: 680; loss: 0.92; acc: 0.7
Batch: 700; loss: 0.97; acc: 0.73
Batch: 720; loss: 0.96; acc: 0.69
Batch: 740; loss: 1.25; acc: 0.61
Batch: 760; loss: 1.03; acc: 0.64
Batch: 780; loss: 0.86; acc: 0.75
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 1.0; acc: 0.67
Batch: 40; loss: 0.68; acc: 0.77
Batch: 60; loss: 0.86; acc: 0.73
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.84; acc: 0.78
Batch: 120; loss: 1.34; acc: 0.56
Batch: 140; loss: 0.7; acc: 0.81
Val Epoch over. val_loss: 0.9009478602819382; val_accuracy: 0.7111863057324841 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.77; acc: 0.81
Batch: 20; loss: 1.01; acc: 0.61
Batch: 40; loss: 0.81; acc: 0.7
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.75; acc: 0.72
Batch: 100; loss: 0.8; acc: 0.78
Batch: 120; loss: 0.96; acc: 0.7
Batch: 140; loss: 1.01; acc: 0.69
Batch: 160; loss: 0.96; acc: 0.69
Batch: 180; loss: 0.82; acc: 0.77
Batch: 200; loss: 1.04; acc: 0.69
Batch: 220; loss: 0.96; acc: 0.67
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.67; acc: 0.83
Batch: 280; loss: 0.96; acc: 0.66
Batch: 300; loss: 1.03; acc: 0.64
Batch: 320; loss: 1.02; acc: 0.64
Batch: 340; loss: 0.79; acc: 0.78
Batch: 360; loss: 0.89; acc: 0.77
Batch: 380; loss: 0.77; acc: 0.75
Batch: 400; loss: 0.95; acc: 0.77
Batch: 420; loss: 0.98; acc: 0.69
Batch: 440; loss: 1.19; acc: 0.55
Batch: 460; loss: 0.76; acc: 0.72
Batch: 480; loss: 0.78; acc: 0.75
Batch: 500; loss: 0.87; acc: 0.72
Batch: 520; loss: 0.72; acc: 0.78
Batch: 540; loss: 0.71; acc: 0.75
Batch: 560; loss: 0.83; acc: 0.75
Batch: 580; loss: 1.15; acc: 0.67
Batch: 600; loss: 1.04; acc: 0.7
Batch: 620; loss: 1.19; acc: 0.59
Batch: 640; loss: 1.0; acc: 0.69
Batch: 660; loss: 0.85; acc: 0.69
Batch: 680; loss: 1.19; acc: 0.56
Batch: 700; loss: 0.66; acc: 0.7
Batch: 720; loss: 1.04; acc: 0.7
Batch: 740; loss: 0.79; acc: 0.75
Batch: 760; loss: 1.14; acc: 0.62
Batch: 780; loss: 1.02; acc: 0.66
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.9; acc: 0.77
Batch: 20; loss: 0.91; acc: 0.7
Batch: 40; loss: 0.6; acc: 0.8
Batch: 60; loss: 0.89; acc: 0.67
Batch: 80; loss: 0.72; acc: 0.75
Batch: 100; loss: 0.94; acc: 0.7
Batch: 120; loss: 1.52; acc: 0.48
Batch: 140; loss: 0.6; acc: 0.78
Val Epoch over. val_loss: 0.8873925841158363; val_accuracy: 0.7054140127388535 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.93; acc: 0.67
Batch: 20; loss: 1.14; acc: 0.72
Batch: 40; loss: 1.02; acc: 0.64
Batch: 60; loss: 1.07; acc: 0.61
Batch: 80; loss: 1.09; acc: 0.7
Batch: 100; loss: 0.92; acc: 0.7
Batch: 120; loss: 0.83; acc: 0.67
Batch: 140; loss: 0.89; acc: 0.75
Batch: 160; loss: 1.2; acc: 0.64
Batch: 180; loss: 0.78; acc: 0.75
Batch: 200; loss: 0.85; acc: 0.78
Batch: 220; loss: 1.4; acc: 0.58
Batch: 240; loss: 1.16; acc: 0.64
Batch: 260; loss: 1.0; acc: 0.73
Batch: 280; loss: 1.05; acc: 0.69
Batch: 300; loss: 0.92; acc: 0.7
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 1.19; acc: 0.72
Batch: 360; loss: 0.74; acc: 0.83
Batch: 380; loss: 0.88; acc: 0.7
Batch: 400; loss: 0.98; acc: 0.69
Batch: 420; loss: 0.9; acc: 0.69
Batch: 440; loss: 1.04; acc: 0.69
Batch: 460; loss: 0.96; acc: 0.67
Batch: 480; loss: 1.18; acc: 0.61
Batch: 500; loss: 1.0; acc: 0.62
Batch: 520; loss: 0.89; acc: 0.67
Batch: 540; loss: 1.14; acc: 0.64
Batch: 560; loss: 0.88; acc: 0.72
Batch: 580; loss: 0.93; acc: 0.7
Batch: 600; loss: 0.87; acc: 0.69
Batch: 620; loss: 0.79; acc: 0.8
Batch: 640; loss: 0.65; acc: 0.73
Batch: 660; loss: 0.9; acc: 0.69
Batch: 680; loss: 0.85; acc: 0.73
Batch: 700; loss: 1.08; acc: 0.66
Batch: 720; loss: 0.82; acc: 0.69
Batch: 740; loss: 0.97; acc: 0.73
Batch: 760; loss: 1.02; acc: 0.66
Batch: 780; loss: 1.34; acc: 0.67
Train Epoch over. train_loss: 0.93; train_accuracy: 0.7 

Batch: 0; loss: 0.91; acc: 0.73
Batch: 20; loss: 1.18; acc: 0.64
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.9; acc: 0.77
Batch: 80; loss: 0.73; acc: 0.78
Batch: 100; loss: 0.96; acc: 0.73
Batch: 120; loss: 1.65; acc: 0.5
Batch: 140; loss: 0.8; acc: 0.73
Val Epoch over. val_loss: 0.9652722079283113; val_accuracy: 0.696656050955414 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.69
Batch: 20; loss: 0.87; acc: 0.69
Batch: 40; loss: 1.01; acc: 0.66
Batch: 60; loss: 0.73; acc: 0.75
Batch: 80; loss: 1.05; acc: 0.67
Batch: 100; loss: 1.21; acc: 0.59
Batch: 120; loss: 0.96; acc: 0.62
Batch: 140; loss: 0.76; acc: 0.78
Batch: 160; loss: 1.02; acc: 0.67
Batch: 180; loss: 0.79; acc: 0.81
Batch: 200; loss: 0.87; acc: 0.77
Batch: 220; loss: 0.79; acc: 0.83
Batch: 240; loss: 0.62; acc: 0.78
Batch: 260; loss: 0.96; acc: 0.7
Batch: 280; loss: 0.95; acc: 0.64
Batch: 300; loss: 0.98; acc: 0.64
Batch: 320; loss: 1.08; acc: 0.66
Batch: 340; loss: 1.02; acc: 0.69
Batch: 360; loss: 1.14; acc: 0.66
Batch: 380; loss: 0.92; acc: 0.7
Batch: 400; loss: 0.85; acc: 0.66
Batch: 420; loss: 0.83; acc: 0.73
Batch: 440; loss: 1.23; acc: 0.62
Batch: 460; loss: 0.9; acc: 0.67
Batch: 480; loss: 1.12; acc: 0.64
Batch: 500; loss: 0.92; acc: 0.73
Batch: 520; loss: 0.79; acc: 0.69
Batch: 540; loss: 0.74; acc: 0.73
Batch: 560; loss: 1.0; acc: 0.7
Batch: 580; loss: 1.02; acc: 0.62
Batch: 600; loss: 0.86; acc: 0.7
Batch: 620; loss: 0.89; acc: 0.72
Batch: 640; loss: 0.75; acc: 0.66
Batch: 660; loss: 0.87; acc: 0.66
Batch: 680; loss: 1.01; acc: 0.59
Batch: 700; loss: 1.07; acc: 0.66
Batch: 720; loss: 1.22; acc: 0.62
Batch: 740; loss: 0.92; acc: 0.69
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 1.0; acc: 0.69
Train Epoch over. train_loss: 0.92; train_accuracy: 0.7 

Batch: 0; loss: 1.01; acc: 0.7
Batch: 20; loss: 1.05; acc: 0.64
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.92; acc: 0.7
Batch: 80; loss: 0.82; acc: 0.73
Batch: 100; loss: 0.95; acc: 0.7
Batch: 120; loss: 1.53; acc: 0.55
Batch: 140; loss: 0.68; acc: 0.78
Val Epoch over. val_loss: 0.9459442093873479; val_accuracy: 0.6900875796178344 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.98; acc: 0.69
Batch: 20; loss: 0.98; acc: 0.59
Batch: 40; loss: 1.04; acc: 0.7
Batch: 60; loss: 0.79; acc: 0.7
Batch: 80; loss: 0.95; acc: 0.7
Batch: 100; loss: 1.03; acc: 0.73
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.58; acc: 0.86
Batch: 160; loss: 1.03; acc: 0.67
Batch: 180; loss: 1.03; acc: 0.64
Batch: 200; loss: 0.98; acc: 0.73
Batch: 220; loss: 0.73; acc: 0.75
Batch: 240; loss: 0.74; acc: 0.72
Batch: 260; loss: 1.09; acc: 0.67
Batch: 280; loss: 0.7; acc: 0.77
Batch: 300; loss: 1.04; acc: 0.67
Batch: 320; loss: 1.02; acc: 0.67
Batch: 340; loss: 1.13; acc: 0.66
Batch: 360; loss: 0.85; acc: 0.69
Batch: 380; loss: 0.84; acc: 0.77
Batch: 400; loss: 0.57; acc: 0.83
Batch: 420; loss: 0.9; acc: 0.69
Batch: 440; loss: 0.93; acc: 0.72
Batch: 460; loss: 1.02; acc: 0.7
Batch: 480; loss: 0.72; acc: 0.8
Batch: 500; loss: 0.77; acc: 0.7
Batch: 520; loss: 0.88; acc: 0.73
Batch: 540; loss: 0.8; acc: 0.77
Batch: 560; loss: 0.88; acc: 0.72
Batch: 580; loss: 0.85; acc: 0.73
Batch: 600; loss: 1.11; acc: 0.66
Batch: 620; loss: 0.68; acc: 0.72
Batch: 640; loss: 1.14; acc: 0.66
Batch: 660; loss: 1.01; acc: 0.73
Batch: 680; loss: 0.97; acc: 0.67
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.85; acc: 0.7
Batch: 740; loss: 0.79; acc: 0.72
Batch: 760; loss: 0.82; acc: 0.69
Batch: 780; loss: 1.01; acc: 0.66
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.91; acc: 0.7
Batch: 20; loss: 1.11; acc: 0.66
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.95; acc: 0.7
Batch: 80; loss: 0.81; acc: 0.77
Batch: 100; loss: 0.94; acc: 0.7
Batch: 120; loss: 1.7; acc: 0.52
Batch: 140; loss: 0.68; acc: 0.77
Val Epoch over. val_loss: 0.9344783077953728; val_accuracy: 0.6951632165605095 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.38; acc: 0.58
Batch: 20; loss: 0.76; acc: 0.8
Batch: 40; loss: 0.66; acc: 0.78
Batch: 60; loss: 0.68; acc: 0.75
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.96; acc: 0.7
Batch: 120; loss: 0.91; acc: 0.73
Batch: 140; loss: 0.89; acc: 0.7
Batch: 160; loss: 0.96; acc: 0.66
Batch: 180; loss: 0.79; acc: 0.73
Batch: 200; loss: 0.71; acc: 0.8
Batch: 220; loss: 1.13; acc: 0.62
Batch: 240; loss: 0.76; acc: 0.75
Batch: 260; loss: 0.82; acc: 0.75
Batch: 280; loss: 0.85; acc: 0.66
Batch: 300; loss: 1.06; acc: 0.67
Batch: 320; loss: 0.97; acc: 0.67
Batch: 340; loss: 1.06; acc: 0.69
Batch: 360; loss: 1.0; acc: 0.72
Batch: 380; loss: 0.67; acc: 0.83
Batch: 400; loss: 0.93; acc: 0.73
Batch: 420; loss: 0.71; acc: 0.77
Batch: 440; loss: 0.67; acc: 0.78
Batch: 460; loss: 0.93; acc: 0.7
Batch: 480; loss: 1.06; acc: 0.67
Batch: 500; loss: 0.96; acc: 0.72
Batch: 520; loss: 0.78; acc: 0.75
Batch: 540; loss: 1.05; acc: 0.7
Batch: 560; loss: 0.97; acc: 0.69
Batch: 580; loss: 0.96; acc: 0.66
Batch: 600; loss: 0.83; acc: 0.8
Batch: 620; loss: 0.69; acc: 0.8
Batch: 640; loss: 0.8; acc: 0.77
Batch: 660; loss: 0.66; acc: 0.8
Batch: 680; loss: 0.83; acc: 0.73
Batch: 700; loss: 0.81; acc: 0.7
Batch: 720; loss: 0.8; acc: 0.77
Batch: 740; loss: 1.27; acc: 0.62
Batch: 760; loss: 0.94; acc: 0.73
Batch: 780; loss: 0.93; acc: 0.66
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 1.03; acc: 0.7
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.82; acc: 0.72
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.72
Batch: 120; loss: 1.51; acc: 0.55
Batch: 140; loss: 0.58; acc: 0.78
Val Epoch over. val_loss: 0.8524652034234089; val_accuracy: 0.7230294585987261 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.89; acc: 0.69
Batch: 20; loss: 1.17; acc: 0.72
Batch: 40; loss: 0.83; acc: 0.73
Batch: 60; loss: 0.9; acc: 0.69
Batch: 80; loss: 0.85; acc: 0.77
Batch: 100; loss: 0.92; acc: 0.67
Batch: 120; loss: 1.04; acc: 0.62
Batch: 140; loss: 0.97; acc: 0.58
Batch: 160; loss: 0.95; acc: 0.72
Batch: 180; loss: 0.62; acc: 0.81
Batch: 200; loss: 0.81; acc: 0.75
Batch: 220; loss: 1.1; acc: 0.64
Batch: 240; loss: 0.75; acc: 0.73
Batch: 260; loss: 1.12; acc: 0.61
Batch: 280; loss: 0.9; acc: 0.7
Batch: 300; loss: 0.86; acc: 0.75
Batch: 320; loss: 0.75; acc: 0.81
Batch: 340; loss: 0.7; acc: 0.81
Batch: 360; loss: 0.94; acc: 0.67
Batch: 380; loss: 1.05; acc: 0.67
Batch: 400; loss: 0.82; acc: 0.75
Batch: 420; loss: 0.86; acc: 0.72
Batch: 440; loss: 0.78; acc: 0.75
Batch: 460; loss: 0.72; acc: 0.77
Batch: 480; loss: 0.81; acc: 0.69
Batch: 500; loss: 0.77; acc: 0.75
Batch: 520; loss: 1.1; acc: 0.66
Batch: 540; loss: 0.88; acc: 0.73
Batch: 560; loss: 0.93; acc: 0.75
Batch: 580; loss: 0.98; acc: 0.7
Batch: 600; loss: 1.21; acc: 0.64
Batch: 620; loss: 0.75; acc: 0.73
Batch: 640; loss: 0.94; acc: 0.72
Batch: 660; loss: 0.94; acc: 0.64
Batch: 680; loss: 0.89; acc: 0.77
Batch: 700; loss: 0.84; acc: 0.72
Batch: 720; loss: 0.82; acc: 0.75
Batch: 740; loss: 0.83; acc: 0.73
Batch: 760; loss: 0.72; acc: 0.77
Batch: 780; loss: 1.0; acc: 0.7
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 0.98; acc: 0.7
Batch: 40; loss: 0.55; acc: 0.88
Batch: 60; loss: 0.84; acc: 0.73
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.91; acc: 0.73
Batch: 120; loss: 1.45; acc: 0.55
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.8417429717103387; val_accuracy: 0.7229299363057324 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.07; acc: 0.67
Batch: 20; loss: 0.88; acc: 0.83
Batch: 40; loss: 0.98; acc: 0.67
Batch: 60; loss: 0.77; acc: 0.7
Batch: 80; loss: 0.81; acc: 0.7
Batch: 100; loss: 1.08; acc: 0.62
Batch: 120; loss: 0.88; acc: 0.78
Batch: 140; loss: 0.75; acc: 0.73
Batch: 160; loss: 0.82; acc: 0.75
Batch: 180; loss: 1.16; acc: 0.62
Batch: 200; loss: 0.85; acc: 0.73
Batch: 220; loss: 0.77; acc: 0.73
Batch: 240; loss: 0.76; acc: 0.81
Batch: 260; loss: 0.84; acc: 0.77
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.81; acc: 0.7
Batch: 320; loss: 0.99; acc: 0.66
Batch: 340; loss: 0.92; acc: 0.73
Batch: 360; loss: 0.9; acc: 0.69
Batch: 380; loss: 0.66; acc: 0.81
Batch: 400; loss: 0.79; acc: 0.69
Batch: 420; loss: 0.75; acc: 0.7
Batch: 440; loss: 1.05; acc: 0.61
Batch: 460; loss: 0.89; acc: 0.75
Batch: 480; loss: 1.0; acc: 0.69
Batch: 500; loss: 0.77; acc: 0.72
Batch: 520; loss: 1.0; acc: 0.66
Batch: 540; loss: 1.19; acc: 0.62
Batch: 560; loss: 0.63; acc: 0.8
Batch: 580; loss: 0.89; acc: 0.73
Batch: 600; loss: 0.89; acc: 0.72
Batch: 620; loss: 0.64; acc: 0.88
Batch: 640; loss: 0.78; acc: 0.73
Batch: 660; loss: 0.98; acc: 0.67
Batch: 680; loss: 0.84; acc: 0.75
Batch: 700; loss: 0.96; acc: 0.73
Batch: 720; loss: 0.91; acc: 0.67
Batch: 740; loss: 1.11; acc: 0.7
Batch: 760; loss: 0.96; acc: 0.67
Batch: 780; loss: 0.67; acc: 0.78
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.86; acc: 0.77
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.56; acc: 0.88
Batch: 60; loss: 0.8; acc: 0.8
Batch: 80; loss: 0.73; acc: 0.75
Batch: 100; loss: 0.86; acc: 0.75
Batch: 120; loss: 1.46; acc: 0.53
Batch: 140; loss: 0.61; acc: 0.78
Val Epoch over. val_loss: 0.8350405719629519; val_accuracy: 0.7296974522292994 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.98; acc: 0.66
Batch: 20; loss: 0.96; acc: 0.73
Batch: 40; loss: 0.86; acc: 0.67
Batch: 60; loss: 0.77; acc: 0.77
Batch: 80; loss: 0.77; acc: 0.73
Batch: 100; loss: 0.87; acc: 0.7
Batch: 120; loss: 1.04; acc: 0.66
Batch: 140; loss: 0.92; acc: 0.66
Batch: 160; loss: 0.72; acc: 0.75
Batch: 180; loss: 0.83; acc: 0.77
Batch: 200; loss: 0.95; acc: 0.67
Batch: 220; loss: 0.78; acc: 0.7
Batch: 240; loss: 0.87; acc: 0.75
Batch: 260; loss: 0.59; acc: 0.78
Batch: 280; loss: 0.91; acc: 0.75
Batch: 300; loss: 0.88; acc: 0.7
Batch: 320; loss: 0.97; acc: 0.73
Batch: 340; loss: 0.75; acc: 0.77
Batch: 360; loss: 0.85; acc: 0.69
Batch: 380; loss: 0.82; acc: 0.75
Batch: 400; loss: 0.87; acc: 0.69
Batch: 420; loss: 0.68; acc: 0.78
Batch: 440; loss: 1.06; acc: 0.64
Batch: 460; loss: 0.99; acc: 0.66
Batch: 480; loss: 1.14; acc: 0.66
Batch: 500; loss: 0.76; acc: 0.73
Batch: 520; loss: 0.88; acc: 0.75
Batch: 540; loss: 1.32; acc: 0.59
Batch: 560; loss: 1.17; acc: 0.59
Batch: 580; loss: 0.8; acc: 0.75
Batch: 600; loss: 0.86; acc: 0.75
Batch: 620; loss: 1.01; acc: 0.67
Batch: 640; loss: 1.1; acc: 0.66
Batch: 660; loss: 0.95; acc: 0.64
Batch: 680; loss: 0.76; acc: 0.77
Batch: 700; loss: 0.93; acc: 0.7
Batch: 720; loss: 0.77; acc: 0.75
Batch: 740; loss: 0.89; acc: 0.69
Batch: 760; loss: 0.85; acc: 0.66
Batch: 780; loss: 0.72; acc: 0.77
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.87; acc: 0.72
Batch: 20; loss: 0.94; acc: 0.72
Batch: 40; loss: 0.6; acc: 0.81
Batch: 60; loss: 0.81; acc: 0.73
Batch: 80; loss: 0.7; acc: 0.77
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 1.43; acc: 0.52
Batch: 140; loss: 0.62; acc: 0.81
Val Epoch over. val_loss: 0.8242567711195369; val_accuracy: 0.7330812101910829 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.98; acc: 0.62
Batch: 40; loss: 0.65; acc: 0.83
Batch: 60; loss: 0.88; acc: 0.7
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 0.69; acc: 0.75
Batch: 120; loss: 0.65; acc: 0.77
Batch: 140; loss: 1.02; acc: 0.7
Batch: 160; loss: 0.91; acc: 0.73
Batch: 180; loss: 0.67; acc: 0.81
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.83; acc: 0.64
Batch: 240; loss: 1.0; acc: 0.62
Batch: 260; loss: 0.79; acc: 0.75
Batch: 280; loss: 0.84; acc: 0.72
Batch: 300; loss: 0.95; acc: 0.67
Batch: 320; loss: 1.02; acc: 0.67
Batch: 340; loss: 0.76; acc: 0.72
Batch: 360; loss: 0.8; acc: 0.73
Batch: 380; loss: 0.81; acc: 0.69
Batch: 400; loss: 0.56; acc: 0.83
Batch: 420; loss: 0.94; acc: 0.64
Batch: 440; loss: 1.19; acc: 0.67
Batch: 460; loss: 0.74; acc: 0.78
Batch: 480; loss: 0.95; acc: 0.7
Batch: 500; loss: 0.88; acc: 0.75
Batch: 520; loss: 1.15; acc: 0.7
Batch: 540; loss: 1.01; acc: 0.75
Batch: 560; loss: 0.71; acc: 0.75
Batch: 580; loss: 0.89; acc: 0.7
Batch: 600; loss: 0.84; acc: 0.73
Batch: 620; loss: 0.9; acc: 0.7
Batch: 640; loss: 1.01; acc: 0.61
Batch: 660; loss: 0.98; acc: 0.69
Batch: 680; loss: 0.91; acc: 0.78
Batch: 700; loss: 0.94; acc: 0.75
Batch: 720; loss: 1.03; acc: 0.67
Batch: 740; loss: 1.12; acc: 0.72
Batch: 760; loss: 1.15; acc: 0.64
Batch: 780; loss: 0.88; acc: 0.62
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 1.0; acc: 0.69
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.79; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.88; acc: 0.8
Batch: 120; loss: 1.49; acc: 0.52
Batch: 140; loss: 0.59; acc: 0.81
Val Epoch over. val_loss: 0.817128929932406; val_accuracy: 0.7372611464968153 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 1.19; acc: 0.7
Batch: 20; loss: 0.71; acc: 0.73
Batch: 40; loss: 1.11; acc: 0.66
Batch: 60; loss: 1.03; acc: 0.7
Batch: 80; loss: 0.94; acc: 0.7
Batch: 100; loss: 1.15; acc: 0.67
Batch: 120; loss: 0.9; acc: 0.66
Batch: 140; loss: 0.86; acc: 0.72
Batch: 160; loss: 1.0; acc: 0.75
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.85; acc: 0.72
Batch: 240; loss: 0.91; acc: 0.73
Batch: 260; loss: 0.96; acc: 0.72
Batch: 280; loss: 0.83; acc: 0.77
Batch: 300; loss: 1.14; acc: 0.58
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.93; acc: 0.69
Batch: 360; loss: 0.88; acc: 0.75
Batch: 380; loss: 1.3; acc: 0.56
Batch: 400; loss: 0.96; acc: 0.69
Batch: 420; loss: 0.82; acc: 0.73
Batch: 440; loss: 0.65; acc: 0.78
Batch: 460; loss: 0.58; acc: 0.8
Batch: 480; loss: 0.98; acc: 0.7
Batch: 500; loss: 0.84; acc: 0.73
Batch: 520; loss: 0.77; acc: 0.69
Batch: 540; loss: 1.01; acc: 0.73
Batch: 560; loss: 0.69; acc: 0.78
Batch: 580; loss: 1.19; acc: 0.67
Batch: 600; loss: 0.93; acc: 0.67
Batch: 620; loss: 0.92; acc: 0.75
Batch: 640; loss: 0.88; acc: 0.77
Batch: 660; loss: 0.74; acc: 0.75
Batch: 680; loss: 0.67; acc: 0.77
Batch: 700; loss: 0.54; acc: 0.84
Batch: 720; loss: 0.88; acc: 0.67
Batch: 740; loss: 0.99; acc: 0.72
Batch: 760; loss: 0.82; acc: 0.8
Batch: 780; loss: 0.77; acc: 0.75
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.92; acc: 0.69
Batch: 20; loss: 1.09; acc: 0.66
Batch: 40; loss: 0.58; acc: 0.86
Batch: 60; loss: 0.86; acc: 0.7
Batch: 80; loss: 0.76; acc: 0.75
Batch: 100; loss: 0.94; acc: 0.72
Batch: 120; loss: 1.59; acc: 0.55
Batch: 140; loss: 0.6; acc: 0.75
Val Epoch over. val_loss: 0.8656631011492127; val_accuracy: 0.7144705414012739 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.9; acc: 0.75
Batch: 20; loss: 0.93; acc: 0.69
Batch: 40; loss: 0.76; acc: 0.78
Batch: 60; loss: 0.91; acc: 0.73
Batch: 80; loss: 1.03; acc: 0.7
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.73; acc: 0.78
Batch: 160; loss: 0.73; acc: 0.78
Batch: 180; loss: 1.18; acc: 0.64
Batch: 200; loss: 0.98; acc: 0.77
Batch: 220; loss: 0.74; acc: 0.77
Batch: 240; loss: 0.71; acc: 0.81
Batch: 260; loss: 0.81; acc: 0.75
Batch: 280; loss: 0.7; acc: 0.78
Batch: 300; loss: 0.91; acc: 0.72
Batch: 320; loss: 0.7; acc: 0.77
Batch: 340; loss: 0.91; acc: 0.72
Batch: 360; loss: 1.05; acc: 0.64
Batch: 380; loss: 0.8; acc: 0.78
Batch: 400; loss: 0.96; acc: 0.69
Batch: 420; loss: 1.11; acc: 0.64
Batch: 440; loss: 0.69; acc: 0.73
Batch: 460; loss: 1.03; acc: 0.69
Batch: 480; loss: 0.72; acc: 0.72
Batch: 500; loss: 0.78; acc: 0.73
Batch: 520; loss: 0.77; acc: 0.73
Batch: 540; loss: 1.21; acc: 0.62
Batch: 560; loss: 1.0; acc: 0.73
Batch: 580; loss: 0.97; acc: 0.72
Batch: 600; loss: 0.89; acc: 0.78
Batch: 620; loss: 0.89; acc: 0.66
Batch: 640; loss: 0.96; acc: 0.69
Batch: 660; loss: 0.92; acc: 0.73
Batch: 680; loss: 0.7; acc: 0.72
Batch: 700; loss: 1.18; acc: 0.66
Batch: 720; loss: 0.76; acc: 0.75
Batch: 740; loss: 1.09; acc: 0.69
Batch: 760; loss: 0.7; acc: 0.83
Batch: 780; loss: 0.87; acc: 0.7
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.89; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.85; acc: 0.7
Batch: 80; loss: 0.75; acc: 0.75
Batch: 100; loss: 0.93; acc: 0.7
Batch: 120; loss: 1.44; acc: 0.52
Batch: 140; loss: 0.65; acc: 0.73
Val Epoch over. val_loss: 0.8550617288631998; val_accuracy: 0.714968152866242 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.94; acc: 0.73
Batch: 20; loss: 0.72; acc: 0.77
Batch: 40; loss: 0.86; acc: 0.8
Batch: 60; loss: 0.75; acc: 0.77
Batch: 80; loss: 0.98; acc: 0.7
Batch: 100; loss: 0.78; acc: 0.69
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.85; acc: 0.69
Batch: 160; loss: 0.83; acc: 0.7
Batch: 180; loss: 0.91; acc: 0.67
Batch: 200; loss: 1.14; acc: 0.67
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 1.04; acc: 0.7
Batch: 260; loss: 0.76; acc: 0.78
Batch: 280; loss: 0.72; acc: 0.78
Batch: 300; loss: 0.8; acc: 0.77
Batch: 320; loss: 0.9; acc: 0.69
Batch: 340; loss: 0.71; acc: 0.77
Batch: 360; loss: 1.06; acc: 0.61
Batch: 380; loss: 0.77; acc: 0.73
Batch: 400; loss: 0.94; acc: 0.73
Batch: 420; loss: 0.93; acc: 0.67
Batch: 440; loss: 0.86; acc: 0.72
Batch: 460; loss: 1.06; acc: 0.75
Batch: 480; loss: 0.97; acc: 0.69
Batch: 500; loss: 0.83; acc: 0.73
Batch: 520; loss: 0.64; acc: 0.77
Batch: 540; loss: 0.75; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.66
Batch: 580; loss: 0.93; acc: 0.77
Batch: 600; loss: 0.92; acc: 0.67
Batch: 620; loss: 1.21; acc: 0.61
Batch: 640; loss: 0.75; acc: 0.73
Batch: 660; loss: 0.97; acc: 0.69
Batch: 680; loss: 0.73; acc: 0.73
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.93; acc: 0.72
Batch: 740; loss: 0.83; acc: 0.7
Batch: 760; loss: 0.86; acc: 0.66
Batch: 780; loss: 0.67; acc: 0.83
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.73
Batch: 20; loss: 1.04; acc: 0.7
Batch: 40; loss: 0.63; acc: 0.78
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.69; acc: 0.83
Batch: 100; loss: 0.87; acc: 0.8
Batch: 120; loss: 1.45; acc: 0.56
Batch: 140; loss: 0.63; acc: 0.78
Val Epoch over. val_loss: 0.8402552310448543; val_accuracy: 0.7319864649681529 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 0.85; acc: 0.7
Batch: 40; loss: 1.02; acc: 0.66
Batch: 60; loss: 0.69; acc: 0.8
Batch: 80; loss: 1.02; acc: 0.69
Batch: 100; loss: 0.86; acc: 0.69
Batch: 120; loss: 0.96; acc: 0.67
Batch: 140; loss: 0.81; acc: 0.72
Batch: 160; loss: 0.85; acc: 0.72
Batch: 180; loss: 0.94; acc: 0.67
Batch: 200; loss: 1.21; acc: 0.66
Batch: 220; loss: 0.82; acc: 0.69
Batch: 240; loss: 1.22; acc: 0.66
Batch: 260; loss: 0.86; acc: 0.75
Batch: 280; loss: 0.85; acc: 0.72
Batch: 300; loss: 0.75; acc: 0.73
Batch: 320; loss: 0.68; acc: 0.78
Batch: 340; loss: 0.91; acc: 0.7
Batch: 360; loss: 0.78; acc: 0.69
Batch: 380; loss: 0.84; acc: 0.73
Batch: 400; loss: 1.06; acc: 0.66
Batch: 420; loss: 0.78; acc: 0.69
Batch: 440; loss: 0.94; acc: 0.64
Batch: 460; loss: 0.75; acc: 0.7
Batch: 480; loss: 1.02; acc: 0.61
Batch: 500; loss: 0.89; acc: 0.73
Batch: 520; loss: 0.89; acc: 0.72
Batch: 540; loss: 0.94; acc: 0.73
Batch: 560; loss: 0.8; acc: 0.72
Batch: 580; loss: 0.81; acc: 0.81
Batch: 600; loss: 0.84; acc: 0.75
Batch: 620; loss: 0.85; acc: 0.78
Batch: 640; loss: 1.14; acc: 0.66
Batch: 660; loss: 0.95; acc: 0.67
Batch: 680; loss: 0.77; acc: 0.77
Batch: 700; loss: 1.02; acc: 0.66
Batch: 720; loss: 1.01; acc: 0.67
Batch: 740; loss: 0.59; acc: 0.78
Batch: 760; loss: 0.89; acc: 0.67
Batch: 780; loss: 0.97; acc: 0.7
Train Epoch over. train_loss: 0.88; train_accuracy: 0.72 

Batch: 0; loss: 0.88; acc: 0.73
Batch: 20; loss: 1.02; acc: 0.67
Batch: 40; loss: 0.64; acc: 0.75
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.69; acc: 0.81
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 1.42; acc: 0.58
Batch: 140; loss: 0.63; acc: 0.81
Val Epoch over. val_loss: 0.8506286421399207; val_accuracy: 0.7269108280254777 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.67; acc: 0.8
Batch: 20; loss: 0.93; acc: 0.73
Batch: 40; loss: 1.01; acc: 0.66
Batch: 60; loss: 1.17; acc: 0.62
Batch: 80; loss: 0.84; acc: 0.7
Batch: 100; loss: 0.91; acc: 0.77
Batch: 120; loss: 0.87; acc: 0.73
Batch: 140; loss: 0.93; acc: 0.73
Batch: 160; loss: 1.04; acc: 0.8
Batch: 180; loss: 0.76; acc: 0.8
Batch: 200; loss: 0.73; acc: 0.8
Batch: 220; loss: 0.68; acc: 0.78
Batch: 240; loss: 0.82; acc: 0.83
Batch: 260; loss: 0.91; acc: 0.69
Batch: 280; loss: 1.09; acc: 0.61
Batch: 300; loss: 1.14; acc: 0.66
Batch: 320; loss: 0.88; acc: 0.7
Batch: 340; loss: 0.75; acc: 0.78
Batch: 360; loss: 1.1; acc: 0.58
Batch: 380; loss: 0.77; acc: 0.73
Batch: 400; loss: 0.94; acc: 0.75
Batch: 420; loss: 0.92; acc: 0.59
Batch: 440; loss: 1.04; acc: 0.67
Batch: 460; loss: 1.1; acc: 0.61
Batch: 480; loss: 0.71; acc: 0.81
Batch: 500; loss: 0.82; acc: 0.78
Batch: 520; loss: 0.8; acc: 0.77
Batch: 540; loss: 0.99; acc: 0.67
Batch: 560; loss: 1.0; acc: 0.69
Batch: 580; loss: 0.85; acc: 0.69
Batch: 600; loss: 0.87; acc: 0.66
Batch: 620; loss: 0.82; acc: 0.7
Batch: 640; loss: 0.87; acc: 0.73
Batch: 660; loss: 0.85; acc: 0.77
Batch: 680; loss: 0.8; acc: 0.75
Batch: 700; loss: 0.81; acc: 0.72
Batch: 720; loss: 0.63; acc: 0.8
Batch: 740; loss: 0.82; acc: 0.75
Batch: 760; loss: 1.01; acc: 0.67
Batch: 780; loss: 0.85; acc: 0.78
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.83; acc: 0.77
Batch: 20; loss: 1.0; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.83
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 1.41; acc: 0.55
Batch: 140; loss: 0.6; acc: 0.78
Val Epoch over. val_loss: 0.8194592422360827; val_accuracy: 0.7351711783439491 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.12; acc: 0.62
Batch: 20; loss: 0.92; acc: 0.64
Batch: 40; loss: 1.18; acc: 0.64
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.85; acc: 0.7
Batch: 120; loss: 0.88; acc: 0.7
Batch: 140; loss: 0.83; acc: 0.72
Batch: 160; loss: 0.95; acc: 0.72
Batch: 180; loss: 0.71; acc: 0.8
Batch: 200; loss: 1.06; acc: 0.67
Batch: 220; loss: 0.99; acc: 0.69
Batch: 240; loss: 0.72; acc: 0.73
Batch: 260; loss: 0.81; acc: 0.78
Batch: 280; loss: 0.95; acc: 0.62
Batch: 300; loss: 0.78; acc: 0.75
Batch: 320; loss: 0.73; acc: 0.73
Batch: 340; loss: 0.98; acc: 0.67
Batch: 360; loss: 0.79; acc: 0.77
Batch: 380; loss: 0.96; acc: 0.64
Batch: 400; loss: 0.81; acc: 0.78
Batch: 420; loss: 0.73; acc: 0.75
Batch: 440; loss: 0.88; acc: 0.66
Batch: 460; loss: 0.79; acc: 0.72
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.82; acc: 0.66
Batch: 520; loss: 1.08; acc: 0.66
Batch: 540; loss: 0.81; acc: 0.72
Batch: 560; loss: 0.85; acc: 0.7
Batch: 580; loss: 0.81; acc: 0.75
Batch: 600; loss: 0.93; acc: 0.67
Batch: 620; loss: 0.62; acc: 0.8
Batch: 640; loss: 1.08; acc: 0.64
Batch: 660; loss: 1.01; acc: 0.66
Batch: 680; loss: 0.69; acc: 0.75
Batch: 700; loss: 0.89; acc: 0.75
Batch: 720; loss: 0.99; acc: 0.69
Batch: 740; loss: 0.63; acc: 0.84
Batch: 760; loss: 0.81; acc: 0.72
Batch: 780; loss: 1.15; acc: 0.67
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.86; acc: 0.73
Batch: 20; loss: 1.01; acc: 0.69
Batch: 40; loss: 0.59; acc: 0.81
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.7; acc: 0.78
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 1.41; acc: 0.55
Batch: 140; loss: 0.62; acc: 0.8
Val Epoch over. val_loss: 0.8195172068040082; val_accuracy: 0.736265923566879 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.03; acc: 0.67
Batch: 20; loss: 0.88; acc: 0.75
Batch: 40; loss: 1.09; acc: 0.67
Batch: 60; loss: 0.77; acc: 0.67
Batch: 80; loss: 0.76; acc: 0.78
Batch: 100; loss: 1.0; acc: 0.67
Batch: 120; loss: 0.9; acc: 0.7
Batch: 140; loss: 1.17; acc: 0.61
Batch: 160; loss: 0.9; acc: 0.66
Batch: 180; loss: 0.69; acc: 0.75
Batch: 200; loss: 0.78; acc: 0.75
Batch: 220; loss: 0.85; acc: 0.72
Batch: 240; loss: 0.85; acc: 0.69
Batch: 260; loss: 0.89; acc: 0.7
Batch: 280; loss: 0.72; acc: 0.7
Batch: 300; loss: 0.77; acc: 0.72
Batch: 320; loss: 0.79; acc: 0.7
Batch: 340; loss: 0.62; acc: 0.78
Batch: 360; loss: 0.74; acc: 0.72
Batch: 380; loss: 0.62; acc: 0.78
Batch: 400; loss: 1.05; acc: 0.67
Batch: 420; loss: 0.79; acc: 0.73
Batch: 440; loss: 0.81; acc: 0.73
Batch: 460; loss: 0.9; acc: 0.69
Batch: 480; loss: 0.91; acc: 0.67
Batch: 500; loss: 0.65; acc: 0.8
Batch: 520; loss: 1.18; acc: 0.62
Batch: 540; loss: 0.94; acc: 0.75
Batch: 560; loss: 0.59; acc: 0.81
Batch: 580; loss: 0.99; acc: 0.69
Batch: 600; loss: 0.72; acc: 0.78
Batch: 620; loss: 0.8; acc: 0.75
Batch: 640; loss: 0.76; acc: 0.77
Batch: 660; loss: 1.08; acc: 0.7
Batch: 680; loss: 1.14; acc: 0.61
Batch: 700; loss: 0.91; acc: 0.7
Batch: 720; loss: 0.86; acc: 0.75
Batch: 740; loss: 0.89; acc: 0.7
Batch: 760; loss: 0.76; acc: 0.75
Batch: 780; loss: 1.16; acc: 0.7
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.98; acc: 0.73
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.78; acc: 0.78
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 1.44; acc: 0.53
Batch: 140; loss: 0.6; acc: 0.78
Val Epoch over. val_loss: 0.8092229840861764; val_accuracy: 0.7401472929936306 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.89; acc: 0.7
Batch: 20; loss: 0.84; acc: 0.75
Batch: 40; loss: 0.94; acc: 0.7
Batch: 60; loss: 0.98; acc: 0.75
Batch: 80; loss: 0.86; acc: 0.73
Batch: 100; loss: 0.58; acc: 0.78
Batch: 120; loss: 0.78; acc: 0.77
Batch: 140; loss: 0.72; acc: 0.75
Batch: 160; loss: 0.97; acc: 0.58
Batch: 180; loss: 0.88; acc: 0.67
Batch: 200; loss: 0.85; acc: 0.78
Batch: 220; loss: 0.76; acc: 0.8
Batch: 240; loss: 0.85; acc: 0.73
Batch: 260; loss: 1.03; acc: 0.73
Batch: 280; loss: 1.07; acc: 0.7
Batch: 300; loss: 0.9; acc: 0.73
Batch: 320; loss: 0.82; acc: 0.72
Batch: 340; loss: 1.23; acc: 0.66
Batch: 360; loss: 0.69; acc: 0.78
Batch: 380; loss: 0.99; acc: 0.66
Batch: 400; loss: 0.72; acc: 0.72
Batch: 420; loss: 0.67; acc: 0.75
Batch: 440; loss: 0.75; acc: 0.72
Batch: 460; loss: 0.85; acc: 0.7
Batch: 480; loss: 0.95; acc: 0.67
Batch: 500; loss: 0.95; acc: 0.75
Batch: 520; loss: 1.09; acc: 0.66
Batch: 540; loss: 0.82; acc: 0.8
Batch: 560; loss: 0.6; acc: 0.81
Batch: 580; loss: 0.94; acc: 0.7
Batch: 600; loss: 0.82; acc: 0.7
Batch: 620; loss: 0.82; acc: 0.73
Batch: 640; loss: 0.59; acc: 0.81
Batch: 660; loss: 0.82; acc: 0.7
Batch: 680; loss: 0.99; acc: 0.66
Batch: 700; loss: 1.19; acc: 0.58
Batch: 720; loss: 0.91; acc: 0.66
Batch: 740; loss: 0.59; acc: 0.84
Batch: 760; loss: 0.67; acc: 0.8
Batch: 780; loss: 0.96; acc: 0.67
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 1.0; acc: 0.72
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.67; acc: 0.8
Batch: 100; loss: 0.89; acc: 0.78
Batch: 120; loss: 1.43; acc: 0.55
Batch: 140; loss: 0.65; acc: 0.75
Val Epoch over. val_loss: 0.8237857575629167; val_accuracy: 0.7337778662420382 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.58; acc: 0.78
Batch: 20; loss: 0.6; acc: 0.78
Batch: 40; loss: 0.62; acc: 0.83
Batch: 60; loss: 1.16; acc: 0.69
Batch: 80; loss: 1.19; acc: 0.66
Batch: 100; loss: 0.94; acc: 0.72
Batch: 120; loss: 0.79; acc: 0.78
Batch: 140; loss: 0.85; acc: 0.73
Batch: 160; loss: 1.19; acc: 0.66
Batch: 180; loss: 1.13; acc: 0.62
Batch: 200; loss: 0.96; acc: 0.67
Batch: 220; loss: 1.11; acc: 0.66
Batch: 240; loss: 0.79; acc: 0.75
Batch: 260; loss: 0.69; acc: 0.81
Batch: 280; loss: 0.79; acc: 0.73
Batch: 300; loss: 1.06; acc: 0.69
Batch: 320; loss: 1.02; acc: 0.66
Batch: 340; loss: 0.78; acc: 0.75
Batch: 360; loss: 1.08; acc: 0.67
Batch: 380; loss: 0.65; acc: 0.81
Batch: 400; loss: 0.77; acc: 0.78
Batch: 420; loss: 0.75; acc: 0.69
Batch: 440; loss: 0.73; acc: 0.73
Batch: 460; loss: 0.97; acc: 0.72
Batch: 480; loss: 0.91; acc: 0.77
Batch: 500; loss: 0.9; acc: 0.66
Batch: 520; loss: 0.97; acc: 0.67
Batch: 540; loss: 1.0; acc: 0.66
Batch: 560; loss: 0.94; acc: 0.77
Batch: 580; loss: 0.92; acc: 0.72
Batch: 600; loss: 1.0; acc: 0.64
Batch: 620; loss: 0.9; acc: 0.67
Batch: 640; loss: 0.94; acc: 0.7
Batch: 660; loss: 0.79; acc: 0.73
Batch: 680; loss: 0.73; acc: 0.78
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.89; acc: 0.73
Batch: 740; loss: 0.95; acc: 0.7
Batch: 760; loss: 0.77; acc: 0.77
Batch: 780; loss: 1.19; acc: 0.59
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.77
Batch: 100; loss: 0.87; acc: 0.73
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.6; acc: 0.77
Val Epoch over. val_loss: 0.8134350209099472; val_accuracy: 0.7366640127388535 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 1.1; acc: 0.64
Batch: 20; loss: 0.92; acc: 0.72
Batch: 40; loss: 0.85; acc: 0.72
Batch: 60; loss: 0.63; acc: 0.77
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.91; acc: 0.64
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.93; acc: 0.8
Batch: 160; loss: 0.76; acc: 0.81
Batch: 180; loss: 0.66; acc: 0.77
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 0.73; acc: 0.75
Batch: 240; loss: 1.16; acc: 0.61
Batch: 260; loss: 0.91; acc: 0.78
Batch: 280; loss: 0.67; acc: 0.78
Batch: 300; loss: 1.07; acc: 0.7
Batch: 320; loss: 0.86; acc: 0.8
Batch: 340; loss: 0.75; acc: 0.77
Batch: 360; loss: 0.63; acc: 0.8
Batch: 380; loss: 1.09; acc: 0.67
Batch: 400; loss: 0.99; acc: 0.69
Batch: 420; loss: 0.73; acc: 0.77
Batch: 440; loss: 0.7; acc: 0.75
Batch: 460; loss: 0.76; acc: 0.77
Batch: 480; loss: 0.99; acc: 0.66
Batch: 500; loss: 0.92; acc: 0.75
Batch: 520; loss: 0.98; acc: 0.66
Batch: 540; loss: 0.88; acc: 0.78
Batch: 560; loss: 0.8; acc: 0.7
Batch: 580; loss: 1.06; acc: 0.69
Batch: 600; loss: 1.09; acc: 0.59
Batch: 620; loss: 0.88; acc: 0.73
Batch: 640; loss: 0.94; acc: 0.72
Batch: 660; loss: 0.83; acc: 0.77
Batch: 680; loss: 1.05; acc: 0.72
Batch: 700; loss: 0.89; acc: 0.7
Batch: 720; loss: 1.03; acc: 0.66
Batch: 740; loss: 0.67; acc: 0.75
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.85; acc: 0.72
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 1.02; acc: 0.69
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.79; acc: 0.78
Batch: 80; loss: 0.71; acc: 0.8
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 1.42; acc: 0.56
Batch: 140; loss: 0.6; acc: 0.78
Val Epoch over. val_loss: 0.8246789680924386; val_accuracy: 0.7337778662420382 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.77; acc: 0.75
Batch: 20; loss: 0.83; acc: 0.72
Batch: 40; loss: 0.82; acc: 0.72
Batch: 60; loss: 0.72; acc: 0.8
Batch: 80; loss: 1.04; acc: 0.73
Batch: 100; loss: 1.23; acc: 0.64
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.8; acc: 0.72
Batch: 160; loss: 0.93; acc: 0.75
Batch: 180; loss: 0.93; acc: 0.73
Batch: 200; loss: 0.78; acc: 0.78
Batch: 220; loss: 0.76; acc: 0.81
Batch: 240; loss: 0.86; acc: 0.69
Batch: 260; loss: 0.66; acc: 0.8
Batch: 280; loss: 0.87; acc: 0.77
Batch: 300; loss: 0.77; acc: 0.78
Batch: 320; loss: 0.89; acc: 0.75
Batch: 340; loss: 0.98; acc: 0.66
Batch: 360; loss: 0.91; acc: 0.77
Batch: 380; loss: 0.85; acc: 0.69
Batch: 400; loss: 1.03; acc: 0.72
Batch: 420; loss: 0.96; acc: 0.72
Batch: 440; loss: 0.94; acc: 0.72
Batch: 460; loss: 0.88; acc: 0.81
Batch: 480; loss: 0.7; acc: 0.72
Batch: 500; loss: 0.8; acc: 0.73
Batch: 520; loss: 1.07; acc: 0.67
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 1.05; acc: 0.66
Batch: 580; loss: 0.92; acc: 0.67
Batch: 600; loss: 1.01; acc: 0.66
Batch: 620; loss: 0.81; acc: 0.7
Batch: 640; loss: 0.73; acc: 0.77
Batch: 660; loss: 1.23; acc: 0.55
Batch: 680; loss: 1.12; acc: 0.64
Batch: 700; loss: 1.07; acc: 0.66
Batch: 720; loss: 0.99; acc: 0.62
Batch: 740; loss: 0.83; acc: 0.75
Batch: 760; loss: 0.74; acc: 0.73
Batch: 780; loss: 0.77; acc: 0.69
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.55; acc: 0.89
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 1.46; acc: 0.53
Batch: 140; loss: 0.61; acc: 0.77
Val Epoch over. val_loss: 0.8212921619415283; val_accuracy: 0.7333797770700637 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.95; acc: 0.72
Batch: 20; loss: 0.75; acc: 0.8
Batch: 40; loss: 0.84; acc: 0.77
Batch: 60; loss: 1.08; acc: 0.64
Batch: 80; loss: 1.24; acc: 0.62
Batch: 100; loss: 0.9; acc: 0.73
Batch: 120; loss: 1.03; acc: 0.62
Batch: 140; loss: 0.97; acc: 0.66
Batch: 160; loss: 0.88; acc: 0.69
Batch: 180; loss: 0.7; acc: 0.89
Batch: 200; loss: 0.98; acc: 0.66
Batch: 220; loss: 0.85; acc: 0.73
Batch: 240; loss: 1.02; acc: 0.69
Batch: 260; loss: 0.88; acc: 0.72
Batch: 280; loss: 0.98; acc: 0.78
Batch: 300; loss: 1.02; acc: 0.66
Batch: 320; loss: 0.96; acc: 0.62
Batch: 340; loss: 0.74; acc: 0.78
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.67; acc: 0.75
Batch: 400; loss: 0.74; acc: 0.72
Batch: 420; loss: 0.81; acc: 0.75
Batch: 440; loss: 0.78; acc: 0.69
Batch: 460; loss: 0.88; acc: 0.69
Batch: 480; loss: 1.05; acc: 0.7
Batch: 500; loss: 0.78; acc: 0.75
Batch: 520; loss: 0.86; acc: 0.7
Batch: 540; loss: 0.61; acc: 0.81
Batch: 560; loss: 0.76; acc: 0.75
Batch: 580; loss: 0.67; acc: 0.7
Batch: 600; loss: 0.84; acc: 0.67
Batch: 620; loss: 1.0; acc: 0.75
Batch: 640; loss: 0.82; acc: 0.75
Batch: 660; loss: 0.96; acc: 0.7
Batch: 680; loss: 1.02; acc: 0.73
Batch: 700; loss: 0.89; acc: 0.8
Batch: 720; loss: 0.97; acc: 0.72
Batch: 740; loss: 0.76; acc: 0.77
Batch: 760; loss: 1.02; acc: 0.61
Batch: 780; loss: 1.0; acc: 0.72
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.8; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.89; acc: 0.73
Batch: 120; loss: 1.42; acc: 0.55
Batch: 140; loss: 0.61; acc: 0.77
Val Epoch over. val_loss: 0.8230461641481728; val_accuracy: 0.7321855095541401 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.94; acc: 0.77
Batch: 20; loss: 1.0; acc: 0.58
Batch: 40; loss: 0.79; acc: 0.73
Batch: 60; loss: 0.68; acc: 0.78
Batch: 80; loss: 0.93; acc: 0.61
Batch: 100; loss: 0.83; acc: 0.73
Batch: 120; loss: 0.94; acc: 0.67
Batch: 140; loss: 0.74; acc: 0.73
Batch: 160; loss: 0.77; acc: 0.73
Batch: 180; loss: 1.1; acc: 0.66
Batch: 200; loss: 1.01; acc: 0.66
Batch: 220; loss: 0.72; acc: 0.8
Batch: 240; loss: 0.82; acc: 0.72
Batch: 260; loss: 0.93; acc: 0.73
Batch: 280; loss: 0.93; acc: 0.69
Batch: 300; loss: 1.02; acc: 0.67
Batch: 320; loss: 0.83; acc: 0.67
Batch: 340; loss: 0.91; acc: 0.72
Batch: 360; loss: 0.78; acc: 0.77
Batch: 380; loss: 0.9; acc: 0.69
Batch: 400; loss: 0.96; acc: 0.73
Batch: 420; loss: 0.79; acc: 0.67
Batch: 440; loss: 0.9; acc: 0.72
Batch: 460; loss: 1.29; acc: 0.53
Batch: 480; loss: 0.78; acc: 0.75
Batch: 500; loss: 1.02; acc: 0.66
Batch: 520; loss: 1.17; acc: 0.64
Batch: 540; loss: 0.95; acc: 0.7
Batch: 560; loss: 0.8; acc: 0.77
Batch: 580; loss: 0.94; acc: 0.69
Batch: 600; loss: 0.86; acc: 0.7
Batch: 620; loss: 0.84; acc: 0.8
Batch: 640; loss: 1.13; acc: 0.59
Batch: 660; loss: 0.8; acc: 0.72
Batch: 680; loss: 1.17; acc: 0.64
Batch: 700; loss: 0.95; acc: 0.69
Batch: 720; loss: 1.01; acc: 0.77
Batch: 740; loss: 0.84; acc: 0.73
Batch: 760; loss: 0.74; acc: 0.73
Batch: 780; loss: 0.9; acc: 0.66
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.86; acc: 0.75
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 0.59; acc: 0.8
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.88; acc: 0.78
Batch: 120; loss: 1.42; acc: 0.56
Batch: 140; loss: 0.64; acc: 0.78
Val Epoch over. val_loss: 0.8256412679982034; val_accuracy: 0.7319864649681529 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.78; acc: 0.69
Batch: 20; loss: 1.0; acc: 0.67
Batch: 40; loss: 0.72; acc: 0.78
Batch: 60; loss: 0.97; acc: 0.73
Batch: 80; loss: 0.94; acc: 0.73
Batch: 100; loss: 0.7; acc: 0.73
Batch: 120; loss: 0.88; acc: 0.75
Batch: 140; loss: 0.61; acc: 0.75
Batch: 160; loss: 0.87; acc: 0.78
Batch: 180; loss: 0.86; acc: 0.73
Batch: 200; loss: 0.99; acc: 0.69
Batch: 220; loss: 1.28; acc: 0.66
Batch: 240; loss: 0.92; acc: 0.64
Batch: 260; loss: 0.85; acc: 0.77
Batch: 280; loss: 0.85; acc: 0.69
Batch: 300; loss: 0.81; acc: 0.78
Batch: 320; loss: 1.21; acc: 0.61
Batch: 340; loss: 0.85; acc: 0.73
Batch: 360; loss: 0.75; acc: 0.77
Batch: 380; loss: 0.85; acc: 0.75
Batch: 400; loss: 0.84; acc: 0.73
Batch: 420; loss: 0.87; acc: 0.75
Batch: 440; loss: 0.61; acc: 0.8
Batch: 460; loss: 0.96; acc: 0.67
Batch: 480; loss: 0.97; acc: 0.66
Batch: 500; loss: 0.66; acc: 0.77
Batch: 520; loss: 0.71; acc: 0.8
Batch: 540; loss: 0.98; acc: 0.64
Batch: 560; loss: 0.68; acc: 0.8
Batch: 580; loss: 0.8; acc: 0.78
Batch: 600; loss: 0.94; acc: 0.72
Batch: 620; loss: 0.77; acc: 0.7
Batch: 640; loss: 1.16; acc: 0.67
Batch: 660; loss: 0.79; acc: 0.72
Batch: 680; loss: 0.74; acc: 0.7
Batch: 700; loss: 0.86; acc: 0.75
Batch: 720; loss: 0.57; acc: 0.77
Batch: 740; loss: 1.13; acc: 0.67
Batch: 760; loss: 0.95; acc: 0.62
Batch: 780; loss: 0.74; acc: 0.72
Train Epoch over. train_loss: 0.87; train_accuracy: 0.72 

Batch: 0; loss: 0.89; acc: 0.73
Batch: 20; loss: 0.97; acc: 0.7
Batch: 40; loss: 0.57; acc: 0.86
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.7; acc: 0.8
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 1.5; acc: 0.52
Batch: 140; loss: 0.61; acc: 0.8
Val Epoch over. val_loss: 0.816739627130472; val_accuracy: 0.7347730891719745 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.04; acc: 0.72
Batch: 20; loss: 0.89; acc: 0.67
Batch: 40; loss: 1.32; acc: 0.69
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.7; acc: 0.77
Batch: 100; loss: 0.9; acc: 0.66
Batch: 120; loss: 0.86; acc: 0.75
Batch: 140; loss: 0.76; acc: 0.75
Batch: 160; loss: 0.74; acc: 0.73
Batch: 180; loss: 0.75; acc: 0.81
Batch: 200; loss: 0.89; acc: 0.62
Batch: 220; loss: 0.99; acc: 0.7
Batch: 240; loss: 0.87; acc: 0.7
Batch: 260; loss: 0.69; acc: 0.77
Batch: 280; loss: 0.99; acc: 0.7
Batch: 300; loss: 0.75; acc: 0.8
Batch: 320; loss: 1.01; acc: 0.64
Batch: 340; loss: 0.65; acc: 0.75
Batch: 360; loss: 1.0; acc: 0.67
Batch: 380; loss: 0.61; acc: 0.78
Batch: 400; loss: 0.66; acc: 0.83
Batch: 420; loss: 0.65; acc: 0.81
Batch: 440; loss: 0.73; acc: 0.75
Batch: 460; loss: 0.85; acc: 0.73
Batch: 480; loss: 1.03; acc: 0.64
Batch: 500; loss: 1.01; acc: 0.67
Batch: 520; loss: 0.79; acc: 0.8
Batch: 540; loss: 0.96; acc: 0.64
Batch: 560; loss: 0.91; acc: 0.67
Batch: 580; loss: 0.96; acc: 0.67
Batch: 600; loss: 1.09; acc: 0.7
Batch: 620; loss: 0.7; acc: 0.78
Batch: 640; loss: 0.75; acc: 0.73
Batch: 660; loss: 0.71; acc: 0.75
Batch: 680; loss: 0.81; acc: 0.73
Batch: 700; loss: 0.83; acc: 0.75
Batch: 720; loss: 1.08; acc: 0.62
Batch: 740; loss: 0.87; acc: 0.77
Batch: 760; loss: 0.98; acc: 0.72
Batch: 780; loss: 0.71; acc: 0.77
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.83; acc: 0.77
Batch: 20; loss: 1.01; acc: 0.72
Batch: 40; loss: 0.55; acc: 0.86
Batch: 60; loss: 0.78; acc: 0.8
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.86; acc: 0.75
Batch: 120; loss: 1.47; acc: 0.56
Batch: 140; loss: 0.62; acc: 0.75
Val Epoch over. val_loss: 0.8145721279511786; val_accuracy: 0.7351711783439491 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.9; acc: 0.69
Batch: 20; loss: 0.77; acc: 0.78
Batch: 40; loss: 1.04; acc: 0.67
Batch: 60; loss: 0.74; acc: 0.75
Batch: 80; loss: 0.93; acc: 0.72
Batch: 100; loss: 0.66; acc: 0.81
Batch: 120; loss: 0.83; acc: 0.8
Batch: 140; loss: 1.23; acc: 0.66
Batch: 160; loss: 0.8; acc: 0.73
Batch: 180; loss: 0.79; acc: 0.73
Batch: 200; loss: 1.13; acc: 0.62
Batch: 220; loss: 0.85; acc: 0.69
Batch: 240; loss: 0.68; acc: 0.84
Batch: 260; loss: 0.71; acc: 0.69
Batch: 280; loss: 0.93; acc: 0.62
Batch: 300; loss: 0.68; acc: 0.78
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.77; acc: 0.73
Batch: 360; loss: 0.61; acc: 0.83
Batch: 380; loss: 0.9; acc: 0.7
Batch: 400; loss: 0.89; acc: 0.7
Batch: 420; loss: 0.78; acc: 0.78
Batch: 440; loss: 0.99; acc: 0.69
Batch: 460; loss: 0.99; acc: 0.69
Batch: 480; loss: 0.91; acc: 0.67
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 0.9; acc: 0.66
Batch: 540; loss: 1.02; acc: 0.66
Batch: 560; loss: 0.86; acc: 0.73
Batch: 580; loss: 0.8; acc: 0.78
Batch: 600; loss: 0.96; acc: 0.72
Batch: 620; loss: 0.86; acc: 0.72
Batch: 640; loss: 1.08; acc: 0.67
Batch: 660; loss: 0.64; acc: 0.8
Batch: 680; loss: 0.98; acc: 0.72
Batch: 700; loss: 0.77; acc: 0.72
Batch: 720; loss: 0.89; acc: 0.7
Batch: 740; loss: 1.36; acc: 0.61
Batch: 760; loss: 0.82; acc: 0.66
Batch: 780; loss: 0.95; acc: 0.7
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.57; acc: 0.83
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 1.43; acc: 0.56
Batch: 140; loss: 0.61; acc: 0.78
Val Epoch over. val_loss: 0.8088443899989888; val_accuracy: 0.7376592356687898 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.12; acc: 0.66
Batch: 20; loss: 0.92; acc: 0.67
Batch: 40; loss: 0.72; acc: 0.77
Batch: 60; loss: 1.0; acc: 0.72
Batch: 80; loss: 0.82; acc: 0.69
Batch: 100; loss: 0.84; acc: 0.75
Batch: 120; loss: 0.96; acc: 0.72
Batch: 140; loss: 0.89; acc: 0.73
Batch: 160; loss: 0.76; acc: 0.72
Batch: 180; loss: 0.9; acc: 0.7
Batch: 200; loss: 0.69; acc: 0.81
Batch: 220; loss: 0.61; acc: 0.8
Batch: 240; loss: 0.74; acc: 0.77
Batch: 260; loss: 0.75; acc: 0.81
Batch: 280; loss: 0.93; acc: 0.69
Batch: 300; loss: 0.9; acc: 0.67
Batch: 320; loss: 0.84; acc: 0.75
Batch: 340; loss: 0.83; acc: 0.7
Batch: 360; loss: 1.19; acc: 0.64
Batch: 380; loss: 0.94; acc: 0.66
Batch: 400; loss: 0.71; acc: 0.77
Batch: 420; loss: 1.03; acc: 0.69
Batch: 440; loss: 0.93; acc: 0.69
Batch: 460; loss: 0.91; acc: 0.69
Batch: 480; loss: 1.01; acc: 0.67
Batch: 500; loss: 0.72; acc: 0.78
Batch: 520; loss: 0.64; acc: 0.75
Batch: 540; loss: 1.04; acc: 0.67
Batch: 560; loss: 0.75; acc: 0.83
Batch: 580; loss: 0.76; acc: 0.73
Batch: 600; loss: 1.15; acc: 0.67
Batch: 620; loss: 1.1; acc: 0.64
Batch: 640; loss: 0.7; acc: 0.78
Batch: 660; loss: 0.87; acc: 0.69
Batch: 680; loss: 0.68; acc: 0.8
Batch: 700; loss: 0.81; acc: 0.7
Batch: 720; loss: 0.85; acc: 0.72
Batch: 740; loss: 0.8; acc: 0.67
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.84; acc: 0.72
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 0.62; acc: 0.73
Val Epoch over. val_loss: 0.8083334901150624; val_accuracy: 0.7385549363057324 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 0.94; acc: 0.66
Batch: 40; loss: 0.87; acc: 0.67
Batch: 60; loss: 0.9; acc: 0.73
Batch: 80; loss: 1.06; acc: 0.67
Batch: 100; loss: 0.76; acc: 0.73
Batch: 120; loss: 0.86; acc: 0.7
Batch: 140; loss: 0.78; acc: 0.72
Batch: 160; loss: 1.12; acc: 0.64
Batch: 180; loss: 0.79; acc: 0.75
Batch: 200; loss: 0.75; acc: 0.78
Batch: 220; loss: 1.18; acc: 0.62
Batch: 240; loss: 0.51; acc: 0.83
Batch: 260; loss: 0.7; acc: 0.81
Batch: 280; loss: 0.67; acc: 0.81
Batch: 300; loss: 1.06; acc: 0.69
Batch: 320; loss: 0.83; acc: 0.77
Batch: 340; loss: 0.81; acc: 0.78
Batch: 360; loss: 1.04; acc: 0.66
Batch: 380; loss: 1.0; acc: 0.69
Batch: 400; loss: 0.72; acc: 0.75
Batch: 420; loss: 0.85; acc: 0.73
Batch: 440; loss: 0.97; acc: 0.64
Batch: 460; loss: 0.99; acc: 0.72
Batch: 480; loss: 0.93; acc: 0.8
Batch: 500; loss: 0.73; acc: 0.73
Batch: 520; loss: 0.94; acc: 0.69
Batch: 540; loss: 0.92; acc: 0.7
Batch: 560; loss: 1.13; acc: 0.62
Batch: 580; loss: 1.04; acc: 0.67
Batch: 600; loss: 0.74; acc: 0.75
Batch: 620; loss: 0.76; acc: 0.72
Batch: 640; loss: 1.11; acc: 0.62
Batch: 660; loss: 0.78; acc: 0.69
Batch: 680; loss: 1.03; acc: 0.64
Batch: 700; loss: 0.76; acc: 0.75
Batch: 720; loss: 0.85; acc: 0.7
Batch: 740; loss: 0.73; acc: 0.8
Batch: 760; loss: 0.75; acc: 0.8
Batch: 780; loss: 0.7; acc: 0.78
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.57; acc: 0.84
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 1.46; acc: 0.55
Batch: 140; loss: 0.62; acc: 0.78
Val Epoch over. val_loss: 0.8107636113455341; val_accuracy: 0.7382563694267515 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.15; acc: 0.64
Batch: 20; loss: 0.74; acc: 0.78
Batch: 40; loss: 0.97; acc: 0.75
Batch: 60; loss: 0.66; acc: 0.75
Batch: 80; loss: 0.76; acc: 0.77
Batch: 100; loss: 0.77; acc: 0.72
Batch: 120; loss: 0.83; acc: 0.81
Batch: 140; loss: 0.76; acc: 0.72
Batch: 160; loss: 0.9; acc: 0.73
Batch: 180; loss: 1.18; acc: 0.69
Batch: 200; loss: 0.69; acc: 0.77
Batch: 220; loss: 1.05; acc: 0.59
Batch: 240; loss: 0.75; acc: 0.77
Batch: 260; loss: 1.13; acc: 0.62
Batch: 280; loss: 0.99; acc: 0.66
Batch: 300; loss: 0.81; acc: 0.73
Batch: 320; loss: 0.86; acc: 0.73
Batch: 340; loss: 1.06; acc: 0.61
Batch: 360; loss: 0.89; acc: 0.75
Batch: 380; loss: 0.91; acc: 0.72
Batch: 400; loss: 0.75; acc: 0.77
Batch: 420; loss: 0.87; acc: 0.7
Batch: 440; loss: 0.98; acc: 0.73
Batch: 460; loss: 0.99; acc: 0.73
Batch: 480; loss: 0.9; acc: 0.7
Batch: 500; loss: 0.87; acc: 0.67
Batch: 520; loss: 0.9; acc: 0.73
Batch: 540; loss: 0.84; acc: 0.7
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 1.07; acc: 0.69
Batch: 600; loss: 0.76; acc: 0.77
Batch: 620; loss: 0.84; acc: 0.73
Batch: 640; loss: 1.06; acc: 0.66
Batch: 660; loss: 0.86; acc: 0.69
Batch: 680; loss: 0.65; acc: 0.81
Batch: 700; loss: 0.93; acc: 0.69
Batch: 720; loss: 0.72; acc: 0.78
Batch: 740; loss: 0.82; acc: 0.72
Batch: 760; loss: 1.09; acc: 0.61
Batch: 780; loss: 1.05; acc: 0.69
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.78; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 1.46; acc: 0.53
Batch: 140; loss: 0.61; acc: 0.78
Val Epoch over. val_loss: 0.8094405979867194; val_accuracy: 0.7358678343949044 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.96; acc: 0.67
Batch: 20; loss: 0.57; acc: 0.89
Batch: 40; loss: 0.94; acc: 0.72
Batch: 60; loss: 0.78; acc: 0.78
Batch: 80; loss: 1.1; acc: 0.64
Batch: 100; loss: 1.05; acc: 0.61
Batch: 120; loss: 0.96; acc: 0.7
Batch: 140; loss: 0.71; acc: 0.75
Batch: 160; loss: 0.54; acc: 0.86
Batch: 180; loss: 0.96; acc: 0.67
Batch: 200; loss: 0.99; acc: 0.72
Batch: 220; loss: 0.84; acc: 0.7
Batch: 240; loss: 0.72; acc: 0.75
Batch: 260; loss: 0.98; acc: 0.66
Batch: 280; loss: 0.62; acc: 0.78
Batch: 300; loss: 0.61; acc: 0.8
Batch: 320; loss: 1.01; acc: 0.72
Batch: 340; loss: 0.87; acc: 0.72
Batch: 360; loss: 0.98; acc: 0.66
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.65; acc: 0.84
Batch: 420; loss: 0.78; acc: 0.77
Batch: 440; loss: 0.83; acc: 0.72
Batch: 460; loss: 0.73; acc: 0.73
Batch: 480; loss: 0.85; acc: 0.77
Batch: 500; loss: 0.69; acc: 0.81
Batch: 520; loss: 0.84; acc: 0.69
Batch: 540; loss: 1.09; acc: 0.72
Batch: 560; loss: 0.98; acc: 0.69
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.79; acc: 0.75
Batch: 620; loss: 0.85; acc: 0.67
Batch: 640; loss: 0.66; acc: 0.8
Batch: 660; loss: 0.99; acc: 0.67
Batch: 680; loss: 0.84; acc: 0.69
Batch: 700; loss: 0.98; acc: 0.64
Batch: 720; loss: 0.88; acc: 0.75
Batch: 740; loss: 1.05; acc: 0.64
Batch: 760; loss: 0.77; acc: 0.78
Batch: 780; loss: 0.75; acc: 0.8
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.85; acc: 0.73
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 1.43; acc: 0.53
Batch: 140; loss: 0.6; acc: 0.78
Val Epoch over. val_loss: 0.8093705520888043; val_accuracy: 0.7353702229299363 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.96; acc: 0.72
Batch: 20; loss: 0.99; acc: 0.69
Batch: 40; loss: 1.04; acc: 0.62
Batch: 60; loss: 1.02; acc: 0.72
Batch: 80; loss: 1.2; acc: 0.61
Batch: 100; loss: 0.97; acc: 0.7
Batch: 120; loss: 1.04; acc: 0.64
Batch: 140; loss: 1.08; acc: 0.75
Batch: 160; loss: 0.96; acc: 0.7
Batch: 180; loss: 0.89; acc: 0.66
Batch: 200; loss: 0.99; acc: 0.7
Batch: 220; loss: 0.55; acc: 0.8
Batch: 240; loss: 0.65; acc: 0.78
Batch: 260; loss: 0.74; acc: 0.73
Batch: 280; loss: 1.16; acc: 0.59
Batch: 300; loss: 0.86; acc: 0.7
Batch: 320; loss: 0.79; acc: 0.72
Batch: 340; loss: 0.7; acc: 0.75
Batch: 360; loss: 0.81; acc: 0.73
Batch: 380; loss: 0.7; acc: 0.73
Batch: 400; loss: 1.05; acc: 0.72
Batch: 420; loss: 0.91; acc: 0.72
Batch: 440; loss: 0.93; acc: 0.7
Batch: 460; loss: 0.83; acc: 0.72
Batch: 480; loss: 0.59; acc: 0.78
Batch: 500; loss: 0.95; acc: 0.69
Batch: 520; loss: 1.05; acc: 0.69
Batch: 540; loss: 0.99; acc: 0.64
Batch: 560; loss: 1.06; acc: 0.69
Batch: 580; loss: 0.99; acc: 0.67
Batch: 600; loss: 0.9; acc: 0.72
Batch: 620; loss: 0.93; acc: 0.69
Batch: 640; loss: 0.87; acc: 0.75
Batch: 660; loss: 0.93; acc: 0.64
Batch: 680; loss: 1.06; acc: 0.73
Batch: 700; loss: 0.88; acc: 0.69
Batch: 720; loss: 0.78; acc: 0.78
Batch: 740; loss: 0.79; acc: 0.83
Batch: 760; loss: 0.68; acc: 0.72
Batch: 780; loss: 0.91; acc: 0.67
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.83; acc: 0.75
Batch: 20; loss: 1.0; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.86; acc: 0.75
Batch: 120; loss: 1.45; acc: 0.55
Batch: 140; loss: 0.61; acc: 0.73
Val Epoch over. val_loss: 0.8079452212828739; val_accuracy: 0.738953025477707 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 1.19; acc: 0.61
Batch: 20; loss: 0.69; acc: 0.78
Batch: 40; loss: 0.97; acc: 0.61
Batch: 60; loss: 0.94; acc: 0.78
Batch: 80; loss: 1.16; acc: 0.7
Batch: 100; loss: 0.69; acc: 0.75
Batch: 120; loss: 0.96; acc: 0.75
Batch: 140; loss: 0.76; acc: 0.69
Batch: 160; loss: 0.47; acc: 0.86
Batch: 180; loss: 0.62; acc: 0.77
Batch: 200; loss: 0.93; acc: 0.75
Batch: 220; loss: 0.96; acc: 0.67
Batch: 240; loss: 1.01; acc: 0.64
Batch: 260; loss: 1.02; acc: 0.64
Batch: 280; loss: 0.85; acc: 0.75
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.6; acc: 0.83
Batch: 340; loss: 0.88; acc: 0.72
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.81; acc: 0.72
Batch: 400; loss: 1.06; acc: 0.66
Batch: 420; loss: 0.81; acc: 0.78
Batch: 440; loss: 0.82; acc: 0.7
Batch: 460; loss: 0.79; acc: 0.81
Batch: 480; loss: 0.95; acc: 0.66
Batch: 500; loss: 0.99; acc: 0.7
Batch: 520; loss: 0.81; acc: 0.83
Batch: 540; loss: 0.96; acc: 0.72
Batch: 560; loss: 0.87; acc: 0.73
Batch: 580; loss: 0.94; acc: 0.69
Batch: 600; loss: 1.02; acc: 0.64
Batch: 620; loss: 0.7; acc: 0.75
Batch: 640; loss: 0.75; acc: 0.73
Batch: 660; loss: 0.99; acc: 0.66
Batch: 680; loss: 0.96; acc: 0.77
Batch: 700; loss: 0.76; acc: 0.72
Batch: 720; loss: 0.93; acc: 0.7
Batch: 740; loss: 1.01; acc: 0.67
Batch: 760; loss: 0.94; acc: 0.66
Batch: 780; loss: 0.91; acc: 0.75
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.84; acc: 0.75
Batch: 20; loss: 0.99; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.79; acc: 0.75
Batch: 80; loss: 0.69; acc: 0.78
Batch: 100; loss: 0.87; acc: 0.77
Batch: 120; loss: 1.45; acc: 0.53
Batch: 140; loss: 0.61; acc: 0.78
Val Epoch over. val_loss: 0.8093035257166359; val_accuracy: 0.7374601910828026 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.83; acc: 0.8
Batch: 20; loss: 0.82; acc: 0.78
Batch: 40; loss: 0.94; acc: 0.66
Batch: 60; loss: 0.86; acc: 0.72
Batch: 80; loss: 1.0; acc: 0.7
Batch: 100; loss: 1.26; acc: 0.56
Batch: 120; loss: 0.85; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.73
Batch: 160; loss: 0.66; acc: 0.83
Batch: 180; loss: 1.11; acc: 0.72
Batch: 200; loss: 0.93; acc: 0.7
Batch: 220; loss: 0.97; acc: 0.72
Batch: 240; loss: 0.88; acc: 0.73
Batch: 260; loss: 0.8; acc: 0.73
Batch: 280; loss: 1.21; acc: 0.62
Batch: 300; loss: 0.71; acc: 0.72
Batch: 320; loss: 0.91; acc: 0.72
Batch: 340; loss: 0.96; acc: 0.7
Batch: 360; loss: 1.08; acc: 0.67
Batch: 380; loss: 0.85; acc: 0.77
Batch: 400; loss: 0.8; acc: 0.7
Batch: 420; loss: 0.97; acc: 0.77
Batch: 440; loss: 0.9; acc: 0.72
Batch: 460; loss: 0.96; acc: 0.7
Batch: 480; loss: 0.82; acc: 0.78
Batch: 500; loss: 0.88; acc: 0.75
Batch: 520; loss: 0.87; acc: 0.72
Batch: 540; loss: 1.1; acc: 0.69
Batch: 560; loss: 0.75; acc: 0.78
Batch: 580; loss: 0.75; acc: 0.72
Batch: 600; loss: 0.98; acc: 0.72
Batch: 620; loss: 0.97; acc: 0.75
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 1.14; acc: 0.67
Batch: 680; loss: 0.99; acc: 0.67
Batch: 700; loss: 0.88; acc: 0.75
Batch: 720; loss: 1.08; acc: 0.64
Batch: 740; loss: 0.85; acc: 0.66
Batch: 760; loss: 0.82; acc: 0.66
Batch: 780; loss: 1.06; acc: 0.59
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.83; acc: 0.73
Batch: 20; loss: 1.0; acc: 0.7
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.78; acc: 0.77
Batch: 80; loss: 0.68; acc: 0.8
Batch: 100; loss: 0.86; acc: 0.77
Batch: 120; loss: 1.46; acc: 0.56
Batch: 140; loss: 0.62; acc: 0.77
Val Epoch over. val_loss: 0.8091616763430796; val_accuracy: 0.7384554140127388 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.9; acc: 0.7
Batch: 20; loss: 0.69; acc: 0.72
Batch: 40; loss: 0.8; acc: 0.8
Batch: 60; loss: 0.89; acc: 0.72
Batch: 80; loss: 0.93; acc: 0.67
Batch: 100; loss: 0.71; acc: 0.72
Batch: 120; loss: 0.83; acc: 0.75
Batch: 140; loss: 0.76; acc: 0.75
Batch: 160; loss: 0.97; acc: 0.66
Batch: 180; loss: 0.83; acc: 0.78
Batch: 200; loss: 0.88; acc: 0.72
Batch: 220; loss: 1.32; acc: 0.64
Batch: 240; loss: 0.95; acc: 0.66
Batch: 260; loss: 0.8; acc: 0.73
Batch: 280; loss: 0.98; acc: 0.69
Batch: 300; loss: 1.04; acc: 0.7
Batch: 320; loss: 1.0; acc: 0.64
Batch: 340; loss: 0.85; acc: 0.7
Batch: 360; loss: 0.7; acc: 0.8
Batch: 380; loss: 0.92; acc: 0.72
Batch: 400; loss: 0.78; acc: 0.78
Batch: 420; loss: 0.87; acc: 0.75
Batch: 440; loss: 0.84; acc: 0.69
Batch: 460; loss: 0.77; acc: 0.73
Batch: 480; loss: 0.71; acc: 0.73
Batch: 500; loss: 0.97; acc: 0.67
Batch: 520; loss: 0.74; acc: 0.78
Batch: 540; loss: 1.06; acc: 0.67
Batch: 560; loss: 0.76; acc: 0.73
Batch: 580; loss: 0.77; acc: 0.77
Batch: 600; loss: 0.63; acc: 0.77
Batch: 620; loss: 0.83; acc: 0.73
Batch: 640; loss: 0.81; acc: 0.72
Batch: 660; loss: 0.76; acc: 0.69
Batch: 680; loss: 1.13; acc: 0.67
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.99; acc: 0.73
Batch: 740; loss: 0.65; acc: 0.77
Batch: 760; loss: 0.64; acc: 0.78
Batch: 780; loss: 1.12; acc: 0.62
Train Epoch over. train_loss: 0.86; train_accuracy: 0.72 

Batch: 0; loss: 0.84; acc: 0.73
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.56; acc: 0.84
Batch: 60; loss: 0.79; acc: 0.77
Batch: 80; loss: 0.69; acc: 0.8
Batch: 100; loss: 0.87; acc: 0.75
Batch: 120; loss: 1.44; acc: 0.55
Batch: 140; loss: 0.61; acc: 0.78
Val Epoch over. val_loss: 0.808428908609281; val_accuracy: 0.7380573248407644 

plots/subspace_training/reg_lenet_2/2020-01-19 19:11:48/d_dim_100_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 28417
elements in E: 3954800
fraction nonzero: 0.007185445534540306
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.31; acc: 0.05
Batch: 140; loss: 2.3; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.08
Batch: 180; loss: 2.31; acc: 0.09
Batch: 200; loss: 2.3; acc: 0.11
Batch: 220; loss: 2.3; acc: 0.11
Batch: 240; loss: 2.3; acc: 0.11
Batch: 260; loss: 2.29; acc: 0.17
Batch: 280; loss: 2.31; acc: 0.08
Batch: 300; loss: 2.3; acc: 0.22
Batch: 320; loss: 2.29; acc: 0.14
Batch: 340; loss: 2.31; acc: 0.19
Batch: 360; loss: 2.29; acc: 0.16
Batch: 380; loss: 2.29; acc: 0.2
Batch: 400; loss: 2.28; acc: 0.17
Batch: 420; loss: 2.3; acc: 0.14
Batch: 440; loss: 2.29; acc: 0.16
Batch: 460; loss: 2.28; acc: 0.25
Batch: 480; loss: 2.29; acc: 0.08
Batch: 500; loss: 2.29; acc: 0.12
Batch: 520; loss: 2.31; acc: 0.05
Batch: 540; loss: 2.28; acc: 0.11
Batch: 560; loss: 2.29; acc: 0.08
Batch: 580; loss: 2.28; acc: 0.16
Batch: 600; loss: 2.28; acc: 0.09
Batch: 620; loss: 2.27; acc: 0.14
Batch: 640; loss: 2.26; acc: 0.12
Batch: 660; loss: 2.24; acc: 0.14
Batch: 680; loss: 2.23; acc: 0.16
Batch: 700; loss: 2.24; acc: 0.16
Batch: 720; loss: 2.19; acc: 0.22
Batch: 740; loss: 2.23; acc: 0.17
Batch: 760; loss: 2.22; acc: 0.16
Batch: 780; loss: 2.18; acc: 0.28
Train Epoch over. train_loss: 2.29; train_accuracy: 0.13 

Batch: 0; loss: 2.22; acc: 0.17
Batch: 20; loss: 2.26; acc: 0.08
Batch: 40; loss: 2.2; acc: 0.17
Batch: 60; loss: 2.21; acc: 0.14
Batch: 80; loss: 2.19; acc: 0.19
Batch: 100; loss: 2.25; acc: 0.09
Batch: 120; loss: 2.24; acc: 0.14
Batch: 140; loss: 2.22; acc: 0.22
Val Epoch over. val_loss: 2.2255344178266587; val_accuracy: 0.15276671974522293 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 2.21; acc: 0.16
Batch: 20; loss: 2.19; acc: 0.19
Batch: 40; loss: 2.21; acc: 0.19
Batch: 60; loss: 2.19; acc: 0.16
Batch: 80; loss: 2.09; acc: 0.25
Batch: 100; loss: 2.19; acc: 0.16
Batch: 120; loss: 2.09; acc: 0.17
Batch: 140; loss: 1.92; acc: 0.39
Batch: 160; loss: 1.9; acc: 0.3
Batch: 180; loss: 1.69; acc: 0.48
Batch: 200; loss: 1.87; acc: 0.33
Batch: 220; loss: 1.67; acc: 0.39
Batch: 240; loss: 1.81; acc: 0.38
Batch: 260; loss: 1.8; acc: 0.34
Batch: 280; loss: 1.5; acc: 0.5
Batch: 300; loss: 1.79; acc: 0.38
Batch: 320; loss: 1.48; acc: 0.53
Batch: 340; loss: 1.61; acc: 0.5
Batch: 360; loss: 1.39; acc: 0.48
Batch: 380; loss: 1.76; acc: 0.38
Batch: 400; loss: 1.33; acc: 0.61
Batch: 420; loss: 1.34; acc: 0.56
Batch: 440; loss: 1.39; acc: 0.55
Batch: 460; loss: 1.43; acc: 0.45
Batch: 480; loss: 1.36; acc: 0.53
Batch: 500; loss: 1.5; acc: 0.56
Batch: 520; loss: 1.3; acc: 0.62
Batch: 540; loss: 1.16; acc: 0.56
Batch: 560; loss: 1.45; acc: 0.5
Batch: 580; loss: 1.09; acc: 0.62
Batch: 600; loss: 1.17; acc: 0.61
Batch: 620; loss: 1.19; acc: 0.55
Batch: 640; loss: 1.12; acc: 0.61
Batch: 660; loss: 1.6; acc: 0.45
Batch: 680; loss: 0.87; acc: 0.7
Batch: 700; loss: 1.28; acc: 0.55
Batch: 720; loss: 1.31; acc: 0.55
Batch: 740; loss: 1.0; acc: 0.66
Batch: 760; loss: 1.42; acc: 0.59
Batch: 780; loss: 0.79; acc: 0.73
Train Epoch over. train_loss: 1.58; train_accuracy: 0.45 

Batch: 0; loss: 1.42; acc: 0.52
Batch: 20; loss: 2.07; acc: 0.39
Batch: 40; loss: 1.0; acc: 0.67
Batch: 60; loss: 1.25; acc: 0.61
Batch: 80; loss: 1.28; acc: 0.55
Batch: 100; loss: 1.3; acc: 0.52
Batch: 120; loss: 1.41; acc: 0.59
Batch: 140; loss: 1.29; acc: 0.58
Val Epoch over. val_loss: 1.459434208596588; val_accuracy: 0.5206011146496815 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.31; acc: 0.55
Batch: 20; loss: 1.26; acc: 0.59
Batch: 40; loss: 1.25; acc: 0.58
Batch: 60; loss: 1.29; acc: 0.48
Batch: 80; loss: 0.95; acc: 0.67
Batch: 100; loss: 1.0; acc: 0.66
Batch: 120; loss: 0.95; acc: 0.7
Batch: 140; loss: 0.98; acc: 0.67
Batch: 160; loss: 1.18; acc: 0.69
Batch: 180; loss: 0.79; acc: 0.73
Batch: 200; loss: 0.94; acc: 0.69
Batch: 220; loss: 0.91; acc: 0.69
Batch: 240; loss: 0.68; acc: 0.77
Batch: 260; loss: 1.32; acc: 0.58
Batch: 280; loss: 1.18; acc: 0.56
Batch: 300; loss: 0.93; acc: 0.7
Batch: 320; loss: 1.05; acc: 0.61
Batch: 340; loss: 1.04; acc: 0.59
Batch: 360; loss: 0.99; acc: 0.72
Batch: 380; loss: 0.77; acc: 0.72
Batch: 400; loss: 1.07; acc: 0.59
Batch: 420; loss: 0.87; acc: 0.7
Batch: 440; loss: 1.19; acc: 0.59
Batch: 460; loss: 0.95; acc: 0.7
Batch: 480; loss: 0.86; acc: 0.72
Batch: 500; loss: 0.86; acc: 0.7
Batch: 520; loss: 0.81; acc: 0.73
Batch: 540; loss: 0.79; acc: 0.72
Batch: 560; loss: 0.86; acc: 0.7
Batch: 580; loss: 0.77; acc: 0.75
Batch: 600; loss: 0.82; acc: 0.73
Batch: 620; loss: 1.11; acc: 0.62
Batch: 640; loss: 0.73; acc: 0.72
Batch: 660; loss: 0.96; acc: 0.72
Batch: 680; loss: 0.7; acc: 0.8
Batch: 700; loss: 0.82; acc: 0.75
Batch: 720; loss: 1.09; acc: 0.64
Batch: 740; loss: 1.02; acc: 0.67
Batch: 760; loss: 0.95; acc: 0.67
Batch: 780; loss: 0.88; acc: 0.73
Train Epoch over. train_loss: 0.96; train_accuracy: 0.68 

Batch: 0; loss: 0.87; acc: 0.73
Batch: 20; loss: 1.18; acc: 0.66
Batch: 40; loss: 0.83; acc: 0.69
Batch: 60; loss: 0.91; acc: 0.72
Batch: 80; loss: 0.75; acc: 0.77
Batch: 100; loss: 0.84; acc: 0.7
Batch: 120; loss: 1.4; acc: 0.52
Batch: 140; loss: 0.9; acc: 0.72
Val Epoch over. val_loss: 1.0359348757251812; val_accuracy: 0.6661027070063694 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 1.0; acc: 0.7
Batch: 20; loss: 1.12; acc: 0.58
Batch: 40; loss: 1.01; acc: 0.72
Batch: 60; loss: 0.63; acc: 0.81
Batch: 80; loss: 0.8; acc: 0.8
Batch: 100; loss: 1.0; acc: 0.69
Batch: 120; loss: 1.36; acc: 0.61
Batch: 140; loss: 0.97; acc: 0.7
Batch: 160; loss: 1.07; acc: 0.69
Batch: 180; loss: 0.8; acc: 0.73
Batch: 200; loss: 0.93; acc: 0.75
Batch: 220; loss: 0.75; acc: 0.75
Batch: 240; loss: 0.99; acc: 0.8
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.78; acc: 0.72
Batch: 300; loss: 0.72; acc: 0.78
Batch: 320; loss: 0.78; acc: 0.8
Batch: 340; loss: 0.79; acc: 0.69
Batch: 360; loss: 0.46; acc: 0.84
Batch: 380; loss: 1.0; acc: 0.73
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.67; acc: 0.75
Batch: 440; loss: 0.95; acc: 0.69
Batch: 460; loss: 0.63; acc: 0.81
Batch: 480; loss: 0.62; acc: 0.75
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.81; acc: 0.69
Batch: 540; loss: 0.72; acc: 0.78
Batch: 560; loss: 1.05; acc: 0.66
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.79; acc: 0.7
Batch: 620; loss: 0.8; acc: 0.7
Batch: 640; loss: 0.66; acc: 0.72
Batch: 660; loss: 0.58; acc: 0.83
Batch: 680; loss: 0.81; acc: 0.81
Batch: 700; loss: 0.98; acc: 0.72
Batch: 720; loss: 0.63; acc: 0.8
Batch: 740; loss: 0.81; acc: 0.72
Batch: 760; loss: 1.04; acc: 0.69
Batch: 780; loss: 0.99; acc: 0.64
Train Epoch over. train_loss: 0.8; train_accuracy: 0.74 

Batch: 0; loss: 0.84; acc: 0.64
Batch: 20; loss: 0.98; acc: 0.72
Batch: 40; loss: 0.82; acc: 0.69
Batch: 60; loss: 0.96; acc: 0.66
Batch: 80; loss: 0.87; acc: 0.72
Batch: 100; loss: 1.11; acc: 0.7
Batch: 120; loss: 1.63; acc: 0.58
Batch: 140; loss: 0.62; acc: 0.77
Val Epoch over. val_loss: 0.9553967518791272; val_accuracy: 0.6943670382165605 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 1.21; acc: 0.53
Batch: 20; loss: 0.85; acc: 0.67
Batch: 40; loss: 0.8; acc: 0.83
Batch: 60; loss: 0.59; acc: 0.86
Batch: 80; loss: 0.69; acc: 0.77
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.79; acc: 0.77
Batch: 140; loss: 0.64; acc: 0.78
Batch: 160; loss: 0.59; acc: 0.81
Batch: 180; loss: 0.54; acc: 0.83
Batch: 200; loss: 0.66; acc: 0.72
Batch: 220; loss: 0.63; acc: 0.78
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.51; acc: 0.81
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.59; acc: 0.81
Batch: 320; loss: 0.92; acc: 0.75
Batch: 340; loss: 1.09; acc: 0.62
Batch: 360; loss: 0.81; acc: 0.78
Batch: 380; loss: 0.8; acc: 0.72
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.92; acc: 0.69
Batch: 440; loss: 1.4; acc: 0.62
Batch: 460; loss: 0.73; acc: 0.78
Batch: 480; loss: 0.77; acc: 0.75
Batch: 500; loss: 0.94; acc: 0.73
Batch: 520; loss: 0.89; acc: 0.72
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.82; acc: 0.66
Batch: 580; loss: 1.19; acc: 0.67
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 1.48; acc: 0.56
Batch: 640; loss: 0.93; acc: 0.73
Batch: 660; loss: 0.66; acc: 0.81
Batch: 680; loss: 0.71; acc: 0.78
Batch: 700; loss: 0.78; acc: 0.75
Batch: 720; loss: 0.77; acc: 0.75
Batch: 740; loss: 0.73; acc: 0.72
Batch: 760; loss: 0.72; acc: 0.8
Batch: 780; loss: 0.91; acc: 0.7
Train Epoch over. train_loss: 0.75; train_accuracy: 0.76 

Batch: 0; loss: 0.76; acc: 0.7
Batch: 20; loss: 1.26; acc: 0.64
Batch: 40; loss: 0.78; acc: 0.73
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.62; acc: 0.8
Batch: 100; loss: 0.86; acc: 0.72
Batch: 120; loss: 1.01; acc: 0.67
Batch: 140; loss: 0.62; acc: 0.72
Val Epoch over. val_loss: 0.887016674515548; val_accuracy: 0.7038216560509554 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.76; acc: 0.75
Batch: 20; loss: 0.88; acc: 0.7
Batch: 40; loss: 0.7; acc: 0.8
Batch: 60; loss: 0.91; acc: 0.7
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.78; acc: 0.75
Batch: 160; loss: 0.69; acc: 0.78
Batch: 180; loss: 0.98; acc: 0.7
Batch: 200; loss: 0.73; acc: 0.69
Batch: 220; loss: 0.84; acc: 0.77
Batch: 240; loss: 0.72; acc: 0.75
Batch: 260; loss: 0.59; acc: 0.81
Batch: 280; loss: 0.7; acc: 0.77
Batch: 300; loss: 0.73; acc: 0.8
Batch: 320; loss: 0.87; acc: 0.77
Batch: 340; loss: 0.47; acc: 0.84
Batch: 360; loss: 0.59; acc: 0.83
Batch: 380; loss: 0.67; acc: 0.77
Batch: 400; loss: 0.56; acc: 0.81
Batch: 420; loss: 0.67; acc: 0.83
Batch: 440; loss: 0.95; acc: 0.64
Batch: 460; loss: 0.57; acc: 0.78
Batch: 480; loss: 0.67; acc: 0.75
Batch: 500; loss: 0.86; acc: 0.72
Batch: 520; loss: 0.86; acc: 0.73
Batch: 540; loss: 0.63; acc: 0.78
Batch: 560; loss: 0.51; acc: 0.81
Batch: 580; loss: 0.52; acc: 0.84
Batch: 600; loss: 0.61; acc: 0.8
Batch: 620; loss: 0.77; acc: 0.75
Batch: 640; loss: 0.66; acc: 0.75
Batch: 660; loss: 0.53; acc: 0.8
Batch: 680; loss: 0.66; acc: 0.75
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.69; acc: 0.78
Batch: 740; loss: 0.63; acc: 0.77
Batch: 760; loss: 0.46; acc: 0.89
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.73; train_accuracy: 0.77 

Batch: 0; loss: 0.65; acc: 0.78
Batch: 20; loss: 0.97; acc: 0.78
Batch: 40; loss: 0.57; acc: 0.81
Batch: 60; loss: 0.56; acc: 0.84
Batch: 80; loss: 0.62; acc: 0.73
Batch: 100; loss: 0.69; acc: 0.77
Batch: 120; loss: 0.89; acc: 0.72
Batch: 140; loss: 0.57; acc: 0.81
Val Epoch over. val_loss: 0.7416910215927537; val_accuracy: 0.7635350318471338 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.93; acc: 0.75
Batch: 20; loss: 0.55; acc: 0.86
Batch: 40; loss: 0.85; acc: 0.75
Batch: 60; loss: 0.81; acc: 0.75
Batch: 80; loss: 0.86; acc: 0.7
Batch: 100; loss: 0.4; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.66
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.59; acc: 0.84
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.97; acc: 0.69
Batch: 220; loss: 0.8; acc: 0.72
Batch: 240; loss: 0.58; acc: 0.75
Batch: 260; loss: 0.61; acc: 0.88
Batch: 280; loss: 1.08; acc: 0.69
Batch: 300; loss: 0.77; acc: 0.8
Batch: 320; loss: 0.87; acc: 0.73
Batch: 340; loss: 0.55; acc: 0.78
Batch: 360; loss: 0.79; acc: 0.67
Batch: 380; loss: 0.74; acc: 0.73
Batch: 400; loss: 0.96; acc: 0.7
Batch: 420; loss: 0.58; acc: 0.83
Batch: 440; loss: 0.42; acc: 0.86
Batch: 460; loss: 0.67; acc: 0.84
Batch: 480; loss: 0.63; acc: 0.86
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.46; acc: 0.84
Batch: 540; loss: 0.49; acc: 0.86
Batch: 560; loss: 0.85; acc: 0.75
Batch: 580; loss: 0.56; acc: 0.8
Batch: 600; loss: 0.74; acc: 0.8
Batch: 620; loss: 0.67; acc: 0.8
Batch: 640; loss: 0.68; acc: 0.77
Batch: 660; loss: 0.69; acc: 0.75
Batch: 680; loss: 0.68; acc: 0.75
Batch: 700; loss: 0.43; acc: 0.84
Batch: 720; loss: 0.95; acc: 0.69
Batch: 740; loss: 0.43; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.88
Batch: 780; loss: 1.05; acc: 0.61
Train Epoch over. train_loss: 0.71; train_accuracy: 0.78 

Batch: 0; loss: 0.52; acc: 0.86
Batch: 20; loss: 0.88; acc: 0.77
Batch: 40; loss: 0.61; acc: 0.77
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.43; acc: 0.88
Batch: 100; loss: 0.61; acc: 0.8
Batch: 120; loss: 0.86; acc: 0.73
Batch: 140; loss: 0.56; acc: 0.81
Val Epoch over. val_loss: 0.7589229314949861; val_accuracy: 0.7533837579617835 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.59; acc: 0.81
Batch: 20; loss: 0.86; acc: 0.73
Batch: 40; loss: 0.69; acc: 0.83
Batch: 60; loss: 0.86; acc: 0.75
Batch: 80; loss: 0.67; acc: 0.78
Batch: 100; loss: 0.85; acc: 0.73
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.49; acc: 0.84
Batch: 160; loss: 0.53; acc: 0.86
Batch: 180; loss: 0.83; acc: 0.72
Batch: 200; loss: 0.61; acc: 0.77
Batch: 220; loss: 0.97; acc: 0.73
Batch: 240; loss: 0.66; acc: 0.8
Batch: 260; loss: 0.75; acc: 0.75
Batch: 280; loss: 1.16; acc: 0.72
Batch: 300; loss: 0.87; acc: 0.7
Batch: 320; loss: 0.7; acc: 0.72
Batch: 340; loss: 0.78; acc: 0.78
Batch: 360; loss: 0.58; acc: 0.86
Batch: 380; loss: 0.56; acc: 0.84
Batch: 400; loss: 1.18; acc: 0.62
Batch: 420; loss: 0.73; acc: 0.78
Batch: 440; loss: 0.66; acc: 0.77
Batch: 460; loss: 0.67; acc: 0.78
Batch: 480; loss: 0.7; acc: 0.78
Batch: 500; loss: 0.7; acc: 0.83
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.51; acc: 0.81
Batch: 560; loss: 0.82; acc: 0.8
Batch: 580; loss: 0.49; acc: 0.84
Batch: 600; loss: 0.54; acc: 0.83
Batch: 620; loss: 0.69; acc: 0.77
Batch: 640; loss: 0.56; acc: 0.86
Batch: 660; loss: 0.83; acc: 0.72
Batch: 680; loss: 0.47; acc: 0.78
Batch: 700; loss: 0.77; acc: 0.78
Batch: 720; loss: 0.9; acc: 0.72
Batch: 740; loss: 0.78; acc: 0.69
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.76; acc: 0.77
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 0.96; acc: 0.69
Batch: 40; loss: 0.72; acc: 0.75
Batch: 60; loss: 0.74; acc: 0.73
Batch: 80; loss: 0.81; acc: 0.7
Batch: 100; loss: 0.9; acc: 0.73
Batch: 120; loss: 1.27; acc: 0.59
Batch: 140; loss: 0.47; acc: 0.83
Val Epoch over. val_loss: 0.8536414773600876; val_accuracy: 0.709593949044586 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.73
Batch: 20; loss: 1.05; acc: 0.77
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.61; acc: 0.81
Batch: 80; loss: 0.75; acc: 0.8
Batch: 100; loss: 1.08; acc: 0.73
Batch: 120; loss: 1.03; acc: 0.69
Batch: 140; loss: 0.85; acc: 0.7
Batch: 160; loss: 0.48; acc: 0.8
Batch: 180; loss: 0.58; acc: 0.86
Batch: 200; loss: 0.5; acc: 0.81
Batch: 220; loss: 0.53; acc: 0.81
Batch: 240; loss: 0.65; acc: 0.77
Batch: 260; loss: 0.8; acc: 0.78
Batch: 280; loss: 0.58; acc: 0.84
Batch: 300; loss: 0.62; acc: 0.75
Batch: 320; loss: 0.64; acc: 0.81
Batch: 340; loss: 0.6; acc: 0.8
Batch: 360; loss: 1.39; acc: 0.7
Batch: 380; loss: 0.77; acc: 0.77
Batch: 400; loss: 0.67; acc: 0.81
Batch: 420; loss: 0.37; acc: 0.88
Batch: 440; loss: 0.7; acc: 0.77
Batch: 460; loss: 0.55; acc: 0.81
Batch: 480; loss: 0.85; acc: 0.77
Batch: 500; loss: 0.79; acc: 0.72
Batch: 520; loss: 0.85; acc: 0.8
Batch: 540; loss: 0.54; acc: 0.81
Batch: 560; loss: 0.67; acc: 0.75
Batch: 580; loss: 0.82; acc: 0.81
Batch: 600; loss: 0.84; acc: 0.75
Batch: 620; loss: 0.8; acc: 0.77
Batch: 640; loss: 0.71; acc: 0.78
Batch: 660; loss: 0.76; acc: 0.8
Batch: 680; loss: 0.63; acc: 0.8
Batch: 700; loss: 0.81; acc: 0.77
Batch: 720; loss: 0.58; acc: 0.81
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.68; acc: 0.81
Batch: 780; loss: 0.78; acc: 0.77
Train Epoch over. train_loss: 0.7; train_accuracy: 0.78 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.79; acc: 0.81
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.89
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.84; acc: 0.72
Batch: 140; loss: 0.39; acc: 0.91
Val Epoch over. val_loss: 0.5649096419097511; val_accuracy: 0.8247412420382165 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.63; acc: 0.75
Batch: 20; loss: 0.53; acc: 0.83
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.53; acc: 0.84
Batch: 80; loss: 0.93; acc: 0.78
Batch: 100; loss: 0.69; acc: 0.72
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.84; acc: 0.75
Batch: 160; loss: 1.21; acc: 0.66
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.84
Batch: 220; loss: 0.94; acc: 0.72
Batch: 240; loss: 0.63; acc: 0.83
Batch: 260; loss: 0.91; acc: 0.73
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.44; acc: 0.89
Batch: 320; loss: 1.04; acc: 0.73
Batch: 340; loss: 0.84; acc: 0.69
Batch: 360; loss: 0.88; acc: 0.72
Batch: 380; loss: 1.05; acc: 0.7
Batch: 400; loss: 0.92; acc: 0.78
Batch: 420; loss: 0.87; acc: 0.77
Batch: 440; loss: 0.96; acc: 0.72
Batch: 460; loss: 0.89; acc: 0.75
Batch: 480; loss: 0.42; acc: 0.88
Batch: 500; loss: 0.59; acc: 0.8
Batch: 520; loss: 0.49; acc: 0.83
Batch: 540; loss: 0.64; acc: 0.81
Batch: 560; loss: 0.62; acc: 0.81
Batch: 580; loss: 0.73; acc: 0.73
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.88; acc: 0.72
Batch: 640; loss: 0.82; acc: 0.73
Batch: 660; loss: 0.93; acc: 0.7
Batch: 680; loss: 0.69; acc: 0.78
Batch: 700; loss: 0.85; acc: 0.73
Batch: 720; loss: 0.79; acc: 0.73
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.71; acc: 0.81
Batch: 780; loss: 0.63; acc: 0.83
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.76; acc: 0.72
Batch: 20; loss: 1.34; acc: 0.62
Batch: 40; loss: 1.0; acc: 0.75
Batch: 60; loss: 0.88; acc: 0.77
Batch: 80; loss: 0.86; acc: 0.72
Batch: 100; loss: 0.71; acc: 0.75
Batch: 120; loss: 1.12; acc: 0.66
Batch: 140; loss: 0.89; acc: 0.7
Val Epoch over. val_loss: 1.0340223786937204; val_accuracy: 0.6785429936305732 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.16; acc: 0.67
Batch: 20; loss: 0.68; acc: 0.83
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.88
Batch: 100; loss: 0.47; acc: 0.88
Batch: 120; loss: 0.51; acc: 0.84
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.56; acc: 0.81
Batch: 180; loss: 0.55; acc: 0.81
Batch: 200; loss: 0.67; acc: 0.81
Batch: 220; loss: 0.6; acc: 0.83
Batch: 240; loss: 0.65; acc: 0.81
Batch: 260; loss: 0.54; acc: 0.83
Batch: 280; loss: 0.51; acc: 0.83
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.48; acc: 0.84
Batch: 360; loss: 0.43; acc: 0.86
Batch: 380; loss: 0.71; acc: 0.8
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.47; acc: 0.81
Batch: 440; loss: 0.57; acc: 0.84
Batch: 460; loss: 0.61; acc: 0.8
Batch: 480; loss: 0.58; acc: 0.8
Batch: 500; loss: 0.71; acc: 0.81
Batch: 520; loss: 0.62; acc: 0.84
Batch: 540; loss: 0.47; acc: 0.84
Batch: 560; loss: 0.59; acc: 0.86
Batch: 580; loss: 0.62; acc: 0.84
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.7; acc: 0.73
Batch: 640; loss: 0.74; acc: 0.75
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.41; acc: 0.88
Batch: 700; loss: 0.31; acc: 0.92
Batch: 720; loss: 0.69; acc: 0.83
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.67; acc: 0.83
Batch: 780; loss: 0.68; acc: 0.75
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.83
Batch: 20; loss: 0.97; acc: 0.64
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.55; acc: 0.86
Batch: 80; loss: 0.55; acc: 0.8
Batch: 100; loss: 0.75; acc: 0.78
Batch: 120; loss: 1.12; acc: 0.67
Batch: 140; loss: 0.43; acc: 0.89
Val Epoch over. val_loss: 0.6484671575818092; val_accuracy: 0.7925955414012739 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.96; acc: 0.7
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.67; acc: 0.78
Batch: 60; loss: 0.62; acc: 0.83
Batch: 80; loss: 0.45; acc: 0.83
Batch: 100; loss: 0.48; acc: 0.8
Batch: 120; loss: 0.77; acc: 0.77
Batch: 140; loss: 0.59; acc: 0.77
Batch: 160; loss: 0.46; acc: 0.83
Batch: 180; loss: 0.53; acc: 0.83
Batch: 200; loss: 0.69; acc: 0.84
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.53; acc: 0.84
Batch: 260; loss: 0.53; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.49; acc: 0.81
Batch: 320; loss: 0.51; acc: 0.81
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.77; acc: 0.8
Batch: 380; loss: 0.72; acc: 0.81
Batch: 400; loss: 0.54; acc: 0.83
Batch: 420; loss: 0.64; acc: 0.84
Batch: 440; loss: 0.45; acc: 0.88
Batch: 460; loss: 0.34; acc: 0.91
Batch: 480; loss: 0.55; acc: 0.81
Batch: 500; loss: 0.74; acc: 0.8
Batch: 520; loss: 0.52; acc: 0.8
Batch: 540; loss: 0.41; acc: 0.89
Batch: 560; loss: 0.73; acc: 0.83
Batch: 580; loss: 0.72; acc: 0.75
Batch: 600; loss: 0.45; acc: 0.84
Batch: 620; loss: 0.53; acc: 0.81
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.48; acc: 0.81
Batch: 680; loss: 0.58; acc: 0.77
Batch: 700; loss: 0.64; acc: 0.78
Batch: 720; loss: 0.43; acc: 0.86
Batch: 740; loss: 0.82; acc: 0.77
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 0.58; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.97; acc: 0.64
Batch: 40; loss: 0.43; acc: 0.89
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 1.23; acc: 0.66
Batch: 140; loss: 0.33; acc: 0.91
Val Epoch over. val_loss: 0.6421169036892569; val_accuracy: 0.7958797770700637 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.73
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.75; acc: 0.81
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.61; acc: 0.8
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.46; acc: 0.88
Batch: 140; loss: 0.51; acc: 0.8
Batch: 160; loss: 0.63; acc: 0.75
Batch: 180; loss: 0.76; acc: 0.73
Batch: 200; loss: 0.26; acc: 0.92
Batch: 220; loss: 0.48; acc: 0.88
Batch: 240; loss: 0.47; acc: 0.89
Batch: 260; loss: 0.58; acc: 0.84
Batch: 280; loss: 0.38; acc: 0.86
Batch: 300; loss: 0.59; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.59; acc: 0.88
Batch: 400; loss: 0.58; acc: 0.83
Batch: 420; loss: 0.43; acc: 0.84
Batch: 440; loss: 0.59; acc: 0.81
Batch: 460; loss: 0.49; acc: 0.83
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.51; acc: 0.83
Batch: 520; loss: 0.58; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.84
Batch: 560; loss: 0.51; acc: 0.81
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.55; acc: 0.83
Batch: 620; loss: 0.79; acc: 0.78
Batch: 640; loss: 0.52; acc: 0.83
Batch: 660; loss: 0.59; acc: 0.78
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.55; acc: 0.8
Batch: 720; loss: 0.71; acc: 0.78
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.69; acc: 0.73
Batch: 780; loss: 0.46; acc: 0.86
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.78; acc: 0.75
Batch: 20; loss: 1.2; acc: 0.66
Batch: 40; loss: 0.67; acc: 0.8
Batch: 60; loss: 0.79; acc: 0.73
Batch: 80; loss: 0.93; acc: 0.75
Batch: 100; loss: 0.85; acc: 0.72
Batch: 120; loss: 1.33; acc: 0.59
Batch: 140; loss: 0.47; acc: 0.81
Val Epoch over. val_loss: 0.861290611658886; val_accuracy: 0.7281050955414012 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.85; acc: 0.7
Batch: 20; loss: 0.51; acc: 0.84
Batch: 40; loss: 0.63; acc: 0.81
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.55; acc: 0.86
Batch: 100; loss: 0.61; acc: 0.83
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.6; acc: 0.78
Batch: 160; loss: 0.51; acc: 0.81
Batch: 180; loss: 0.43; acc: 0.84
Batch: 200; loss: 0.54; acc: 0.84
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.45; acc: 0.84
Batch: 260; loss: 0.65; acc: 0.78
Batch: 280; loss: 0.66; acc: 0.83
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.53; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.54; acc: 0.81
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.59; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.83
Batch: 440; loss: 0.56; acc: 0.8
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.81
Batch: 520; loss: 0.57; acc: 0.86
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.81; acc: 0.78
Batch: 600; loss: 0.67; acc: 0.77
Batch: 620; loss: 0.33; acc: 0.84
Batch: 640; loss: 0.71; acc: 0.73
Batch: 660; loss: 0.64; acc: 0.81
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.58; acc: 0.81
Batch: 720; loss: 0.45; acc: 0.83
Batch: 740; loss: 0.26; acc: 0.91
Batch: 760; loss: 0.8; acc: 0.72
Batch: 780; loss: 0.52; acc: 0.8
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.36; acc: 0.91
Batch: 20; loss: 0.83; acc: 0.72
Batch: 40; loss: 0.42; acc: 0.92
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.5; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.81
Batch: 120; loss: 0.8; acc: 0.73
Batch: 140; loss: 0.37; acc: 0.89
Val Epoch over. val_loss: 0.5941192718448153; val_accuracy: 0.8104100318471338 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.83
Batch: 20; loss: 0.71; acc: 0.8
Batch: 40; loss: 0.55; acc: 0.84
Batch: 60; loss: 0.64; acc: 0.8
Batch: 80; loss: 0.43; acc: 0.83
Batch: 100; loss: 0.49; acc: 0.86
Batch: 120; loss: 0.61; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.8
Batch: 160; loss: 0.53; acc: 0.81
Batch: 180; loss: 0.45; acc: 0.84
Batch: 200; loss: 0.67; acc: 0.84
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.34; acc: 0.86
Batch: 280; loss: 0.49; acc: 0.88
Batch: 300; loss: 0.36; acc: 0.86
Batch: 320; loss: 0.95; acc: 0.73
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.67; acc: 0.78
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.69; acc: 0.77
Batch: 420; loss: 0.58; acc: 0.8
Batch: 440; loss: 0.55; acc: 0.81
Batch: 460; loss: 0.5; acc: 0.84
Batch: 480; loss: 0.55; acc: 0.86
Batch: 500; loss: 0.94; acc: 0.75
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.77; acc: 0.81
Batch: 560; loss: 0.56; acc: 0.84
Batch: 580; loss: 0.57; acc: 0.86
Batch: 600; loss: 0.61; acc: 0.83
Batch: 620; loss: 0.51; acc: 0.81
Batch: 640; loss: 0.6; acc: 0.81
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.26; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.88
Batch: 720; loss: 0.52; acc: 0.78
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.49; acc: 0.86
Batch: 780; loss: 0.55; acc: 0.81
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.49; acc: 0.86
Batch: 20; loss: 0.7; acc: 0.81
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.73
Batch: 140; loss: 0.34; acc: 0.86
Val Epoch over. val_loss: 0.5800291390935327; val_accuracy: 0.8165804140127388 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.64; acc: 0.77
Batch: 20; loss: 0.55; acc: 0.78
Batch: 40; loss: 0.68; acc: 0.73
Batch: 60; loss: 0.52; acc: 0.84
Batch: 80; loss: 0.59; acc: 0.8
Batch: 100; loss: 0.51; acc: 0.81
Batch: 120; loss: 0.38; acc: 0.88
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.92; acc: 0.69
Batch: 180; loss: 0.74; acc: 0.81
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.53; acc: 0.83
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.53; acc: 0.81
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.5; acc: 0.83
Batch: 320; loss: 0.68; acc: 0.75
Batch: 340; loss: 0.66; acc: 0.83
Batch: 360; loss: 0.68; acc: 0.8
Batch: 380; loss: 0.52; acc: 0.83
Batch: 400; loss: 0.55; acc: 0.81
Batch: 420; loss: 0.48; acc: 0.86
Batch: 440; loss: 0.28; acc: 0.91
Batch: 460; loss: 0.72; acc: 0.8
Batch: 480; loss: 0.5; acc: 0.84
Batch: 500; loss: 0.76; acc: 0.86
Batch: 520; loss: 0.39; acc: 0.88
Batch: 540; loss: 0.51; acc: 0.81
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.45; acc: 0.88
Batch: 620; loss: 0.88; acc: 0.73
Batch: 640; loss: 0.46; acc: 0.84
Batch: 660; loss: 0.36; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.88
Batch: 700; loss: 0.44; acc: 0.86
Batch: 720; loss: 0.45; acc: 0.89
Batch: 740; loss: 0.63; acc: 0.75
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.68; acc: 0.81
Train Epoch over. train_loss: 0.53; train_accuracy: 0.84 

Batch: 0; loss: 0.4; acc: 0.89
Batch: 20; loss: 0.78; acc: 0.77
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.21; acc: 0.97
Val Epoch over. val_loss: 0.5222984546688711; val_accuracy: 0.8407643312101911 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.53; acc: 0.84
Batch: 20; loss: 0.54; acc: 0.88
Batch: 40; loss: 0.58; acc: 0.8
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.85; acc: 0.7
Batch: 100; loss: 0.45; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.86
Batch: 140; loss: 0.52; acc: 0.81
Batch: 160; loss: 0.52; acc: 0.83
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.56; acc: 0.8
Batch: 220; loss: 0.73; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.64; acc: 0.81
Batch: 280; loss: 0.75; acc: 0.81
Batch: 300; loss: 0.57; acc: 0.81
Batch: 320; loss: 0.69; acc: 0.77
Batch: 340; loss: 0.34; acc: 0.88
Batch: 360; loss: 0.44; acc: 0.84
Batch: 380; loss: 0.66; acc: 0.73
Batch: 400; loss: 0.56; acc: 0.8
Batch: 420; loss: 0.68; acc: 0.78
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.66; acc: 0.75
Batch: 480; loss: 0.54; acc: 0.84
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.48; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.84
Batch: 560; loss: 0.43; acc: 0.8
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.76; acc: 0.78
Batch: 620; loss: 0.56; acc: 0.77
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.56; acc: 0.8
Batch: 680; loss: 0.48; acc: 0.83
Batch: 700; loss: 0.52; acc: 0.81
Batch: 720; loss: 0.46; acc: 0.81
Batch: 740; loss: 0.8; acc: 0.78
Batch: 760; loss: 0.69; acc: 0.78
Batch: 780; loss: 0.5; acc: 0.81
Train Epoch over. train_loss: 0.53; train_accuracy: 0.83 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.8
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.89
Batch: 120; loss: 0.72; acc: 0.73
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.5036983416908106; val_accuracy: 0.8427547770700637 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.46; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.84
Batch: 100; loss: 0.58; acc: 0.81
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.58; acc: 0.77
Batch: 160; loss: 0.42; acc: 0.83
Batch: 180; loss: 0.53; acc: 0.84
Batch: 200; loss: 0.91; acc: 0.75
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.42; acc: 0.89
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.52; acc: 0.84
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.59; acc: 0.81
Batch: 340; loss: 0.43; acc: 0.88
Batch: 360; loss: 0.7; acc: 0.81
Batch: 380; loss: 0.5; acc: 0.83
Batch: 400; loss: 0.43; acc: 0.83
Batch: 420; loss: 0.48; acc: 0.83
Batch: 440; loss: 0.77; acc: 0.8
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.64; acc: 0.8
Batch: 520; loss: 0.49; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.55; acc: 0.84
Batch: 580; loss: 0.69; acc: 0.84
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.63; acc: 0.78
Batch: 640; loss: 0.56; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.83
Batch: 680; loss: 0.6; acc: 0.73
Batch: 700; loss: 0.39; acc: 0.88
Batch: 720; loss: 0.57; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.89
Batch: 780; loss: 0.59; acc: 0.83
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.8
Batch: 40; loss: 0.41; acc: 0.84
Batch: 60; loss: 0.29; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.89
Batch: 120; loss: 0.77; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.47087275896482406; val_accuracy: 0.8548964968152867 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.68; acc: 0.77
Batch: 20; loss: 0.51; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.83
Batch: 60; loss: 0.72; acc: 0.78
Batch: 80; loss: 0.5; acc: 0.8
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.62; acc: 0.81
Batch: 140; loss: 0.68; acc: 0.77
Batch: 160; loss: 0.45; acc: 0.86
Batch: 180; loss: 0.56; acc: 0.8
Batch: 200; loss: 0.51; acc: 0.83
Batch: 220; loss: 0.97; acc: 0.75
Batch: 240; loss: 0.43; acc: 0.81
Batch: 260; loss: 0.8; acc: 0.8
Batch: 280; loss: 0.67; acc: 0.72
Batch: 300; loss: 0.4; acc: 0.86
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.57; acc: 0.83
Batch: 360; loss: 0.49; acc: 0.88
Batch: 380; loss: 0.69; acc: 0.8
Batch: 400; loss: 0.55; acc: 0.81
Batch: 420; loss: 0.69; acc: 0.8
Batch: 440; loss: 0.61; acc: 0.83
Batch: 460; loss: 0.65; acc: 0.81
Batch: 480; loss: 0.63; acc: 0.78
Batch: 500; loss: 0.75; acc: 0.84
Batch: 520; loss: 0.46; acc: 0.86
Batch: 540; loss: 0.74; acc: 0.8
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.61; acc: 0.83
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.41; acc: 0.91
Batch: 640; loss: 0.48; acc: 0.83
Batch: 660; loss: 0.52; acc: 0.84
Batch: 680; loss: 0.86; acc: 0.8
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.65; acc: 0.81
Batch: 740; loss: 0.44; acc: 0.88
Batch: 760; loss: 0.59; acc: 0.84
Batch: 780; loss: 0.85; acc: 0.73
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.7; acc: 0.73
Batch: 40; loss: 0.55; acc: 0.78
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.83
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.4; acc: 0.83
Val Epoch over. val_loss: 0.5571851380121936; val_accuracy: 0.8223527070063694 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.52; acc: 0.88
Batch: 40; loss: 0.46; acc: 0.86
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.68; acc: 0.78
Batch: 100; loss: 0.43; acc: 0.81
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.47; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.84
Batch: 180; loss: 0.6; acc: 0.83
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.45; acc: 0.84
Batch: 280; loss: 0.6; acc: 0.81
Batch: 300; loss: 0.64; acc: 0.83
Batch: 320; loss: 0.76; acc: 0.78
Batch: 340; loss: 0.78; acc: 0.73
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.55; acc: 0.8
Batch: 400; loss: 0.63; acc: 0.73
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.39; acc: 0.91
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.92
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.56; acc: 0.77
Batch: 560; loss: 0.51; acc: 0.84
Batch: 580; loss: 0.7; acc: 0.73
Batch: 600; loss: 0.45; acc: 0.8
Batch: 620; loss: 0.44; acc: 0.91
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.86
Batch: 700; loss: 0.62; acc: 0.81
Batch: 720; loss: 0.79; acc: 0.77
Batch: 740; loss: 0.48; acc: 0.83
Batch: 760; loss: 0.43; acc: 0.86
Batch: 780; loss: 0.51; acc: 0.84
Train Epoch over. train_loss: 0.52; train_accuracy: 0.84 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.29; acc: 0.92
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.73
Batch: 140; loss: 0.22; acc: 0.94
Val Epoch over. val_loss: 0.48378764994584833; val_accuracy: 0.8525079617834395 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.46; acc: 0.91
Batch: 20; loss: 0.56; acc: 0.86
Batch: 40; loss: 0.39; acc: 0.86
Batch: 60; loss: 0.59; acc: 0.81
Batch: 80; loss: 0.51; acc: 0.84
Batch: 100; loss: 0.37; acc: 0.86
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.34; acc: 0.88
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.4; acc: 0.89
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.42; acc: 0.83
Batch: 260; loss: 0.42; acc: 0.88
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.37; acc: 0.84
Batch: 320; loss: 0.79; acc: 0.77
Batch: 340; loss: 0.42; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.44; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.81
Batch: 420; loss: 0.66; acc: 0.81
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.32; acc: 0.89
Batch: 500; loss: 0.79; acc: 0.81
Batch: 520; loss: 0.65; acc: 0.88
Batch: 540; loss: 0.6; acc: 0.83
Batch: 560; loss: 0.66; acc: 0.8
Batch: 580; loss: 0.29; acc: 0.91
Batch: 600; loss: 0.64; acc: 0.81
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.58; acc: 0.84
Batch: 660; loss: 0.78; acc: 0.75
Batch: 680; loss: 0.71; acc: 0.78
Batch: 700; loss: 0.68; acc: 0.81
Batch: 720; loss: 0.61; acc: 0.83
Batch: 740; loss: 0.45; acc: 0.84
Batch: 760; loss: 0.54; acc: 0.83
Batch: 780; loss: 0.63; acc: 0.8
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.41; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.88
Batch: 100; loss: 0.46; acc: 0.84
Batch: 120; loss: 0.97; acc: 0.7
Batch: 140; loss: 0.17; acc: 0.97
Val Epoch over. val_loss: 0.5208211272575294; val_accuracy: 0.8365843949044586 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.74; acc: 0.77
Batch: 20; loss: 0.51; acc: 0.86
Batch: 40; loss: 0.4; acc: 0.88
Batch: 60; loss: 0.51; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.84
Batch: 100; loss: 0.69; acc: 0.8
Batch: 120; loss: 0.57; acc: 0.78
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.6; acc: 0.83
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.44; acc: 0.86
Batch: 220; loss: 0.67; acc: 0.81
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.41; acc: 0.83
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.42; acc: 0.91
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.56; acc: 0.88
Batch: 360; loss: 0.64; acc: 0.8
Batch: 380; loss: 0.31; acc: 0.88
Batch: 400; loss: 0.69; acc: 0.78
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.86
Batch: 480; loss: 0.47; acc: 0.81
Batch: 500; loss: 0.63; acc: 0.83
Batch: 520; loss: 0.37; acc: 0.89
Batch: 540; loss: 0.6; acc: 0.78
Batch: 560; loss: 0.59; acc: 0.8
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.73; acc: 0.81
Batch: 620; loss: 0.34; acc: 0.89
Batch: 640; loss: 0.54; acc: 0.83
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.44; acc: 0.89
Batch: 700; loss: 0.74; acc: 0.86
Batch: 720; loss: 0.29; acc: 0.86
Batch: 740; loss: 0.49; acc: 0.81
Batch: 760; loss: 0.61; acc: 0.84
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.83; acc: 0.73
Batch: 140; loss: 0.15; acc: 0.97
Val Epoch over. val_loss: 0.45497715036580516; val_accuracy: 0.8586783439490446 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.81
Batch: 40; loss: 0.45; acc: 0.86
Batch: 60; loss: 0.41; acc: 0.8
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.71; acc: 0.77
Batch: 160; loss: 0.39; acc: 0.88
Batch: 180; loss: 0.37; acc: 0.84
Batch: 200; loss: 0.52; acc: 0.81
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.34; acc: 0.91
Batch: 260; loss: 0.39; acc: 0.83
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.84
Batch: 320; loss: 0.49; acc: 0.88
Batch: 340; loss: 0.3; acc: 0.95
Batch: 360; loss: 0.51; acc: 0.83
Batch: 380; loss: 0.52; acc: 0.81
Batch: 400; loss: 0.53; acc: 0.88
Batch: 420; loss: 0.48; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.41; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.63; acc: 0.83
Batch: 540; loss: 0.38; acc: 0.92
Batch: 560; loss: 0.23; acc: 0.95
Batch: 580; loss: 0.5; acc: 0.84
Batch: 600; loss: 0.55; acc: 0.88
Batch: 620; loss: 0.62; acc: 0.88
Batch: 640; loss: 0.52; acc: 0.81
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.89
Batch: 700; loss: 0.51; acc: 0.8
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.52; acc: 0.84
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.78; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.45639146930852514; val_accuracy: 0.8607683121019108 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.72; acc: 0.77
Batch: 20; loss: 0.43; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.78
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.84
Batch: 120; loss: 0.63; acc: 0.78
Batch: 140; loss: 0.58; acc: 0.83
Batch: 160; loss: 0.39; acc: 0.92
Batch: 180; loss: 0.81; acc: 0.73
Batch: 200; loss: 0.4; acc: 0.83
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.37; acc: 0.88
Batch: 260; loss: 0.51; acc: 0.83
Batch: 280; loss: 0.37; acc: 0.88
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.68; acc: 0.8
Batch: 340; loss: 0.49; acc: 0.83
Batch: 360; loss: 0.36; acc: 0.89
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.46; acc: 0.84
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.62; acc: 0.77
Batch: 460; loss: 0.52; acc: 0.83
Batch: 480; loss: 0.68; acc: 0.83
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.42; acc: 0.94
Batch: 540; loss: 0.62; acc: 0.83
Batch: 560; loss: 0.33; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.81
Batch: 620; loss: 0.51; acc: 0.84
Batch: 640; loss: 0.48; acc: 0.8
Batch: 660; loss: 0.75; acc: 0.75
Batch: 680; loss: 0.38; acc: 0.89
Batch: 700; loss: 0.4; acc: 0.89
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.47; acc: 0.81
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.68; acc: 0.72
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.4404978109582974; val_accuracy: 0.8660429936305732 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.48; acc: 0.86
Batch: 20; loss: 0.6; acc: 0.88
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.43; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.66; acc: 0.83
Batch: 140; loss: 0.46; acc: 0.89
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.37; acc: 0.86
Batch: 200; loss: 0.56; acc: 0.88
Batch: 220; loss: 0.5; acc: 0.81
Batch: 240; loss: 0.36; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.84
Batch: 280; loss: 0.32; acc: 0.91
Batch: 300; loss: 0.58; acc: 0.84
Batch: 320; loss: 0.44; acc: 0.83
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.65; acc: 0.8
Batch: 380; loss: 0.4; acc: 0.91
Batch: 400; loss: 0.52; acc: 0.83
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.61; acc: 0.88
Batch: 460; loss: 0.65; acc: 0.77
Batch: 480; loss: 0.43; acc: 0.91
Batch: 500; loss: 0.55; acc: 0.83
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.56; acc: 0.88
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.86
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.64; acc: 0.8
Batch: 640; loss: 0.45; acc: 0.83
Batch: 660; loss: 0.45; acc: 0.86
Batch: 680; loss: 0.43; acc: 0.89
Batch: 700; loss: 0.47; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.86
Batch: 740; loss: 0.34; acc: 0.86
Batch: 760; loss: 0.41; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.59; acc: 0.83
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.4575791418742222; val_accuracy: 0.8590764331210191 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.41; acc: 0.81
Batch: 40; loss: 0.35; acc: 0.89
Batch: 60; loss: 0.71; acc: 0.73
Batch: 80; loss: 0.52; acc: 0.84
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.38; acc: 0.86
Batch: 240; loss: 0.56; acc: 0.86
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.67; acc: 0.78
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.83
Batch: 360; loss: 0.57; acc: 0.83
Batch: 380; loss: 0.6; acc: 0.8
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.82; acc: 0.81
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.59; acc: 0.78
Batch: 500; loss: 0.38; acc: 0.89
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.4; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.69; acc: 0.8
Batch: 620; loss: 0.68; acc: 0.77
Batch: 640; loss: 0.47; acc: 0.83
Batch: 660; loss: 0.54; acc: 0.88
Batch: 680; loss: 0.39; acc: 0.92
Batch: 700; loss: 0.5; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.88
Batch: 740; loss: 0.57; acc: 0.86
Batch: 760; loss: 0.61; acc: 0.81
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.35; acc: 0.86
Batch: 20; loss: 0.61; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.84
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.36; acc: 0.84
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.81; acc: 0.73
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.46587470468062503; val_accuracy: 0.8562898089171974 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.51; acc: 0.83
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.81; acc: 0.77
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.44; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.53; acc: 0.8
Batch: 160; loss: 0.58; acc: 0.84
Batch: 180; loss: 0.4; acc: 0.88
Batch: 200; loss: 0.51; acc: 0.86
Batch: 220; loss: 0.42; acc: 0.86
Batch: 240; loss: 0.41; acc: 0.86
Batch: 260; loss: 0.48; acc: 0.86
Batch: 280; loss: 0.37; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.81; acc: 0.84
Batch: 340; loss: 0.64; acc: 0.83
Batch: 360; loss: 0.69; acc: 0.8
Batch: 380; loss: 0.61; acc: 0.8
Batch: 400; loss: 0.49; acc: 0.84
Batch: 420; loss: 0.41; acc: 0.84
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.27; acc: 0.91
Batch: 480; loss: 0.57; acc: 0.8
Batch: 500; loss: 0.46; acc: 0.84
Batch: 520; loss: 0.45; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.84
Batch: 560; loss: 0.5; acc: 0.86
Batch: 580; loss: 0.7; acc: 0.8
Batch: 600; loss: 0.5; acc: 0.84
Batch: 620; loss: 0.6; acc: 0.8
Batch: 640; loss: 0.71; acc: 0.86
Batch: 660; loss: 0.56; acc: 0.83
Batch: 680; loss: 0.49; acc: 0.83
Batch: 700; loss: 0.36; acc: 0.86
Batch: 720; loss: 0.63; acc: 0.8
Batch: 740; loss: 0.54; acc: 0.86
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.45; acc: 0.88
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.65; acc: 0.8
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.38; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.38; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.21; acc: 0.94
Val Epoch over. val_loss: 0.4683167782558757; val_accuracy: 0.8562898089171974 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.81; acc: 0.78
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.64; acc: 0.78
Batch: 80; loss: 0.45; acc: 0.86
Batch: 100; loss: 0.57; acc: 0.83
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.81
Batch: 180; loss: 0.7; acc: 0.75
Batch: 200; loss: 0.58; acc: 0.84
Batch: 220; loss: 0.46; acc: 0.88
Batch: 240; loss: 0.37; acc: 0.89
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.6; acc: 0.84
Batch: 320; loss: 0.43; acc: 0.84
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.66; acc: 0.77
Batch: 380; loss: 0.62; acc: 0.86
Batch: 400; loss: 0.46; acc: 0.86
Batch: 420; loss: 0.8; acc: 0.72
Batch: 440; loss: 0.35; acc: 0.86
Batch: 460; loss: 0.35; acc: 0.92
Batch: 480; loss: 0.56; acc: 0.86
Batch: 500; loss: 0.48; acc: 0.84
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.75; acc: 0.78
Batch: 560; loss: 0.45; acc: 0.83
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.51; acc: 0.81
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.61; acc: 0.83
Batch: 700; loss: 0.61; acc: 0.8
Batch: 720; loss: 0.4; acc: 0.88
Batch: 740; loss: 0.53; acc: 0.86
Batch: 760; loss: 0.58; acc: 0.78
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.81
Batch: 40; loss: 0.46; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.86
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.71; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4462196763343872; val_accuracy: 0.8635549363057324 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.57; acc: 0.78
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.42; acc: 0.83
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.5; acc: 0.88
Batch: 180; loss: 0.58; acc: 0.8
Batch: 200; loss: 0.79; acc: 0.78
Batch: 220; loss: 0.31; acc: 0.89
Batch: 240; loss: 0.58; acc: 0.84
Batch: 260; loss: 0.55; acc: 0.86
Batch: 280; loss: 0.47; acc: 0.8
Batch: 300; loss: 0.43; acc: 0.86
Batch: 320; loss: 0.61; acc: 0.8
Batch: 340; loss: 0.33; acc: 0.92
Batch: 360; loss: 0.47; acc: 0.83
Batch: 380; loss: 0.46; acc: 0.84
Batch: 400; loss: 0.34; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.84
Batch: 440; loss: 0.52; acc: 0.83
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.56; acc: 0.84
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.92
Batch: 540; loss: 0.4; acc: 0.89
Batch: 560; loss: 0.5; acc: 0.83
Batch: 580; loss: 0.51; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.91
Batch: 620; loss: 0.55; acc: 0.88
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.55; acc: 0.83
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.41; acc: 0.91
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.47; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.39; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.84
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.63; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.92
Val Epoch over. val_loss: 0.4302002891993067; val_accuracy: 0.8722133757961783 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.58; acc: 0.81
Batch: 20; loss: 0.56; acc: 0.77
Batch: 40; loss: 0.48; acc: 0.91
Batch: 60; loss: 0.6; acc: 0.83
Batch: 80; loss: 0.48; acc: 0.8
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.5; acc: 0.83
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.47; acc: 0.88
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.69; acc: 0.84
Batch: 260; loss: 0.51; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.92
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.57; acc: 0.84
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.37; acc: 0.89
Batch: 400; loss: 0.7; acc: 0.73
Batch: 420; loss: 0.44; acc: 0.83
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.51; acc: 0.88
Batch: 480; loss: 0.62; acc: 0.84
Batch: 500; loss: 0.4; acc: 0.89
Batch: 520; loss: 0.41; acc: 0.89
Batch: 540; loss: 0.34; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.83
Batch: 580; loss: 0.53; acc: 0.89
Batch: 600; loss: 0.48; acc: 0.89
Batch: 620; loss: 0.48; acc: 0.84
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.84; acc: 0.73
Batch: 680; loss: 0.37; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.83
Batch: 720; loss: 0.4; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.42; acc: 0.89
Batch: 780; loss: 0.74; acc: 0.75
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.62; acc: 0.88
Batch: 40; loss: 0.47; acc: 0.89
Batch: 60; loss: 0.33; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.63; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.42810830217637835; val_accuracy: 0.8710191082802548 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.53; acc: 0.88
Batch: 40; loss: 0.66; acc: 0.77
Batch: 60; loss: 0.5; acc: 0.81
Batch: 80; loss: 0.37; acc: 0.88
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.39; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.51; acc: 0.88
Batch: 200; loss: 0.38; acc: 0.88
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.5; acc: 0.83
Batch: 260; loss: 0.5; acc: 0.81
Batch: 280; loss: 0.81; acc: 0.81
Batch: 300; loss: 0.62; acc: 0.8
Batch: 320; loss: 0.62; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.91
Batch: 360; loss: 0.8; acc: 0.75
Batch: 380; loss: 0.56; acc: 0.86
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.43; acc: 0.86
Batch: 440; loss: 0.55; acc: 0.83
Batch: 460; loss: 0.57; acc: 0.77
Batch: 480; loss: 0.52; acc: 0.83
Batch: 500; loss: 0.39; acc: 0.84
Batch: 520; loss: 0.48; acc: 0.89
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.47; acc: 0.84
Batch: 580; loss: 0.48; acc: 0.83
Batch: 600; loss: 0.57; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.53; acc: 0.84
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.5; acc: 0.83
Batch: 700; loss: 0.31; acc: 0.95
Batch: 720; loss: 0.33; acc: 0.91
Batch: 740; loss: 0.45; acc: 0.89
Batch: 760; loss: 0.38; acc: 0.84
Batch: 780; loss: 0.35; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.94
Val Epoch over. val_loss: 0.4146717954782923; val_accuracy: 0.8749004777070064 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.62; acc: 0.8
Batch: 20; loss: 0.62; acc: 0.83
Batch: 40; loss: 0.64; acc: 0.83
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.55; acc: 0.84
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.7; acc: 0.81
Batch: 160; loss: 0.41; acc: 0.88
Batch: 180; loss: 0.41; acc: 0.83
Batch: 200; loss: 0.78; acc: 0.77
Batch: 220; loss: 0.48; acc: 0.83
Batch: 240; loss: 0.28; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.91
Batch: 280; loss: 0.49; acc: 0.89
Batch: 300; loss: 0.31; acc: 0.92
Batch: 320; loss: 0.37; acc: 0.89
Batch: 340; loss: 0.54; acc: 0.8
Batch: 360; loss: 0.48; acc: 0.89
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.32; acc: 0.86
Batch: 420; loss: 0.41; acc: 0.89
Batch: 440; loss: 0.45; acc: 0.86
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.41; acc: 0.81
Batch: 500; loss: 0.56; acc: 0.78
Batch: 520; loss: 0.53; acc: 0.81
Batch: 540; loss: 0.42; acc: 0.84
Batch: 560; loss: 0.44; acc: 0.89
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.74; acc: 0.81
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.57; acc: 0.78
Batch: 660; loss: 0.47; acc: 0.86
Batch: 680; loss: 0.48; acc: 0.91
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.41; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.37; acc: 0.92
Batch: 20; loss: 0.57; acc: 0.86
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.92
Val Epoch over. val_loss: 0.41813785160423084; val_accuracy: 0.8747014331210191 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.39; acc: 0.84
Batch: 100; loss: 0.5; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.44; acc: 0.84
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.28; acc: 0.92
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.83; acc: 0.73
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.28; acc: 0.89
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.41; acc: 0.89
Batch: 340; loss: 0.39; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.55; acc: 0.88
Batch: 400; loss: 0.62; acc: 0.75
Batch: 420; loss: 0.29; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.45; acc: 0.89
Batch: 480; loss: 0.37; acc: 0.86
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.89; acc: 0.77
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.32; acc: 0.92
Batch: 580; loss: 0.68; acc: 0.83
Batch: 600; loss: 0.58; acc: 0.86
Batch: 620; loss: 0.58; acc: 0.83
Batch: 640; loss: 0.49; acc: 0.83
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.74; acc: 0.78
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.56; acc: 0.81
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.77; acc: 0.83
Batch: 780; loss: 0.9; acc: 0.78
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.88
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.41572300368433546; val_accuracy: 0.8726114649681529 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.42; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.34; acc: 0.94
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.51; acc: 0.81
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.88
Batch: 180; loss: 0.66; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.91
Batch: 220; loss: 0.49; acc: 0.83
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.68; acc: 0.73
Batch: 280; loss: 0.46; acc: 0.86
Batch: 300; loss: 0.64; acc: 0.8
Batch: 320; loss: 0.33; acc: 0.89
Batch: 340; loss: 0.65; acc: 0.81
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.33; acc: 0.91
Batch: 420; loss: 0.43; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.91
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.47; acc: 0.84
Batch: 500; loss: 0.49; acc: 0.83
Batch: 520; loss: 0.47; acc: 0.83
Batch: 540; loss: 0.47; acc: 0.86
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.65; acc: 0.77
Batch: 600; loss: 0.55; acc: 0.81
Batch: 620; loss: 0.46; acc: 0.83
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.46; acc: 0.86
Batch: 700; loss: 0.65; acc: 0.8
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.36; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.89
Batch: 780; loss: 0.53; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.7; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.92
Val Epoch over. val_loss: 0.41786182477216055; val_accuracy: 0.8728105095541401 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.83
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.81
Batch: 80; loss: 0.63; acc: 0.81
Batch: 100; loss: 0.38; acc: 0.81
Batch: 120; loss: 0.51; acc: 0.81
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.48; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.84
Batch: 200; loss: 0.58; acc: 0.8
Batch: 220; loss: 0.45; acc: 0.86
Batch: 240; loss: 0.43; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.53; acc: 0.84
Batch: 320; loss: 0.54; acc: 0.84
Batch: 340; loss: 0.42; acc: 0.89
Batch: 360; loss: 0.84; acc: 0.86
Batch: 380; loss: 0.54; acc: 0.83
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.54; acc: 0.81
Batch: 460; loss: 0.47; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.47; acc: 0.89
Batch: 520; loss: 0.79; acc: 0.8
Batch: 540; loss: 0.49; acc: 0.94
Batch: 560; loss: 0.42; acc: 0.86
Batch: 580; loss: 0.67; acc: 0.86
Batch: 600; loss: 0.3; acc: 0.91
Batch: 620; loss: 0.53; acc: 0.88
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.39; acc: 0.88
Batch: 700; loss: 0.46; acc: 0.89
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.94
Batch: 760; loss: 0.51; acc: 0.78
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.4199214294837539; val_accuracy: 0.8736066878980892 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.66; acc: 0.8
Batch: 20; loss: 0.54; acc: 0.81
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.58; acc: 0.84
Batch: 100; loss: 0.65; acc: 0.78
Batch: 120; loss: 0.36; acc: 0.84
Batch: 140; loss: 0.47; acc: 0.81
Batch: 160; loss: 0.43; acc: 0.84
Batch: 180; loss: 0.52; acc: 0.8
Batch: 200; loss: 0.43; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.91
Batch: 240; loss: 0.38; acc: 0.84
Batch: 260; loss: 0.47; acc: 0.89
Batch: 280; loss: 0.47; acc: 0.88
Batch: 300; loss: 0.63; acc: 0.78
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.44; acc: 0.84
Batch: 360; loss: 0.31; acc: 0.86
Batch: 380; loss: 0.6; acc: 0.84
Batch: 400; loss: 0.47; acc: 0.88
Batch: 420; loss: 0.63; acc: 0.88
Batch: 440; loss: 0.43; acc: 0.88
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.6; acc: 0.81
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.51; acc: 0.83
Batch: 580; loss: 0.61; acc: 0.86
Batch: 600; loss: 0.56; acc: 0.86
Batch: 620; loss: 0.46; acc: 0.81
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.34; acc: 0.88
Batch: 680; loss: 0.47; acc: 0.89
Batch: 700; loss: 0.53; acc: 0.81
Batch: 720; loss: 0.61; acc: 0.81
Batch: 740; loss: 0.51; acc: 0.83
Batch: 760; loss: 0.21; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.61; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.73
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.41769124130914165; val_accuracy: 0.873109076433121 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.49; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.88
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.68; acc: 0.8
Batch: 120; loss: 0.3; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.86
Batch: 160; loss: 0.48; acc: 0.91
Batch: 180; loss: 0.49; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.41; acc: 0.86
Batch: 240; loss: 0.31; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.86
Batch: 280; loss: 0.54; acc: 0.81
Batch: 300; loss: 0.27; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.52; acc: 0.81
Batch: 360; loss: 0.42; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.45; acc: 0.88
Batch: 420; loss: 0.56; acc: 0.8
Batch: 440; loss: 0.5; acc: 0.88
Batch: 460; loss: 0.43; acc: 0.84
Batch: 480; loss: 0.51; acc: 0.8
Batch: 500; loss: 0.34; acc: 0.83
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.47; acc: 0.81
Batch: 580; loss: 0.47; acc: 0.83
Batch: 600; loss: 0.44; acc: 0.91
Batch: 620; loss: 0.63; acc: 0.81
Batch: 640; loss: 0.37; acc: 0.91
Batch: 660; loss: 0.57; acc: 0.8
Batch: 680; loss: 0.47; acc: 0.86
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.58; acc: 0.83
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.32; acc: 0.89
Batch: 780; loss: 0.63; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.71; acc: 0.73
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.41913567407495655; val_accuracy: 0.8744028662420382 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.37; acc: 0.86
Batch: 40; loss: 0.61; acc: 0.83
Batch: 60; loss: 0.61; acc: 0.78
Batch: 80; loss: 0.53; acc: 0.84
Batch: 100; loss: 0.77; acc: 0.78
Batch: 120; loss: 0.37; acc: 0.89
Batch: 140; loss: 0.32; acc: 0.91
Batch: 160; loss: 0.41; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.91
Batch: 200; loss: 0.3; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.89
Batch: 240; loss: 0.4; acc: 0.88
Batch: 260; loss: 0.72; acc: 0.8
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.52; acc: 0.83
Batch: 320; loss: 0.46; acc: 0.77
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.86
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.8
Batch: 500; loss: 0.54; acc: 0.8
Batch: 520; loss: 0.43; acc: 0.83
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.68; acc: 0.81
Batch: 640; loss: 0.46; acc: 0.91
Batch: 660; loss: 0.76; acc: 0.8
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.7; acc: 0.83
Batch: 720; loss: 0.59; acc: 0.81
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.52; acc: 0.83
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.83
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.68; acc: 0.77
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4276844276364442; val_accuracy: 0.8691281847133758 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.53; acc: 0.81
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.42; acc: 0.84
Batch: 60; loss: 0.45; acc: 0.83
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.43; acc: 0.84
Batch: 120; loss: 0.56; acc: 0.83
Batch: 140; loss: 0.44; acc: 0.86
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.43; acc: 0.91
Batch: 200; loss: 0.87; acc: 0.78
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.45; acc: 0.88
Batch: 260; loss: 0.54; acc: 0.81
Batch: 280; loss: 0.68; acc: 0.81
Batch: 300; loss: 0.36; acc: 0.92
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.34; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.6; acc: 0.83
Batch: 400; loss: 0.68; acc: 0.86
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.55; acc: 0.8
Batch: 460; loss: 0.95; acc: 0.72
Batch: 480; loss: 0.25; acc: 0.94
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.56; acc: 0.81
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.45; acc: 0.84
Batch: 600; loss: 0.6; acc: 0.81
Batch: 620; loss: 0.33; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.41; acc: 0.86
Batch: 680; loss: 0.61; acc: 0.8
Batch: 700; loss: 0.32; acc: 0.91
Batch: 720; loss: 0.62; acc: 0.86
Batch: 740; loss: 0.32; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.38; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.45; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.6; acc: 0.77
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.4222643980934362; val_accuracy: 0.8722133757961783 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.84
Batch: 20; loss: 0.58; acc: 0.78
Batch: 40; loss: 0.35; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.84
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.37; acc: 0.88
Batch: 140; loss: 0.27; acc: 0.94
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.88
Batch: 200; loss: 0.63; acc: 0.77
Batch: 220; loss: 0.64; acc: 0.81
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.66; acc: 0.84
Batch: 280; loss: 0.53; acc: 0.88
Batch: 300; loss: 0.41; acc: 0.86
Batch: 320; loss: 0.62; acc: 0.77
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.86
Batch: 400; loss: 0.57; acc: 0.88
Batch: 420; loss: 0.58; acc: 0.86
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.48; acc: 0.84
Batch: 480; loss: 0.54; acc: 0.86
Batch: 500; loss: 0.31; acc: 0.88
Batch: 520; loss: 0.46; acc: 0.83
Batch: 540; loss: 0.57; acc: 0.77
Batch: 560; loss: 0.25; acc: 0.92
Batch: 580; loss: 0.39; acc: 0.89
Batch: 600; loss: 0.52; acc: 0.83
Batch: 620; loss: 0.42; acc: 0.89
Batch: 640; loss: 0.72; acc: 0.8
Batch: 660; loss: 0.63; acc: 0.8
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.61; acc: 0.84
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.45; acc: 0.89
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.33; acc: 0.86
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.7; acc: 0.7
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.42535503901493776; val_accuracy: 0.8700238853503185 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.49; acc: 0.81
Batch: 20; loss: 0.43; acc: 0.89
Batch: 40; loss: 0.77; acc: 0.77
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 0.39; acc: 0.91
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.24; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.4; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.49; acc: 0.83
Batch: 300; loss: 0.32; acc: 0.86
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.39; acc: 0.84
Batch: 360; loss: 0.4; acc: 0.88
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.94
Batch: 420; loss: 0.45; acc: 0.88
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.57; acc: 0.83
Batch: 480; loss: 0.66; acc: 0.81
Batch: 500; loss: 0.52; acc: 0.81
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.52; acc: 0.81
Batch: 560; loss: 0.33; acc: 0.92
Batch: 580; loss: 0.4; acc: 0.86
Batch: 600; loss: 0.51; acc: 0.8
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.4; acc: 0.84
Batch: 700; loss: 0.38; acc: 0.86
Batch: 720; loss: 0.54; acc: 0.84
Batch: 740; loss: 0.57; acc: 0.92
Batch: 760; loss: 0.72; acc: 0.83
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.41; acc: 0.89
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.72
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4138692948658755; val_accuracy: 0.8748009554140127 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.5; acc: 0.84
Batch: 20; loss: 0.3; acc: 0.94
Batch: 40; loss: 0.53; acc: 0.84
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.47; acc: 0.86
Batch: 100; loss: 0.46; acc: 0.83
Batch: 120; loss: 0.32; acc: 0.89
Batch: 140; loss: 0.53; acc: 0.86
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.42; acc: 0.88
Batch: 200; loss: 0.61; acc: 0.83
Batch: 220; loss: 0.54; acc: 0.88
Batch: 240; loss: 0.26; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.57; acc: 0.8
Batch: 300; loss: 0.52; acc: 0.88
Batch: 320; loss: 0.39; acc: 0.88
Batch: 340; loss: 0.46; acc: 0.86
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.52; acc: 0.8
Batch: 420; loss: 0.59; acc: 0.86
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.46; acc: 0.88
Batch: 480; loss: 0.57; acc: 0.83
Batch: 500; loss: 0.34; acc: 0.88
Batch: 520; loss: 0.36; acc: 0.91
Batch: 540; loss: 0.51; acc: 0.81
Batch: 560; loss: 0.35; acc: 0.89
Batch: 580; loss: 0.43; acc: 0.84
Batch: 600; loss: 0.67; acc: 0.81
Batch: 620; loss: 0.6; acc: 0.78
Batch: 640; loss: 0.63; acc: 0.81
Batch: 660; loss: 0.4; acc: 0.91
Batch: 680; loss: 0.94; acc: 0.7
Batch: 700; loss: 0.4; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.78
Batch: 740; loss: 0.63; acc: 0.83
Batch: 760; loss: 0.5; acc: 0.8
Batch: 780; loss: 0.56; acc: 0.81
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.58; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.75
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.4121512861768152; val_accuracy: 0.8757961783439491 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.51; acc: 0.86
Batch: 20; loss: 0.68; acc: 0.72
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.84
Batch: 80; loss: 0.42; acc: 0.89
Batch: 100; loss: 0.53; acc: 0.84
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.5; acc: 0.83
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.84
Batch: 240; loss: 0.4; acc: 0.91
Batch: 260; loss: 0.64; acc: 0.83
Batch: 280; loss: 0.57; acc: 0.86
Batch: 300; loss: 0.55; acc: 0.86
Batch: 320; loss: 0.47; acc: 0.81
Batch: 340; loss: 0.39; acc: 0.88
Batch: 360; loss: 0.64; acc: 0.77
Batch: 380; loss: 0.43; acc: 0.81
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.54; acc: 0.83
Batch: 440; loss: 0.49; acc: 0.88
Batch: 460; loss: 0.55; acc: 0.91
Batch: 480; loss: 0.71; acc: 0.78
Batch: 500; loss: 0.61; acc: 0.84
Batch: 520; loss: 0.44; acc: 0.88
Batch: 540; loss: 0.74; acc: 0.83
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.41; acc: 0.89
Batch: 600; loss: 0.47; acc: 0.88
Batch: 620; loss: 0.47; acc: 0.83
Batch: 640; loss: 0.43; acc: 0.89
Batch: 660; loss: 0.5; acc: 0.86
Batch: 680; loss: 0.18; acc: 0.97
Batch: 700; loss: 0.37; acc: 0.89
Batch: 720; loss: 0.46; acc: 0.88
Batch: 740; loss: 0.49; acc: 0.81
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.41; acc: 0.81
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.43; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.72
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.4141212571768244; val_accuracy: 0.8741042993630573 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.52; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.72; acc: 0.78
Batch: 100; loss: 0.48; acc: 0.86
Batch: 120; loss: 0.56; acc: 0.77
Batch: 140; loss: 0.3; acc: 0.89
Batch: 160; loss: 0.79; acc: 0.75
Batch: 180; loss: 0.38; acc: 0.89
Batch: 200; loss: 0.5; acc: 0.86
Batch: 220; loss: 0.57; acc: 0.75
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.41; acc: 0.81
Batch: 300; loss: 0.62; acc: 0.83
Batch: 320; loss: 0.69; acc: 0.81
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.67; acc: 0.81
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.51; acc: 0.84
Batch: 420; loss: 0.52; acc: 0.77
Batch: 440; loss: 0.39; acc: 0.89
Batch: 460; loss: 0.45; acc: 0.86
Batch: 480; loss: 0.56; acc: 0.88
Batch: 500; loss: 0.46; acc: 0.81
Batch: 520; loss: 0.59; acc: 0.78
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.64; acc: 0.83
Batch: 580; loss: 0.62; acc: 0.81
Batch: 600; loss: 0.29; acc: 0.88
Batch: 620; loss: 0.51; acc: 0.83
Batch: 640; loss: 0.6; acc: 0.81
Batch: 660; loss: 0.5; acc: 0.81
Batch: 680; loss: 0.57; acc: 0.81
Batch: 700; loss: 0.45; acc: 0.84
Batch: 720; loss: 0.54; acc: 0.86
Batch: 740; loss: 0.41; acc: 0.89
Batch: 760; loss: 0.31; acc: 0.91
Batch: 780; loss: 0.54; acc: 0.81
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.18; acc: 0.91
Val Epoch over. val_loss: 0.4125325062851997; val_accuracy: 0.87609474522293 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.45; acc: 0.8
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.45; acc: 0.81
Batch: 60; loss: 0.34; acc: 0.84
Batch: 80; loss: 0.41; acc: 0.89
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.88
Batch: 180; loss: 0.69; acc: 0.78
Batch: 200; loss: 0.55; acc: 0.84
Batch: 220; loss: 0.25; acc: 0.91
Batch: 240; loss: 0.67; acc: 0.8
Batch: 260; loss: 0.42; acc: 0.89
Batch: 280; loss: 0.62; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.89
Batch: 320; loss: 0.48; acc: 0.86
Batch: 340; loss: 0.67; acc: 0.8
Batch: 360; loss: 0.3; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.43; acc: 0.84
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.49; acc: 0.81
Batch: 460; loss: 0.67; acc: 0.81
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.46; acc: 0.86
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.22; acc: 0.98
Batch: 560; loss: 0.41; acc: 0.92
Batch: 580; loss: 0.46; acc: 0.84
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.53; acc: 0.86
Batch: 660; loss: 0.53; acc: 0.86
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.53; acc: 0.86
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.42; acc: 0.83
Batch: 760; loss: 0.55; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.43; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.7
Batch: 140; loss: 0.17; acc: 0.95
Val Epoch over. val_loss: 0.4142181240259462; val_accuracy: 0.8754976114649682 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.47; acc: 0.81
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.69; acc: 0.72
Batch: 60; loss: 0.57; acc: 0.83
Batch: 80; loss: 0.36; acc: 0.92
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.6; acc: 0.83
Batch: 140; loss: 0.4; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.92
Batch: 180; loss: 0.39; acc: 0.86
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.39; acc: 0.86
Batch: 240; loss: 0.48; acc: 0.84
Batch: 260; loss: 0.49; acc: 0.81
Batch: 280; loss: 0.27; acc: 0.91
Batch: 300; loss: 0.46; acc: 0.86
Batch: 320; loss: 0.48; acc: 0.88
Batch: 340; loss: 0.5; acc: 0.84
Batch: 360; loss: 0.38; acc: 0.88
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.43; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.89
Batch: 440; loss: 0.48; acc: 0.83
Batch: 460; loss: 0.45; acc: 0.81
Batch: 480; loss: 0.5; acc: 0.86
Batch: 500; loss: 0.42; acc: 0.88
Batch: 520; loss: 0.42; acc: 0.86
Batch: 540; loss: 0.51; acc: 0.8
Batch: 560; loss: 0.55; acc: 0.86
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.63; acc: 0.81
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.28; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.42; acc: 0.92
Batch: 740; loss: 0.41; acc: 0.91
Batch: 760; loss: 0.67; acc: 0.78
Batch: 780; loss: 0.33; acc: 0.88
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.36; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.89
Batch: 100; loss: 0.3; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.72
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.41397815496678564; val_accuracy: 0.8757961783439491 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.53; acc: 0.86
Batch: 20; loss: 0.54; acc: 0.77
Batch: 40; loss: 0.43; acc: 0.84
Batch: 60; loss: 0.51; acc: 0.86
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.34; acc: 0.91
Batch: 120; loss: 0.7; acc: 0.78
Batch: 140; loss: 0.63; acc: 0.88
Batch: 160; loss: 0.51; acc: 0.81
Batch: 180; loss: 0.51; acc: 0.86
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.28; acc: 0.92
Batch: 240; loss: 0.32; acc: 0.89
Batch: 260; loss: 0.36; acc: 0.88
Batch: 280; loss: 0.73; acc: 0.84
Batch: 300; loss: 0.48; acc: 0.81
Batch: 320; loss: 0.52; acc: 0.84
Batch: 340; loss: 0.53; acc: 0.86
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.44; acc: 0.86
Batch: 420; loss: 0.39; acc: 0.88
Batch: 440; loss: 0.45; acc: 0.84
Batch: 460; loss: 0.51; acc: 0.83
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.45; acc: 0.84
Batch: 520; loss: 0.55; acc: 0.84
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.58; acc: 0.81
Batch: 580; loss: 0.44; acc: 0.91
Batch: 600; loss: 0.36; acc: 0.84
Batch: 620; loss: 0.36; acc: 0.91
Batch: 640; loss: 0.49; acc: 0.81
Batch: 660; loss: 0.55; acc: 0.86
Batch: 680; loss: 0.65; acc: 0.78
Batch: 700; loss: 0.41; acc: 0.86
Batch: 720; loss: 0.52; acc: 0.88
Batch: 740; loss: 0.52; acc: 0.78
Batch: 760; loss: 0.33; acc: 0.92
Batch: 780; loss: 0.46; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.59; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.66; acc: 0.72
Batch: 140; loss: 0.18; acc: 0.92
Val Epoch over. val_loss: 0.4110907932185823; val_accuracy: 0.8763933121019108 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.69; acc: 0.83
Batch: 20; loss: 0.39; acc: 0.91
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.88
Batch: 80; loss: 0.63; acc: 0.8
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.56; acc: 0.77
Batch: 140; loss: 0.61; acc: 0.86
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.22; acc: 0.95
Batch: 200; loss: 0.4; acc: 0.89
Batch: 220; loss: 0.6; acc: 0.81
Batch: 240; loss: 0.61; acc: 0.86
Batch: 260; loss: 0.29; acc: 0.88
Batch: 280; loss: 0.39; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.89
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.44; acc: 0.8
Batch: 360; loss: 0.3; acc: 0.92
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.38; acc: 0.86
Batch: 420; loss: 0.89; acc: 0.78
Batch: 440; loss: 0.5; acc: 0.83
Batch: 460; loss: 0.56; acc: 0.86
Batch: 480; loss: 0.4; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.8
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.68; acc: 0.81
Batch: 560; loss: 0.73; acc: 0.78
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.83
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.86
Batch: 660; loss: 0.45; acc: 0.84
Batch: 680; loss: 0.32; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.8
Batch: 740; loss: 0.5; acc: 0.84
Batch: 760; loss: 0.53; acc: 0.81
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.94
Batch: 20; loss: 0.58; acc: 0.84
Batch: 40; loss: 0.41; acc: 0.88
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.65; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4129435408172334; val_accuracy: 0.8744028662420382 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.81
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.46; acc: 0.89
Batch: 80; loss: 0.52; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.83
Batch: 120; loss: 0.33; acc: 0.84
Batch: 140; loss: 0.36; acc: 0.88
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.7; acc: 0.73
Batch: 200; loss: 0.49; acc: 0.81
Batch: 220; loss: 0.43; acc: 0.88
Batch: 240; loss: 0.7; acc: 0.83
Batch: 260; loss: 0.25; acc: 0.89
Batch: 280; loss: 0.51; acc: 0.81
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.62; acc: 0.73
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.38; acc: 0.91
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.54; acc: 0.86
Batch: 420; loss: 0.51; acc: 0.84
Batch: 440; loss: 0.42; acc: 0.8
Batch: 460; loss: 0.38; acc: 0.86
Batch: 480; loss: 0.35; acc: 0.88
Batch: 500; loss: 0.5; acc: 0.83
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.86
Batch: 560; loss: 0.24; acc: 0.91
Batch: 580; loss: 0.5; acc: 0.8
Batch: 600; loss: 0.48; acc: 0.86
Batch: 620; loss: 0.57; acc: 0.89
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.47; acc: 0.83
Batch: 680; loss: 0.42; acc: 0.88
Batch: 700; loss: 0.69; acc: 0.81
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.42; acc: 0.86
Batch: 760; loss: 0.54; acc: 0.86
Batch: 780; loss: 0.43; acc: 0.91
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.86
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.28; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.72
Batch: 140; loss: 0.18; acc: 0.94
Val Epoch over. val_loss: 0.4123183615078592; val_accuracy: 0.8744028662420382 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.32; acc: 0.89
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.84
Batch: 80; loss: 0.38; acc: 0.84
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.59; acc: 0.81
Batch: 140; loss: 0.55; acc: 0.88
Batch: 160; loss: 0.57; acc: 0.78
Batch: 180; loss: 0.36; acc: 0.89
Batch: 200; loss: 0.37; acc: 0.84
Batch: 220; loss: 0.66; acc: 0.78
Batch: 240; loss: 0.41; acc: 0.88
Batch: 260; loss: 0.48; acc: 0.83
Batch: 280; loss: 0.52; acc: 0.86
Batch: 300; loss: 0.61; acc: 0.77
Batch: 320; loss: 0.38; acc: 0.84
Batch: 340; loss: 0.54; acc: 0.84
Batch: 360; loss: 0.37; acc: 0.86
Batch: 380; loss: 0.53; acc: 0.83
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.53; acc: 0.81
Batch: 440; loss: 0.6; acc: 0.86
Batch: 460; loss: 0.65; acc: 0.83
Batch: 480; loss: 0.46; acc: 0.83
Batch: 500; loss: 0.49; acc: 0.81
Batch: 520; loss: 0.59; acc: 0.81
Batch: 540; loss: 0.45; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.94
Batch: 580; loss: 0.44; acc: 0.88
Batch: 600; loss: 0.22; acc: 0.95
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.78
Batch: 660; loss: 0.36; acc: 0.88
Batch: 680; loss: 0.6; acc: 0.86
Batch: 700; loss: 0.52; acc: 0.86
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.33; acc: 0.91
Batch: 780; loss: 0.57; acc: 0.83
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.35; acc: 0.95
Batch: 20; loss: 0.59; acc: 0.84
Batch: 40; loss: 0.42; acc: 0.88
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.29; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.67; acc: 0.72
Batch: 140; loss: 0.17; acc: 0.94
Val Epoch over. val_loss: 0.4125325432059112; val_accuracy: 0.8762937898089171 

plots/subspace_training/reg_lenet_2/2020-01-19 19:11:48/d_dim_200_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 42532
elements in E: 5932200
fraction nonzero: 0.007169684096962341
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.31; acc: 0.05
Batch: 140; loss: 2.29; acc: 0.09
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.31; acc: 0.16
Batch: 200; loss: 2.29; acc: 0.23
Batch: 220; loss: 2.29; acc: 0.27
Batch: 240; loss: 2.3; acc: 0.16
Batch: 260; loss: 2.28; acc: 0.22
Batch: 280; loss: 2.3; acc: 0.09
Batch: 300; loss: 2.29; acc: 0.16
Batch: 320; loss: 2.29; acc: 0.14
Batch: 340; loss: 2.3; acc: 0.14
Batch: 360; loss: 2.29; acc: 0.09
Batch: 380; loss: 2.26; acc: 0.14
Batch: 400; loss: 2.25; acc: 0.14
Batch: 420; loss: 2.27; acc: 0.14
Batch: 440; loss: 2.25; acc: 0.16
Batch: 460; loss: 2.16; acc: 0.27
Batch: 480; loss: 2.25; acc: 0.19
Batch: 500; loss: 2.12; acc: 0.22
Batch: 520; loss: 2.21; acc: 0.14
Batch: 540; loss: 1.99; acc: 0.28
Batch: 560; loss: 1.95; acc: 0.36
Batch: 580; loss: 1.81; acc: 0.41
Batch: 600; loss: 1.89; acc: 0.36
Batch: 620; loss: 1.67; acc: 0.39
Batch: 640; loss: 1.65; acc: 0.41
Batch: 660; loss: 2.03; acc: 0.34
Batch: 680; loss: 1.64; acc: 0.42
Batch: 700; loss: 1.56; acc: 0.5
Batch: 720; loss: 1.4; acc: 0.55
Batch: 740; loss: 1.88; acc: 0.39
Batch: 760; loss: 1.46; acc: 0.59
Batch: 780; loss: 1.42; acc: 0.52
Train Epoch over. train_loss: 2.07; train_accuracy: 0.25 

Batch: 0; loss: 1.61; acc: 0.36
Batch: 20; loss: 2.02; acc: 0.3
Batch: 40; loss: 1.43; acc: 0.45
Batch: 60; loss: 1.55; acc: 0.42
Batch: 80; loss: 1.55; acc: 0.42
Batch: 100; loss: 1.6; acc: 0.34
Batch: 120; loss: 1.8; acc: 0.36
Batch: 140; loss: 1.92; acc: 0.3
Val Epoch over. val_loss: 1.6759229177122663; val_accuracy: 0.37589570063694266 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.61; acc: 0.48
Batch: 20; loss: 1.27; acc: 0.53
Batch: 40; loss: 1.15; acc: 0.62
Batch: 60; loss: 1.12; acc: 0.55
Batch: 80; loss: 0.93; acc: 0.72
Batch: 100; loss: 1.25; acc: 0.5
Batch: 120; loss: 1.66; acc: 0.39
Batch: 140; loss: 0.8; acc: 0.77
Batch: 160; loss: 1.22; acc: 0.58
Batch: 180; loss: 1.04; acc: 0.62
Batch: 200; loss: 1.28; acc: 0.55
Batch: 220; loss: 1.03; acc: 0.66
Batch: 240; loss: 1.08; acc: 0.67
Batch: 260; loss: 1.29; acc: 0.62
Batch: 280; loss: 0.76; acc: 0.78
Batch: 300; loss: 0.95; acc: 0.7
Batch: 320; loss: 0.93; acc: 0.64
Batch: 340; loss: 1.34; acc: 0.53
Batch: 360; loss: 0.42; acc: 0.86
Batch: 380; loss: 0.8; acc: 0.73
Batch: 400; loss: 0.8; acc: 0.7
Batch: 420; loss: 0.91; acc: 0.75
Batch: 440; loss: 0.71; acc: 0.8
Batch: 460; loss: 0.66; acc: 0.78
Batch: 480; loss: 0.92; acc: 0.69
Batch: 500; loss: 0.73; acc: 0.77
Batch: 520; loss: 0.67; acc: 0.78
Batch: 540; loss: 0.52; acc: 0.83
Batch: 560; loss: 0.65; acc: 0.78
Batch: 580; loss: 0.63; acc: 0.77
Batch: 600; loss: 0.82; acc: 0.75
Batch: 620; loss: 0.56; acc: 0.81
Batch: 640; loss: 0.63; acc: 0.78
Batch: 660; loss: 0.76; acc: 0.78
Batch: 680; loss: 0.66; acc: 0.77
Batch: 700; loss: 0.73; acc: 0.73
Batch: 720; loss: 0.89; acc: 0.7
Batch: 740; loss: 0.6; acc: 0.77
Batch: 760; loss: 0.74; acc: 0.75
Batch: 780; loss: 0.6; acc: 0.75
Train Epoch over. train_loss: 0.92; train_accuracy: 0.7 

Batch: 0; loss: 1.11; acc: 0.62
Batch: 20; loss: 2.03; acc: 0.5
Batch: 40; loss: 0.9; acc: 0.75
Batch: 60; loss: 1.09; acc: 0.64
Batch: 80; loss: 1.04; acc: 0.69
Batch: 100; loss: 1.42; acc: 0.59
Batch: 120; loss: 1.69; acc: 0.48
Batch: 140; loss: 1.15; acc: 0.64
Val Epoch over. val_loss: 1.3360957936116844; val_accuracy: 0.5979299363057324 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 1.56; acc: 0.53
Batch: 20; loss: 0.75; acc: 0.75
Batch: 40; loss: 0.56; acc: 0.78
Batch: 60; loss: 1.24; acc: 0.55
Batch: 80; loss: 0.52; acc: 0.86
Batch: 100; loss: 0.64; acc: 0.77
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.71; acc: 0.78
Batch: 160; loss: 0.71; acc: 0.78
Batch: 180; loss: 0.49; acc: 0.83
Batch: 200; loss: 0.6; acc: 0.84
Batch: 220; loss: 0.56; acc: 0.81
Batch: 240; loss: 0.35; acc: 0.92
Batch: 260; loss: 0.75; acc: 0.73
Batch: 280; loss: 0.53; acc: 0.86
Batch: 300; loss: 0.47; acc: 0.81
Batch: 320; loss: 0.64; acc: 0.8
Batch: 340; loss: 0.56; acc: 0.78
Batch: 360; loss: 0.65; acc: 0.78
Batch: 380; loss: 0.33; acc: 0.89
Batch: 400; loss: 0.74; acc: 0.69
Batch: 420; loss: 0.6; acc: 0.86
Batch: 440; loss: 0.81; acc: 0.7
Batch: 460; loss: 0.52; acc: 0.8
Batch: 480; loss: 1.0; acc: 0.8
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.52; acc: 0.78
Batch: 540; loss: 0.59; acc: 0.83
Batch: 560; loss: 0.48; acc: 0.78
Batch: 580; loss: 0.66; acc: 0.81
Batch: 600; loss: 0.71; acc: 0.77
Batch: 620; loss: 0.66; acc: 0.8
Batch: 640; loss: 0.54; acc: 0.81
Batch: 660; loss: 0.42; acc: 0.91
Batch: 680; loss: 0.54; acc: 0.78
Batch: 700; loss: 0.57; acc: 0.81
Batch: 720; loss: 0.47; acc: 0.88
Batch: 740; loss: 0.58; acc: 0.81
Batch: 760; loss: 0.58; acc: 0.81
Batch: 780; loss: 0.47; acc: 0.8
Train Epoch over. train_loss: 0.63; train_accuracy: 0.8 

Batch: 0; loss: 0.58; acc: 0.84
Batch: 20; loss: 1.46; acc: 0.64
Batch: 40; loss: 0.51; acc: 0.84
Batch: 60; loss: 0.52; acc: 0.77
Batch: 80; loss: 0.48; acc: 0.83
Batch: 100; loss: 0.77; acc: 0.77
Batch: 120; loss: 0.89; acc: 0.73
Batch: 140; loss: 0.69; acc: 0.78
Val Epoch over. val_loss: 0.7752488659825295; val_accuracy: 0.7621417197452229 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.79; acc: 0.77
Batch: 20; loss: 0.66; acc: 0.78
Batch: 40; loss: 0.58; acc: 0.75
Batch: 60; loss: 0.64; acc: 0.83
Batch: 80; loss: 0.58; acc: 0.83
Batch: 100; loss: 0.6; acc: 0.8
Batch: 120; loss: 0.66; acc: 0.8
Batch: 140; loss: 0.51; acc: 0.81
Batch: 160; loss: 0.92; acc: 0.75
Batch: 180; loss: 0.55; acc: 0.84
Batch: 200; loss: 0.56; acc: 0.81
Batch: 220; loss: 0.67; acc: 0.78
Batch: 240; loss: 0.6; acc: 0.83
Batch: 260; loss: 0.38; acc: 0.86
Batch: 280; loss: 0.77; acc: 0.83
Batch: 300; loss: 0.6; acc: 0.81
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.62; acc: 0.77
Batch: 360; loss: 0.36; acc: 0.86
Batch: 380; loss: 0.65; acc: 0.8
Batch: 400; loss: 0.43; acc: 0.81
Batch: 420; loss: 0.59; acc: 0.78
Batch: 440; loss: 1.29; acc: 0.67
Batch: 460; loss: 0.41; acc: 0.88
Batch: 480; loss: 0.47; acc: 0.83
Batch: 500; loss: 0.53; acc: 0.81
Batch: 520; loss: 0.5; acc: 0.81
Batch: 540; loss: 0.37; acc: 0.88
Batch: 560; loss: 0.54; acc: 0.81
Batch: 580; loss: 0.31; acc: 0.89
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.5; acc: 0.84
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.52; acc: 0.83
Batch: 720; loss: 0.35; acc: 0.88
Batch: 740; loss: 1.44; acc: 0.66
Batch: 760; loss: 0.48; acc: 0.86
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.68; acc: 0.78
Batch: 40; loss: 0.48; acc: 0.84
Batch: 60; loss: 0.71; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.83
Batch: 100; loss: 0.63; acc: 0.8
Batch: 120; loss: 1.11; acc: 0.7
Batch: 140; loss: 0.24; acc: 0.91
Val Epoch over. val_loss: 0.6232128553329759; val_accuracy: 0.8012539808917197 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.84; acc: 0.72
Batch: 20; loss: 0.64; acc: 0.77
Batch: 40; loss: 0.81; acc: 0.81
Batch: 60; loss: 0.42; acc: 0.86
Batch: 80; loss: 0.48; acc: 0.8
Batch: 100; loss: 0.67; acc: 0.86
Batch: 120; loss: 0.68; acc: 0.84
Batch: 140; loss: 0.46; acc: 0.88
Batch: 160; loss: 0.72; acc: 0.83
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.53; acc: 0.78
Batch: 220; loss: 0.62; acc: 0.8
Batch: 240; loss: 0.44; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.6; acc: 0.83
Batch: 300; loss: 0.61; acc: 0.86
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.64; acc: 0.81
Batch: 360; loss: 0.48; acc: 0.86
Batch: 380; loss: 0.49; acc: 0.86
Batch: 400; loss: 0.71; acc: 0.8
Batch: 420; loss: 0.59; acc: 0.8
Batch: 440; loss: 0.51; acc: 0.84
Batch: 460; loss: 0.75; acc: 0.81
Batch: 480; loss: 0.41; acc: 0.89
Batch: 500; loss: 0.33; acc: 0.86
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.55; acc: 0.88
Batch: 560; loss: 0.56; acc: 0.81
Batch: 580; loss: 0.5; acc: 0.81
Batch: 600; loss: 0.33; acc: 0.86
Batch: 620; loss: 0.59; acc: 0.81
Batch: 640; loss: 0.49; acc: 0.78
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.39; acc: 0.86
Batch: 700; loss: 0.68; acc: 0.75
Batch: 720; loss: 0.66; acc: 0.78
Batch: 740; loss: 0.41; acc: 0.84
Batch: 760; loss: 0.66; acc: 0.83
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.51; train_accuracy: 0.84 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.68; acc: 0.77
Batch: 40; loss: 0.37; acc: 0.88
Batch: 60; loss: 0.5; acc: 0.8
Batch: 80; loss: 0.45; acc: 0.88
Batch: 100; loss: 0.36; acc: 0.84
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.2; acc: 0.94
Val Epoch over. val_loss: 0.5098863002031472; val_accuracy: 0.8310111464968153 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.44; acc: 0.86
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.51; acc: 0.83
Batch: 80; loss: 0.53; acc: 0.86
Batch: 100; loss: 0.6; acc: 0.78
Batch: 120; loss: 1.08; acc: 0.73
Batch: 140; loss: 0.56; acc: 0.8
Batch: 160; loss: 0.65; acc: 0.8
Batch: 180; loss: 0.74; acc: 0.8
Batch: 200; loss: 0.64; acc: 0.86
Batch: 220; loss: 0.61; acc: 0.84
Batch: 240; loss: 0.61; acc: 0.77
Batch: 260; loss: 0.36; acc: 0.91
Batch: 280; loss: 0.65; acc: 0.78
Batch: 300; loss: 0.56; acc: 0.86
Batch: 320; loss: 0.58; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.86
Batch: 360; loss: 0.6; acc: 0.83
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.88
Batch: 420; loss: 0.65; acc: 0.83
Batch: 440; loss: 0.5; acc: 0.86
Batch: 460; loss: 0.26; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.94
Batch: 500; loss: 0.69; acc: 0.77
Batch: 520; loss: 0.61; acc: 0.83
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.33; acc: 0.91
Batch: 580; loss: 0.67; acc: 0.78
Batch: 600; loss: 0.67; acc: 0.83
Batch: 620; loss: 0.54; acc: 0.81
Batch: 640; loss: 0.37; acc: 0.88
Batch: 660; loss: 0.61; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.66; acc: 0.81
Batch: 720; loss: 0.66; acc: 0.77
Batch: 740; loss: 0.6; acc: 0.81
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.48; train_accuracy: 0.85 

Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.41; acc: 0.89
Batch: 40; loss: 0.31; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.95
Batch: 120; loss: 0.71; acc: 0.8
Batch: 140; loss: 0.14; acc: 0.97
Val Epoch over. val_loss: 0.4113632280165982; val_accuracy: 0.8743033439490446 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.7; acc: 0.8
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.91
Batch: 80; loss: 0.55; acc: 0.81
Batch: 100; loss: 0.48; acc: 0.81
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.7; acc: 0.8
Batch: 160; loss: 0.33; acc: 0.92
Batch: 180; loss: 0.44; acc: 0.84
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.35; acc: 0.88
Batch: 260; loss: 0.4; acc: 0.83
Batch: 280; loss: 0.69; acc: 0.77
Batch: 300; loss: 0.45; acc: 0.84
Batch: 320; loss: 0.45; acc: 0.88
Batch: 340; loss: 0.41; acc: 0.91
Batch: 360; loss: 0.62; acc: 0.78
Batch: 380; loss: 0.43; acc: 0.89
Batch: 400; loss: 0.5; acc: 0.83
Batch: 420; loss: 0.51; acc: 0.81
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.86; acc: 0.75
Batch: 480; loss: 0.42; acc: 0.92
Batch: 500; loss: 0.68; acc: 0.77
Batch: 520; loss: 0.53; acc: 0.83
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.46; acc: 0.86
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.86
Batch: 640; loss: 0.57; acc: 0.81
Batch: 660; loss: 0.53; acc: 0.83
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.54; acc: 0.83
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.6; acc: 0.78
Train Epoch over. train_loss: 0.47; train_accuracy: 0.85 

Batch: 0; loss: 1.59; acc: 0.64
Batch: 20; loss: 1.4; acc: 0.61
Batch: 40; loss: 1.56; acc: 0.7
Batch: 60; loss: 1.38; acc: 0.67
Batch: 80; loss: 1.56; acc: 0.7
Batch: 100; loss: 1.28; acc: 0.7
Batch: 120; loss: 1.49; acc: 0.67
Batch: 140; loss: 0.8; acc: 0.8
Val Epoch over. val_loss: 1.4453583522966713; val_accuracy: 0.666202229299363 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 1.36; acc: 0.66
Batch: 20; loss: 0.49; acc: 0.86
Batch: 40; loss: 0.6; acc: 0.88
Batch: 60; loss: 0.73; acc: 0.78
Batch: 80; loss: 0.43; acc: 0.89
Batch: 100; loss: 0.46; acc: 0.77
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.34; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.89
Batch: 180; loss: 0.91; acc: 0.72
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.58; acc: 0.81
Batch: 240; loss: 0.41; acc: 0.84
Batch: 260; loss: 0.33; acc: 0.88
Batch: 280; loss: 0.51; acc: 0.84
Batch: 300; loss: 0.37; acc: 0.91
Batch: 320; loss: 0.45; acc: 0.86
Batch: 340; loss: 0.59; acc: 0.88
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.88
Batch: 400; loss: 0.4; acc: 0.86
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.81
Batch: 480; loss: 0.42; acc: 0.84
Batch: 500; loss: 0.55; acc: 0.84
Batch: 520; loss: 0.59; acc: 0.86
Batch: 540; loss: 0.42; acc: 0.91
Batch: 560; loss: 0.4; acc: 0.84
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.25; acc: 0.97
Batch: 620; loss: 0.49; acc: 0.83
Batch: 640; loss: 0.65; acc: 0.77
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.41; acc: 0.89
Batch: 720; loss: 0.49; acc: 0.88
Batch: 740; loss: 0.25; acc: 0.94
Batch: 760; loss: 0.39; acc: 0.84
Batch: 780; loss: 0.6; acc: 0.84
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.47; acc: 0.83
Batch: 20; loss: 0.45; acc: 0.83
Batch: 40; loss: 0.42; acc: 0.86
Batch: 60; loss: 0.45; acc: 0.81
Batch: 80; loss: 0.56; acc: 0.84
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.73
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.4541999810630349; val_accuracy: 0.8482285031847133 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.36; acc: 0.81
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.55; acc: 0.91
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.64; acc: 0.81
Batch: 120; loss: 0.64; acc: 0.78
Batch: 140; loss: 0.99; acc: 0.72
Batch: 160; loss: 0.31; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.92
Batch: 220; loss: 0.57; acc: 0.83
Batch: 240; loss: 0.48; acc: 0.86
Batch: 260; loss: 0.33; acc: 0.91
Batch: 280; loss: 0.72; acc: 0.83
Batch: 300; loss: 0.47; acc: 0.83
Batch: 320; loss: 0.51; acc: 0.84
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.4; acc: 0.92
Batch: 380; loss: 0.54; acc: 0.86
Batch: 400; loss: 0.68; acc: 0.78
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.54; acc: 0.78
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.64; acc: 0.84
Batch: 500; loss: 0.36; acc: 0.86
Batch: 520; loss: 0.47; acc: 0.86
Batch: 540; loss: 0.43; acc: 0.86
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.57; acc: 0.81
Batch: 600; loss: 0.73; acc: 0.81
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.57; acc: 0.88
Batch: 660; loss: 0.56; acc: 0.78
Batch: 680; loss: 0.41; acc: 0.86
Batch: 700; loss: 0.37; acc: 0.92
Batch: 720; loss: 0.59; acc: 0.83
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.73; acc: 0.8
Batch: 780; loss: 0.45; acc: 0.84
Train Epoch over. train_loss: 0.45; train_accuracy: 0.86 

Batch: 0; loss: 0.33; acc: 0.89
Batch: 20; loss: 0.55; acc: 0.84
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.43; acc: 0.89
Batch: 120; loss: 0.95; acc: 0.75
Batch: 140; loss: 0.15; acc: 0.98
Val Epoch over. val_loss: 0.39706857991256533; val_accuracy: 0.8824641719745223 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.89
Batch: 40; loss: 0.29; acc: 0.95
Batch: 60; loss: 0.41; acc: 0.86
Batch: 80; loss: 0.51; acc: 0.83
Batch: 100; loss: 0.73; acc: 0.78
Batch: 120; loss: 0.46; acc: 0.91
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.48; acc: 0.81
Batch: 180; loss: 0.47; acc: 0.88
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.69; acc: 0.8
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.29; acc: 0.86
Batch: 280; loss: 0.4; acc: 0.89
Batch: 300; loss: 0.15; acc: 0.94
Batch: 320; loss: 0.7; acc: 0.81
Batch: 340; loss: 0.58; acc: 0.81
Batch: 360; loss: 0.56; acc: 0.86
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.83; acc: 0.73
Batch: 420; loss: 0.57; acc: 0.8
Batch: 440; loss: 0.78; acc: 0.81
Batch: 460; loss: 0.62; acc: 0.89
Batch: 480; loss: 0.43; acc: 0.83
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.43; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.84
Batch: 560; loss: 0.77; acc: 0.72
Batch: 580; loss: 0.44; acc: 0.84
Batch: 600; loss: 0.32; acc: 0.86
Batch: 620; loss: 0.33; acc: 0.86
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.97; acc: 0.66
Batch: 700; loss: 0.56; acc: 0.81
Batch: 720; loss: 0.37; acc: 0.86
Batch: 740; loss: 0.53; acc: 0.84
Batch: 760; loss: 0.76; acc: 0.81
Batch: 780; loss: 0.43; acc: 0.86
Train Epoch over. train_loss: 0.44; train_accuracy: 0.86 

Batch: 0; loss: 1.29; acc: 0.72
Batch: 20; loss: 1.62; acc: 0.66
Batch: 40; loss: 1.17; acc: 0.72
Batch: 60; loss: 1.09; acc: 0.8
Batch: 80; loss: 1.17; acc: 0.75
Batch: 100; loss: 1.38; acc: 0.72
Batch: 120; loss: 1.49; acc: 0.67
Batch: 140; loss: 1.21; acc: 0.77
Val Epoch over. val_loss: 1.3506487444707542; val_accuracy: 0.7126791401273885 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 1.19; acc: 0.73
Batch: 20; loss: 0.42; acc: 0.88
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.36; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.88
Batch: 120; loss: 0.25; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.95
Batch: 160; loss: 0.36; acc: 0.89
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.84
Batch: 220; loss: 0.37; acc: 0.92
Batch: 240; loss: 0.57; acc: 0.88
Batch: 260; loss: 0.42; acc: 0.86
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.37; acc: 0.88
Batch: 320; loss: 0.22; acc: 0.92
Batch: 340; loss: 0.36; acc: 0.88
Batch: 360; loss: 0.45; acc: 0.86
Batch: 380; loss: 0.32; acc: 0.94
Batch: 400; loss: 0.49; acc: 0.88
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.25; acc: 0.88
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.57; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.91
Batch: 540; loss: 0.27; acc: 0.91
Batch: 560; loss: 0.38; acc: 0.89
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.36; acc: 0.89
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.35; acc: 0.91
Batch: 660; loss: 0.37; acc: 0.88
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.51; acc: 0.83
Batch: 740; loss: 0.27; acc: 0.88
Batch: 760; loss: 0.34; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.36; train_accuracy: 0.89 

Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.16; acc: 0.95
Val Epoch over. val_loss: 0.3380326951859863; val_accuracy: 0.8969944267515924 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.47; acc: 0.86
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.54; acc: 0.89
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.27; acc: 0.88
Batch: 100; loss: 0.29; acc: 0.84
Batch: 120; loss: 0.46; acc: 0.83
Batch: 140; loss: 0.22; acc: 0.92
Batch: 160; loss: 0.54; acc: 0.81
Batch: 180; loss: 0.46; acc: 0.86
Batch: 200; loss: 0.27; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.95
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.97
Batch: 360; loss: 0.34; acc: 0.89
Batch: 380; loss: 0.51; acc: 0.88
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.5; acc: 0.83
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.28; acc: 0.94
Batch: 500; loss: 0.26; acc: 0.89
Batch: 520; loss: 0.34; acc: 0.89
Batch: 540; loss: 0.27; acc: 0.94
Batch: 560; loss: 0.36; acc: 0.91
Batch: 580; loss: 0.53; acc: 0.84
Batch: 600; loss: 0.3; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.84
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.83
Batch: 680; loss: 0.32; acc: 0.92
Batch: 700; loss: 0.28; acc: 0.88
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.62; acc: 0.8
Batch: 760; loss: 0.4; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.4; acc: 0.86
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.3889209573530847; val_accuracy: 0.8765923566878981 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.46; acc: 0.81
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.52; acc: 0.81
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.37; acc: 0.84
Batch: 120; loss: 0.25; acc: 0.94
Batch: 140; loss: 0.3; acc: 0.91
Batch: 160; loss: 0.44; acc: 0.81
Batch: 180; loss: 0.25; acc: 0.94
Batch: 200; loss: 0.21; acc: 0.95
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.83
Batch: 300; loss: 0.48; acc: 0.86
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.7; acc: 0.8
Batch: 400; loss: 0.33; acc: 0.88
Batch: 420; loss: 0.24; acc: 0.92
Batch: 440; loss: 0.36; acc: 0.86
Batch: 460; loss: 0.34; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.39; acc: 0.84
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.95
Batch: 580; loss: 0.42; acc: 0.91
Batch: 600; loss: 0.43; acc: 0.88
Batch: 620; loss: 0.57; acc: 0.83
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.25; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.81
Batch: 20; loss: 0.42; acc: 0.89
Batch: 40; loss: 0.37; acc: 0.83
Batch: 60; loss: 0.5; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.86
Batch: 100; loss: 0.54; acc: 0.81
Batch: 120; loss: 0.97; acc: 0.77
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.4817716060265614; val_accuracy: 0.841062898089172 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.43; acc: 0.83
Batch: 20; loss: 0.61; acc: 0.81
Batch: 40; loss: 0.39; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.66; acc: 0.81
Batch: 100; loss: 0.45; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.86
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.84
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.94
Batch: 260; loss: 0.38; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.88
Batch: 300; loss: 0.26; acc: 0.88
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.31; acc: 0.91
Batch: 360; loss: 0.34; acc: 0.91
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.35; acc: 0.91
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.89
Batch: 460; loss: 0.36; acc: 0.86
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.3; acc: 0.88
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.89
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.47; acc: 0.81
Batch: 600; loss: 0.24; acc: 0.89
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.49; acc: 0.88
Batch: 660; loss: 0.37; acc: 0.94
Batch: 680; loss: 0.26; acc: 0.89
Batch: 700; loss: 0.54; acc: 0.88
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.26; acc: 0.89
Batch: 760; loss: 0.4; acc: 0.84
Batch: 780; loss: 0.3; acc: 0.86
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.32; acc: 0.94
Batch: 20; loss: 0.4; acc: 0.88
Batch: 40; loss: 0.25; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.1; acc: 0.95
Val Epoch over. val_loss: 0.3353355843929728; val_accuracy: 0.8956011146496815 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.83
Batch: 60; loss: 0.42; acc: 0.84
Batch: 80; loss: 0.34; acc: 0.89
Batch: 100; loss: 0.23; acc: 0.83
Batch: 120; loss: 0.45; acc: 0.83
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.35; acc: 0.89
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.4; acc: 0.86
Batch: 280; loss: 0.32; acc: 0.88
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.42; acc: 0.88
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.19; acc: 0.92
Batch: 400; loss: 0.39; acc: 0.88
Batch: 420; loss: 0.38; acc: 0.88
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.3; acc: 0.89
Batch: 500; loss: 0.6; acc: 0.83
Batch: 520; loss: 0.5; acc: 0.84
Batch: 540; loss: 0.41; acc: 0.88
Batch: 560; loss: 0.42; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.54; acc: 0.86
Batch: 620; loss: 0.25; acc: 0.91
Batch: 640; loss: 0.35; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.88
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.42; acc: 0.89
Batch: 760; loss: 0.16; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.39; acc: 0.88
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.66; acc: 0.77
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.34072134079067573; val_accuracy: 0.8932125796178344 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.61; acc: 0.83
Batch: 40; loss: 0.36; acc: 0.86
Batch: 60; loss: 0.33; acc: 0.88
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.35; acc: 0.88
Batch: 140; loss: 0.33; acc: 0.84
Batch: 160; loss: 0.52; acc: 0.84
Batch: 180; loss: 0.34; acc: 0.89
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.44; acc: 0.88
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.46; acc: 0.84
Batch: 280; loss: 0.36; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.4; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.91
Batch: 360; loss: 0.31; acc: 0.89
Batch: 380; loss: 0.39; acc: 0.86
Batch: 400; loss: 0.41; acc: 0.86
Batch: 420; loss: 0.45; acc: 0.89
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.86
Batch: 480; loss: 0.39; acc: 0.89
Batch: 500; loss: 0.65; acc: 0.81
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.6; acc: 0.8
Batch: 560; loss: 0.43; acc: 0.88
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.58; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.91
Batch: 680; loss: 0.26; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.45; acc: 0.88
Batch: 740; loss: 0.26; acc: 0.97
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.47; acc: 0.84
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.84
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.81; acc: 0.75
Batch: 140; loss: 0.19; acc: 0.95
Val Epoch over. val_loss: 0.33933579945450376; val_accuracy: 0.8914211783439491 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.49; acc: 0.86
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.88
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.37; acc: 0.91
Batch: 180; loss: 0.39; acc: 0.89
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.4; acc: 0.84
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.47; acc: 0.84
Batch: 300; loss: 0.34; acc: 0.88
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.27; acc: 0.89
Batch: 360; loss: 0.28; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.94
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.49; acc: 0.83
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.49; acc: 0.8
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.51; acc: 0.84
Batch: 540; loss: 0.29; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.33; acc: 0.91
Batch: 600; loss: 0.39; acc: 0.91
Batch: 620; loss: 0.48; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.97
Batch: 660; loss: 0.25; acc: 0.89
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.51; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.88
Batch: 740; loss: 0.59; acc: 0.86
Batch: 760; loss: 0.44; acc: 0.88
Batch: 780; loss: 0.18; acc: 0.91
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.48; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.83
Batch: 40; loss: 0.32; acc: 0.91
Batch: 60; loss: 0.44; acc: 0.88
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.77
Batch: 140; loss: 0.14; acc: 0.95
Val Epoch over. val_loss: 0.36197697627506437; val_accuracy: 0.8851512738853503 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.45; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.94
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.88
Batch: 120; loss: 0.38; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.88
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.47; acc: 0.84
Batch: 220; loss: 0.65; acc: 0.81
Batch: 240; loss: 0.21; acc: 0.91
Batch: 260; loss: 0.21; acc: 0.89
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.45; acc: 0.88
Batch: 320; loss: 0.44; acc: 0.89
Batch: 340; loss: 0.28; acc: 0.91
Batch: 360; loss: 0.4; acc: 0.84
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.36; acc: 0.86
Batch: 440; loss: 0.48; acc: 0.89
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.58; acc: 0.84
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.45; acc: 0.91
Batch: 600; loss: 0.53; acc: 0.91
Batch: 620; loss: 0.43; acc: 0.86
Batch: 640; loss: 0.43; acc: 0.84
Batch: 660; loss: 0.27; acc: 0.91
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.18; acc: 0.94
Batch: 720; loss: 0.57; acc: 0.84
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.47; acc: 0.89
Batch: 780; loss: 0.38; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.5; acc: 0.8
Batch: 20; loss: 0.4; acc: 0.89
Batch: 40; loss: 0.32; acc: 0.89
Batch: 60; loss: 0.51; acc: 0.88
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.86
Batch: 120; loss: 1.0; acc: 0.75
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.4079067379615869; val_accuracy: 0.8701234076433121 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.66; acc: 0.77
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.27; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.38; acc: 0.84
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.37; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.88
Batch: 240; loss: 0.35; acc: 0.83
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.48; acc: 0.83
Batch: 300; loss: 0.16; acc: 0.97
Batch: 320; loss: 0.37; acc: 0.92
Batch: 340; loss: 0.43; acc: 0.91
Batch: 360; loss: 0.3; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.88
Batch: 400; loss: 0.2; acc: 0.94
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.38; acc: 0.84
Batch: 480; loss: 0.57; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.88
Batch: 520; loss: 0.45; acc: 0.88
Batch: 540; loss: 0.56; acc: 0.84
Batch: 560; loss: 0.37; acc: 0.86
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.34; acc: 0.91
Batch: 620; loss: 0.17; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.94
Batch: 660; loss: 0.19; acc: 0.94
Batch: 680; loss: 0.46; acc: 0.81
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.52; acc: 0.84
Batch: 740; loss: 0.34; acc: 0.89
Batch: 760; loss: 0.42; acc: 0.88
Batch: 780; loss: 0.65; acc: 0.8
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.49; acc: 0.89
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.4; acc: 0.83
Batch: 120; loss: 1.08; acc: 0.7
Batch: 140; loss: 0.16; acc: 0.97
Val Epoch over. val_loss: 0.38287262665997646; val_accuracy: 0.8775875796178344 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.81
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.54; acc: 0.83
Batch: 100; loss: 0.37; acc: 0.88
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.89
Batch: 160; loss: 0.45; acc: 0.84
Batch: 180; loss: 0.39; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.32; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.92
Batch: 280; loss: 0.39; acc: 0.86
Batch: 300; loss: 0.48; acc: 0.88
Batch: 320; loss: 0.31; acc: 0.91
Batch: 340; loss: 0.41; acc: 0.83
Batch: 360; loss: 0.44; acc: 0.86
Batch: 380; loss: 0.28; acc: 0.89
Batch: 400; loss: 0.3; acc: 0.89
Batch: 420; loss: 0.39; acc: 0.89
Batch: 440; loss: 0.38; acc: 0.86
Batch: 460; loss: 0.22; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.48; acc: 0.81
Batch: 540; loss: 0.31; acc: 0.89
Batch: 560; loss: 0.46; acc: 0.89
Batch: 580; loss: 0.53; acc: 0.8
Batch: 600; loss: 0.35; acc: 0.88
Batch: 620; loss: 0.31; acc: 0.92
Batch: 640; loss: 0.23; acc: 0.94
Batch: 660; loss: 0.38; acc: 0.84
Batch: 680; loss: 0.23; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.42; acc: 0.83
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.38; acc: 0.88
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.22; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.3; acc: 0.88
Batch: 120; loss: 0.93; acc: 0.72
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3264355919068786; val_accuracy: 0.8961982484076433 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.27; acc: 0.92
Batch: 40; loss: 0.47; acc: 0.86
Batch: 60; loss: 0.36; acc: 0.89
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.92
Batch: 180; loss: 0.2; acc: 0.94
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.28; acc: 0.91
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.86
Batch: 300; loss: 0.25; acc: 0.88
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.35; acc: 0.89
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.89
Batch: 500; loss: 0.67; acc: 0.84
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.19; acc: 0.97
Batch: 600; loss: 0.51; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.89
Batch: 660; loss: 0.33; acc: 0.92
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.4; acc: 0.88
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.92
Batch: 760; loss: 0.32; acc: 0.88
Batch: 780; loss: 0.29; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.42; acc: 0.92
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.32; acc: 0.88
Batch: 120; loss: 0.93; acc: 0.73
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.33303166899806375; val_accuracy: 0.894406847133758 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.1; acc: 0.98
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.52; acc: 0.84
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.28; acc: 0.91
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.52; acc: 0.81
Batch: 240; loss: 0.52; acc: 0.86
Batch: 260; loss: 0.27; acc: 0.91
Batch: 280; loss: 0.25; acc: 0.91
Batch: 300; loss: 0.24; acc: 0.89
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.45; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.88
Batch: 380; loss: 0.27; acc: 0.91
Batch: 400; loss: 0.44; acc: 0.84
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.84
Batch: 500; loss: 0.3; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.39; acc: 0.88
Batch: 560; loss: 0.46; acc: 0.84
Batch: 580; loss: 0.23; acc: 0.98
Batch: 600; loss: 0.41; acc: 0.83
Batch: 620; loss: 0.27; acc: 0.89
Batch: 640; loss: 0.44; acc: 0.86
Batch: 660; loss: 0.09; acc: 1.0
Batch: 680; loss: 0.24; acc: 0.92
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.3; acc: 0.83
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.32; acc: 0.91
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.34; acc: 0.91
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.28; acc: 0.89
Batch: 120; loss: 0.89; acc: 0.75
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.3094983550061466; val_accuracy: 0.9040605095541401 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.42; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.92
Batch: 40; loss: 0.59; acc: 0.86
Batch: 60; loss: 0.35; acc: 0.91
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.21; acc: 0.94
Batch: 220; loss: 0.52; acc: 0.84
Batch: 240; loss: 0.27; acc: 0.91
Batch: 260; loss: 0.26; acc: 0.91
Batch: 280; loss: 0.26; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.94
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.35; acc: 0.88
Batch: 380; loss: 0.4; acc: 0.86
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.39; acc: 0.88
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.23; acc: 0.95
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.43; acc: 0.86
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.3; acc: 0.91
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.28; acc: 0.88
Batch: 700; loss: 0.24; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.84
Batch: 740; loss: 0.26; acc: 0.92
Batch: 760; loss: 0.15; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.92
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.43; acc: 0.86
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.3110600896179676; val_accuracy: 0.903562898089172 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.49; acc: 0.88
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.98
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.35; acc: 0.92
Batch: 140; loss: 0.37; acc: 0.91
Batch: 160; loss: 0.31; acc: 0.92
Batch: 180; loss: 0.28; acc: 0.89
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.89
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.42; acc: 0.81
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.88
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.32; acc: 0.91
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.22; acc: 0.94
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.27; acc: 0.89
Batch: 480; loss: 0.6; acc: 0.84
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.09; acc: 0.98
Batch: 580; loss: 0.39; acc: 0.88
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.35; acc: 0.89
Batch: 680; loss: 0.28; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.28; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.88
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.39; acc: 0.86
Batch: 40; loss: 0.24; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.78; acc: 0.78
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.30986283835806666; val_accuracy: 0.9030652866242038 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.94
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.38; acc: 0.86
Batch: 60; loss: 0.22; acc: 0.94
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.39; acc: 0.91
Batch: 180; loss: 0.32; acc: 0.89
Batch: 200; loss: 0.35; acc: 0.86
Batch: 220; loss: 0.2; acc: 0.92
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.33; acc: 0.89
Batch: 280; loss: 0.34; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.91
Batch: 380; loss: 0.26; acc: 0.92
Batch: 400; loss: 0.46; acc: 0.88
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.41; acc: 0.86
Batch: 480; loss: 0.3; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.89
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.65; acc: 0.84
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.35; acc: 0.88
Batch: 640; loss: 0.42; acc: 0.84
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.35; acc: 0.92
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.32; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.91
Batch: 20; loss: 0.35; acc: 0.86
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.78
Batch: 140; loss: 0.11; acc: 0.97
Val Epoch over. val_loss: 0.3071261724326641; val_accuracy: 0.9031648089171974 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.4; acc: 0.84
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.91
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.38; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.25; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.47; acc: 0.86
Batch: 320; loss: 0.41; acc: 0.86
Batch: 340; loss: 0.32; acc: 0.92
Batch: 360; loss: 0.25; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.89
Batch: 400; loss: 0.06; acc: 1.0
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.55; acc: 0.84
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.4; acc: 0.86
Batch: 560; loss: 0.29; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.89
Batch: 640; loss: 0.4; acc: 0.86
Batch: 660; loss: 0.49; acc: 0.83
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.41; acc: 0.88
Batch: 720; loss: 0.28; acc: 0.91
Batch: 740; loss: 0.3; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.89
Batch: 780; loss: 0.17; acc: 0.97
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.29939789306016484; val_accuracy: 0.9070461783439491 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.43; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.64; acc: 0.8
Batch: 60; loss: 0.27; acc: 0.94
Batch: 80; loss: 0.28; acc: 0.92
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.43; acc: 0.86
Batch: 160; loss: 0.33; acc: 0.89
Batch: 180; loss: 0.44; acc: 0.89
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.23; acc: 0.91
Batch: 240; loss: 0.37; acc: 0.91
Batch: 260; loss: 0.73; acc: 0.78
Batch: 280; loss: 0.22; acc: 0.95
Batch: 300; loss: 0.37; acc: 0.89
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.45; acc: 0.86
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.39; acc: 0.89
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.39; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.89
Batch: 520; loss: 0.4; acc: 0.91
Batch: 540; loss: 0.44; acc: 0.86
Batch: 560; loss: 0.34; acc: 0.89
Batch: 580; loss: 0.98; acc: 0.81
Batch: 600; loss: 0.36; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.44; acc: 0.91
Batch: 660; loss: 0.45; acc: 0.89
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.22; acc: 0.94
Batch: 720; loss: 0.33; acc: 0.84
Batch: 740; loss: 0.43; acc: 0.89
Batch: 760; loss: 0.24; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.41; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.95
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.82; acc: 0.73
Batch: 140; loss: 0.11; acc: 0.95
Val Epoch over. val_loss: 0.29981658655177257; val_accuracy: 0.9049562101910829 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.45; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.39; acc: 0.84
Batch: 60; loss: 0.4; acc: 0.84
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.31; acc: 0.91
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.28; acc: 0.92
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.19; acc: 0.95
Batch: 260; loss: 0.3; acc: 0.88
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.4; acc: 0.86
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.44; acc: 0.89
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.86
Batch: 420; loss: 0.47; acc: 0.86
Batch: 440; loss: 0.27; acc: 0.91
Batch: 460; loss: 0.36; acc: 0.89
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.24; acc: 0.91
Batch: 560; loss: 0.49; acc: 0.84
Batch: 580; loss: 0.32; acc: 0.89
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.25; acc: 0.95
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.27; acc: 0.94
Batch: 680; loss: 0.37; acc: 0.95
Batch: 700; loss: 0.46; acc: 0.88
Batch: 720; loss: 0.27; acc: 0.89
Batch: 740; loss: 0.41; acc: 0.88
Batch: 760; loss: 0.43; acc: 0.88
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.35; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.22; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.89
Batch: 120; loss: 0.85; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.2997021586841838; val_accuracy: 0.9059514331210191 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.95
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.37; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.29; acc: 0.91
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.92
Batch: 200; loss: 0.5; acc: 0.89
Batch: 220; loss: 0.16; acc: 0.94
Batch: 240; loss: 0.46; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.97
Batch: 280; loss: 0.38; acc: 0.89
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.48; acc: 0.84
Batch: 340; loss: 0.25; acc: 0.95
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.26; acc: 0.92
Batch: 420; loss: 0.31; acc: 0.91
Batch: 440; loss: 0.46; acc: 0.83
Batch: 460; loss: 0.43; acc: 0.88
Batch: 480; loss: 0.25; acc: 0.92
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.44; acc: 0.83
Batch: 580; loss: 0.27; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.3; acc: 0.91
Batch: 700; loss: 0.16; acc: 0.94
Batch: 720; loss: 0.41; acc: 0.86
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.21; acc: 0.89
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.36; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.8; acc: 0.75
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.30017633636476126; val_accuracy: 0.9046576433121019 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.35; acc: 0.92
Batch: 40; loss: 0.45; acc: 0.89
Batch: 60; loss: 0.35; acc: 0.89
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.25; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.32; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.39; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.89
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.31; acc: 0.95
Batch: 340; loss: 0.36; acc: 0.89
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.49; acc: 0.81
Batch: 500; loss: 0.32; acc: 0.91
Batch: 520; loss: 0.2; acc: 0.92
Batch: 540; loss: 0.51; acc: 0.84
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.89
Batch: 600; loss: 0.26; acc: 0.97
Batch: 620; loss: 0.49; acc: 0.88
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.43; acc: 0.84
Batch: 680; loss: 0.28; acc: 0.89
Batch: 700; loss: 0.35; acc: 0.92
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.92
Batch: 760; loss: 0.34; acc: 0.91
Batch: 780; loss: 0.48; acc: 0.83
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.73
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2926647112390418; val_accuracy: 0.9082404458598726 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.86
Batch: 40; loss: 0.51; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.32; acc: 0.89
Batch: 100; loss: 0.41; acc: 0.86
Batch: 120; loss: 0.36; acc: 0.91
Batch: 140; loss: 0.39; acc: 0.84
Batch: 160; loss: 0.39; acc: 0.86
Batch: 180; loss: 0.22; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.3; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.92
Batch: 280; loss: 0.44; acc: 0.91
Batch: 300; loss: 0.56; acc: 0.89
Batch: 320; loss: 0.44; acc: 0.84
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.41; acc: 0.83
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.33; acc: 0.86
Batch: 440; loss: 0.3; acc: 0.88
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.4; acc: 0.91
Batch: 500; loss: 0.32; acc: 0.84
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.61; acc: 0.84
Batch: 560; loss: 0.29; acc: 0.89
Batch: 580; loss: 0.29; acc: 0.88
Batch: 600; loss: 0.34; acc: 0.92
Batch: 620; loss: 0.2; acc: 0.94
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.88
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.21; acc: 0.94
Batch: 740; loss: 0.46; acc: 0.86
Batch: 760; loss: 0.37; acc: 0.88
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.37; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.79; acc: 0.75
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.29427794008782715; val_accuracy: 0.9082404458598726 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.56; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.41; acc: 0.91
Batch: 60; loss: 0.55; acc: 0.88
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.34; acc: 0.94
Batch: 120; loss: 0.36; acc: 0.89
Batch: 140; loss: 0.35; acc: 0.91
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.52; acc: 0.84
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.33; acc: 0.84
Batch: 240; loss: 0.33; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.4; acc: 0.86
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.24; acc: 0.95
Batch: 340; loss: 0.5; acc: 0.83
Batch: 360; loss: 0.23; acc: 0.92
Batch: 380; loss: 0.33; acc: 0.88
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.35; acc: 0.92
Batch: 460; loss: 0.42; acc: 0.88
Batch: 480; loss: 0.34; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.89
Batch: 520; loss: 0.32; acc: 0.88
Batch: 540; loss: 0.28; acc: 0.91
Batch: 560; loss: 0.34; acc: 0.94
Batch: 580; loss: 0.21; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.89
Batch: 620; loss: 0.27; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.25; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.92
Batch: 760; loss: 0.41; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.86
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.77; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.98
Val Epoch over. val_loss: 0.2924713554204839; val_accuracy: 0.9083399681528662 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.28; acc: 0.86
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.91
Batch: 60; loss: 0.12; acc: 0.94
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.35; acc: 0.88
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.37; acc: 0.88
Batch: 160; loss: 0.35; acc: 0.94
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.36; acc: 0.88
Batch: 220; loss: 0.42; acc: 0.91
Batch: 240; loss: 0.5; acc: 0.86
Batch: 260; loss: 0.34; acc: 0.91
Batch: 280; loss: 0.18; acc: 0.92
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.44; acc: 0.88
Batch: 340; loss: 0.24; acc: 0.91
Batch: 360; loss: 0.38; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.92
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.23; acc: 0.94
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.45; acc: 0.84
Batch: 500; loss: 0.12; acc: 0.94
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.33; acc: 0.91
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.28; acc: 0.88
Batch: 600; loss: 0.47; acc: 0.92
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.24; acc: 0.95
Batch: 660; loss: 0.48; acc: 0.86
Batch: 680; loss: 0.42; acc: 0.86
Batch: 700; loss: 0.33; acc: 0.84
Batch: 720; loss: 0.56; acc: 0.83
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.39; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.33; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.2873923740331914; val_accuracy: 0.9095342356687898 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.39; acc: 0.91
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.97
Batch: 60; loss: 0.25; acc: 0.95
Batch: 80; loss: 0.3; acc: 0.92
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.14; acc: 0.97
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.91
Batch: 220; loss: 0.37; acc: 0.86
Batch: 240; loss: 0.33; acc: 0.88
Batch: 260; loss: 0.37; acc: 0.91
Batch: 280; loss: 0.43; acc: 0.86
Batch: 300; loss: 0.3; acc: 0.92
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.39; acc: 0.91
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.41; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.46; acc: 0.86
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.26; acc: 0.86
Batch: 500; loss: 0.64; acc: 0.83
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.29; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.17; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.92
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.97
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2894703176608131; val_accuracy: 0.9092356687898089 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.44; acc: 0.86
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.34; acc: 0.88
Batch: 180; loss: 0.42; acc: 0.84
Batch: 200; loss: 0.33; acc: 0.88
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.92
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.23; acc: 0.91
Batch: 320; loss: 0.41; acc: 0.84
Batch: 340; loss: 0.38; acc: 0.89
Batch: 360; loss: 0.41; acc: 0.84
Batch: 380; loss: 0.33; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.97
Batch: 420; loss: 0.35; acc: 0.88
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.49; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.92
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.38; acc: 0.92
Batch: 540; loss: 0.37; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.17; acc: 0.95
Batch: 600; loss: 0.1; acc: 0.98
Batch: 620; loss: 0.24; acc: 0.95
Batch: 640; loss: 0.29; acc: 0.92
Batch: 660; loss: 0.33; acc: 0.91
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.34; acc: 0.92
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.34; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.71; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.2918853190531776; val_accuracy: 0.9088375796178344 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.34; acc: 0.84
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.31; acc: 0.88
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.47; acc: 0.83
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.15; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.94
Batch: 200; loss: 0.32; acc: 0.86
Batch: 220; loss: 0.19; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.15; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.44; acc: 0.86
Batch: 320; loss: 0.34; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.41; acc: 0.86
Batch: 400; loss: 0.36; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.94
Batch: 440; loss: 0.28; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.31; acc: 0.92
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.44; acc: 0.83
Batch: 540; loss: 0.23; acc: 0.95
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.45; acc: 0.89
Batch: 600; loss: 0.41; acc: 0.86
Batch: 620; loss: 0.23; acc: 0.92
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.21; acc: 0.92
Batch: 680; loss: 0.35; acc: 0.89
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.25; acc: 0.97
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.38; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.89
Batch: 120; loss: 0.73; acc: 0.75
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.28714182695290846; val_accuracy: 0.9098328025477707 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.91
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.91
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.26; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.27; acc: 0.89
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.24; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.5; acc: 0.91
Batch: 420; loss: 0.25; acc: 0.94
Batch: 440; loss: 0.32; acc: 0.94
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.34; acc: 0.88
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.16; acc: 0.92
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.29; acc: 0.88
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.89
Batch: 640; loss: 0.29; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.88
Batch: 680; loss: 0.43; acc: 0.88
Batch: 700; loss: 0.3; acc: 0.95
Batch: 720; loss: 0.3; acc: 0.91
Batch: 740; loss: 0.28; acc: 0.88
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.37; acc: 0.89
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.75; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2888410084281757; val_accuracy: 0.908937101910828 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.41; acc: 0.81
Batch: 80; loss: 0.53; acc: 0.83
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.37; acc: 0.89
Batch: 180; loss: 0.3; acc: 0.89
Batch: 200; loss: 0.39; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.31; acc: 0.92
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.21; acc: 0.95
Batch: 320; loss: 0.14; acc: 0.98
Batch: 340; loss: 0.28; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.22; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.88
Batch: 440; loss: 0.24; acc: 0.91
Batch: 460; loss: 0.19; acc: 0.95
Batch: 480; loss: 0.26; acc: 0.89
Batch: 500; loss: 0.34; acc: 0.86
Batch: 520; loss: 0.35; acc: 0.91
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.27; acc: 0.91
Batch: 580; loss: 0.22; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.91
Batch: 620; loss: 0.41; acc: 0.86
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.44; acc: 0.84
Batch: 680; loss: 0.27; acc: 0.91
Batch: 700; loss: 0.36; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.92
Batch: 740; loss: 0.11; acc: 0.97
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.25; acc: 0.88
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.29302416699137657; val_accuracy: 0.9088375796178344 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.36; acc: 0.86
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.34; acc: 0.88
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.26; acc: 0.91
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.94
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.38; acc: 0.84
Batch: 200; loss: 0.44; acc: 0.92
Batch: 220; loss: 0.26; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.37; acc: 0.84
Batch: 280; loss: 0.51; acc: 0.88
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.38; acc: 0.83
Batch: 340; loss: 0.19; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.43; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.33; acc: 0.88
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.26; acc: 0.89
Batch: 560; loss: 0.23; acc: 0.89
Batch: 580; loss: 0.31; acc: 0.84
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.28; acc: 0.97
Batch: 640; loss: 0.32; acc: 0.88
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.47; acc: 0.83
Batch: 700; loss: 0.2; acc: 0.92
Batch: 720; loss: 0.48; acc: 0.86
Batch: 740; loss: 0.13; acc: 0.97
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.92
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.68; acc: 0.75
Batch: 140; loss: 0.1; acc: 0.97
Val Epoch over. val_loss: 0.29189942255141627; val_accuracy: 0.9075437898089171 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.16; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.94
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.38; acc: 0.89
Batch: 180; loss: 0.25; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.36; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.89
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.36; acc: 0.86
Batch: 340; loss: 0.38; acc: 0.88
Batch: 360; loss: 0.17; acc: 0.94
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.28; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.16; acc: 0.98
Batch: 460; loss: 0.32; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.24; acc: 0.92
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.42; acc: 0.83
Batch: 660; loss: 0.32; acc: 0.88
Batch: 680; loss: 0.37; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.38; acc: 0.81
Batch: 760; loss: 0.43; acc: 0.91
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.38; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.92
Batch: 120; loss: 0.83; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.292717584853719; val_accuracy: 0.910031847133758 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.41; acc: 0.84
Batch: 40; loss: 0.53; acc: 0.86
Batch: 60; loss: 0.39; acc: 0.88
Batch: 80; loss: 0.28; acc: 0.91
Batch: 100; loss: 0.49; acc: 0.84
Batch: 120; loss: 0.22; acc: 0.95
Batch: 140; loss: 0.26; acc: 0.91
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.49; acc: 0.86
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.45; acc: 0.83
Batch: 320; loss: 0.25; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.88
Batch: 480; loss: 0.36; acc: 0.89
Batch: 500; loss: 0.29; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.48; acc: 0.83
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.37; acc: 0.83
Batch: 620; loss: 0.36; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.95
Batch: 660; loss: 0.12; acc: 0.94
Batch: 680; loss: 0.33; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.89
Batch: 720; loss: 0.36; acc: 0.89
Batch: 740; loss: 0.39; acc: 0.88
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.16; acc: 0.95
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28605944606339095; val_accuracy: 0.9110270700636943 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.34; acc: 0.89
Batch: 60; loss: 0.21; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.33; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.84
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.2; acc: 0.97
Batch: 200; loss: 0.43; acc: 0.84
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.3; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.89
Batch: 340; loss: 0.35; acc: 0.86
Batch: 360; loss: 0.38; acc: 0.84
Batch: 380; loss: 0.29; acc: 0.91
Batch: 400; loss: 0.27; acc: 0.89
Batch: 420; loss: 0.27; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.44; acc: 0.86
Batch: 500; loss: 0.36; acc: 0.88
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.52; acc: 0.84
Batch: 580; loss: 0.2; acc: 0.95
Batch: 600; loss: 0.53; acc: 0.83
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.12; acc: 0.95
Batch: 680; loss: 0.48; acc: 0.86
Batch: 700; loss: 0.15; acc: 0.94
Batch: 720; loss: 0.38; acc: 0.89
Batch: 740; loss: 0.68; acc: 0.77
Batch: 760; loss: 0.26; acc: 0.91
Batch: 780; loss: 0.44; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.72; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2843621854735598; val_accuracy: 0.9113256369426752 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.88
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.28; acc: 0.95
Batch: 140; loss: 0.4; acc: 0.88
Batch: 160; loss: 0.32; acc: 0.91
Batch: 180; loss: 0.33; acc: 0.91
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.29; acc: 0.89
Batch: 320; loss: 0.28; acc: 0.92
Batch: 340; loss: 0.25; acc: 0.89
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.35; acc: 0.88
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.5; acc: 0.86
Batch: 440; loss: 0.16; acc: 0.94
Batch: 460; loss: 0.39; acc: 0.89
Batch: 480; loss: 0.44; acc: 0.88
Batch: 500; loss: 0.31; acc: 0.84
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.35; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.5; acc: 0.88
Batch: 620; loss: 0.4; acc: 0.86
Batch: 640; loss: 0.37; acc: 0.86
Batch: 660; loss: 0.25; acc: 0.88
Batch: 680; loss: 0.1; acc: 1.0
Batch: 700; loss: 0.21; acc: 0.97
Batch: 720; loss: 0.24; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.26; acc: 0.91
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.76; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28589780128021147; val_accuracy: 0.911922770700637 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.43; acc: 0.94
Batch: 60; loss: 0.29; acc: 0.88
Batch: 80; loss: 0.4; acc: 0.84
Batch: 100; loss: 0.29; acc: 0.94
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.89
Batch: 180; loss: 0.19; acc: 0.95
Batch: 200; loss: 0.35; acc: 0.92
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.91
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.42; acc: 0.89
Batch: 320; loss: 0.58; acc: 0.88
Batch: 340; loss: 0.49; acc: 0.86
Batch: 360; loss: 0.42; acc: 0.89
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.37; acc: 0.86
Batch: 440; loss: 0.38; acc: 0.83
Batch: 460; loss: 0.38; acc: 0.91
Batch: 480; loss: 0.25; acc: 0.91
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.92
Batch: 540; loss: 0.21; acc: 0.91
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.4; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.33; acc: 0.92
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.21; acc: 0.92
Batch: 740; loss: 0.49; acc: 0.84
Batch: 760; loss: 0.19; acc: 0.95
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2840511395839179; val_accuracy: 0.9123208598726115 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.36; acc: 0.88
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.33; acc: 0.89
Batch: 160; loss: 0.49; acc: 0.86
Batch: 180; loss: 0.5; acc: 0.84
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.44; acc: 0.84
Batch: 260; loss: 0.37; acc: 0.88
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.31; acc: 0.92
Batch: 340; loss: 0.47; acc: 0.83
Batch: 360; loss: 0.39; acc: 0.86
Batch: 380; loss: 0.29; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.2; acc: 0.94
Batch: 440; loss: 0.36; acc: 0.84
Batch: 460; loss: 0.36; acc: 0.92
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.27; acc: 0.95
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.28; acc: 0.91
Batch: 580; loss: 0.4; acc: 0.89
Batch: 600; loss: 0.31; acc: 0.91
Batch: 620; loss: 0.18; acc: 0.95
Batch: 640; loss: 0.31; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.95
Batch: 680; loss: 0.26; acc: 0.91
Batch: 700; loss: 0.37; acc: 0.86
Batch: 720; loss: 0.16; acc: 0.97
Batch: 740; loss: 0.39; acc: 0.89
Batch: 760; loss: 0.23; acc: 0.92
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.92
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.91
Batch: 120; loss: 0.76; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2854330649207922; val_accuracy: 0.9103304140127388 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.46; acc: 0.88
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.38; acc: 0.88
Batch: 80; loss: 0.35; acc: 0.89
Batch: 100; loss: 0.39; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.29; acc: 0.94
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.56; acc: 0.8
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.23; acc: 0.94
Batch: 240; loss: 0.37; acc: 0.86
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.38; acc: 0.92
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.34; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.95
Batch: 640; loss: 0.2; acc: 0.91
Batch: 660; loss: 0.21; acc: 0.95
Batch: 680; loss: 0.37; acc: 0.91
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.31; acc: 0.89
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.283980112226241; val_accuracy: 0.9122213375796179 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.35; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.23; acc: 0.89
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.36; acc: 0.91
Batch: 160; loss: 0.35; acc: 0.95
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.21; acc: 0.89
Batch: 220; loss: 0.21; acc: 0.92
Batch: 240; loss: 0.3; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.86
Batch: 280; loss: 0.55; acc: 0.86
Batch: 300; loss: 0.28; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.91
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.45; acc: 0.89
Batch: 460; loss: 0.4; acc: 0.91
Batch: 480; loss: 0.15; acc: 0.95
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.45; acc: 0.89
Batch: 540; loss: 0.3; acc: 0.94
Batch: 560; loss: 0.45; acc: 0.86
Batch: 580; loss: 0.24; acc: 0.94
Batch: 600; loss: 0.2; acc: 0.92
Batch: 620; loss: 0.35; acc: 0.89
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.35; acc: 0.88
Batch: 680; loss: 0.48; acc: 0.84
Batch: 700; loss: 0.3; acc: 0.86
Batch: 720; loss: 0.31; acc: 0.91
Batch: 740; loss: 0.32; acc: 0.89
Batch: 760; loss: 0.33; acc: 0.88
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.72; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28538522406653233; val_accuracy: 0.9117237261146497 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.42; acc: 0.84
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.48; acc: 0.83
Batch: 60; loss: 0.39; acc: 0.86
Batch: 80; loss: 0.33; acc: 0.89
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.92
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.95
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.37; acc: 0.91
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.92
Batch: 300; loss: 0.25; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.89
Batch: 340; loss: 0.27; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.89
Batch: 400; loss: 0.34; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.86
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.31; acc: 0.88
Batch: 500; loss: 0.28; acc: 0.92
Batch: 520; loss: 0.29; acc: 0.94
Batch: 540; loss: 0.36; acc: 0.86
Batch: 560; loss: 0.41; acc: 0.86
Batch: 580; loss: 0.28; acc: 0.92
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.97
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.29; acc: 0.89
Batch: 780; loss: 0.23; acc: 0.92
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.33; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.71; acc: 0.77
Batch: 140; loss: 0.08; acc: 1.0
Val Epoch over. val_loss: 0.2840866464884228; val_accuracy: 0.9121218152866242 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.6; acc: 0.84
Batch: 120; loss: 0.29; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.84
Batch: 160; loss: 0.15; acc: 0.92
Batch: 180; loss: 0.42; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.18; acc: 0.95
Batch: 240; loss: 0.39; acc: 0.91
Batch: 260; loss: 0.19; acc: 0.95
Batch: 280; loss: 0.39; acc: 0.88
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.46; acc: 0.84
Batch: 340; loss: 0.37; acc: 0.86
Batch: 360; loss: 0.3; acc: 0.86
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.34; acc: 0.94
Batch: 420; loss: 0.35; acc: 0.86
Batch: 440; loss: 0.4; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.26; acc: 0.94
Batch: 500; loss: 0.23; acc: 0.92
Batch: 520; loss: 0.28; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.91
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.42; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.97
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.28; acc: 0.95
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.23; acc: 0.91
Batch: 780; loss: 0.4; acc: 0.89
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.36; acc: 0.94
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.77
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.28467209601573124; val_accuracy: 0.9110270700636943 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.95
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.3; acc: 0.88
Batch: 160; loss: 0.23; acc: 0.91
Batch: 180; loss: 0.31; acc: 0.88
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.58; acc: 0.78
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.35; acc: 0.86
Batch: 320; loss: 0.3; acc: 0.95
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.25; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.89
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.33; acc: 0.86
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.36; acc: 0.86
Batch: 500; loss: 0.22; acc: 0.91
Batch: 520; loss: 0.15; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.91
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.52; acc: 0.84
Batch: 660; loss: 0.29; acc: 0.91
Batch: 680; loss: 0.36; acc: 0.91
Batch: 700; loss: 0.29; acc: 0.88
Batch: 720; loss: 0.22; acc: 0.91
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.23; acc: 0.94
Batch: 780; loss: 0.42; acc: 0.88
Train Epoch over. train_loss: 0.29; train_accuracy: 0.91 

Batch: 0; loss: 0.32; acc: 0.91
Batch: 20; loss: 0.34; acc: 0.88
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.94
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.23; acc: 0.91
Batch: 120; loss: 0.74; acc: 0.75
Batch: 140; loss: 0.08; acc: 0.98
Val Epoch over. val_loss: 0.2831859977405732; val_accuracy: 0.9116242038216561 

plots/subspace_training/reg_lenet_2/2020-01-19 19:11:48/d_dim_300_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 56242
elements in E: 7909600
fraction nonzero: 0.007110599777485587
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.29; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.31; acc: 0.05
Batch: 140; loss: 2.29; acc: 0.2
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.28; acc: 0.17
Batch: 220; loss: 2.28; acc: 0.17
Batch: 240; loss: 2.29; acc: 0.12
Batch: 260; loss: 2.27; acc: 0.14
Batch: 280; loss: 2.29; acc: 0.06
Batch: 300; loss: 2.27; acc: 0.17
Batch: 320; loss: 2.27; acc: 0.16
Batch: 340; loss: 2.27; acc: 0.11
Batch: 360; loss: 2.24; acc: 0.17
Batch: 380; loss: 2.15; acc: 0.27
Batch: 400; loss: 2.19; acc: 0.17
Batch: 420; loss: 2.11; acc: 0.25
Batch: 440; loss: 1.98; acc: 0.25
Batch: 460; loss: 1.69; acc: 0.42
Batch: 480; loss: 1.86; acc: 0.39
Batch: 500; loss: 1.57; acc: 0.41
Batch: 520; loss: 1.83; acc: 0.3
Batch: 540; loss: 1.63; acc: 0.45
Batch: 560; loss: 1.42; acc: 0.5
Batch: 580; loss: 1.4; acc: 0.55
Batch: 600; loss: 1.66; acc: 0.44
Batch: 620; loss: 1.24; acc: 0.59
Batch: 640; loss: 1.56; acc: 0.41
Batch: 660; loss: 1.97; acc: 0.44
Batch: 680; loss: 0.82; acc: 0.8
Batch: 700; loss: 1.0; acc: 0.7
Batch: 720; loss: 1.28; acc: 0.62
Batch: 740; loss: 1.19; acc: 0.61
Batch: 760; loss: 1.4; acc: 0.58
Batch: 780; loss: 1.03; acc: 0.69
Train Epoch over. train_loss: 1.89; train_accuracy: 0.32 

Batch: 0; loss: 1.24; acc: 0.59
Batch: 20; loss: 1.8; acc: 0.45
Batch: 40; loss: 1.17; acc: 0.62
Batch: 60; loss: 1.38; acc: 0.48
Batch: 80; loss: 1.27; acc: 0.55
Batch: 100; loss: 1.36; acc: 0.55
Batch: 120; loss: 1.56; acc: 0.53
Batch: 140; loss: 1.41; acc: 0.55
Val Epoch over. val_loss: 1.454578599732393; val_accuracy: 0.5269705414012739 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.04; acc: 0.64
Batch: 20; loss: 0.99; acc: 0.72
Batch: 40; loss: 0.89; acc: 0.77
Batch: 60; loss: 1.02; acc: 0.67
Batch: 80; loss: 0.55; acc: 0.84
Batch: 100; loss: 0.72; acc: 0.81
Batch: 120; loss: 0.88; acc: 0.69
Batch: 140; loss: 0.64; acc: 0.77
Batch: 160; loss: 0.85; acc: 0.7
Batch: 180; loss: 0.59; acc: 0.8
Batch: 200; loss: 0.55; acc: 0.8
Batch: 220; loss: 0.65; acc: 0.78
Batch: 240; loss: 0.82; acc: 0.72
Batch: 260; loss: 0.77; acc: 0.78
Batch: 280; loss: 0.67; acc: 0.75
Batch: 300; loss: 1.08; acc: 0.69
Batch: 320; loss: 0.84; acc: 0.73
Batch: 340; loss: 0.94; acc: 0.64
Batch: 360; loss: 0.28; acc: 0.94
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.74; acc: 0.7
Batch: 420; loss: 0.53; acc: 0.86
Batch: 440; loss: 0.74; acc: 0.77
Batch: 460; loss: 0.76; acc: 0.73
Batch: 480; loss: 0.45; acc: 0.91
Batch: 500; loss: 0.79; acc: 0.75
Batch: 520; loss: 0.58; acc: 0.8
Batch: 540; loss: 0.39; acc: 0.86
Batch: 560; loss: 0.43; acc: 0.86
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.55; acc: 0.8
Batch: 620; loss: 0.5; acc: 0.86
Batch: 640; loss: 0.66; acc: 0.8
Batch: 660; loss: 0.38; acc: 0.88
Batch: 680; loss: 0.38; acc: 0.88
Batch: 700; loss: 0.77; acc: 0.81
Batch: 720; loss: 1.71; acc: 0.53
Batch: 740; loss: 0.48; acc: 0.8
Batch: 760; loss: 0.56; acc: 0.86
Batch: 780; loss: 0.66; acc: 0.77
Train Epoch over. train_loss: 0.68; train_accuracy: 0.79 

Batch: 0; loss: 0.57; acc: 0.8
Batch: 20; loss: 0.71; acc: 0.75
Batch: 40; loss: 0.37; acc: 0.89
Batch: 60; loss: 0.55; acc: 0.81
Batch: 80; loss: 0.4; acc: 0.83
Batch: 100; loss: 0.56; acc: 0.83
Batch: 120; loss: 0.87; acc: 0.72
Batch: 140; loss: 0.3; acc: 0.94
Val Epoch over. val_loss: 0.5197873383190981; val_accuracy: 0.8302149681528662 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.64; acc: 0.8
Batch: 20; loss: 0.49; acc: 0.83
Batch: 40; loss: 0.34; acc: 0.92
Batch: 60; loss: 0.88; acc: 0.75
Batch: 80; loss: 0.59; acc: 0.78
Batch: 100; loss: 0.66; acc: 0.86
Batch: 120; loss: 0.59; acc: 0.83
Batch: 140; loss: 0.39; acc: 0.86
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.33; acc: 0.94
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.54; acc: 0.86
Batch: 240; loss: 0.52; acc: 0.83
Batch: 260; loss: 0.65; acc: 0.8
Batch: 280; loss: 0.64; acc: 0.77
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.47; acc: 0.84
Batch: 340; loss: 0.41; acc: 0.89
Batch: 360; loss: 0.51; acc: 0.89
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.65; acc: 0.81
Batch: 420; loss: 0.42; acc: 0.88
Batch: 440; loss: 0.36; acc: 0.89
Batch: 460; loss: 0.39; acc: 0.86
Batch: 480; loss: 0.45; acc: 0.89
Batch: 500; loss: 0.37; acc: 0.84
Batch: 520; loss: 0.26; acc: 0.94
Batch: 540; loss: 0.34; acc: 0.92
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.43; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.32; acc: 0.89
Batch: 640; loss: 0.63; acc: 0.89
Batch: 660; loss: 0.39; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.91
Batch: 700; loss: 0.24; acc: 0.92
Batch: 720; loss: 0.5; acc: 0.84
Batch: 740; loss: 0.3; acc: 0.91
Batch: 760; loss: 0.66; acc: 0.84
Batch: 780; loss: 0.39; acc: 0.92
Train Epoch over. train_loss: 0.46; train_accuracy: 0.86 

Batch: 0; loss: 0.57; acc: 0.86
Batch: 20; loss: 0.74; acc: 0.73
Batch: 40; loss: 0.49; acc: 0.86
Batch: 60; loss: 0.57; acc: 0.8
Batch: 80; loss: 0.32; acc: 0.88
Batch: 100; loss: 0.8; acc: 0.8
Batch: 120; loss: 0.6; acc: 0.81
Batch: 140; loss: 0.32; acc: 0.88
Val Epoch over. val_loss: 0.5893235602386439; val_accuracy: 0.8173765923566879 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.58; acc: 0.83
Batch: 20; loss: 0.57; acc: 0.75
Batch: 40; loss: 0.73; acc: 0.77
Batch: 60; loss: 0.48; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.88
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.68; acc: 0.81
Batch: 160; loss: 0.48; acc: 0.86
Batch: 180; loss: 0.28; acc: 0.91
Batch: 200; loss: 0.33; acc: 0.89
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.54; acc: 0.84
Batch: 260; loss: 0.39; acc: 0.86
Batch: 280; loss: 0.43; acc: 0.84
Batch: 300; loss: 0.4; acc: 0.91
Batch: 320; loss: 0.49; acc: 0.91
Batch: 340; loss: 0.34; acc: 0.89
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.38; acc: 0.86
Batch: 400; loss: 0.29; acc: 0.89
Batch: 420; loss: 0.37; acc: 0.91
Batch: 440; loss: 0.58; acc: 0.84
Batch: 460; loss: 0.37; acc: 0.89
Batch: 480; loss: 0.45; acc: 0.81
Batch: 500; loss: 0.43; acc: 0.84
Batch: 520; loss: 0.37; acc: 0.86
Batch: 540; loss: 0.34; acc: 0.89
Batch: 560; loss: 0.36; acc: 0.86
Batch: 580; loss: 0.31; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.46; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.89
Batch: 660; loss: 0.26; acc: 0.94
Batch: 680; loss: 0.17; acc: 0.97
Batch: 700; loss: 0.52; acc: 0.84
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.88; acc: 0.78
Batch: 760; loss: 0.19; acc: 0.97
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.4; train_accuracy: 0.87 

Batch: 0; loss: 2.45; acc: 0.39
Batch: 20; loss: 1.78; acc: 0.5
Batch: 40; loss: 1.52; acc: 0.61
Batch: 60; loss: 1.81; acc: 0.56
Batch: 80; loss: 1.98; acc: 0.55
Batch: 100; loss: 1.68; acc: 0.5
Batch: 120; loss: 2.56; acc: 0.44
Batch: 140; loss: 0.74; acc: 0.69
Val Epoch over. val_loss: 1.6694911498173026; val_accuracy: 0.5620023885350318 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 2.2; acc: 0.48
Batch: 20; loss: 0.38; acc: 0.84
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.41; acc: 0.88
Batch: 120; loss: 0.75; acc: 0.8
Batch: 140; loss: 0.31; acc: 0.89
Batch: 160; loss: 0.28; acc: 0.91
Batch: 180; loss: 0.18; acc: 0.91
Batch: 200; loss: 0.45; acc: 0.89
Batch: 220; loss: 0.45; acc: 0.88
Batch: 240; loss: 0.24; acc: 0.91
Batch: 260; loss: 0.2; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.84
Batch: 300; loss: 0.78; acc: 0.8
Batch: 320; loss: 0.75; acc: 0.83
Batch: 340; loss: 0.54; acc: 0.88
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.48; acc: 0.83
Batch: 420; loss: 0.31; acc: 0.88
Batch: 440; loss: 0.31; acc: 0.89
Batch: 460; loss: 0.54; acc: 0.83
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.31; acc: 0.91
Batch: 540; loss: 0.5; acc: 0.84
Batch: 560; loss: 0.45; acc: 0.84
Batch: 580; loss: 0.44; acc: 0.8
Batch: 600; loss: 0.22; acc: 0.94
Batch: 620; loss: 0.59; acc: 0.78
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.23; acc: 0.92
Batch: 680; loss: 0.49; acc: 0.84
Batch: 700; loss: 0.42; acc: 0.88
Batch: 720; loss: 0.31; acc: 0.89
Batch: 740; loss: 0.23; acc: 0.89
Batch: 760; loss: 0.7; acc: 0.81
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.39; train_accuracy: 0.88 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.53; acc: 0.86
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.37; acc: 0.86
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.42; acc: 0.88
Batch: 120; loss: 0.69; acc: 0.8
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.44314900315870903; val_accuracy: 0.8625597133757962 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.41; acc: 0.89
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.46; acc: 0.84
Batch: 80; loss: 0.55; acc: 0.83
Batch: 100; loss: 0.41; acc: 0.84
Batch: 120; loss: 0.71; acc: 0.83
Batch: 140; loss: 0.5; acc: 0.81
Batch: 160; loss: 0.36; acc: 0.91
Batch: 180; loss: 0.51; acc: 0.81
Batch: 200; loss: 0.42; acc: 0.88
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.27; acc: 0.89
Batch: 280; loss: 0.43; acc: 0.88
Batch: 300; loss: 0.47; acc: 0.84
Batch: 320; loss: 0.47; acc: 0.89
Batch: 340; loss: 0.18; acc: 0.92
Batch: 360; loss: 0.42; acc: 0.84
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.95
Batch: 420; loss: 0.42; acc: 0.89
Batch: 440; loss: 0.74; acc: 0.83
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.38; acc: 0.86
Batch: 520; loss: 0.28; acc: 0.91
Batch: 540; loss: 0.39; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.32; acc: 0.88
Batch: 600; loss: 0.54; acc: 0.91
Batch: 620; loss: 0.3; acc: 0.89
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.46; acc: 0.86
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.91
Batch: 720; loss: 0.67; acc: 0.83
Batch: 740; loss: 0.34; acc: 0.88
Batch: 760; loss: 0.27; acc: 0.91
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.34; acc: 0.89
Batch: 20; loss: 0.36; acc: 0.88
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.49; acc: 0.8
Batch: 80; loss: 0.3; acc: 0.94
Batch: 100; loss: 0.32; acc: 0.89
Batch: 120; loss: 0.69; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.4042901608879399; val_accuracy: 0.8750995222929936 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.71; acc: 0.77
Batch: 20; loss: 0.35; acc: 0.88
Batch: 40; loss: 0.36; acc: 0.88
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.19; acc: 0.91
Batch: 120; loss: 0.27; acc: 0.92
Batch: 140; loss: 0.42; acc: 0.88
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.32; acc: 0.86
Batch: 200; loss: 0.25; acc: 0.91
Batch: 220; loss: 0.41; acc: 0.88
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.4; acc: 0.92
Batch: 320; loss: 0.33; acc: 0.91
Batch: 340; loss: 0.43; acc: 0.92
Batch: 360; loss: 0.51; acc: 0.88
Batch: 380; loss: 0.42; acc: 0.89
Batch: 400; loss: 0.35; acc: 0.88
Batch: 420; loss: 0.32; acc: 0.91
Batch: 440; loss: 0.22; acc: 0.91
Batch: 460; loss: 0.53; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.89
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.26; acc: 0.91
Batch: 560; loss: 0.35; acc: 0.92
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.27; acc: 0.94
Batch: 620; loss: 0.35; acc: 0.92
Batch: 640; loss: 0.36; acc: 0.89
Batch: 660; loss: 0.31; acc: 0.91
Batch: 680; loss: 0.46; acc: 0.89
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.16; acc: 0.95
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.18; acc: 0.94
Batch: 780; loss: 0.3; acc: 0.89
Train Epoch over. train_loss: 0.35; train_accuracy: 0.89 

Batch: 0; loss: 0.23; acc: 0.95
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.17; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.66; acc: 0.84
Batch: 140; loss: 0.11; acc: 0.94
Val Epoch over. val_loss: 0.3012165468969163; val_accuracy: 0.9079418789808917 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.91
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.32; acc: 0.89
Batch: 80; loss: 0.44; acc: 0.89
Batch: 100; loss: 0.31; acc: 0.92
Batch: 120; loss: 0.18; acc: 0.92
Batch: 140; loss: 0.2; acc: 0.95
Batch: 160; loss: 0.31; acc: 0.89
Batch: 180; loss: 0.53; acc: 0.8
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.86
Batch: 240; loss: 0.35; acc: 0.89
Batch: 260; loss: 0.25; acc: 0.94
Batch: 280; loss: 0.31; acc: 0.91
Batch: 300; loss: 0.39; acc: 0.88
Batch: 320; loss: 0.37; acc: 0.86
Batch: 340; loss: 0.37; acc: 0.89
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.28; acc: 0.92
Batch: 420; loss: 0.21; acc: 0.91
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.49; acc: 0.84
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.37; acc: 0.91
Batch: 520; loss: 0.47; acc: 0.88
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.36; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.46; acc: 0.84
Batch: 620; loss: 0.45; acc: 0.88
Batch: 640; loss: 0.26; acc: 0.91
Batch: 660; loss: 0.4; acc: 0.88
Batch: 680; loss: 0.36; acc: 0.94
Batch: 700; loss: 0.42; acc: 0.89
Batch: 720; loss: 0.37; acc: 0.91
Batch: 740; loss: 0.19; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.92
Batch: 780; loss: 0.4; acc: 0.86
Train Epoch over. train_loss: 0.34; train_accuracy: 0.89 

Batch: 0; loss: 0.31; acc: 0.91
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.2; acc: 0.91
Batch: 60; loss: 0.33; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.62; acc: 0.77
Batch: 140; loss: 0.1; acc: 0.98
Val Epoch over. val_loss: 0.3250432716338498; val_accuracy: 0.8918192675159236 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.22; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.3; acc: 0.89
Batch: 60; loss: 0.23; acc: 0.95
Batch: 80; loss: 0.33; acc: 0.91
Batch: 100; loss: 0.4; acc: 0.88
Batch: 120; loss: 0.49; acc: 0.84
Batch: 140; loss: 0.66; acc: 0.78
Batch: 160; loss: 0.3; acc: 0.86
Batch: 180; loss: 0.31; acc: 0.92
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.4; acc: 0.84
Batch: 240; loss: 0.58; acc: 0.78
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.24; acc: 0.94
Batch: 300; loss: 0.2; acc: 0.91
Batch: 320; loss: 0.26; acc: 0.89
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.23; acc: 0.91
Batch: 420; loss: 0.15; acc: 0.97
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.39; acc: 0.88
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.3; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.32; acc: 0.88
Batch: 580; loss: 0.26; acc: 0.89
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.41; acc: 0.89
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.34; acc: 0.92
Batch: 700; loss: 0.29; acc: 0.89
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.35; acc: 0.88
Batch: 760; loss: 0.45; acc: 0.86
Batch: 780; loss: 0.28; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.81; acc: 0.8
Batch: 140; loss: 0.08; acc: 0.97
Val Epoch over. val_loss: 0.2724231019331391; val_accuracy: 0.9151074840764332 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.28; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.24; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.28; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.84
Batch: 120; loss: 0.35; acc: 0.86
Batch: 140; loss: 0.38; acc: 0.86
Batch: 160; loss: 0.53; acc: 0.84
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.49; acc: 0.86
Batch: 340; loss: 0.4; acc: 0.88
Batch: 360; loss: 0.4; acc: 0.86
Batch: 380; loss: 0.43; acc: 0.88
Batch: 400; loss: 0.36; acc: 0.88
Batch: 420; loss: 0.44; acc: 0.88
Batch: 440; loss: 0.34; acc: 0.88
Batch: 460; loss: 0.57; acc: 0.81
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.97
Batch: 520; loss: 0.36; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.2; acc: 0.94
Batch: 580; loss: 0.3; acc: 0.92
Batch: 600; loss: 0.21; acc: 0.91
Batch: 620; loss: 0.31; acc: 0.88
Batch: 640; loss: 0.18; acc: 0.91
Batch: 660; loss: 0.28; acc: 0.92
Batch: 680; loss: 0.36; acc: 0.88
Batch: 700; loss: 0.43; acc: 0.88
Batch: 720; loss: 0.38; acc: 0.86
Batch: 740; loss: 0.43; acc: 0.84
Batch: 760; loss: 0.62; acc: 0.84
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.31; train_accuracy: 0.9 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.3; acc: 0.89
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.81
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.26; acc: 0.94
Batch: 120; loss: 0.68; acc: 0.83
Batch: 140; loss: 0.12; acc: 0.97
Val Epoch over. val_loss: 0.36385025264351234; val_accuracy: 0.8880374203821656 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.94
Batch: 20; loss: 0.36; acc: 0.91
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.91
Batch: 100; loss: 0.35; acc: 0.86
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.91
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.95
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.13; acc: 0.98
Batch: 360; loss: 0.19; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.3; acc: 0.94
Batch: 420; loss: 0.44; acc: 0.86
Batch: 440; loss: 0.35; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.91
Batch: 480; loss: 0.32; acc: 0.92
Batch: 500; loss: 0.42; acc: 0.91
Batch: 520; loss: 0.28; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.91
Batch: 600; loss: 0.21; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.14; acc: 0.97
Batch: 680; loss: 0.13; acc: 0.97
Batch: 700; loss: 0.19; acc: 0.95
Batch: 720; loss: 0.36; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.37; acc: 0.88
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.27; acc: 0.91
Batch: 20; loss: 0.36; acc: 0.86
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.88
Batch: 80; loss: 0.15; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.8; acc: 0.78
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.28124618219437114; val_accuracy: 0.9136146496815286 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.36; acc: 0.88
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.35; acc: 0.94
Batch: 60; loss: 0.28; acc: 0.89
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.88
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.91
Batch: 180; loss: 0.25; acc: 0.91
Batch: 200; loss: 0.22; acc: 0.91
Batch: 220; loss: 0.16; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.91
Batch: 260; loss: 0.28; acc: 0.89
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.31; acc: 0.91
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.31; acc: 0.91
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.91
Batch: 420; loss: 0.36; acc: 0.91
Batch: 440; loss: 0.13; acc: 0.95
Batch: 460; loss: 0.2; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.98
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.2; acc: 0.95
Batch: 580; loss: 0.37; acc: 0.88
Batch: 600; loss: 0.07; acc: 1.0
Batch: 620; loss: 0.34; acc: 0.88
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.27; acc: 0.92
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.36; acc: 0.86
Batch: 760; loss: 0.4; acc: 0.89
Batch: 780; loss: 0.37; acc: 0.92
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.44; acc: 0.86
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.25; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.91
Batch: 120; loss: 0.69; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.3298507183552927; val_accuracy: 0.9023686305732485 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.24; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.16; acc: 0.97
Batch: 120; loss: 0.3; acc: 0.88
Batch: 140; loss: 0.07; acc: 1.0
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.14; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.97
Batch: 220; loss: 0.31; acc: 0.94
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.22; acc: 0.91
Batch: 300; loss: 0.33; acc: 0.91
Batch: 320; loss: 0.29; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.53; acc: 0.83
Batch: 380; loss: 0.39; acc: 0.84
Batch: 400; loss: 0.11; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.23; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.91
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.28; acc: 0.88
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.95
Batch: 620; loss: 0.31; acc: 0.91
Batch: 640; loss: 0.33; acc: 0.92
Batch: 660; loss: 0.41; acc: 0.84
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.31; acc: 0.89
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.43; acc: 0.89
Batch: 780; loss: 0.41; acc: 0.86
Train Epoch over. train_loss: 0.24; train_accuracy: 0.92 

Batch: 0; loss: 0.29; acc: 0.89
Batch: 20; loss: 0.28; acc: 0.89
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.42; acc: 0.88
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.91
Batch: 120; loss: 0.73; acc: 0.83
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.3049537690391966; val_accuracy: 0.9021695859872612 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.27; acc: 0.92
Batch: 20; loss: 0.45; acc: 0.86
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.28; acc: 0.86
Batch: 100; loss: 0.34; acc: 0.88
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.14; acc: 0.97
Batch: 160; loss: 0.42; acc: 0.86
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.17; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.29; acc: 0.94
Batch: 260; loss: 0.17; acc: 0.95
Batch: 280; loss: 0.3; acc: 0.92
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.91
Batch: 380; loss: 0.25; acc: 0.89
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.47; acc: 0.88
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.95
Batch: 480; loss: 0.31; acc: 0.91
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.94
Batch: 560; loss: 0.19; acc: 0.92
Batch: 580; loss: 0.47; acc: 0.88
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.34; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.3; acc: 0.89
Batch: 720; loss: 0.14; acc: 0.94
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.26; acc: 0.89
Batch: 780; loss: 0.2; acc: 0.94
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.31; acc: 0.88
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.13; acc: 0.94
Batch: 120; loss: 0.59; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.22776461418741828; val_accuracy: 0.9290406050955414 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.37; acc: 0.89
Batch: 40; loss: 0.21; acc: 0.91
Batch: 60; loss: 0.2; acc: 0.97
Batch: 80; loss: 0.31; acc: 0.89
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.26; acc: 0.95
Batch: 160; loss: 0.29; acc: 0.95
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.27; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.36; acc: 0.89
Batch: 340; loss: 0.1; acc: 0.98
Batch: 360; loss: 0.15; acc: 0.92
Batch: 380; loss: 0.23; acc: 0.91
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.3; acc: 0.92
Batch: 460; loss: 0.17; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.94
Batch: 500; loss: 0.72; acc: 0.81
Batch: 520; loss: 0.29; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.94
Batch: 560; loss: 0.31; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.92
Batch: 600; loss: 0.32; acc: 0.86
Batch: 620; loss: 0.32; acc: 0.91
Batch: 640; loss: 0.3; acc: 0.89
Batch: 660; loss: 0.36; acc: 0.92
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.21; acc: 0.92
Batch: 720; loss: 0.26; acc: 0.92
Batch: 740; loss: 0.31; acc: 0.92
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.26; acc: 0.89
Train Epoch over. train_loss: 0.24; train_accuracy: 0.93 

Batch: 0; loss: 0.15; acc: 0.94
Batch: 20; loss: 0.29; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.53; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23329947212604202; val_accuracy: 0.9270501592356688 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.33; acc: 0.84
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.34; acc: 0.92
Batch: 80; loss: 0.26; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.26; acc: 0.95
Batch: 140; loss: 0.34; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.91
Batch: 180; loss: 0.27; acc: 0.92
Batch: 200; loss: 0.23; acc: 0.94
Batch: 220; loss: 0.34; acc: 0.89
Batch: 240; loss: 0.23; acc: 0.94
Batch: 260; loss: 0.32; acc: 0.92
Batch: 280; loss: 0.32; acc: 0.89
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.34; acc: 0.92
Batch: 380; loss: 0.14; acc: 0.94
Batch: 400; loss: 0.33; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.94
Batch: 440; loss: 0.34; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.21; acc: 0.92
Batch: 500; loss: 0.45; acc: 0.88
Batch: 520; loss: 0.07; acc: 0.98
Batch: 540; loss: 0.33; acc: 0.88
Batch: 560; loss: 0.39; acc: 0.91
Batch: 580; loss: 0.37; acc: 0.89
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.22; acc: 0.97
Batch: 640; loss: 0.46; acc: 0.86
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.27; acc: 0.92
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.29; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.92
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.28; acc: 0.88
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.58; acc: 0.89
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.228396797434065; val_accuracy: 0.932921974522293 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.2; acc: 0.94
Batch: 60; loss: 0.07; acc: 0.98
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.97
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.14; acc: 0.95
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.42; acc: 0.86
Batch: 300; loss: 0.22; acc: 0.97
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.95
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.98
Batch: 420; loss: 0.33; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.31; acc: 0.88
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.34; acc: 0.84
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.97
Batch: 600; loss: 0.24; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.3; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.26; acc: 0.94
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.63; acc: 0.83
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.23626751413533262; val_accuracy: 0.9289410828025477 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.21; acc: 0.91
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.23; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.22; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.92
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.22; acc: 0.92
Batch: 200; loss: 0.26; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.27; acc: 0.88
Batch: 300; loss: 0.11; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.91
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.33; acc: 0.89
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.32; acc: 0.95
Batch: 420; loss: 0.33; acc: 0.84
Batch: 440; loss: 0.37; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.43; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.95
Batch: 540; loss: 0.15; acc: 0.92
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.94
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.4; acc: 0.84
Batch: 640; loss: 0.55; acc: 0.86
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.25; acc: 0.91
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.97
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.31; acc: 0.91
Batch: 40; loss: 0.23; acc: 0.91
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.58; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2537261388578992; val_accuracy: 0.9182921974522293 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.95
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.13; acc: 0.97
Batch: 80; loss: 0.33; acc: 0.88
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.21; acc: 0.97
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.37; acc: 0.92
Batch: 220; loss: 0.56; acc: 0.86
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.26; acc: 0.94
Batch: 280; loss: 0.28; acc: 0.92
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.24; acc: 0.92
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.24; acc: 0.91
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.26; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.94
Batch: 480; loss: 0.32; acc: 0.88
Batch: 500; loss: 0.27; acc: 0.91
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.44; acc: 0.83
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.28; acc: 0.86
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.2; acc: 0.94
Batch: 660; loss: 0.32; acc: 0.89
Batch: 680; loss: 0.52; acc: 0.84
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.29; acc: 0.92
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.35; acc: 0.91
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.52; acc: 0.84
Batch: 40; loss: 0.22; acc: 0.91
Batch: 60; loss: 0.53; acc: 0.83
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.83
Batch: 120; loss: 0.77; acc: 0.78
Batch: 140; loss: 0.29; acc: 0.89
Val Epoch over. val_loss: 0.45630159622924343; val_accuracy: 0.8599721337579618 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.91
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.92
Batch: 100; loss: 0.26; acc: 0.88
Batch: 120; loss: 0.26; acc: 0.94
Batch: 140; loss: 0.29; acc: 0.92
Batch: 160; loss: 0.31; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.94
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.1; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.37; acc: 0.89
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.36; acc: 0.84
Batch: 340; loss: 0.16; acc: 0.98
Batch: 360; loss: 0.45; acc: 0.92
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.92
Batch: 420; loss: 0.18; acc: 0.92
Batch: 440; loss: 0.27; acc: 0.94
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.09; acc: 0.98
Batch: 520; loss: 0.06; acc: 1.0
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.88
Batch: 600; loss: 0.23; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.95
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.28; acc: 0.89
Batch: 680; loss: 0.14; acc: 0.98
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.26; acc: 0.95
Train Epoch over. train_loss: 0.23; train_accuracy: 0.93 

Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.62; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.22623159921473; val_accuracy: 0.9335191082802548 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.25; acc: 0.94
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.91
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.92
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.21; acc: 0.94
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.24; acc: 0.91
Batch: 200; loss: 0.27; acc: 0.89
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.24; acc: 0.92
Batch: 280; loss: 0.23; acc: 0.89
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.38; acc: 0.89
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.92
Batch: 400; loss: 0.07; acc: 0.97
Batch: 420; loss: 0.18; acc: 0.91
Batch: 440; loss: 0.14; acc: 0.97
Batch: 460; loss: 0.14; acc: 0.95
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.94
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.37; acc: 0.84
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.05; acc: 1.0
Batch: 600; loss: 0.44; acc: 0.89
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.44; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.31; acc: 0.88
Batch: 720; loss: 0.14; acc: 0.98
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.26; acc: 0.92
Batch: 780; loss: 0.25; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.34; acc: 0.89
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.56; acc: 0.88
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2250515735073454; val_accuracy: 0.931031050955414 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.11; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.89
Batch: 140; loss: 0.28; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.3; acc: 0.89
Batch: 220; loss: 0.4; acc: 0.91
Batch: 240; loss: 0.22; acc: 0.95
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.32; acc: 0.89
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.48; acc: 0.86
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.17; acc: 0.94
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.23; acc: 0.95
Batch: 520; loss: 0.18; acc: 0.92
Batch: 540; loss: 0.28; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.89
Batch: 580; loss: 0.15; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.25; acc: 0.92
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.94
Batch: 700; loss: 0.32; acc: 0.92
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.15; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.54; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21091364636115587; val_accuracy: 0.9362062101910829 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.23; acc: 0.91
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.27; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.35; acc: 0.88
Batch: 160; loss: 0.36; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.97
Batch: 220; loss: 0.28; acc: 0.89
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.25; acc: 0.91
Batch: 320; loss: 0.35; acc: 0.92
Batch: 340; loss: 0.17; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.26; acc: 0.91
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.21; acc: 0.89
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.14; acc: 0.98
Batch: 520; loss: 0.26; acc: 0.91
Batch: 540; loss: 0.21; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.25; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.39; acc: 0.89
Batch: 660; loss: 0.15; acc: 0.98
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.16; acc: 0.95
Batch: 760; loss: 0.16; acc: 0.95
Batch: 780; loss: 0.43; acc: 0.89
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2182869960548012; val_accuracy: 0.9353105095541401 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.1; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.36; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.14; acc: 0.95
Batch: 180; loss: 0.36; acc: 0.91
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.18; acc: 0.92
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.22; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.13; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.94
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.25; acc: 0.92
Batch: 480; loss: 0.4; acc: 0.89
Batch: 500; loss: 0.17; acc: 0.94
Batch: 520; loss: 0.26; acc: 0.95
Batch: 540; loss: 0.18; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.14; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.92
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.12; acc: 0.97
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.35; acc: 0.88
Batch: 780; loss: 0.48; acc: 0.88
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.16; acc: 0.98
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.89
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.2095646137597075; val_accuracy: 0.9373009554140127 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.94
Batch: 20; loss: 0.39; acc: 0.88
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.97
Batch: 100; loss: 0.16; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.14; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.29; acc: 0.89
Batch: 280; loss: 0.21; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.92
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.19; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.17; acc: 0.95
Batch: 440; loss: 0.5; acc: 0.91
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.45; acc: 0.81
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.17; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.92
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.46; acc: 0.89
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.1; acc: 0.98
Batch: 720; loss: 0.37; acc: 0.88
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.17; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.93 

Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.20885799433680097; val_accuracy: 0.9372014331210191 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.91
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.98
Batch: 60; loss: 0.24; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.89
Batch: 100; loss: 0.08; acc: 0.98
Batch: 120; loss: 0.06; acc: 0.98
Batch: 140; loss: 0.16; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.16; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.97
Batch: 220; loss: 0.12; acc: 0.95
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.94
Batch: 300; loss: 0.26; acc: 0.91
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.26; acc: 0.91
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.35; acc: 0.91
Batch: 400; loss: 0.05; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.43; acc: 0.91
Batch: 460; loss: 0.12; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.95
Batch: 500; loss: 0.36; acc: 0.91
Batch: 520; loss: 0.3; acc: 0.89
Batch: 540; loss: 0.32; acc: 0.88
Batch: 560; loss: 0.18; acc: 0.92
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.97
Batch: 640; loss: 0.3; acc: 0.91
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.95
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.38; acc: 0.91
Batch: 780; loss: 0.07; acc: 0.98
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.08; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.53; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.21249813141574145; val_accuracy: 0.9353105095541401 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.5; acc: 0.88
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.19; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.95
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.27; acc: 0.89
Batch: 160; loss: 0.35; acc: 0.91
Batch: 180; loss: 0.23; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.98
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.91
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.31; acc: 0.94
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.16; acc: 0.95
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.29; acc: 0.91
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.16; acc: 0.94
Batch: 520; loss: 0.37; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.94
Batch: 580; loss: 0.42; acc: 0.86
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.42; acc: 0.84
Batch: 640; loss: 0.35; acc: 0.88
Batch: 660; loss: 0.4; acc: 0.89
Batch: 680; loss: 0.19; acc: 0.91
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.2; acc: 0.92
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.91
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.51; acc: 0.89
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.2157771943647201; val_accuracy: 0.9362062101910829 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.3; acc: 0.89
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.26; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.37; acc: 0.86
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.98
Batch: 180; loss: 0.26; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.06; acc: 1.0
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.45; acc: 0.88
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.41; acc: 0.88
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.92
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.08; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.17; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.91
Batch: 620; loss: 0.2; acc: 0.95
Batch: 640; loss: 0.24; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.1; acc: 0.98
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.3; acc: 0.88
Batch: 760; loss: 0.36; acc: 0.91
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.91
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.52; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21070912900339267; val_accuracy: 0.9352109872611465 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.94
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.91
Batch: 140; loss: 0.11; acc: 0.98
Batch: 160; loss: 0.11; acc: 0.98
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.42; acc: 0.89
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.39; acc: 0.92
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.15; acc: 0.98
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.26; acc: 0.97
Batch: 480; loss: 0.19; acc: 0.92
Batch: 500; loss: 0.13; acc: 0.98
Batch: 520; loss: 0.2; acc: 0.95
Batch: 540; loss: 0.17; acc: 0.95
Batch: 560; loss: 0.29; acc: 0.86
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.33; acc: 0.89
Batch: 620; loss: 0.26; acc: 0.92
Batch: 640; loss: 0.26; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.23; acc: 0.94
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.15; acc: 0.95
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.94
Batch: 60; loss: 0.37; acc: 0.91
Batch: 80; loss: 0.07; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21875714012392009; val_accuracy: 0.9350119426751592 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.2; acc: 0.92
Batch: 60; loss: 0.34; acc: 0.94
Batch: 80; loss: 0.23; acc: 0.94
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.15; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.94
Batch: 180; loss: 0.21; acc: 0.91
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.94
Batch: 240; loss: 0.33; acc: 0.89
Batch: 260; loss: 0.19; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.14; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.92
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.09; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.88
Batch: 420; loss: 0.13; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.25; acc: 0.95
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.91
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.88
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.26; acc: 0.95
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.13; acc: 0.97
Batch: 720; loss: 0.31; acc: 0.86
Batch: 740; loss: 0.22; acc: 0.95
Batch: 760; loss: 0.12; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.21; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.89
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21436085534181185; val_accuracy: 0.9357085987261147 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.25; acc: 0.94
Batch: 100; loss: 0.38; acc: 0.89
Batch: 120; loss: 0.23; acc: 0.97
Batch: 140; loss: 0.26; acc: 0.94
Batch: 160; loss: 0.28; acc: 0.89
Batch: 180; loss: 0.12; acc: 0.95
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.41; acc: 0.89
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.52; acc: 0.89
Batch: 300; loss: 0.29; acc: 0.91
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.17; acc: 0.92
Batch: 360; loss: 0.23; acc: 0.91
Batch: 380; loss: 0.31; acc: 0.92
Batch: 400; loss: 0.42; acc: 0.92
Batch: 420; loss: 0.13; acc: 0.95
Batch: 440; loss: 0.39; acc: 0.81
Batch: 460; loss: 0.19; acc: 0.94
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.17; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.98
Batch: 540; loss: 0.46; acc: 0.86
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.42; acc: 0.89
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.33; acc: 0.89
Batch: 660; loss: 0.2; acc: 0.92
Batch: 680; loss: 0.17; acc: 0.95
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.23; acc: 0.91
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.06; acc: 0.98
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.20357819085430567; val_accuracy: 0.9384952229299363 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.23; acc: 0.89
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.38; acc: 0.89
Batch: 60; loss: 0.28; acc: 0.91
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.98
Batch: 120; loss: 0.08; acc: 0.98
Batch: 140; loss: 0.33; acc: 0.94
Batch: 160; loss: 0.16; acc: 0.94
Batch: 180; loss: 0.35; acc: 0.91
Batch: 200; loss: 0.31; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.07; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.92
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.36; acc: 0.91
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.29; acc: 0.95
Batch: 440; loss: 0.24; acc: 0.89
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.16; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.39; acc: 0.92
Batch: 540; loss: 0.14; acc: 0.97
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.27; acc: 0.91
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.37; acc: 0.89
Batch: 680; loss: 0.26; acc: 0.94
Batch: 700; loss: 0.29; acc: 0.92
Batch: 720; loss: 0.15; acc: 0.94
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.36; acc: 0.89
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.94
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.5; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2039308176037802; val_accuracy: 0.9387937898089171 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.25; acc: 0.91
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.18; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.89
Batch: 120; loss: 0.2; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.27; acc: 0.95
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.28; acc: 0.92
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.89
Batch: 260; loss: 0.33; acc: 0.92
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.19; acc: 0.91
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.89
Batch: 420; loss: 0.17; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.18; acc: 0.92
Batch: 500; loss: 0.25; acc: 0.91
Batch: 520; loss: 0.34; acc: 0.91
Batch: 540; loss: 0.22; acc: 0.94
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.48; acc: 0.92
Batch: 620; loss: 0.21; acc: 0.91
Batch: 640; loss: 0.27; acc: 0.94
Batch: 660; loss: 0.26; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.24; acc: 0.89
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.98
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.3; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.97
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20386674851890962; val_accuracy: 0.9372014331210191 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.21; acc: 0.97
Batch: 20; loss: 0.22; acc: 0.94
Batch: 40; loss: 0.09; acc: 0.97
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.34; acc: 0.86
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.17; acc: 0.92
Batch: 160; loss: 0.23; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.95
Batch: 200; loss: 0.18; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.91
Batch: 240; loss: 0.31; acc: 0.97
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.29; acc: 0.92
Batch: 300; loss: 0.43; acc: 0.88
Batch: 320; loss: 0.18; acc: 0.94
Batch: 340; loss: 0.23; acc: 0.91
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.28; acc: 0.95
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.16; acc: 0.95
Batch: 440; loss: 0.21; acc: 0.94
Batch: 460; loss: 0.23; acc: 0.92
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.39; acc: 0.89
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.14; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.18; acc: 0.95
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.23; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.05; acc: 0.98
Batch: 760; loss: 0.18; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2066071541256206; val_accuracy: 0.9372014331210191 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.35; acc: 0.92
Batch: 80; loss: 0.22; acc: 0.91
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.13; acc: 0.95
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.92
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.32; acc: 0.91
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.16; acc: 0.97
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.18; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.95
Batch: 340; loss: 0.23; acc: 0.92
Batch: 360; loss: 0.39; acc: 0.89
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.19; acc: 0.94
Batch: 440; loss: 0.35; acc: 0.89
Batch: 460; loss: 0.18; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.95
Batch: 500; loss: 0.19; acc: 0.91
Batch: 520; loss: 0.39; acc: 0.91
Batch: 540; loss: 0.32; acc: 0.92
Batch: 560; loss: 0.38; acc: 0.92
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.25; acc: 0.91
Batch: 620; loss: 0.23; acc: 0.91
Batch: 640; loss: 0.25; acc: 0.94
Batch: 660; loss: 0.25; acc: 0.94
Batch: 680; loss: 0.16; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.09; acc: 0.98
Batch: 740; loss: 0.22; acc: 0.94
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.28; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20518930168573263; val_accuracy: 0.9378980891719745 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.44; acc: 0.88
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.3; acc: 0.89
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.12; acc: 0.97
Batch: 180; loss: 0.37; acc: 0.89
Batch: 200; loss: 0.3; acc: 0.92
Batch: 220; loss: 0.12; acc: 0.97
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.18; acc: 0.95
Batch: 300; loss: 0.3; acc: 0.91
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.95
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.37; acc: 0.91
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.3; acc: 0.92
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.13; acc: 0.95
Batch: 480; loss: 0.34; acc: 0.91
Batch: 500; loss: 0.1; acc: 0.98
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.11; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.35; acc: 0.91
Batch: 600; loss: 0.31; acc: 0.89
Batch: 620; loss: 0.24; acc: 0.92
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.25; acc: 0.92
Batch: 680; loss: 0.31; acc: 0.91
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.29; acc: 0.89
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.11; acc: 0.97
Batch: 780; loss: 0.17; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.20823449175211656; val_accuracy: 0.9370023885350318 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.09; acc: 0.97
Batch: 40; loss: 0.3; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.26; acc: 0.92
Batch: 160; loss: 0.18; acc: 0.95
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.07; acc: 0.98
Batch: 220; loss: 0.33; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.94
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.94
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.92
Batch: 440; loss: 0.22; acc: 0.94
Batch: 460; loss: 0.12; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.12; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.23; acc: 0.91
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.17; acc: 0.91
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.31; acc: 0.89
Batch: 700; loss: 0.11; acc: 0.95
Batch: 720; loss: 0.41; acc: 0.91
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.2068869399654258; val_accuracy: 0.9376990445859873 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.24; acc: 0.89
Batch: 20; loss: 0.29; acc: 0.94
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.36; acc: 0.91
Batch: 80; loss: 0.39; acc: 0.91
Batch: 100; loss: 0.17; acc: 0.95
Batch: 120; loss: 0.17; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.19; acc: 0.94
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.21; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.97
Batch: 240; loss: 0.34; acc: 0.89
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.16; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.11; acc: 0.95
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.29; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.21; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.88
Batch: 520; loss: 0.16; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.13; acc: 0.94
Batch: 580; loss: 0.15; acc: 0.95
Batch: 600; loss: 0.11; acc: 0.94
Batch: 620; loss: 0.26; acc: 0.94
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.24; acc: 0.94
Batch: 740; loss: 0.18; acc: 0.94
Batch: 760; loss: 0.27; acc: 0.95
Batch: 780; loss: 0.12; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.48; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20666658910359167; val_accuracy: 0.9378980891719745 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.14; acc: 0.95
Batch: 20; loss: 0.2; acc: 0.95
Batch: 40; loss: 0.31; acc: 0.91
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.21; acc: 0.94
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.27; acc: 0.94
Batch: 140; loss: 0.21; acc: 0.92
Batch: 160; loss: 0.16; acc: 0.97
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.44; acc: 0.94
Batch: 220; loss: 0.07; acc: 0.98
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.35; acc: 0.91
Batch: 280; loss: 0.62; acc: 0.83
Batch: 300; loss: 0.25; acc: 0.92
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.22; acc: 0.95
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.06; acc: 0.97
Batch: 400; loss: 0.26; acc: 0.94
Batch: 420; loss: 0.16; acc: 0.94
Batch: 440; loss: 0.22; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.24; acc: 0.92
Batch: 540; loss: 0.17; acc: 0.94
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.12; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.92
Batch: 640; loss: 0.22; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.45; acc: 0.84
Batch: 700; loss: 0.19; acc: 0.94
Batch: 720; loss: 0.34; acc: 0.91
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.32; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2034701731791542; val_accuracy: 0.9378980891719745 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.91
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.16; acc: 0.95
Batch: 200; loss: 0.11; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.91
Batch: 240; loss: 0.26; acc: 0.92
Batch: 260; loss: 0.3; acc: 0.92
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.2; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.3; acc: 0.89
Batch: 400; loss: 0.16; acc: 0.94
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.1; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.27; acc: 0.92
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.07; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.06; acc: 0.98
Batch: 640; loss: 0.34; acc: 0.88
Batch: 660; loss: 0.07; acc: 1.0
Batch: 680; loss: 0.13; acc: 0.95
Batch: 700; loss: 0.14; acc: 0.97
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.37; acc: 0.91
Batch: 760; loss: 0.38; acc: 0.89
Batch: 780; loss: 0.28; acc: 0.88
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.18; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2041963311090211; val_accuracy: 0.9392914012738853 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.95
Batch: 20; loss: 0.28; acc: 0.91
Batch: 40; loss: 0.4; acc: 0.94
Batch: 60; loss: 0.44; acc: 0.89
Batch: 80; loss: 0.16; acc: 0.92
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.11; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.15; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.95
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.21; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.97
Batch: 280; loss: 0.49; acc: 0.86
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.94
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.17; acc: 0.91
Batch: 380; loss: 0.09; acc: 0.98
Batch: 400; loss: 0.18; acc: 0.97
Batch: 420; loss: 0.08; acc: 0.98
Batch: 440; loss: 0.16; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.32; acc: 0.89
Batch: 560; loss: 0.24; acc: 0.94
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.25; acc: 0.94
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.95
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.92
Batch: 720; loss: 0.27; acc: 0.94
Batch: 740; loss: 0.32; acc: 0.91
Batch: 760; loss: 0.24; acc: 0.97
Batch: 780; loss: 0.16; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.26; acc: 0.91
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20230850151437482; val_accuracy: 0.9384952229299363 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.3; acc: 0.88
Batch: 60; loss: 0.21; acc: 0.94
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.14; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.92
Batch: 160; loss: 0.1; acc: 0.97
Batch: 180; loss: 0.23; acc: 0.98
Batch: 200; loss: 0.34; acc: 0.89
Batch: 220; loss: 0.1; acc: 0.97
Batch: 240; loss: 0.07; acc: 1.0
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.24; acc: 0.91
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.95
Batch: 380; loss: 0.32; acc: 0.92
Batch: 400; loss: 0.1; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.26; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.09; acc: 0.98
Batch: 540; loss: 0.21; acc: 0.94
Batch: 560; loss: 0.35; acc: 0.88
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.25; acc: 0.88
Batch: 620; loss: 0.27; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.95
Batch: 700; loss: 0.09; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.95
Batch: 740; loss: 0.57; acc: 0.81
Batch: 760; loss: 0.25; acc: 0.92
Batch: 780; loss: 0.31; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.95
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2004289930437211; val_accuracy: 0.9386942675159236 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.31; acc: 0.89
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.95
Batch: 100; loss: 0.24; acc: 0.95
Batch: 120; loss: 0.15; acc: 0.97
Batch: 140; loss: 0.24; acc: 0.89
Batch: 160; loss: 0.06; acc: 1.0
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.1; acc: 0.97
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.34; acc: 0.88
Batch: 280; loss: 0.16; acc: 0.95
Batch: 300; loss: 0.22; acc: 0.92
Batch: 320; loss: 0.29; acc: 0.91
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.16; acc: 0.97
Batch: 380; loss: 0.18; acc: 0.92
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.92
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.2; acc: 0.97
Batch: 480; loss: 0.27; acc: 0.86
Batch: 500; loss: 0.35; acc: 0.89
Batch: 520; loss: 0.09; acc: 0.97
Batch: 540; loss: 0.38; acc: 0.83
Batch: 560; loss: 0.26; acc: 0.94
Batch: 580; loss: 0.19; acc: 0.92
Batch: 600; loss: 0.29; acc: 0.91
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.4; acc: 0.89
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.95
Batch: 700; loss: 0.2; acc: 0.89
Batch: 720; loss: 0.22; acc: 0.95
Batch: 740; loss: 0.16; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.13; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2000551695466801; val_accuracy: 0.9388933121019108 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.91
Batch: 80; loss: 0.24; acc: 0.89
Batch: 100; loss: 0.14; acc: 0.97
Batch: 120; loss: 0.07; acc: 1.0
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.34; acc: 0.91
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.34; acc: 0.92
Batch: 220; loss: 0.2; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.98
Batch: 260; loss: 0.32; acc: 0.91
Batch: 280; loss: 0.09; acc: 0.95
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.2; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.29; acc: 0.89
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.28; acc: 0.86
Batch: 440; loss: 0.23; acc: 0.92
Batch: 460; loss: 0.29; acc: 0.91
Batch: 480; loss: 0.27; acc: 0.88
Batch: 500; loss: 0.17; acc: 0.97
Batch: 520; loss: 0.15; acc: 0.97
Batch: 540; loss: 0.25; acc: 0.92
Batch: 560; loss: 0.16; acc: 0.92
Batch: 580; loss: 0.2; acc: 0.92
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.97
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.12; acc: 0.98
Batch: 720; loss: 0.17; acc: 0.94
Batch: 740; loss: 0.34; acc: 0.91
Batch: 760; loss: 0.03; acc: 1.0
Batch: 780; loss: 0.1; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2012676006408444; val_accuracy: 0.9394904458598726 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.94
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.94
Batch: 100; loss: 0.25; acc: 0.94
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.54; acc: 0.89
Batch: 200; loss: 0.16; acc: 0.91
Batch: 220; loss: 0.11; acc: 0.97
Batch: 240; loss: 0.29; acc: 0.91
Batch: 260; loss: 0.14; acc: 0.95
Batch: 280; loss: 0.19; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.97
Batch: 320; loss: 0.19; acc: 0.95
Batch: 340; loss: 0.29; acc: 0.92
Batch: 360; loss: 0.2; acc: 0.94
Batch: 380; loss: 0.18; acc: 0.94
Batch: 400; loss: 0.25; acc: 0.88
Batch: 420; loss: 0.29; acc: 0.91
Batch: 440; loss: 0.25; acc: 0.91
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.95
Batch: 520; loss: 0.32; acc: 0.92
Batch: 540; loss: 0.05; acc: 1.0
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.3; acc: 0.94
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.32; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.92
Batch: 700; loss: 0.27; acc: 0.94
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.16; acc: 0.94
Batch: 780; loss: 0.27; acc: 0.91
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.06; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20164318135969198; val_accuracy: 0.9396894904458599 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.84
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.29; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.28; acc: 0.92
Batch: 120; loss: 0.22; acc: 0.97
Batch: 140; loss: 0.31; acc: 0.91
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.25; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.29; acc: 0.92
Batch: 340; loss: 0.12; acc: 0.95
Batch: 360; loss: 0.17; acc: 0.92
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.95
Batch: 440; loss: 0.12; acc: 0.95
Batch: 460; loss: 0.32; acc: 0.88
Batch: 480; loss: 0.31; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.26; acc: 0.92
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.2; acc: 0.94
Batch: 620; loss: 0.16; acc: 0.94
Batch: 640; loss: 0.11; acc: 0.97
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.24; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.95
Batch: 760; loss: 0.18; acc: 0.92
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.94
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.47; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.2000266623558702; val_accuracy: 0.9395899681528662 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.45; acc: 0.91
Batch: 80; loss: 0.31; acc: 0.91
Batch: 100; loss: 0.24; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.88
Batch: 140; loss: 0.24; acc: 0.92
Batch: 160; loss: 0.22; acc: 0.94
Batch: 180; loss: 0.31; acc: 0.89
Batch: 200; loss: 0.21; acc: 0.89
Batch: 220; loss: 0.19; acc: 0.95
Batch: 240; loss: 0.22; acc: 0.94
Batch: 260; loss: 0.18; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.24; acc: 0.94
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.1; acc: 0.97
Batch: 380; loss: 0.2; acc: 0.92
Batch: 400; loss: 0.17; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.95
Batch: 440; loss: 0.27; acc: 0.88
Batch: 460; loss: 0.19; acc: 0.92
Batch: 480; loss: 0.2; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.97
Batch: 520; loss: 0.31; acc: 0.92
Batch: 540; loss: 0.32; acc: 0.91
Batch: 560; loss: 0.44; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.95
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.91
Batch: 700; loss: 0.23; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.22; acc: 0.92
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.94
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20152697918616283; val_accuracy: 0.9406847133757962 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.37; acc: 0.88
Batch: 20; loss: 0.07; acc: 1.0
Batch: 40; loss: 0.11; acc: 0.98
Batch: 60; loss: 0.43; acc: 0.91
Batch: 80; loss: 0.27; acc: 0.91
Batch: 100; loss: 0.1; acc: 0.97
Batch: 120; loss: 0.1; acc: 0.95
Batch: 140; loss: 0.16; acc: 0.98
Batch: 160; loss: 0.18; acc: 0.94
Batch: 180; loss: 0.08; acc: 0.98
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.39; acc: 0.88
Batch: 260; loss: 0.1; acc: 0.97
Batch: 280; loss: 0.12; acc: 0.97
Batch: 300; loss: 0.12; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.92
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.24; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.92
Batch: 440; loss: 0.23; acc: 0.95
Batch: 460; loss: 0.41; acc: 0.89
Batch: 480; loss: 0.22; acc: 0.94
Batch: 500; loss: 0.13; acc: 0.97
Batch: 520; loss: 0.19; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.94
Batch: 560; loss: 0.3; acc: 0.91
Batch: 580; loss: 0.14; acc: 0.97
Batch: 600; loss: 0.16; acc: 0.95
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.89
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.91
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20057928897914992; val_accuracy: 0.9392914012738853 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.89
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.24; acc: 0.91
Batch: 100; loss: 0.32; acc: 0.91
Batch: 120; loss: 0.11; acc: 0.98
Batch: 140; loss: 0.06; acc: 0.98
Batch: 160; loss: 0.21; acc: 0.92
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.13; acc: 0.94
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.28; acc: 0.92
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.24; acc: 0.89
Batch: 300; loss: 0.13; acc: 0.97
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.19; acc: 0.94
Batch: 360; loss: 0.28; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.31; acc: 0.89
Batch: 420; loss: 0.35; acc: 0.89
Batch: 440; loss: 0.42; acc: 0.89
Batch: 460; loss: 0.3; acc: 0.91
Batch: 480; loss: 0.08; acc: 0.97
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.98
Batch: 540; loss: 0.24; acc: 0.94
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.95
Batch: 600; loss: 0.35; acc: 0.91
Batch: 620; loss: 0.14; acc: 0.97
Batch: 640; loss: 0.07; acc: 0.98
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.34; acc: 0.88
Batch: 700; loss: 0.26; acc: 0.91
Batch: 720; loss: 0.21; acc: 0.88
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.25; acc: 0.92
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.19; acc: 0.95
Batch: 120; loss: 0.46; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.20051348739701091; val_accuracy: 0.9396894904458599 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.14; acc: 0.97
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.1; acc: 0.97
Batch: 80; loss: 0.15; acc: 0.94
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.22; acc: 0.92
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.26; acc: 0.88
Batch: 280; loss: 0.14; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.94
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.97
Batch: 380; loss: 0.21; acc: 0.91
Batch: 400; loss: 0.11; acc: 0.97
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.35; acc: 0.91
Batch: 480; loss: 0.26; acc: 0.92
Batch: 500; loss: 0.21; acc: 0.91
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.2; acc: 0.92
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.08; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.26; acc: 0.89
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.29; acc: 0.92
Batch: 700; loss: 0.08; acc: 0.98
Batch: 720; loss: 0.13; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.98
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.39; acc: 0.88
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.16; acc: 0.97
Batch: 20; loss: 0.24; acc: 0.94
Batch: 40; loss: 0.14; acc: 0.97
Batch: 60; loss: 0.31; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.18; acc: 0.95
Batch: 120; loss: 0.45; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.19988329772641705; val_accuracy: 0.940187101910828 

plots/subspace_training/reg_lenet_2/2020-01-19 19:11:48/d_dim_400_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
nonzero elements in E: 70560
elements in E: 9887000
fraction nonzero: 0.007136644078082331
Epoch 1 start
The current lr is: 1.0
Batch: 0; loss: 2.3; acc: 0.16
Batch: 20; loss: 2.3; acc: 0.12
Batch: 40; loss: 2.3; acc: 0.16
Batch: 60; loss: 2.3; acc: 0.12
Batch: 80; loss: 2.3; acc: 0.12
Batch: 100; loss: 2.3; acc: 0.08
Batch: 120; loss: 2.3; acc: 0.05
Batch: 140; loss: 2.29; acc: 0.23
Batch: 160; loss: 2.3; acc: 0.12
Batch: 180; loss: 2.3; acc: 0.14
Batch: 200; loss: 2.27; acc: 0.23
Batch: 220; loss: 2.26; acc: 0.23
Batch: 240; loss: 2.27; acc: 0.2
Batch: 260; loss: 2.23; acc: 0.25
Batch: 280; loss: 2.23; acc: 0.12
Batch: 300; loss: 2.14; acc: 0.31
Batch: 320; loss: 2.04; acc: 0.25
Batch: 340; loss: 1.99; acc: 0.2
Batch: 360; loss: 1.84; acc: 0.34
Batch: 380; loss: 1.63; acc: 0.45
Batch: 400; loss: 1.56; acc: 0.45
Batch: 420; loss: 1.64; acc: 0.39
Batch: 440; loss: 1.59; acc: 0.48
Batch: 460; loss: 1.23; acc: 0.62
Batch: 480; loss: 1.75; acc: 0.38
Batch: 500; loss: 1.19; acc: 0.61
Batch: 520; loss: 1.47; acc: 0.5
Batch: 540; loss: 1.07; acc: 0.69
Batch: 560; loss: 0.89; acc: 0.69
Batch: 580; loss: 0.91; acc: 0.77
Batch: 600; loss: 0.99; acc: 0.72
Batch: 620; loss: 0.79; acc: 0.75
Batch: 640; loss: 0.67; acc: 0.77
Batch: 660; loss: 0.92; acc: 0.75
Batch: 680; loss: 0.87; acc: 0.72
Batch: 700; loss: 0.69; acc: 0.72
Batch: 720; loss: 0.96; acc: 0.75
Batch: 740; loss: 0.86; acc: 0.67
Batch: 760; loss: 0.56; acc: 0.78
Batch: 780; loss: 0.62; acc: 0.75
Train Epoch over. train_loss: 1.64; train_accuracy: 0.42 

Batch: 0; loss: 1.41; acc: 0.56
Batch: 20; loss: 1.81; acc: 0.44
Batch: 40; loss: 1.05; acc: 0.69
Batch: 60; loss: 1.55; acc: 0.58
Batch: 80; loss: 1.82; acc: 0.56
Batch: 100; loss: 1.6; acc: 0.5
Batch: 120; loss: 1.8; acc: 0.53
Batch: 140; loss: 1.19; acc: 0.61
Val Epoch over. val_loss: 1.5088239129941174; val_accuracy: 0.549562101910828 

Epoch 2 start
The current lr is: 1.0
Batch: 0; loss: 1.1; acc: 0.61
Batch: 20; loss: 0.65; acc: 0.75
Batch: 40; loss: 0.73; acc: 0.84
Batch: 60; loss: 0.75; acc: 0.73
Batch: 80; loss: 0.64; acc: 0.8
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 1.22; acc: 0.66
Batch: 140; loss: 0.56; acc: 0.83
Batch: 160; loss: 0.55; acc: 0.81
Batch: 180; loss: 0.36; acc: 0.88
Batch: 200; loss: 0.41; acc: 0.89
Batch: 220; loss: 0.96; acc: 0.69
Batch: 240; loss: 0.57; acc: 0.81
Batch: 260; loss: 0.79; acc: 0.77
Batch: 280; loss: 0.62; acc: 0.8
Batch: 300; loss: 0.9; acc: 0.73
Batch: 320; loss: 0.66; acc: 0.8
Batch: 340; loss: 0.43; acc: 0.83
Batch: 360; loss: 0.33; acc: 0.91
Batch: 380; loss: 0.47; acc: 0.89
Batch: 400; loss: 0.59; acc: 0.81
Batch: 420; loss: 0.53; acc: 0.84
Batch: 440; loss: 0.58; acc: 0.83
Batch: 460; loss: 0.74; acc: 0.77
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.4; acc: 0.84
Batch: 520; loss: 0.38; acc: 0.88
Batch: 540; loss: 0.25; acc: 0.95
Batch: 560; loss: 0.39; acc: 0.88
Batch: 580; loss: 0.33; acc: 0.92
Batch: 600; loss: 0.53; acc: 0.88
Batch: 620; loss: 0.45; acc: 0.86
Batch: 640; loss: 0.4; acc: 0.83
Batch: 660; loss: 0.39; acc: 0.86
Batch: 680; loss: 0.31; acc: 0.88
Batch: 700; loss: 0.42; acc: 0.91
Batch: 720; loss: 0.53; acc: 0.83
Batch: 740; loss: 0.36; acc: 0.88
Batch: 760; loss: 0.44; acc: 0.91
Batch: 780; loss: 0.46; acc: 0.84
Train Epoch over. train_loss: 0.54; train_accuracy: 0.83 

Batch: 0; loss: 0.36; acc: 0.92
Batch: 20; loss: 0.6; acc: 0.81
Batch: 40; loss: 0.28; acc: 0.92
Batch: 60; loss: 0.56; acc: 0.86
Batch: 80; loss: 0.3; acc: 0.89
Batch: 100; loss: 0.42; acc: 0.89
Batch: 120; loss: 0.9; acc: 0.75
Batch: 140; loss: 0.25; acc: 0.91
Val Epoch over. val_loss: 0.44157274613145053; val_accuracy: 0.8667396496815286 

Epoch 3 start
The current lr is: 1.0
Batch: 0; loss: 0.4; acc: 0.88
Batch: 20; loss: 0.52; acc: 0.83
Batch: 40; loss: 0.29; acc: 0.88
Batch: 60; loss: 0.6; acc: 0.81
Batch: 80; loss: 0.68; acc: 0.81
Batch: 100; loss: 0.33; acc: 0.89
Batch: 120; loss: 0.27; acc: 0.89
Batch: 140; loss: 0.38; acc: 0.88
Batch: 160; loss: 0.3; acc: 0.91
Batch: 180; loss: 0.24; acc: 0.89
Batch: 200; loss: 0.32; acc: 0.88
Batch: 220; loss: 0.32; acc: 0.88
Batch: 240; loss: 0.3; acc: 0.91
Batch: 260; loss: 0.6; acc: 0.83
Batch: 280; loss: 0.33; acc: 0.92
Batch: 300; loss: 0.22; acc: 0.94
Batch: 320; loss: 0.27; acc: 0.92
Batch: 340; loss: 0.39; acc: 0.89
Batch: 360; loss: 0.37; acc: 0.88
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.41; acc: 0.88
Batch: 420; loss: 0.41; acc: 0.86
Batch: 440; loss: 0.39; acc: 0.91
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.48; acc: 0.86
Batch: 500; loss: 0.43; acc: 0.83
Batch: 520; loss: 0.34; acc: 0.92
Batch: 540; loss: 0.31; acc: 0.88
Batch: 560; loss: 0.31; acc: 0.89
Batch: 580; loss: 0.25; acc: 0.92
Batch: 600; loss: 0.38; acc: 0.88
Batch: 620; loss: 0.38; acc: 0.89
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.38; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.35; acc: 0.89
Batch: 740; loss: 0.38; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.86
Batch: 780; loss: 0.5; acc: 0.88
Train Epoch over. train_loss: 0.38; train_accuracy: 0.88 

Batch: 0; loss: 0.7; acc: 0.81
Batch: 20; loss: 0.81; acc: 0.73
Batch: 40; loss: 0.56; acc: 0.86
Batch: 60; loss: 0.77; acc: 0.78
Batch: 80; loss: 0.6; acc: 0.84
Batch: 100; loss: 0.53; acc: 0.86
Batch: 120; loss: 0.9; acc: 0.72
Batch: 140; loss: 0.74; acc: 0.8
Val Epoch over. val_loss: 0.8110106276099089; val_accuracy: 0.779359076433121 

Epoch 4 start
The current lr is: 1.0
Batch: 0; loss: 0.72; acc: 0.73
Batch: 20; loss: 0.45; acc: 0.88
Batch: 40; loss: 0.4; acc: 0.89
Batch: 60; loss: 0.39; acc: 0.89
Batch: 80; loss: 0.46; acc: 0.89
Batch: 100; loss: 0.36; acc: 0.89
Batch: 120; loss: 0.33; acc: 0.91
Batch: 140; loss: 0.41; acc: 0.84
Batch: 160; loss: 0.36; acc: 0.86
Batch: 180; loss: 0.29; acc: 0.89
Batch: 200; loss: 0.17; acc: 0.95
Batch: 220; loss: 0.29; acc: 0.91
Batch: 240; loss: 0.35; acc: 0.84
Batch: 260; loss: 0.29; acc: 0.92
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.36; acc: 0.94
Batch: 320; loss: 0.46; acc: 0.88
Batch: 340; loss: 0.45; acc: 0.88
Batch: 360; loss: 0.14; acc: 0.98
Batch: 380; loss: 0.36; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.24; acc: 0.94
Batch: 440; loss: 0.33; acc: 0.88
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.35; acc: 0.86
Batch: 520; loss: 0.42; acc: 0.88
Batch: 540; loss: 0.19; acc: 0.97
Batch: 560; loss: 0.25; acc: 0.91
Batch: 580; loss: 0.21; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.61; acc: 0.84
Batch: 640; loss: 0.24; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.97
Batch: 680; loss: 0.11; acc: 0.95
Batch: 700; loss: 0.45; acc: 0.83
Batch: 720; loss: 0.32; acc: 0.86
Batch: 740; loss: 0.57; acc: 0.88
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.3; acc: 0.91
Train Epoch over. train_loss: 0.32; train_accuracy: 0.9 

Batch: 0; loss: 0.62; acc: 0.78
Batch: 20; loss: 0.74; acc: 0.83
Batch: 40; loss: 0.46; acc: 0.84
Batch: 60; loss: 0.63; acc: 0.84
Batch: 80; loss: 0.44; acc: 0.84
Batch: 100; loss: 0.83; acc: 0.77
Batch: 120; loss: 1.06; acc: 0.7
Batch: 140; loss: 0.34; acc: 0.88
Val Epoch over. val_loss: 0.6590309652743066; val_accuracy: 0.7926950636942676 

Epoch 5 start
The current lr is: 1.0
Batch: 0; loss: 0.69; acc: 0.8
Batch: 20; loss: 0.25; acc: 0.89
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.29; acc: 0.91
Batch: 80; loss: 0.13; acc: 0.98
Batch: 100; loss: 0.56; acc: 0.89
Batch: 120; loss: 0.55; acc: 0.81
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.44; acc: 0.89
Batch: 180; loss: 0.21; acc: 0.92
Batch: 200; loss: 0.39; acc: 0.92
Batch: 220; loss: 0.47; acc: 0.81
Batch: 240; loss: 0.31; acc: 0.95
Batch: 260; loss: 0.21; acc: 0.91
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.49; acc: 0.92
Batch: 320; loss: 0.52; acc: 0.88
Batch: 340; loss: 0.31; acc: 0.86
Batch: 360; loss: 0.15; acc: 0.95
Batch: 380; loss: 0.23; acc: 0.92
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.44; acc: 0.89
Batch: 460; loss: 0.5; acc: 0.86
Batch: 480; loss: 0.22; acc: 0.92
Batch: 500; loss: 0.27; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.36; acc: 0.89
Batch: 560; loss: 0.27; acc: 0.92
Batch: 580; loss: 0.12; acc: 0.98
Batch: 600; loss: 0.09; acc: 0.98
Batch: 620; loss: 0.44; acc: 0.86
Batch: 640; loss: 0.18; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.92
Batch: 680; loss: 0.23; acc: 0.89
Batch: 700; loss: 0.43; acc: 0.89
Batch: 720; loss: 0.29; acc: 0.88
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.18; acc: 0.97
Train Epoch over. train_loss: 0.3; train_accuracy: 0.91 

Batch: 0; loss: 0.51; acc: 0.88
Batch: 20; loss: 0.43; acc: 0.84
Batch: 40; loss: 0.38; acc: 0.88
Batch: 60; loss: 0.47; acc: 0.86
Batch: 80; loss: 0.2; acc: 0.94
Batch: 100; loss: 0.61; acc: 0.77
Batch: 120; loss: 0.67; acc: 0.8
Batch: 140; loss: 0.09; acc: 0.97
Val Epoch over. val_loss: 0.4505127543571648; val_accuracy: 0.8458399681528662 

Epoch 6 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.91
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.43; acc: 0.86
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.2; acc: 0.97
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.43; acc: 0.91
Batch: 160; loss: 0.51; acc: 0.88
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.33; acc: 0.92
Batch: 220; loss: 0.38; acc: 0.91
Batch: 240; loss: 0.27; acc: 0.92
Batch: 260; loss: 0.16; acc: 0.95
Batch: 280; loss: 0.4; acc: 0.88
Batch: 300; loss: 0.23; acc: 0.94
Batch: 320; loss: 0.38; acc: 0.88
Batch: 340; loss: 0.11; acc: 0.95
Batch: 360; loss: 0.49; acc: 0.84
Batch: 380; loss: 0.27; acc: 0.92
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.47; acc: 0.89
Batch: 440; loss: 0.52; acc: 0.84
Batch: 460; loss: 0.11; acc: 0.97
Batch: 480; loss: 0.29; acc: 0.89
Batch: 500; loss: 0.32; acc: 0.88
Batch: 520; loss: 0.18; acc: 0.94
Batch: 540; loss: 0.17; acc: 0.92
Batch: 560; loss: 0.24; acc: 0.95
Batch: 580; loss: 0.34; acc: 0.89
Batch: 600; loss: 0.4; acc: 0.89
Batch: 620; loss: 0.38; acc: 0.88
Batch: 640; loss: 0.29; acc: 0.91
Batch: 660; loss: 0.33; acc: 0.89
Batch: 680; loss: 0.36; acc: 0.86
Batch: 700; loss: 0.29; acc: 0.91
Batch: 720; loss: 0.19; acc: 0.94
Batch: 740; loss: 0.31; acc: 0.89
Batch: 760; loss: 0.14; acc: 0.97
Batch: 780; loss: 0.28; acc: 0.89
Train Epoch over. train_loss: 0.28; train_accuracy: 0.91 

Batch: 0; loss: 0.21; acc: 0.92
Batch: 20; loss: 0.27; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.89
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.61; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.27291025022032916; val_accuracy: 0.9174960191082803 

Epoch 7 start
The current lr is: 1.0
Batch: 0; loss: 0.38; acc: 0.89
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.32; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.92
Batch: 80; loss: 0.4; acc: 0.89
Batch: 100; loss: 0.18; acc: 0.94
Batch: 120; loss: 0.24; acc: 0.91
Batch: 140; loss: 0.27; acc: 0.88
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.13; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.3; acc: 0.89
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.28; acc: 0.91
Batch: 280; loss: 0.36; acc: 0.89
Batch: 300; loss: 0.51; acc: 0.84
Batch: 320; loss: 0.35; acc: 0.86
Batch: 340; loss: 0.15; acc: 0.97
Batch: 360; loss: 0.19; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.91
Batch: 400; loss: 0.29; acc: 0.91
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.2; acc: 0.92
Batch: 460; loss: 0.38; acc: 0.89
Batch: 480; loss: 0.21; acc: 0.94
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.41; acc: 0.84
Batch: 540; loss: 0.14; acc: 0.94
Batch: 560; loss: 0.18; acc: 0.97
Batch: 580; loss: 0.22; acc: 0.91
Batch: 600; loss: 0.32; acc: 0.92
Batch: 620; loss: 0.26; acc: 0.89
Batch: 640; loss: 0.62; acc: 0.86
Batch: 660; loss: 0.29; acc: 0.92
Batch: 680; loss: 0.3; acc: 0.94
Batch: 700; loss: 0.27; acc: 0.89
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.24; acc: 0.91
Batch: 760; loss: 0.15; acc: 0.97
Batch: 780; loss: 0.28; acc: 0.95
Train Epoch over. train_loss: 0.27; train_accuracy: 0.92 

Batch: 0; loss: 0.18; acc: 0.92
Batch: 20; loss: 0.29; acc: 0.89
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.24; acc: 0.94
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.2560926997547696; val_accuracy: 0.9177945859872612 

Epoch 8 start
The current lr is: 1.0
Batch: 0; loss: 0.13; acc: 0.97
Batch: 20; loss: 0.38; acc: 0.91
Batch: 40; loss: 0.33; acc: 0.92
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.38; acc: 0.92
Batch: 100; loss: 0.39; acc: 0.86
Batch: 120; loss: 0.16; acc: 0.94
Batch: 140; loss: 0.15; acc: 0.94
Batch: 160; loss: 0.32; acc: 0.92
Batch: 180; loss: 0.3; acc: 0.92
Batch: 200; loss: 0.24; acc: 0.92
Batch: 220; loss: 0.29; acc: 0.88
Batch: 240; loss: 0.23; acc: 0.92
Batch: 260; loss: 0.13; acc: 0.97
Batch: 280; loss: 0.35; acc: 0.89
Batch: 300; loss: 0.34; acc: 0.92
Batch: 320; loss: 0.23; acc: 0.95
Batch: 340; loss: 0.28; acc: 0.86
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.39; acc: 0.88
Batch: 400; loss: 0.32; acc: 0.88
Batch: 420; loss: 0.31; acc: 0.89
Batch: 440; loss: 0.19; acc: 0.92
Batch: 460; loss: 0.4; acc: 0.88
Batch: 480; loss: 0.28; acc: 0.88
Batch: 500; loss: 0.34; acc: 0.95
Batch: 520; loss: 0.3; acc: 0.94
Batch: 540; loss: 0.23; acc: 0.94
Batch: 560; loss: 0.47; acc: 0.86
Batch: 580; loss: 0.22; acc: 0.95
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.33; acc: 0.88
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.36; acc: 0.92
Batch: 700; loss: 0.24; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.14; acc: 0.94
Batch: 760; loss: 0.32; acc: 0.92
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.26; train_accuracy: 0.92 

Batch: 0; loss: 0.38; acc: 0.84
Batch: 20; loss: 0.41; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.88
Batch: 60; loss: 0.43; acc: 0.8
Batch: 80; loss: 0.35; acc: 0.88
Batch: 100; loss: 0.52; acc: 0.83
Batch: 120; loss: 0.54; acc: 0.83
Batch: 140; loss: 0.06; acc: 0.97
Val Epoch over. val_loss: 0.39387702448352885; val_accuracy: 0.8779856687898089 

Epoch 9 start
The current lr is: 1.0
Batch: 0; loss: 0.25; acc: 0.89
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.43; acc: 0.91
Batch: 60; loss: 0.09; acc: 0.98
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.39; acc: 0.89
Batch: 120; loss: 0.35; acc: 0.89
Batch: 140; loss: 0.34; acc: 0.86
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.1; acc: 0.95
Batch: 200; loss: 0.29; acc: 0.91
Batch: 220; loss: 0.13; acc: 0.97
Batch: 240; loss: 0.36; acc: 0.88
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.92
Batch: 300; loss: 0.17; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.2; acc: 0.94
Batch: 360; loss: 0.37; acc: 0.89
Batch: 380; loss: 0.28; acc: 0.92
Batch: 400; loss: 0.45; acc: 0.89
Batch: 420; loss: 0.28; acc: 0.91
Batch: 440; loss: 0.43; acc: 0.86
Batch: 460; loss: 0.18; acc: 0.97
Batch: 480; loss: 0.37; acc: 0.91
Batch: 500; loss: 0.29; acc: 0.89
Batch: 520; loss: 0.52; acc: 0.84
Batch: 540; loss: 0.08; acc: 0.97
Batch: 560; loss: 0.42; acc: 0.88
Batch: 580; loss: 0.24; acc: 0.92
Batch: 600; loss: 0.12; acc: 0.97
Batch: 620; loss: 0.19; acc: 0.94
Batch: 640; loss: 0.34; acc: 0.89
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.25; acc: 0.92
Batch: 700; loss: 0.45; acc: 0.86
Batch: 720; loss: 0.6; acc: 0.83
Batch: 740; loss: 0.08; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.86
Batch: 780; loss: 0.27; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.21; acc: 0.94
Batch: 20; loss: 0.57; acc: 0.88
Batch: 40; loss: 0.44; acc: 0.84
Batch: 60; loss: 0.3; acc: 0.84
Batch: 80; loss: 0.2; acc: 0.91
Batch: 100; loss: 0.58; acc: 0.83
Batch: 120; loss: 0.67; acc: 0.81
Batch: 140; loss: 0.15; acc: 0.94
Val Epoch over. val_loss: 0.44109563728806317; val_accuracy: 0.8614649681528662 

Epoch 10 start
The current lr is: 1.0
Batch: 0; loss: 0.54; acc: 0.8
Batch: 20; loss: 0.16; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.95
Batch: 60; loss: 0.27; acc: 0.89
Batch: 80; loss: 0.41; acc: 0.92
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.3; acc: 0.92
Batch: 140; loss: 0.13; acc: 0.94
Batch: 160; loss: 0.4; acc: 0.88
Batch: 180; loss: 0.18; acc: 0.95
Batch: 200; loss: 0.1; acc: 0.95
Batch: 220; loss: 0.36; acc: 0.91
Batch: 240; loss: 0.25; acc: 0.92
Batch: 260; loss: 0.26; acc: 0.89
Batch: 280; loss: 0.23; acc: 0.94
Batch: 300; loss: 0.11; acc: 0.97
Batch: 320; loss: 0.21; acc: 0.95
Batch: 340; loss: 0.19; acc: 0.95
Batch: 360; loss: 0.08; acc: 0.97
Batch: 380; loss: 0.32; acc: 0.91
Batch: 400; loss: 0.31; acc: 0.92
Batch: 420; loss: 0.35; acc: 0.95
Batch: 440; loss: 0.2; acc: 0.91
Batch: 460; loss: 0.48; acc: 0.83
Batch: 480; loss: 0.14; acc: 0.97
Batch: 500; loss: 0.25; acc: 0.89
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.13; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.19; acc: 0.95
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.16; acc: 0.97
Batch: 660; loss: 0.2; acc: 0.97
Batch: 680; loss: 0.36; acc: 0.89
Batch: 700; loss: 0.17; acc: 0.94
Batch: 720; loss: 0.26; acc: 0.89
Batch: 740; loss: 0.29; acc: 0.89
Batch: 760; loss: 0.46; acc: 0.88
Batch: 780; loss: 0.18; acc: 0.92
Train Epoch over. train_loss: 0.25; train_accuracy: 0.92 

Batch: 0; loss: 0.12; acc: 0.95
Batch: 20; loss: 0.21; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.92
Batch: 60; loss: 0.21; acc: 0.91
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.55; acc: 0.88
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21187346867600065; val_accuracy: 0.9349124203821656 

Epoch 11 start
The current lr is: 0.4
Batch: 0; loss: 0.28; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.36; acc: 0.89
Batch: 60; loss: 0.32; acc: 0.92
Batch: 80; loss: 0.14; acc: 0.97
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.95
Batch: 140; loss: 0.18; acc: 0.94
Batch: 160; loss: 0.25; acc: 0.89
Batch: 180; loss: 0.36; acc: 0.86
Batch: 200; loss: 0.28; acc: 0.91
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.2; acc: 0.92
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.12; acc: 0.95
Batch: 300; loss: 0.24; acc: 0.91
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.27; acc: 0.92
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.1; acc: 0.97
Batch: 400; loss: 0.16; acc: 0.95
Batch: 420; loss: 0.37; acc: 0.89
Batch: 440; loss: 0.15; acc: 0.97
Batch: 460; loss: 0.13; acc: 0.97
Batch: 480; loss: 0.1; acc: 0.98
Batch: 500; loss: 0.27; acc: 0.94
Batch: 520; loss: 0.24; acc: 0.91
Batch: 540; loss: 0.33; acc: 0.86
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.19; acc: 0.91
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.92
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.18; acc: 0.94
Batch: 740; loss: 0.12; acc: 0.94
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.26; acc: 0.92
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.15; acc: 0.92
Batch: 20; loss: 0.2; acc: 0.94
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.25; acc: 0.94
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.83; acc: 0.84
Batch: 140; loss: 0.07; acc: 0.98
Val Epoch over. val_loss: 0.268669978306172; val_accuracy: 0.9157046178343949 

Epoch 12 start
The current lr is: 0.4
Batch: 0; loss: 0.31; acc: 0.89
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.66; acc: 0.88
Batch: 60; loss: 0.27; acc: 0.91
Batch: 80; loss: 0.12; acc: 0.98
Batch: 100; loss: 0.14; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.92
Batch: 180; loss: 0.19; acc: 0.92
Batch: 200; loss: 0.16; acc: 0.94
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.11; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.97
Batch: 280; loss: 0.09; acc: 0.97
Batch: 300; loss: 0.24; acc: 0.92
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.26; acc: 0.91
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.32; acc: 0.92
Batch: 420; loss: 0.38; acc: 0.91
Batch: 440; loss: 0.2; acc: 0.95
Batch: 460; loss: 0.26; acc: 0.95
Batch: 480; loss: 0.2; acc: 0.91
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.1; acc: 0.95
Batch: 540; loss: 0.27; acc: 0.92
Batch: 560; loss: 0.25; acc: 0.94
Batch: 580; loss: 0.26; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.21; acc: 0.92
Batch: 640; loss: 0.15; acc: 0.94
Batch: 660; loss: 0.17; acc: 0.91
Batch: 680; loss: 0.35; acc: 0.91
Batch: 700; loss: 0.25; acc: 0.94
Batch: 720; loss: 0.1; acc: 0.97
Batch: 740; loss: 0.45; acc: 0.88
Batch: 760; loss: 0.09; acc: 0.97
Batch: 780; loss: 0.11; acc: 0.95
Train Epoch over. train_loss: 0.2; train_accuracy: 0.94 

Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.44; acc: 0.89
Batch: 40; loss: 0.26; acc: 0.95
Batch: 60; loss: 0.22; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.35; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.78
Batch: 140; loss: 0.08; acc: 0.95
Val Epoch over. val_loss: 0.30509771791043555; val_accuracy: 0.9075437898089171 

Epoch 13 start
The current lr is: 0.4
Batch: 0; loss: 0.23; acc: 0.88
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.27; acc: 0.89
Batch: 60; loss: 0.16; acc: 0.95
Batch: 80; loss: 0.15; acc: 0.95
Batch: 100; loss: 0.16; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.18; acc: 0.95
Batch: 160; loss: 0.26; acc: 0.89
Batch: 180; loss: 0.1; acc: 0.98
Batch: 200; loss: 0.09; acc: 0.95
Batch: 220; loss: 0.3; acc: 0.92
Batch: 240; loss: 0.23; acc: 0.95
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.23; acc: 0.91
Batch: 300; loss: 0.17; acc: 0.95
Batch: 320; loss: 0.27; acc: 0.94
Batch: 340; loss: 0.17; acc: 0.94
Batch: 360; loss: 0.24; acc: 0.89
Batch: 380; loss: 0.19; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.94
Batch: 420; loss: 0.04; acc: 1.0
Batch: 440; loss: 0.14; acc: 0.95
Batch: 460; loss: 0.15; acc: 0.97
Batch: 480; loss: 0.32; acc: 0.91
Batch: 500; loss: 0.07; acc: 0.97
Batch: 520; loss: 0.22; acc: 0.92
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.19; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.95
Batch: 600; loss: 0.18; acc: 0.97
Batch: 620; loss: 0.23; acc: 0.97
Batch: 640; loss: 0.2; acc: 0.92
Batch: 660; loss: 0.29; acc: 0.94
Batch: 680; loss: 0.21; acc: 0.94
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.19; acc: 0.95
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.45; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.14; acc: 0.98
Batch: 20; loss: 0.24; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.94
Batch: 60; loss: 0.4; acc: 0.89
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.31; acc: 0.89
Batch: 120; loss: 0.74; acc: 0.77
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.2892633837167245; val_accuracy: 0.9072452229299363 

Epoch 14 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.89
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.28; acc: 0.94
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.43; acc: 0.86
Batch: 100; loss: 0.17; acc: 0.92
Batch: 120; loss: 0.24; acc: 0.92
Batch: 140; loss: 0.14; acc: 0.94
Batch: 160; loss: 0.29; acc: 0.92
Batch: 180; loss: 0.22; acc: 0.94
Batch: 200; loss: 0.18; acc: 0.95
Batch: 220; loss: 0.15; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.29; acc: 0.91
Batch: 280; loss: 0.13; acc: 0.97
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.18; acc: 0.95
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.95
Batch: 380; loss: 0.27; acc: 0.95
Batch: 400; loss: 0.28; acc: 0.88
Batch: 420; loss: 0.28; acc: 0.92
Batch: 440; loss: 0.13; acc: 0.97
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.17; acc: 0.95
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.09; acc: 0.95
Batch: 560; loss: 0.22; acc: 0.94
Batch: 580; loss: 0.28; acc: 0.94
Batch: 600; loss: 0.1; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.95
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.16; acc: 0.94
Batch: 740; loss: 0.07; acc: 0.97
Batch: 760; loss: 0.22; acc: 0.88
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.2; acc: 0.94
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.42; acc: 0.84
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.17911706658400547; val_accuracy: 0.9467555732484076 

Epoch 15 start
The current lr is: 0.4
Batch: 0; loss: 0.08; acc: 0.98
Batch: 20; loss: 0.25; acc: 0.95
Batch: 40; loss: 0.27; acc: 0.91
Batch: 60; loss: 0.26; acc: 0.92
Batch: 80; loss: 0.21; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.97
Batch: 120; loss: 0.28; acc: 0.92
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.27; acc: 0.92
Batch: 180; loss: 0.04; acc: 1.0
Batch: 200; loss: 0.25; acc: 0.92
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.07; acc: 1.0
Batch: 260; loss: 0.14; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.95
Batch: 320; loss: 0.45; acc: 0.84
Batch: 340; loss: 0.06; acc: 0.97
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.13; acc: 0.97
Batch: 400; loss: 0.14; acc: 0.98
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.92
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.43; acc: 0.88
Batch: 520; loss: 0.22; acc: 0.89
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.07; acc: 0.98
Batch: 600; loss: 0.13; acc: 0.98
Batch: 620; loss: 0.16; acc: 0.97
Batch: 640; loss: 0.17; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.14; acc: 0.97
Batch: 700; loss: 0.11; acc: 0.98
Batch: 720; loss: 0.3; acc: 0.92
Batch: 740; loss: 0.3; acc: 0.94
Batch: 760; loss: 0.1; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.16; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.18003560728423154; val_accuracy: 0.9436703821656051 

Epoch 16 start
The current lr is: 0.4
Batch: 0; loss: 0.18; acc: 0.95
Batch: 20; loss: 0.23; acc: 0.92
Batch: 40; loss: 0.21; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.26; acc: 0.92
Batch: 140; loss: 0.21; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.91
Batch: 180; loss: 0.29; acc: 0.94
Batch: 200; loss: 0.24; acc: 0.95
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.2; acc: 0.94
Batch: 280; loss: 0.22; acc: 0.94
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.14; acc: 0.95
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.91
Batch: 380; loss: 0.08; acc: 0.95
Batch: 400; loss: 0.24; acc: 0.97
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.3; acc: 0.97
Batch: 460; loss: 0.15; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.28; acc: 0.91
Batch: 520; loss: 0.04; acc: 0.98
Batch: 540; loss: 0.22; acc: 0.92
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.15; acc: 0.95
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.28; acc: 0.89
Batch: 660; loss: 0.06; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.92
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.17; acc: 0.92
Batch: 740; loss: 0.23; acc: 0.95
Batch: 760; loss: 0.17; acc: 0.95
Batch: 780; loss: 0.24; acc: 0.91
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.06; acc: 0.98
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.17200201516081193; val_accuracy: 0.9476512738853503 

Epoch 17 start
The current lr is: 0.4
Batch: 0; loss: 0.19; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.06; acc: 1.0
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.34; acc: 0.91
Batch: 100; loss: 0.33; acc: 0.91
Batch: 120; loss: 0.26; acc: 0.89
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.19; acc: 0.97
Batch: 180; loss: 0.21; acc: 0.94
Batch: 200; loss: 0.09; acc: 1.0
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.42; acc: 0.86
Batch: 260; loss: 0.11; acc: 0.95
Batch: 280; loss: 0.41; acc: 0.89
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.13; acc: 0.97
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.14; acc: 0.97
Batch: 380; loss: 0.24; acc: 0.94
Batch: 400; loss: 0.23; acc: 0.92
Batch: 420; loss: 0.34; acc: 0.92
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.31; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.05; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.17; acc: 0.98
Batch: 560; loss: 0.13; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.98
Batch: 600; loss: 0.17; acc: 0.94
Batch: 620; loss: 0.28; acc: 0.94
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.2; acc: 0.92
Batch: 700; loss: 0.22; acc: 0.92
Batch: 720; loss: 0.13; acc: 0.97
Batch: 740; loss: 0.21; acc: 0.92
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.19; acc: 0.95
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.17; acc: 0.94
Batch: 20; loss: 0.26; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.37; acc: 0.91
Batch: 120; loss: 0.38; acc: 0.86
Batch: 140; loss: 0.04; acc: 1.0
Val Epoch over. val_loss: 0.22263988684032374; val_accuracy: 0.9309315286624203 

Epoch 18 start
The current lr is: 0.4
Batch: 0; loss: 0.26; acc: 0.89
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.97
Batch: 80; loss: 0.14; acc: 0.95
Batch: 100; loss: 0.19; acc: 0.97
Batch: 120; loss: 0.09; acc: 1.0
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.08; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.95
Batch: 200; loss: 0.24; acc: 0.94
Batch: 220; loss: 0.26; acc: 0.94
Batch: 240; loss: 0.15; acc: 0.97
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.11; acc: 0.97
Batch: 300; loss: 0.14; acc: 0.97
Batch: 320; loss: 0.09; acc: 0.97
Batch: 340; loss: 0.3; acc: 0.91
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.89
Batch: 440; loss: 0.36; acc: 0.88
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.31; acc: 0.91
Batch: 520; loss: 0.18; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.04; acc: 0.98
Batch: 580; loss: 0.58; acc: 0.88
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.27; acc: 0.94
Batch: 640; loss: 0.39; acc: 0.88
Batch: 660; loss: 0.15; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.21; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.24; acc: 0.94
Batch: 760; loss: 0.22; acc: 0.91
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.19; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.28; acc: 0.92
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.91
Batch: 120; loss: 0.43; acc: 0.89
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.21162809042414282; val_accuracy: 0.9347133757961783 

Epoch 19 start
The current lr is: 0.4
Batch: 0; loss: 0.3; acc: 0.94
Batch: 20; loss: 0.31; acc: 0.92
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.94
Batch: 80; loss: 0.13; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.91
Batch: 140; loss: 0.25; acc: 0.95
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.98
Batch: 200; loss: 0.35; acc: 0.95
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.13; acc: 0.97
Batch: 260; loss: 0.11; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.95
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.17; acc: 0.95
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.98
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.15; acc: 0.95
Batch: 420; loss: 0.14; acc: 0.97
Batch: 440; loss: 0.11; acc: 0.97
Batch: 460; loss: 0.24; acc: 0.94
Batch: 480; loss: 0.11; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.95
Batch: 520; loss: 0.28; acc: 0.88
Batch: 540; loss: 0.23; acc: 0.92
Batch: 560; loss: 0.28; acc: 0.92
Batch: 580; loss: 0.21; acc: 0.92
Batch: 600; loss: 0.09; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.92
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.13; acc: 0.95
Batch: 680; loss: 0.57; acc: 0.88
Batch: 700; loss: 0.07; acc: 0.98
Batch: 720; loss: 0.23; acc: 0.91
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.32; acc: 0.94
Batch: 780; loss: 0.36; acc: 0.88
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.19; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.11; acc: 0.95
Batch: 100; loss: 0.25; acc: 0.95
Batch: 120; loss: 0.49; acc: 0.86
Batch: 140; loss: 0.05; acc: 0.97
Val Epoch over. val_loss: 0.18259168406770487; val_accuracy: 0.9434713375796179 

Epoch 20 start
The current lr is: 0.4
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.22; acc: 0.89
Batch: 100; loss: 0.15; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.15; acc: 0.97
Batch: 160; loss: 0.21; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.98
Batch: 200; loss: 0.1; acc: 0.98
Batch: 220; loss: 0.08; acc: 1.0
Batch: 240; loss: 0.12; acc: 0.94
Batch: 260; loss: 0.06; acc: 0.98
Batch: 280; loss: 0.46; acc: 0.84
Batch: 300; loss: 0.09; acc: 0.98
Batch: 320; loss: 0.3; acc: 0.91
Batch: 340; loss: 0.22; acc: 0.94
Batch: 360; loss: 0.3; acc: 0.91
Batch: 380; loss: 0.12; acc: 0.95
Batch: 400; loss: 0.11; acc: 0.98
Batch: 420; loss: 0.1; acc: 0.95
Batch: 440; loss: 0.18; acc: 0.94
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.16; acc: 0.95
Batch: 500; loss: 0.07; acc: 0.98
Batch: 520; loss: 0.29; acc: 0.89
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.45; acc: 0.91
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.18; acc: 0.92
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.09; acc: 0.95
Batch: 660; loss: 0.16; acc: 0.94
Batch: 680; loss: 0.12; acc: 0.98
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.18; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.19; train_accuracy: 0.94 

Batch: 0; loss: 0.1; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.21; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.17; acc: 0.94
Batch: 120; loss: 0.55; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.18220188411747573; val_accuracy: 0.9421775477707006 

Epoch 21 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.3; acc: 0.91
Batch: 40; loss: 0.12; acc: 0.95
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.23; acc: 0.89
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.1; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.98
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.94
Batch: 280; loss: 0.36; acc: 0.91
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.19; acc: 0.92
Batch: 360; loss: 0.18; acc: 0.94
Batch: 380; loss: 0.17; acc: 0.92
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.89
Batch: 440; loss: 0.17; acc: 0.95
Batch: 460; loss: 0.08; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.22; acc: 0.92
Batch: 520; loss: 0.21; acc: 0.95
Batch: 540; loss: 0.3; acc: 0.89
Batch: 560; loss: 0.26; acc: 0.91
Batch: 580; loss: 0.13; acc: 0.94
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.11; acc: 0.95
Batch: 660; loss: 0.43; acc: 0.89
Batch: 680; loss: 0.21; acc: 0.95
Batch: 700; loss: 0.36; acc: 0.91
Batch: 720; loss: 0.18; acc: 0.95
Batch: 740; loss: 0.05; acc: 1.0
Batch: 760; loss: 0.15; acc: 0.94
Batch: 780; loss: 0.21; acc: 0.94
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.98
Batch: 20; loss: 0.14; acc: 0.97
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.26; acc: 0.91
Batch: 120; loss: 0.48; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.16223254638492682; val_accuracy: 0.9488455414012739 

Epoch 22 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.14; acc: 0.94
Batch: 20; loss: 0.22; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.08; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.28; acc: 0.91
Batch: 140; loss: 0.34; acc: 0.91
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.12; acc: 0.94
Batch: 200; loss: 0.11; acc: 0.98
Batch: 220; loss: 0.24; acc: 0.91
Batch: 240; loss: 0.13; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.06; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.05; acc: 1.0
Batch: 340; loss: 0.3; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.97
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.38; acc: 0.89
Batch: 420; loss: 0.12; acc: 0.97
Batch: 440; loss: 0.04; acc: 0.98
Batch: 460; loss: 0.09; acc: 0.98
Batch: 480; loss: 0.27; acc: 0.89
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.13; acc: 0.97
Batch: 540; loss: 0.19; acc: 0.92
Batch: 560; loss: 0.18; acc: 0.95
Batch: 580; loss: 0.1; acc: 0.97
Batch: 600; loss: 0.23; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.94
Batch: 640; loss: 0.21; acc: 0.94
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.22; acc: 0.92
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.17; acc: 0.97
Batch: 760; loss: 0.33; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.1636357954258372; val_accuracy: 0.9492436305732485 

Epoch 23 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.27; acc: 0.94
Batch: 20; loss: 0.18; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.14; acc: 0.97
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.25; acc: 0.98
Batch: 120; loss: 0.16; acc: 0.95
Batch: 140; loss: 0.19; acc: 0.95
Batch: 160; loss: 0.32; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.97
Batch: 220; loss: 0.21; acc: 0.94
Batch: 240; loss: 0.06; acc: 0.98
Batch: 260; loss: 0.15; acc: 0.95
Batch: 280; loss: 0.18; acc: 0.94
Batch: 300; loss: 0.36; acc: 0.91
Batch: 320; loss: 0.24; acc: 0.91
Batch: 340; loss: 0.06; acc: 0.98
Batch: 360; loss: 0.12; acc: 0.95
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.18; acc: 0.94
Batch: 420; loss: 0.23; acc: 0.94
Batch: 440; loss: 0.1; acc: 0.94
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.06; acc: 0.98
Batch: 500; loss: 0.12; acc: 0.98
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.07; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.19; acc: 0.94
Batch: 600; loss: 0.28; acc: 0.92
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.13; acc: 0.95
Batch: 660; loss: 0.09; acc: 0.98
Batch: 680; loss: 0.08; acc: 0.98
Batch: 700; loss: 0.09; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.13; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.98
Batch: 780; loss: 0.4; acc: 0.91
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.23; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.16472476328112137; val_accuracy: 0.948546974522293 

Epoch 24 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.29; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.17; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.23; acc: 0.97
Batch: 100; loss: 0.15; acc: 0.95
Batch: 120; loss: 0.34; acc: 0.89
Batch: 140; loss: 0.36; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.97
Batch: 180; loss: 0.31; acc: 0.91
Batch: 200; loss: 0.25; acc: 0.89
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.11; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.08; acc: 1.0
Batch: 300; loss: 0.11; acc: 0.98
Batch: 320; loss: 0.12; acc: 0.98
Batch: 340; loss: 0.14; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.15; acc: 0.94
Batch: 400; loss: 0.19; acc: 0.92
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.1; acc: 0.97
Batch: 480; loss: 0.3; acc: 0.91
Batch: 500; loss: 0.02; acc: 1.0
Batch: 520; loss: 0.12; acc: 0.97
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.04; acc: 1.0
Batch: 580; loss: 0.1; acc: 0.98
Batch: 600; loss: 0.07; acc: 0.97
Batch: 620; loss: 0.28; acc: 0.95
Batch: 640; loss: 0.12; acc: 0.94
Batch: 660; loss: 0.18; acc: 0.97
Batch: 680; loss: 0.15; acc: 0.97
Batch: 700; loss: 0.25; acc: 0.91
Batch: 720; loss: 0.23; acc: 0.92
Batch: 740; loss: 0.14; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.89
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.48; acc: 0.84
Batch: 140; loss: 0.04; acc: 0.98
Val Epoch over. val_loss: 0.1671816673200981; val_accuracy: 0.9469546178343949 

Epoch 25 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.13; acc: 0.94
Batch: 20; loss: 0.32; acc: 0.91
Batch: 40; loss: 0.16; acc: 0.92
Batch: 60; loss: 0.06; acc: 1.0
Batch: 80; loss: 0.14; acc: 0.94
Batch: 100; loss: 0.13; acc: 0.95
Batch: 120; loss: 0.16; acc: 0.97
Batch: 140; loss: 0.05; acc: 0.98
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.15; acc: 0.97
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.11; acc: 0.95
Batch: 240; loss: 0.14; acc: 0.95
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.2; acc: 0.91
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.12; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.98
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.22; acc: 0.92
Batch: 420; loss: 0.03; acc: 1.0
Batch: 440; loss: 0.28; acc: 0.94
Batch: 460; loss: 0.28; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.98
Batch: 500; loss: 0.09; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.3; acc: 0.91
Batch: 560; loss: 0.24; acc: 0.92
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.21; acc: 0.94
Batch: 640; loss: 0.3; acc: 0.92
Batch: 660; loss: 0.18; acc: 0.91
Batch: 680; loss: 0.11; acc: 0.97
Batch: 700; loss: 0.18; acc: 0.95
Batch: 720; loss: 0.2; acc: 0.89
Batch: 740; loss: 0.07; acc: 0.98
Batch: 760; loss: 0.06; acc: 0.98
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.97
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.06; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.5; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1668235120737249; val_accuracy: 0.9476512738853503 

Epoch 26 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.94
Batch: 80; loss: 0.15; acc: 0.92
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.18; acc: 0.94
Batch: 140; loss: 0.07; acc: 0.98
Batch: 160; loss: 0.12; acc: 0.94
Batch: 180; loss: 0.09; acc: 0.98
Batch: 200; loss: 0.2; acc: 0.94
Batch: 220; loss: 0.09; acc: 0.95
Batch: 240; loss: 0.2; acc: 0.94
Batch: 260; loss: 0.22; acc: 0.94
Batch: 280; loss: 0.14; acc: 0.94
Batch: 300; loss: 0.24; acc: 0.95
Batch: 320; loss: 0.34; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.97
Batch: 380; loss: 0.26; acc: 0.89
Batch: 400; loss: 0.06; acc: 0.98
Batch: 420; loss: 0.15; acc: 0.98
Batch: 440; loss: 0.26; acc: 0.95
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.24; acc: 0.92
Batch: 520; loss: 0.32; acc: 0.89
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.94
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.13; acc: 0.97
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.23; acc: 0.94
Batch: 680; loss: 0.13; acc: 0.94
Batch: 700; loss: 0.26; acc: 0.92
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.06; acc: 1.0
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.09; acc: 0.97
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.2; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.1655453549828499; val_accuracy: 0.9479498407643312 

Epoch 27 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.22; acc: 0.94
Batch: 20; loss: 0.07; acc: 0.97
Batch: 40; loss: 0.29; acc: 0.91
Batch: 60; loss: 0.25; acc: 0.91
Batch: 80; loss: 0.1; acc: 0.98
Batch: 100; loss: 0.3; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.51; acc: 0.91
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.26; acc: 0.92
Batch: 200; loss: 0.28; acc: 0.94
Batch: 220; loss: 0.12; acc: 0.92
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.38; acc: 0.95
Batch: 280; loss: 0.13; acc: 0.95
Batch: 300; loss: 0.35; acc: 0.92
Batch: 320; loss: 0.27; acc: 0.91
Batch: 340; loss: 0.2; acc: 0.95
Batch: 360; loss: 0.2; acc: 0.92
Batch: 380; loss: 0.22; acc: 0.94
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.11; acc: 0.97
Batch: 440; loss: 0.1; acc: 0.97
Batch: 460; loss: 0.11; acc: 0.95
Batch: 480; loss: 0.13; acc: 0.95
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.18; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.94
Batch: 560; loss: 0.28; acc: 0.89
Batch: 580; loss: 0.23; acc: 0.92
Batch: 600; loss: 0.15; acc: 0.94
Batch: 620; loss: 0.19; acc: 0.92
Batch: 640; loss: 0.37; acc: 0.92
Batch: 660; loss: 0.36; acc: 0.95
Batch: 680; loss: 0.06; acc: 1.0
Batch: 700; loss: 0.05; acc: 1.0
Batch: 720; loss: 0.26; acc: 0.91
Batch: 740; loss: 0.35; acc: 0.92
Batch: 760; loss: 0.12; acc: 0.95
Batch: 780; loss: 0.09; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.1606197733740518; val_accuracy: 0.9496417197452229 

Epoch 28 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.19; acc: 0.95
Batch: 20; loss: 0.06; acc: 0.98
Batch: 40; loss: 0.24; acc: 0.94
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.95
Batch: 100; loss: 0.11; acc: 0.97
Batch: 120; loss: 0.32; acc: 0.91
Batch: 140; loss: 0.17; acc: 0.94
Batch: 160; loss: 0.14; acc: 0.94
Batch: 180; loss: 0.11; acc: 0.97
Batch: 200; loss: 0.14; acc: 0.95
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.27; acc: 0.95
Batch: 260; loss: 0.06; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.18; acc: 0.98
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.11; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.2; acc: 0.92
Batch: 420; loss: 0.3; acc: 0.91
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.09; acc: 0.97
Batch: 500; loss: 0.08; acc: 0.98
Batch: 520; loss: 0.05; acc: 0.98
Batch: 540; loss: 0.18; acc: 0.92
Batch: 560; loss: 0.21; acc: 0.95
Batch: 580; loss: 0.04; acc: 1.0
Batch: 600; loss: 0.25; acc: 0.92
Batch: 620; loss: 0.07; acc: 1.0
Batch: 640; loss: 0.19; acc: 0.91
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.18; acc: 0.92
Batch: 740; loss: 0.33; acc: 0.91
Batch: 760; loss: 0.23; acc: 0.97
Batch: 780; loss: 0.12; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.92
Batch: 80; loss: 0.07; acc: 0.97
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.52; acc: 0.86
Batch: 140; loss: 0.04; acc: 0.97
Val Epoch over. val_loss: 0.16825345764826438; val_accuracy: 0.9475517515923567 

Epoch 29 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.16; acc: 0.95
Batch: 20; loss: 0.22; acc: 0.92
Batch: 40; loss: 0.19; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.1; acc: 0.95
Batch: 120; loss: 0.22; acc: 0.91
Batch: 140; loss: 0.16; acc: 0.94
Batch: 160; loss: 0.09; acc: 0.97
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.29; acc: 0.89
Batch: 220; loss: 0.05; acc: 0.98
Batch: 240; loss: 0.19; acc: 0.94
Batch: 260; loss: 0.12; acc: 0.95
Batch: 280; loss: 0.17; acc: 0.91
Batch: 300; loss: 0.14; acc: 0.94
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.09; acc: 0.95
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.17; acc: 0.95
Batch: 400; loss: 0.19; acc: 0.94
Batch: 420; loss: 0.18; acc: 0.88
Batch: 440; loss: 0.09; acc: 0.97
Batch: 460; loss: 0.35; acc: 0.89
Batch: 480; loss: 0.24; acc: 0.91
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.17; acc: 0.97
Batch: 580; loss: 0.13; acc: 0.97
Batch: 600; loss: 0.12; acc: 0.95
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.23; acc: 0.92
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.07; acc: 0.98
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.95
Batch: 740; loss: 0.08; acc: 0.98
Batch: 760; loss: 0.3; acc: 0.91
Batch: 780; loss: 0.13; acc: 0.95
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.94
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.16252104636685105; val_accuracy: 0.9490445859872612 

Epoch 30 start
The current lr is: 0.16000000000000003
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.11; acc: 0.95
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.2; acc: 0.95
Batch: 80; loss: 0.24; acc: 0.92
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.23; acc: 0.91
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.1; acc: 0.98
Batch: 180; loss: 0.17; acc: 0.94
Batch: 200; loss: 0.23; acc: 0.91
Batch: 220; loss: 0.25; acc: 0.95
Batch: 240; loss: 0.21; acc: 0.92
Batch: 260; loss: 0.21; acc: 0.92
Batch: 280; loss: 0.15; acc: 0.94
Batch: 300; loss: 0.06; acc: 0.98
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.25; acc: 0.94
Batch: 360; loss: 0.12; acc: 0.97
Batch: 380; loss: 0.25; acc: 0.91
Batch: 400; loss: 0.25; acc: 0.92
Batch: 420; loss: 0.08; acc: 0.95
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.21; acc: 0.94
Batch: 480; loss: 0.29; acc: 0.92
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.36; acc: 0.89
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.91
Batch: 620; loss: 0.26; acc: 0.91
Batch: 640; loss: 0.13; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.23; acc: 0.95
Batch: 700; loss: 0.22; acc: 0.95
Batch: 720; loss: 0.15; acc: 0.98
Batch: 740; loss: 0.11; acc: 0.95
Batch: 760; loss: 0.1; acc: 0.95
Batch: 780; loss: 0.34; acc: 0.91
Train Epoch over. train_loss: 0.17; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.95
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1730198637362878; val_accuracy: 0.9460589171974523 

Epoch 31 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.95
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.36; acc: 0.91
Batch: 60; loss: 0.22; acc: 0.92
Batch: 80; loss: 0.18; acc: 0.95
Batch: 100; loss: 0.39; acc: 0.91
Batch: 120; loss: 0.21; acc: 0.91
Batch: 140; loss: 0.18; acc: 0.91
Batch: 160; loss: 0.38; acc: 0.91
Batch: 180; loss: 0.09; acc: 0.97
Batch: 200; loss: 0.06; acc: 0.98
Batch: 220; loss: 0.16; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.95
Batch: 260; loss: 0.43; acc: 0.94
Batch: 280; loss: 0.3; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.23; acc: 0.92
Batch: 340; loss: 0.07; acc: 0.98
Batch: 360; loss: 0.21; acc: 0.92
Batch: 380; loss: 0.16; acc: 0.95
Batch: 400; loss: 0.39; acc: 0.91
Batch: 420; loss: 0.12; acc: 0.95
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.22; acc: 0.95
Batch: 480; loss: 0.1; acc: 0.97
Batch: 500; loss: 0.1; acc: 0.95
Batch: 520; loss: 0.23; acc: 0.92
Batch: 540; loss: 0.44; acc: 0.89
Batch: 560; loss: 0.16; acc: 0.97
Batch: 580; loss: 0.34; acc: 0.92
Batch: 600; loss: 0.14; acc: 0.95
Batch: 620; loss: 0.24; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.97
Batch: 660; loss: 0.22; acc: 0.94
Batch: 680; loss: 0.2; acc: 0.97
Batch: 700; loss: 0.05; acc: 0.98
Batch: 720; loss: 0.1; acc: 0.98
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.05; acc: 1.0
Batch: 780; loss: 0.11; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.97
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15827392926736242; val_accuracy: 0.9508359872611465 

Epoch 32 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.27; acc: 0.88
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.32; acc: 0.88
Batch: 60; loss: 0.2; acc: 0.92
Batch: 80; loss: 0.06; acc: 0.97
Batch: 100; loss: 0.22; acc: 0.92
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.21; acc: 0.91
Batch: 160; loss: 0.13; acc: 0.95
Batch: 180; loss: 0.24; acc: 0.94
Batch: 200; loss: 0.41; acc: 0.91
Batch: 220; loss: 0.17; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.94
Batch: 260; loss: 0.03; acc: 1.0
Batch: 280; loss: 0.21; acc: 0.95
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.06; acc: 0.98
Batch: 380; loss: 0.2; acc: 0.94
Batch: 400; loss: 0.02; acc: 1.0
Batch: 420; loss: 0.16; acc: 0.92
Batch: 440; loss: 0.32; acc: 0.92
Batch: 460; loss: 0.14; acc: 0.97
Batch: 480; loss: 0.09; acc: 0.98
Batch: 500; loss: 0.24; acc: 0.94
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.25; acc: 0.91
Batch: 560; loss: 0.15; acc: 0.97
Batch: 580; loss: 0.07; acc: 0.97
Batch: 600; loss: 0.11; acc: 0.98
Batch: 620; loss: 0.22; acc: 0.94
Batch: 640; loss: 0.17; acc: 0.94
Batch: 660; loss: 0.24; acc: 0.94
Batch: 680; loss: 0.19; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.95
Batch: 720; loss: 0.07; acc: 0.98
Batch: 740; loss: 0.1; acc: 0.97
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.19; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.95
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1599144178567229; val_accuracy: 0.9492436305732485 

Epoch 33 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.15; acc: 0.97
Batch: 20; loss: 0.12; acc: 0.97
Batch: 40; loss: 0.15; acc: 0.94
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.27; acc: 0.92
Batch: 120; loss: 0.14; acc: 0.98
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.24; acc: 0.95
Batch: 180; loss: 0.01; acc: 1.0
Batch: 200; loss: 0.13; acc: 0.95
Batch: 220; loss: 0.18; acc: 0.94
Batch: 240; loss: 0.23; acc: 0.91
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.07; acc: 0.97
Batch: 320; loss: 0.33; acc: 0.95
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.18; acc: 0.92
Batch: 380; loss: 0.08; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.92
Batch: 420; loss: 0.12; acc: 0.98
Batch: 440; loss: 0.12; acc: 0.94
Batch: 460; loss: 0.06; acc: 0.98
Batch: 480; loss: 0.12; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.14; acc: 0.97
Batch: 540; loss: 0.13; acc: 0.97
Batch: 560; loss: 0.06; acc: 0.97
Batch: 580; loss: 0.17; acc: 0.94
Batch: 600; loss: 0.3; acc: 0.94
Batch: 620; loss: 0.25; acc: 0.94
Batch: 640; loss: 0.38; acc: 0.86
Batch: 660; loss: 0.14; acc: 0.98
Batch: 680; loss: 0.19; acc: 0.95
Batch: 700; loss: 0.24; acc: 0.91
Batch: 720; loss: 0.11; acc: 0.95
Batch: 740; loss: 0.09; acc: 0.97
Batch: 760; loss: 0.39; acc: 0.88
Batch: 780; loss: 0.25; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.92
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.15675824833144048; val_accuracy: 0.9515326433121019 

Epoch 34 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.2; acc: 0.92
Batch: 20; loss: 0.12; acc: 0.94
Batch: 40; loss: 0.07; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.26; acc: 0.89
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.12; acc: 0.95
Batch: 140; loss: 0.04; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.12; acc: 0.98
Batch: 200; loss: 0.22; acc: 0.98
Batch: 220; loss: 0.14; acc: 0.92
Batch: 240; loss: 0.26; acc: 0.94
Batch: 260; loss: 0.24; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.16; acc: 0.92
Batch: 340; loss: 0.12; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.22; acc: 0.92
Batch: 400; loss: 0.14; acc: 0.92
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.16; acc: 0.95
Batch: 460; loss: 0.06; acc: 0.97
Batch: 480; loss: 0.11; acc: 0.95
Batch: 500; loss: 0.32; acc: 0.92
Batch: 520; loss: 0.07; acc: 0.97
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.94
Batch: 580; loss: 0.2; acc: 0.94
Batch: 600; loss: 0.32; acc: 0.94
Batch: 620; loss: 0.07; acc: 0.97
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.97
Batch: 680; loss: 0.18; acc: 0.94
Batch: 700; loss: 0.25; acc: 0.89
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.05; acc: 0.98
Batch: 780; loss: 0.21; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15903409785193623; val_accuracy: 0.9482484076433121 

Epoch 35 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.05; acc: 0.98
Batch: 20; loss: 0.08; acc: 0.97
Batch: 40; loss: 0.17; acc: 0.98
Batch: 60; loss: 0.25; acc: 0.92
Batch: 80; loss: 0.19; acc: 0.94
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.15; acc: 0.94
Batch: 140; loss: 0.12; acc: 0.97
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.12; acc: 0.97
Batch: 200; loss: 0.16; acc: 0.95
Batch: 220; loss: 0.22; acc: 0.94
Batch: 240; loss: 0.18; acc: 0.95
Batch: 260; loss: 0.06; acc: 0.97
Batch: 280; loss: 0.17; acc: 0.97
Batch: 300; loss: 0.15; acc: 0.95
Batch: 320; loss: 0.11; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.94
Batch: 360; loss: 0.33; acc: 0.92
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.16; acc: 0.97
Batch: 420; loss: 0.15; acc: 0.94
Batch: 440; loss: 0.29; acc: 0.88
Batch: 460; loss: 0.18; acc: 0.91
Batch: 480; loss: 0.13; acc: 0.94
Batch: 500; loss: 0.15; acc: 0.97
Batch: 520; loss: 0.24; acc: 0.94
Batch: 540; loss: 0.2; acc: 0.95
Batch: 560; loss: 0.13; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.06; acc: 0.97
Batch: 620; loss: 0.11; acc: 0.98
Batch: 640; loss: 0.12; acc: 0.95
Batch: 660; loss: 0.15; acc: 0.94
Batch: 680; loss: 0.1; acc: 0.97
Batch: 700; loss: 0.06; acc: 0.97
Batch: 720; loss: 0.11; acc: 0.97
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.21; acc: 0.95
Batch: 780; loss: 0.14; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.95
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.42; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15734341171137087; val_accuracy: 0.9507364649681529 

Epoch 36 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.41; acc: 0.81
Batch: 20; loss: 0.12; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.98
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.18; acc: 0.97
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.19; acc: 0.94
Batch: 160; loss: 0.13; acc: 0.97
Batch: 180; loss: 0.13; acc: 0.94
Batch: 200; loss: 0.22; acc: 0.94
Batch: 220; loss: 0.08; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.97
Batch: 260; loss: 0.07; acc: 0.98
Batch: 280; loss: 0.06; acc: 0.98
Batch: 300; loss: 0.21; acc: 0.94
Batch: 320; loss: 0.14; acc: 0.97
Batch: 340; loss: 0.13; acc: 0.94
Batch: 360; loss: 0.13; acc: 0.95
Batch: 380; loss: 0.15; acc: 0.97
Batch: 400; loss: 0.21; acc: 0.94
Batch: 420; loss: 0.1; acc: 0.97
Batch: 440; loss: 0.24; acc: 0.92
Batch: 460; loss: 0.1; acc: 0.95
Batch: 480; loss: 0.23; acc: 0.94
Batch: 500; loss: 0.08; acc: 0.95
Batch: 520; loss: 0.26; acc: 0.92
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.18; acc: 0.94
Batch: 580; loss: 0.27; acc: 0.91
Batch: 600; loss: 0.33; acc: 0.91
Batch: 620; loss: 0.12; acc: 0.97
Batch: 640; loss: 0.14; acc: 0.95
Batch: 660; loss: 0.22; acc: 0.92
Batch: 680; loss: 0.14; acc: 0.94
Batch: 700; loss: 0.1; acc: 0.95
Batch: 720; loss: 0.26; acc: 0.94
Batch: 740; loss: 0.2; acc: 0.92
Batch: 760; loss: 0.08; acc: 0.98
Batch: 780; loss: 0.23; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.11; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.4; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.16274220173715787; val_accuracy: 0.9481488853503185 

Epoch 37 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.1; acc: 0.97
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.14; acc: 0.95
Batch: 80; loss: 0.09; acc: 0.95
Batch: 100; loss: 0.29; acc: 0.92
Batch: 120; loss: 0.12; acc: 0.97
Batch: 140; loss: 0.13; acc: 0.95
Batch: 160; loss: 0.15; acc: 0.97
Batch: 180; loss: 0.18; acc: 0.94
Batch: 200; loss: 0.12; acc: 0.97
Batch: 220; loss: 0.2; acc: 0.95
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.14; acc: 0.97
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.09; acc: 0.95
Batch: 320; loss: 0.05; acc: 0.98
Batch: 340; loss: 0.11; acc: 0.94
Batch: 360; loss: 0.22; acc: 0.94
Batch: 380; loss: 0.25; acc: 0.94
Batch: 400; loss: 0.13; acc: 0.95
Batch: 420; loss: 0.16; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.05; acc: 1.0
Batch: 480; loss: 0.14; acc: 0.95
Batch: 500; loss: 0.17; acc: 0.98
Batch: 520; loss: 0.04; acc: 1.0
Batch: 540; loss: 0.1; acc: 0.95
Batch: 560; loss: 0.12; acc: 0.97
Batch: 580; loss: 0.11; acc: 0.95
Batch: 600; loss: 0.05; acc: 0.98
Batch: 620; loss: 0.14; acc: 0.94
Batch: 640; loss: 0.1; acc: 0.95
Batch: 660; loss: 0.21; acc: 0.94
Batch: 680; loss: 0.09; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.92
Batch: 740; loss: 0.1; acc: 0.95
Batch: 760; loss: 0.08; acc: 0.97
Batch: 780; loss: 0.29; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.06; acc: 0.97
Batch: 20; loss: 0.13; acc: 0.94
Batch: 40; loss: 0.22; acc: 0.95
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.22; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.15713445944296325; val_accuracy: 0.950437898089172 

Epoch 38 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.17; acc: 0.92
Batch: 20; loss: 0.31; acc: 0.95
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.13; acc: 0.95
Batch: 80; loss: 0.23; acc: 0.92
Batch: 100; loss: 0.08; acc: 0.97
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.08; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.95
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.17; acc: 0.94
Batch: 220; loss: 0.24; acc: 0.92
Batch: 240; loss: 0.07; acc: 0.98
Batch: 260; loss: 0.23; acc: 0.91
Batch: 280; loss: 0.1; acc: 0.98
Batch: 300; loss: 0.2; acc: 0.92
Batch: 320; loss: 0.08; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.97
Batch: 360; loss: 0.04; acc: 1.0
Batch: 380; loss: 0.11; acc: 0.98
Batch: 400; loss: 0.17; acc: 0.95
Batch: 420; loss: 0.09; acc: 0.97
Batch: 440; loss: 0.07; acc: 0.97
Batch: 460; loss: 0.09; acc: 0.95
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.18; acc: 0.94
Batch: 520; loss: 0.21; acc: 0.98
Batch: 540; loss: 0.16; acc: 0.97
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.06; acc: 1.0
Batch: 620; loss: 0.29; acc: 0.91
Batch: 640; loss: 0.09; acc: 0.98
Batch: 660; loss: 0.25; acc: 0.91
Batch: 680; loss: 0.2; acc: 0.94
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.32; acc: 0.91
Batch: 740; loss: 0.03; acc: 1.0
Batch: 760; loss: 0.11; acc: 0.95
Batch: 780; loss: 0.19; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.16; acc: 0.94
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.39; acc: 0.84
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1586721231508407; val_accuracy: 0.9501393312101911 

Epoch 39 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.26; acc: 0.92
Batch: 40; loss: 0.16; acc: 0.95
Batch: 60; loss: 0.12; acc: 0.97
Batch: 80; loss: 0.11; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.92
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.2; acc: 0.95
Batch: 180; loss: 0.3; acc: 0.91
Batch: 200; loss: 0.37; acc: 0.91
Batch: 220; loss: 0.15; acc: 0.94
Batch: 240; loss: 0.05; acc: 1.0
Batch: 260; loss: 0.23; acc: 0.92
Batch: 280; loss: 0.42; acc: 0.91
Batch: 300; loss: 0.21; acc: 0.92
Batch: 320; loss: 0.26; acc: 0.91
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.09; acc: 0.95
Batch: 380; loss: 0.06; acc: 0.98
Batch: 400; loss: 0.27; acc: 0.94
Batch: 420; loss: 0.02; acc: 1.0
Batch: 440; loss: 0.27; acc: 0.95
Batch: 460; loss: 0.25; acc: 0.91
Batch: 480; loss: 0.05; acc: 0.98
Batch: 500; loss: 0.2; acc: 0.91
Batch: 520; loss: 0.1; acc: 0.97
Batch: 540; loss: 0.15; acc: 0.95
Batch: 560; loss: 0.17; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.97
Batch: 600; loss: 0.09; acc: 1.0
Batch: 620; loss: 0.04; acc: 1.0
Batch: 640; loss: 0.22; acc: 0.94
Batch: 660; loss: 0.16; acc: 0.95
Batch: 680; loss: 0.24; acc: 0.89
Batch: 700; loss: 0.11; acc: 0.97
Batch: 720; loss: 0.2; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.97
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.09; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.95
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.03; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.4; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15643092929415262; val_accuracy: 0.9503383757961783 

Epoch 40 start
The current lr is: 0.06400000000000002
Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.11; acc: 0.97
Batch: 40; loss: 0.16; acc: 0.94
Batch: 60; loss: 0.34; acc: 0.91
Batch: 80; loss: 0.18; acc: 0.92
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.07; acc: 0.98
Batch: 140; loss: 0.1; acc: 0.97
Batch: 160; loss: 0.25; acc: 0.95
Batch: 180; loss: 0.06; acc: 0.98
Batch: 200; loss: 0.07; acc: 1.0
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.09; acc: 0.98
Batch: 260; loss: 0.3; acc: 0.94
Batch: 280; loss: 0.15; acc: 0.97
Batch: 300; loss: 0.2; acc: 0.95
Batch: 320; loss: 0.19; acc: 0.92
Batch: 340; loss: 0.14; acc: 0.94
Batch: 360; loss: 0.15; acc: 0.94
Batch: 380; loss: 0.34; acc: 0.91
Batch: 400; loss: 0.08; acc: 0.95
Batch: 420; loss: 0.31; acc: 0.92
Batch: 440; loss: 0.08; acc: 0.98
Batch: 460; loss: 0.54; acc: 0.84
Batch: 480; loss: 0.15; acc: 0.94
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.2; acc: 0.94
Batch: 540; loss: 0.1; acc: 0.97
Batch: 560; loss: 0.03; acc: 0.98
Batch: 580; loss: 0.18; acc: 0.94
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.07; acc: 0.98
Batch: 640; loss: 0.37; acc: 0.84
Batch: 660; loss: 0.11; acc: 0.95
Batch: 680; loss: 0.2; acc: 0.95
Batch: 700; loss: 0.1; acc: 0.97
Batch: 720; loss: 0.1; acc: 0.95
Batch: 740; loss: 0.27; acc: 0.92
Batch: 760; loss: 0.24; acc: 0.92
Batch: 780; loss: 0.24; acc: 0.92
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.25; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.05; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.47; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15956899908128058; val_accuracy: 0.9502388535031847 

Epoch 41 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.98
Batch: 20; loss: 0.17; acc: 0.92
Batch: 40; loss: 0.38; acc: 0.91
Batch: 60; loss: 0.23; acc: 0.92
Batch: 80; loss: 0.09; acc: 0.97
Batch: 100; loss: 0.24; acc: 0.94
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.19; acc: 0.97
Batch: 160; loss: 0.04; acc: 0.98
Batch: 180; loss: 0.08; acc: 0.97
Batch: 200; loss: 0.3; acc: 0.94
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.09; acc: 0.95
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.21; acc: 0.94
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.15; acc: 0.95
Batch: 340; loss: 0.1; acc: 0.97
Batch: 360; loss: 0.13; acc: 0.94
Batch: 380; loss: 0.09; acc: 0.95
Batch: 400; loss: 0.1; acc: 0.98
Batch: 420; loss: 0.19; acc: 0.97
Batch: 440; loss: 0.19; acc: 0.95
Batch: 460; loss: 0.16; acc: 0.94
Batch: 480; loss: 0.19; acc: 0.91
Batch: 500; loss: 0.15; acc: 0.95
Batch: 520; loss: 0.19; acc: 0.91
Batch: 540; loss: 0.3; acc: 0.92
Batch: 560; loss: 0.08; acc: 0.98
Batch: 580; loss: 0.13; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.23; acc: 0.94
Batch: 640; loss: 0.18; acc: 0.92
Batch: 660; loss: 0.07; acc: 0.98
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.26; acc: 0.94
Batch: 720; loss: 0.31; acc: 0.95
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.15; acc: 0.95
Batch: 780; loss: 0.05; acc: 0.98
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.97
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15551459563860467; val_accuracy: 0.9505374203821656 

Epoch 42 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.09; acc: 0.95
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.21; acc: 0.94
Batch: 60; loss: 0.08; acc: 0.95
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.1; acc: 0.92
Batch: 120; loss: 0.11; acc: 0.97
Batch: 140; loss: 0.23; acc: 0.94
Batch: 160; loss: 0.07; acc: 0.98
Batch: 180; loss: 0.13; acc: 0.97
Batch: 200; loss: 0.26; acc: 0.89
Batch: 220; loss: 0.06; acc: 0.98
Batch: 240; loss: 0.08; acc: 0.98
Batch: 260; loss: 0.18; acc: 0.95
Batch: 280; loss: 0.16; acc: 0.97
Batch: 300; loss: 0.1; acc: 0.98
Batch: 320; loss: 0.08; acc: 0.98
Batch: 340; loss: 0.15; acc: 0.95
Batch: 360; loss: 0.22; acc: 0.92
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.14; acc: 0.97
Batch: 420; loss: 0.21; acc: 0.97
Batch: 440; loss: 0.15; acc: 0.95
Batch: 460; loss: 0.23; acc: 0.94
Batch: 480; loss: 0.27; acc: 0.94
Batch: 500; loss: 0.05; acc: 1.0
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.19; acc: 0.95
Batch: 560; loss: 0.15; acc: 0.95
Batch: 580; loss: 0.09; acc: 0.97
Batch: 600; loss: 0.19; acc: 0.92
Batch: 620; loss: 0.15; acc: 0.95
Batch: 640; loss: 0.27; acc: 0.91
Batch: 660; loss: 0.06; acc: 0.98
Batch: 680; loss: 0.16; acc: 0.92
Batch: 700; loss: 0.15; acc: 0.97
Batch: 720; loss: 0.15; acc: 0.95
Batch: 740; loss: 0.44; acc: 0.86
Batch: 760; loss: 0.07; acc: 0.98
Batch: 780; loss: 0.12; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1544869339010518; val_accuracy: 0.950437898089172 

Epoch 43 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.33; acc: 0.92
Batch: 20; loss: 0.25; acc: 0.94
Batch: 40; loss: 0.12; acc: 0.97
Batch: 60; loss: 0.26; acc: 0.91
Batch: 80; loss: 0.09; acc: 0.98
Batch: 100; loss: 0.13; acc: 0.97
Batch: 120; loss: 0.16; acc: 0.92
Batch: 140; loss: 0.17; acc: 0.95
Batch: 160; loss: 0.08; acc: 0.97
Batch: 180; loss: 0.2; acc: 0.92
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.1; acc: 0.95
Batch: 260; loss: 0.23; acc: 0.94
Batch: 280; loss: 0.19; acc: 0.92
Batch: 300; loss: 0.1; acc: 0.97
Batch: 320; loss: 0.24; acc: 0.92
Batch: 340; loss: 0.21; acc: 0.94
Batch: 360; loss: 0.16; acc: 0.92
Batch: 380; loss: 0.2; acc: 0.91
Batch: 400; loss: 0.12; acc: 0.97
Batch: 420; loss: 0.34; acc: 0.91
Batch: 440; loss: 0.12; acc: 0.97
Batch: 460; loss: 0.29; acc: 0.92
Batch: 480; loss: 0.17; acc: 0.94
Batch: 500; loss: 0.12; acc: 0.95
Batch: 520; loss: 0.06; acc: 0.97
Batch: 540; loss: 0.4; acc: 0.88
Batch: 560; loss: 0.26; acc: 0.95
Batch: 580; loss: 0.12; acc: 0.95
Batch: 600; loss: 0.26; acc: 0.95
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.35; acc: 0.92
Batch: 660; loss: 0.2; acc: 0.91
Batch: 680; loss: 0.06; acc: 0.98
Batch: 700; loss: 0.21; acc: 0.94
Batch: 720; loss: 0.15; acc: 0.97
Batch: 740; loss: 0.12; acc: 0.95
Batch: 760; loss: 0.09; acc: 0.98
Batch: 780; loss: 0.15; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1541354594289497; val_accuracy: 0.9515326433121019 

Epoch 44 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.95
Batch: 20; loss: 0.17; acc: 0.97
Batch: 40; loss: 0.13; acc: 0.97
Batch: 60; loss: 0.12; acc: 0.95
Batch: 80; loss: 0.18; acc: 0.94
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.21; acc: 0.92
Batch: 140; loss: 0.07; acc: 0.97
Batch: 160; loss: 0.18; acc: 0.92
Batch: 180; loss: 0.15; acc: 0.94
Batch: 200; loss: 0.15; acc: 0.95
Batch: 220; loss: 0.19; acc: 0.92
Batch: 240; loss: 0.08; acc: 0.97
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.05; acc: 0.98
Batch: 300; loss: 0.26; acc: 0.92
Batch: 320; loss: 0.28; acc: 0.91
Batch: 340; loss: 0.18; acc: 0.94
Batch: 360; loss: 0.27; acc: 0.91
Batch: 380; loss: 0.14; acc: 0.95
Batch: 400; loss: 0.17; acc: 0.91
Batch: 420; loss: 0.23; acc: 0.91
Batch: 440; loss: 0.26; acc: 0.94
Batch: 460; loss: 0.08; acc: 0.98
Batch: 480; loss: 0.27; acc: 0.92
Batch: 500; loss: 0.06; acc: 0.98
Batch: 520; loss: 0.13; acc: 0.95
Batch: 540; loss: 0.23; acc: 0.91
Batch: 560; loss: 0.14; acc: 0.95
Batch: 580; loss: 0.31; acc: 0.95
Batch: 600; loss: 0.16; acc: 0.97
Batch: 620; loss: 0.1; acc: 0.95
Batch: 640; loss: 0.15; acc: 0.92
Batch: 660; loss: 0.12; acc: 0.97
Batch: 680; loss: 0.21; acc: 0.88
Batch: 700; loss: 0.14; acc: 0.95
Batch: 720; loss: 0.12; acc: 0.95
Batch: 740; loss: 0.2; acc: 0.94
Batch: 760; loss: 0.13; acc: 0.97
Batch: 780; loss: 0.08; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.17; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.2; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15420442140975577; val_accuracy: 0.9517316878980892 

Epoch 45 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.23; acc: 0.92
Batch: 20; loss: 0.07; acc: 0.98
Batch: 40; loss: 0.12; acc: 0.94
Batch: 60; loss: 0.09; acc: 0.97
Batch: 80; loss: 0.19; acc: 0.98
Batch: 100; loss: 0.19; acc: 0.94
Batch: 120; loss: 0.07; acc: 0.97
Batch: 140; loss: 0.09; acc: 0.97
Batch: 160; loss: 0.09; acc: 0.98
Batch: 180; loss: 0.34; acc: 0.91
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.13; acc: 0.95
Batch: 240; loss: 0.18; acc: 0.91
Batch: 260; loss: 0.13; acc: 0.95
Batch: 280; loss: 0.25; acc: 0.92
Batch: 300; loss: 0.08; acc: 0.98
Batch: 320; loss: 0.32; acc: 0.92
Batch: 340; loss: 0.26; acc: 0.94
Batch: 360; loss: 0.2; acc: 0.95
Batch: 380; loss: 0.21; acc: 0.94
Batch: 400; loss: 0.11; acc: 0.95
Batch: 420; loss: 0.21; acc: 0.94
Batch: 440; loss: 0.37; acc: 0.91
Batch: 460; loss: 0.16; acc: 0.98
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.2; acc: 0.92
Batch: 520; loss: 0.19; acc: 0.95
Batch: 540; loss: 0.08; acc: 0.98
Batch: 560; loss: 0.09; acc: 0.97
Batch: 580; loss: 0.16; acc: 0.97
Batch: 600; loss: 0.14; acc: 0.97
Batch: 620; loss: 0.17; acc: 0.91
Batch: 640; loss: 0.12; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.92
Batch: 680; loss: 0.08; acc: 0.97
Batch: 700; loss: 0.28; acc: 0.92
Batch: 720; loss: 0.07; acc: 0.97
Batch: 740; loss: 0.31; acc: 0.94
Batch: 760; loss: 0.11; acc: 0.98
Batch: 780; loss: 0.22; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.07; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.05; acc: 0.98
Batch: 100; loss: 0.23; acc: 0.92
Batch: 120; loss: 0.46; acc: 0.86
Batch: 140; loss: 0.03; acc: 0.98
Val Epoch over. val_loss: 0.1559869963913017; val_accuracy: 0.9503383757961783 

Epoch 46 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.26; acc: 0.92
Batch: 20; loss: 0.1; acc: 0.95
Batch: 40; loss: 0.19; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.97
Batch: 80; loss: 0.2; acc: 0.95
Batch: 100; loss: 0.07; acc: 0.98
Batch: 120; loss: 0.15; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.89
Batch: 160; loss: 0.1; acc: 0.95
Batch: 180; loss: 0.18; acc: 0.92
Batch: 200; loss: 0.12; acc: 0.98
Batch: 220; loss: 0.09; acc: 0.97
Batch: 240; loss: 0.19; acc: 0.92
Batch: 260; loss: 0.25; acc: 0.95
Batch: 280; loss: 0.08; acc: 0.95
Batch: 300; loss: 0.13; acc: 0.95
Batch: 320; loss: 0.17; acc: 0.92
Batch: 340; loss: 0.18; acc: 0.95
Batch: 360; loss: 0.21; acc: 0.95
Batch: 380; loss: 0.05; acc: 0.98
Batch: 400; loss: 0.07; acc: 0.98
Batch: 420; loss: 0.14; acc: 0.95
Batch: 440; loss: 0.06; acc: 1.0
Batch: 460; loss: 0.22; acc: 0.94
Batch: 480; loss: 0.13; acc: 0.97
Batch: 500; loss: 0.06; acc: 0.97
Batch: 520; loss: 0.14; acc: 0.95
Batch: 540; loss: 0.31; acc: 0.91
Batch: 560; loss: 0.16; acc: 0.94
Batch: 580; loss: 0.11; acc: 0.98
Batch: 600; loss: 0.12; acc: 0.94
Batch: 620; loss: 0.13; acc: 0.95
Batch: 640; loss: 0.03; acc: 1.0
Batch: 660; loss: 0.1; acc: 0.98
Batch: 680; loss: 0.12; acc: 0.97
Batch: 700; loss: 0.2; acc: 0.94
Batch: 720; loss: 0.13; acc: 0.94
Batch: 740; loss: 0.19; acc: 0.95
Batch: 760; loss: 0.2; acc: 0.94
Batch: 780; loss: 0.14; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.88
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.1546035542799409; val_accuracy: 0.9507364649681529 

Epoch 47 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.11; acc: 0.95
Batch: 20; loss: 0.18; acc: 0.94
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.19; acc: 0.95
Batch: 80; loss: 0.12; acc: 0.95
Batch: 100; loss: 0.15; acc: 0.94
Batch: 120; loss: 0.34; acc: 0.91
Batch: 140; loss: 0.13; acc: 0.97
Batch: 160; loss: 0.14; acc: 0.98
Batch: 180; loss: 0.23; acc: 0.91
Batch: 200; loss: 0.08; acc: 0.97
Batch: 220; loss: 0.14; acc: 0.97
Batch: 240; loss: 0.18; acc: 0.92
Batch: 260; loss: 0.17; acc: 0.94
Batch: 280; loss: 0.33; acc: 0.89
Batch: 300; loss: 0.19; acc: 0.94
Batch: 320; loss: 0.11; acc: 0.97
Batch: 340; loss: 0.08; acc: 0.98
Batch: 360; loss: 0.03; acc: 0.98
Batch: 380; loss: 0.1; acc: 0.95
Batch: 400; loss: 0.09; acc: 0.98
Batch: 420; loss: 0.2; acc: 0.92
Batch: 440; loss: 0.25; acc: 0.94
Batch: 460; loss: 0.18; acc: 0.95
Batch: 480; loss: 0.04; acc: 0.98
Batch: 500; loss: 0.08; acc: 0.97
Batch: 520; loss: 0.25; acc: 0.89
Batch: 540; loss: 0.15; acc: 0.91
Batch: 560; loss: 0.19; acc: 0.95
Batch: 580; loss: 0.04; acc: 0.98
Batch: 600; loss: 0.05; acc: 1.0
Batch: 620; loss: 0.08; acc: 0.97
Batch: 640; loss: 0.08; acc: 0.98
Batch: 660; loss: 0.19; acc: 0.91
Batch: 680; loss: 0.28; acc: 0.94
Batch: 700; loss: 0.17; acc: 0.95
Batch: 720; loss: 0.28; acc: 0.92
Batch: 740; loss: 0.21; acc: 0.95
Batch: 760; loss: 0.07; acc: 1.0
Batch: 780; loss: 0.17; acc: 0.94
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 1.0
Batch: 100; loss: 0.21; acc: 0.95
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.1544833046140944; val_accuracy: 0.9508359872611465 

Epoch 48 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.28; acc: 0.94
Batch: 20; loss: 0.16; acc: 0.95
Batch: 40; loss: 0.08; acc: 0.97
Batch: 60; loss: 0.15; acc: 0.95
Batch: 80; loss: 0.17; acc: 0.95
Batch: 100; loss: 0.12; acc: 0.94
Batch: 120; loss: 0.17; acc: 0.94
Batch: 140; loss: 0.06; acc: 1.0
Batch: 160; loss: 0.07; acc: 0.97
Batch: 180; loss: 0.11; acc: 0.94
Batch: 200; loss: 0.19; acc: 0.92
Batch: 220; loss: 0.08; acc: 0.95
Batch: 240; loss: 0.16; acc: 0.89
Batch: 260; loss: 0.09; acc: 0.97
Batch: 280; loss: 0.1; acc: 0.95
Batch: 300; loss: 0.16; acc: 0.92
Batch: 320; loss: 0.06; acc: 0.98
Batch: 340; loss: 0.07; acc: 0.97
Batch: 360; loss: 0.1; acc: 0.94
Batch: 380; loss: 0.11; acc: 0.97
Batch: 400; loss: 0.13; acc: 0.94
Batch: 420; loss: 0.22; acc: 0.94
Batch: 440; loss: 0.12; acc: 0.98
Batch: 460; loss: 0.28; acc: 0.92
Batch: 480; loss: 0.11; acc: 0.97
Batch: 500; loss: 0.11; acc: 0.95
Batch: 520; loss: 0.22; acc: 0.95
Batch: 540; loss: 0.36; acc: 0.92
Batch: 560; loss: 0.3; acc: 0.92
Batch: 580; loss: 0.24; acc: 0.95
Batch: 600; loss: 0.21; acc: 0.92
Batch: 620; loss: 0.03; acc: 1.0
Batch: 640; loss: 0.11; acc: 0.94
Batch: 660; loss: 0.2; acc: 0.95
Batch: 680; loss: 0.04; acc: 0.98
Batch: 700; loss: 0.12; acc: 0.97
Batch: 720; loss: 0.21; acc: 0.95
Batch: 740; loss: 0.25; acc: 0.92
Batch: 760; loss: 0.14; acc: 0.94
Batch: 780; loss: 0.15; acc: 0.97
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.43; acc: 0.86
Batch: 140; loss: 0.02; acc: 1.0
Val Epoch over. val_loss: 0.15479189852715297; val_accuracy: 0.9511345541401274 

Epoch 49 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.1; acc: 0.97
Batch: 20; loss: 0.21; acc: 0.92
Batch: 40; loss: 0.23; acc: 0.92
Batch: 60; loss: 0.11; acc: 0.97
Batch: 80; loss: 0.16; acc: 0.94
Batch: 100; loss: 0.3; acc: 0.91
Batch: 120; loss: 0.04; acc: 1.0
Batch: 140; loss: 0.12; acc: 0.95
Batch: 160; loss: 0.05; acc: 0.98
Batch: 180; loss: 0.27; acc: 0.95
Batch: 200; loss: 0.15; acc: 0.94
Batch: 220; loss: 0.31; acc: 0.92
Batch: 240; loss: 0.38; acc: 0.89
Batch: 260; loss: 0.05; acc: 1.0
Batch: 280; loss: 0.08; acc: 0.98
Batch: 300; loss: 0.04; acc: 1.0
Batch: 320; loss: 0.13; acc: 0.94
Batch: 340; loss: 0.16; acc: 0.97
Batch: 360; loss: 0.28; acc: 0.89
Batch: 380; loss: 0.12; acc: 0.97
Batch: 400; loss: 0.37; acc: 0.88
Batch: 420; loss: 0.1; acc: 0.98
Batch: 440; loss: 0.25; acc: 0.92
Batch: 460; loss: 0.24; acc: 0.95
Batch: 480; loss: 0.07; acc: 0.98
Batch: 500; loss: 0.18; acc: 0.95
Batch: 520; loss: 0.11; acc: 0.95
Batch: 540; loss: 0.12; acc: 0.97
Batch: 560; loss: 0.14; acc: 0.97
Batch: 580; loss: 0.08; acc: 0.98
Batch: 600; loss: 0.22; acc: 0.92
Batch: 620; loss: 0.05; acc: 0.98
Batch: 640; loss: 0.04; acc: 1.0
Batch: 660; loss: 0.08; acc: 0.98
Batch: 680; loss: 0.32; acc: 0.91
Batch: 700; loss: 0.28; acc: 0.94
Batch: 720; loss: 0.14; acc: 0.97
Batch: 740; loss: 0.14; acc: 0.95
Batch: 760; loss: 0.08; acc: 1.0
Batch: 780; loss: 0.3; acc: 0.95
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.15; acc: 0.92
Batch: 40; loss: 0.24; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.94
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.2; acc: 0.94
Batch: 120; loss: 0.45; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15482457869561614; val_accuracy: 0.9515326433121019 

Epoch 50 start
The current lr is: 0.025600000000000005
Batch: 0; loss: 0.22; acc: 0.92
Batch: 20; loss: 0.05; acc: 0.98
Batch: 40; loss: 0.15; acc: 0.95
Batch: 60; loss: 0.05; acc: 0.98
Batch: 80; loss: 0.07; acc: 0.98
Batch: 100; loss: 0.09; acc: 0.97
Batch: 120; loss: 0.19; acc: 0.95
Batch: 140; loss: 0.22; acc: 0.94
Batch: 160; loss: 0.23; acc: 0.94
Batch: 180; loss: 0.07; acc: 0.97
Batch: 200; loss: 0.13; acc: 0.97
Batch: 220; loss: 0.3; acc: 0.94
Batch: 240; loss: 0.12; acc: 0.97
Batch: 260; loss: 0.08; acc: 0.98
Batch: 280; loss: 0.12; acc: 0.98
Batch: 300; loss: 0.19; acc: 0.95
Batch: 320; loss: 0.15; acc: 0.94
Batch: 340; loss: 0.04; acc: 0.98
Batch: 360; loss: 0.23; acc: 0.94
Batch: 380; loss: 0.16; acc: 0.92
Batch: 400; loss: 0.08; acc: 0.98
Batch: 420; loss: 0.09; acc: 0.98
Batch: 440; loss: 0.2; acc: 0.94
Batch: 460; loss: 0.2; acc: 0.94
Batch: 480; loss: 0.26; acc: 0.91
Batch: 500; loss: 0.2; acc: 0.94
Batch: 520; loss: 0.12; acc: 0.98
Batch: 540; loss: 0.14; acc: 0.95
Batch: 560; loss: 0.11; acc: 0.95
Batch: 580; loss: 0.23; acc: 0.94
Batch: 600; loss: 0.07; acc: 0.98
Batch: 620; loss: 0.1; acc: 0.98
Batch: 640; loss: 0.34; acc: 0.91
Batch: 660; loss: 0.14; acc: 0.92
Batch: 680; loss: 0.19; acc: 0.97
Batch: 700; loss: 0.16; acc: 0.95
Batch: 720; loss: 0.06; acc: 0.98
Batch: 740; loss: 0.06; acc: 0.98
Batch: 760; loss: 0.13; acc: 0.95
Batch: 780; loss: 0.33; acc: 0.91
Train Epoch over. train_loss: 0.16; train_accuracy: 0.95 

Batch: 0; loss: 0.08; acc: 0.97
Batch: 20; loss: 0.14; acc: 0.94
Batch: 40; loss: 0.23; acc: 0.95
Batch: 60; loss: 0.18; acc: 0.95
Batch: 80; loss: 0.04; acc: 0.98
Batch: 100; loss: 0.21; acc: 0.94
Batch: 120; loss: 0.44; acc: 0.86
Batch: 140; loss: 0.03; acc: 1.0
Val Epoch over. val_loss: 0.15456144184253778; val_accuracy: 0.9516321656050956 

plots/subspace_training/reg_lenet_2/2020-01-19 19:11:48/d_dim_500_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
plots/subspace_training/reg_lenet_2/2020-01-19 19:11:48/d_dim_XXXXX_lr_1.0_gamma_0.4_sched_freq_10_seed_1_epochs_50_batchsize_64
/var/spool/slurm-llnl/slurmd/job4385835/slurm_script: line 25: --print_freq=20: command not found
