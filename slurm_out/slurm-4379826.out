nonzero elements in E: 15561
elements in E: 4823500
fraction nonzero: 0.0032260806468332125
Epoch 1 start
The current lr is: 0.1
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
/tmp/pip-req-build-4baxydiv/aten/src/ATen/native/IndexingUtils.h:20: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead.
Batch: 0; loss: 278.61; acc: 0.16
Batch: 20; loss: 280.04; acc: 0.06
Batch: 40; loss: 409.56; acc: 0.11
Batch: 60; loss: 150.17; acc: 0.11
Batch: 80; loss: 193.63; acc: 0.09
Batch: 100; loss: 88.76; acc: 0.16
Batch: 120; loss: 88.87; acc: 0.19
Batch: 140; loss: 92.11; acc: 0.14
Batch: 160; loss: 123.82; acc: 0.14
Batch: 180; loss: 73.93; acc: 0.14
Batch: 200; loss: 128.5; acc: 0.19
Batch: 220; loss: 111.67; acc: 0.23
Batch: 240; loss: 438.54; acc: 0.05
Batch: 260; loss: 86.75; acc: 0.11
Batch: 280; loss: 98.25; acc: 0.09
Batch: 300; loss: 165.04; acc: 0.11
Batch: 320; loss: 65.67; acc: 0.27
Batch: 340; loss: 99.41; acc: 0.12
Batch: 360; loss: 57.01; acc: 0.28
Batch: 380; loss: 88.23; acc: 0.19
Batch: 400; loss: 80.41; acc: 0.23
Batch: 420; loss: 45.22; acc: 0.31
Batch: 440; loss: 168.42; acc: 0.09
Batch: 460; loss: 118.77; acc: 0.23
Batch: 480; loss: 157.29; acc: 0.12
Batch: 500; loss: 161.69; acc: 0.08
Batch: 520; loss: 218.73; acc: 0.11
Batch: 540; loss: 99.92; acc: 0.22
Batch: 560; loss: 100.84; acc: 0.09
Batch: 580; loss: 85.74; acc: 0.2
Batch: 600; loss: 131.11; acc: 0.2
Batch: 620; loss: 254.43; acc: 0.11
Train Epoch over. train_loss: 153.62; train_accuracy: 0.14 

Batch: 0; loss: 180.3; acc: 0.22
Batch: 20; loss: 215.34; acc: 0.17
Batch: 40; loss: 159.25; acc: 0.19
Batch: 60; loss: 142.55; acc: 0.25
Batch: 80; loss: 174.51; acc: 0.23
Batch: 100; loss: 179.07; acc: 0.19
Batch: 120; loss: 181.78; acc: 0.14
Batch: 140; loss: 140.45; acc: 0.22
Val Epoch over. val_loss: 163.90602845598937; val_accuracy: 0.18610668789808918 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: 164.93; acc: 0.19
Batch: 20; loss: 154.13; acc: 0.08
Batch: 40; loss: 72.25; acc: 0.08
Batch: 60; loss: 110.54; acc: 0.03
Batch: 80; loss: 135.3; acc: 0.09
Batch: 100; loss: 65.58; acc: 0.14
Batch: 120; loss: 116.16; acc: 0.14
Batch: 140; loss: 135.45; acc: 0.17
Batch: 160; loss: 160.08; acc: 0.08
Batch: 180; loss: 147.92; acc: 0.17
Batch: 200; loss: 102.15; acc: 0.2
Batch: 220; loss: 134.58; acc: 0.2
Batch: 240; loss: 98.81; acc: 0.12
Batch: 260; loss: 133.17; acc: 0.17
Batch: 280; loss: 104.05; acc: 0.12
Batch: 300; loss: 105.84; acc: 0.12
Batch: 320; loss: 156.95; acc: 0.19
Batch: 340; loss: 108.43; acc: 0.19
Batch: 360; loss: 88.42; acc: 0.2
Batch: 380; loss: 115.4; acc: 0.14
Batch: 400; loss: 80.56; acc: 0.12
Batch: 420; loss: 160.79; acc: 0.12
Batch: 440; loss: 240.3; acc: 0.09
Batch: 460; loss: 98.51; acc: 0.08
Batch: 480; loss: 106.94; acc: 0.17
Batch: 500; loss: 116.89; acc: 0.23
Batch: 520; loss: 84.16; acc: 0.19
Batch: 540; loss: 131.74; acc: 0.12
Batch: 560; loss: 268.07; acc: 0.09
Batch: 580; loss: 325.22; acc: 0.06
Batch: 600; loss: 126.84; acc: 0.08
Batch: 620; loss: 63.34; acc: 0.22
Train Epoch over. train_loss: 137.43; train_accuracy: 0.13 

Batch: 0; loss: 119.48; acc: 0.09
Batch: 20; loss: 146.36; acc: 0.08
Batch: 40; loss: 113.59; acc: 0.12
Batch: 60; loss: 117.34; acc: 0.08
Batch: 80; loss: 132.63; acc: 0.11
Batch: 100; loss: 139.54; acc: 0.06
Batch: 120; loss: 120.44; acc: 0.11
Batch: 140; loss: 117.01; acc: 0.09
Val Epoch over. val_loss: 122.77726920546999; val_accuracy: 0.08996815286624203 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: 130.46; acc: 0.09
Batch: 20; loss: 83.08; acc: 0.16
Batch: 40; loss: 58.31; acc: 0.28
Batch: 60; loss: 234.08; acc: 0.14
Batch: 80; loss: 129.13; acc: 0.19
Batch: 100; loss: 120.61; acc: 0.11
Batch: 120; loss: 191.26; acc: 0.12
Batch: 140; loss: 167.53; acc: 0.11
Batch: 160; loss: 89.91; acc: 0.19
Batch: 180; loss: 137.7; acc: 0.17
Batch: 200; loss: 109.37; acc: 0.06
Batch: 220; loss: 123.69; acc: 0.08
Batch: 240; loss: 162.57; acc: 0.05
Batch: 260; loss: 201.52; acc: 0.12
Batch: 280; loss: 87.19; acc: 0.11
Batch: 300; loss: 126.91; acc: 0.14
Batch: 320; loss: 72.03; acc: 0.17
Batch: 340; loss: 110.71; acc: 0.12
Batch: 360; loss: 52.65; acc: 0.22
Batch: 380; loss: 93.6; acc: 0.17
Batch: 400; loss: 157.82; acc: 0.12
Batch: 420; loss: 83.2; acc: 0.16
Batch: 440; loss: 159.39; acc: 0.09
Batch: 460; loss: 181.69; acc: 0.11
Batch: 480; loss: 154.97; acc: 0.03
Batch: 500; loss: 318.3; acc: 0.16
Batch: 520; loss: 77.78; acc: 0.22
Batch: 540; loss: 98.59; acc: 0.17
Batch: 560; loss: 126.41; acc: 0.11
Batch: 580; loss: 100.09; acc: 0.11
Batch: 600; loss: 64.44; acc: 0.16
Batch: 620; loss: 176.82; acc: 0.14
Train Epoch over. train_loss: 127.26; train_accuracy: 0.14 

Batch: 0; loss: 115.14; acc: 0.11
Batch: 20; loss: 111.61; acc: 0.14
Batch: 40; loss: 109.26; acc: 0.09
Batch: 60; loss: 153.37; acc: 0.03
Batch: 80; loss: 129.52; acc: 0.12
Batch: 100; loss: 105.45; acc: 0.17
Batch: 120; loss: 102.43; acc: 0.14
Batch: 140; loss: 112.05; acc: 0.09
Val Epoch over. val_loss: 118.8713548502345; val_accuracy: 0.10589171974522293 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: 133.52; acc: 0.09
Batch: 20; loss: 143.06; acc: 0.03
Batch: 40; loss: 82.87; acc: 0.19
Batch: 60; loss: 87.73; acc: 0.17
Batch: 80; loss: 109.17; acc: 0.2
Batch: 100; loss: 151.01; acc: 0.08
Batch: 120; loss: 72.08; acc: 0.08
Batch: 140; loss: 98.18; acc: 0.08
Batch: 160; loss: 79.87; acc: 0.14
Batch: 180; loss: 78.39; acc: 0.12
Batch: 200; loss: 176.88; acc: 0.09
Batch: 220; loss: 205.3; acc: 0.09
Batch: 240; loss: 93.84; acc: 0.19
Batch: 260; loss: 91.16; acc: 0.12
Batch: 280; loss: 72.77; acc: 0.17
Batch: 300; loss: 127.09; acc: 0.14
Batch: 320; loss: 134.69; acc: 0.11
Batch: 340; loss: 129.26; acc: 0.05
Batch: 360; loss: 94.9; acc: 0.05
Batch: 380; loss: 110.57; acc: 0.2
Batch: 400; loss: 59.94; acc: 0.2
Batch: 420; loss: 126.55; acc: 0.08
Batch: 440; loss: 502.94; acc: 0.06
Batch: 460; loss: 98.05; acc: 0.22
Batch: 480; loss: 81.52; acc: 0.2
Batch: 500; loss: 140.56; acc: 0.08
Batch: 520; loss: 109.95; acc: 0.22
Batch: 540; loss: 55.36; acc: 0.2
Batch: 560; loss: 193.66; acc: 0.09
Batch: 580; loss: 71.96; acc: 0.22
Batch: 600; loss: 87.02; acc: 0.19
Batch: 620; loss: 110.24; acc: 0.12
Train Epoch over. train_loss: 135.84; train_accuracy: 0.14 

Batch: 0; loss: 91.72; acc: 0.06
Batch: 20; loss: 105.83; acc: 0.14
Batch: 40; loss: 89.34; acc: 0.08
Batch: 60; loss: 73.65; acc: 0.14
Batch: 80; loss: 102.56; acc: 0.05
Batch: 100; loss: 98.46; acc: 0.06
Batch: 120; loss: 97.79; acc: 0.14
Batch: 140; loss: 75.62; acc: 0.16
Val Epoch over. val_loss: 93.54044555858442; val_accuracy: 0.11753582802547771 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: 96.15; acc: 0.08
Batch: 20; loss: 246.87; acc: 0.08
Batch: 40; loss: 180.45; acc: 0.09
Batch: 60; loss: 134.85; acc: 0.11
Batch: 80; loss: 136.19; acc: 0.11
Batch: 100; loss: 199.36; acc: 0.09
Batch: 120; loss: 123.44; acc: 0.09
Batch: 140; loss: 188.24; acc: 0.11
Batch: 160; loss: 161.8; acc: 0.09
Batch: 180; loss: 120.64; acc: 0.06
Batch: 200; loss: 317.89; acc: 0.05
Batch: 220; loss: 102.47; acc: 0.16
Batch: 240; loss: 282.67; acc: 0.03
Batch: 260; loss: 178.07; acc: 0.16
Batch: 280; loss: 148.93; acc: 0.2
Batch: 300; loss: 206.9; acc: 0.09
Batch: 320; loss: 136.23; acc: 0.03
Batch: 340; loss: 124.17; acc: 0.08
Batch: 360; loss: 155.73; acc: 0.17
Batch: 380; loss: 301.42; acc: 0.05
Batch: 400; loss: 167.09; acc: 0.08
Batch: 420; loss: 180.81; acc: 0.19
Batch: 440; loss: 91.58; acc: 0.14
Batch: 460; loss: 82.73; acc: 0.17
Batch: 480; loss: 48.79; acc: 0.16
Batch: 500; loss: 45.46; acc: 0.12
Batch: 520; loss: 112.14; acc: 0.12
Batch: 540; loss: 241.51; acc: 0.09
Batch: 560; loss: 85.73; acc: 0.06
Batch: 580; loss: 197.56; acc: 0.14
Batch: 600; loss: 146.16; acc: 0.12
Batch: 620; loss: 212.12; acc: 0.14
Train Epoch over. train_loss: 150.66; train_accuracy: 0.13 

Batch: 0; loss: 117.78; acc: 0.19
Batch: 20; loss: 119.6; acc: 0.16
Batch: 40; loss: 110.12; acc: 0.16
Batch: 60; loss: 118.34; acc: 0.11
Batch: 80; loss: 132.12; acc: 0.11
Batch: 100; loss: 107.02; acc: 0.19
Batch: 120; loss: 106.94; acc: 0.16
Batch: 140; loss: 92.15; acc: 0.2
Val Epoch over. val_loss: 117.67367670338625; val_accuracy: 0.16789410828025478 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: 111.61; acc: 0.09
Batch: 20; loss: 105.43; acc: 0.19
Batch: 40; loss: 85.21; acc: 0.25
Batch: 60; loss: 208.49; acc: 0.22
Batch: 80; loss: 110.82; acc: 0.14
Batch: 100; loss: 119.42; acc: 0.09
Batch: 120; loss: 79.25; acc: 0.11
Batch: 140; loss: 42.29; acc: 0.23
Batch: 160; loss: 121.98; acc: 0.11
Batch: 180; loss: 76.61; acc: 0.22
Batch: 200; loss: 197.57; acc: 0.14
Batch: 220; loss: 110.66; acc: 0.17
Batch: 240; loss: 145.59; acc: 0.12
Batch: 260; loss: 142.08; acc: 0.12
Batch: 280; loss: 129.92; acc: 0.16
Batch: 300; loss: 68.18; acc: 0.17
Batch: 320; loss: 92.63; acc: 0.08
Batch: 340; loss: 82.99; acc: 0.2
Batch: 360; loss: 451.77; acc: 0.17
Batch: 380; loss: 167.98; acc: 0.12
Batch: 400; loss: 131.71; acc: 0.09
Batch: 420; loss: 83.11; acc: 0.16
Batch: 440; loss: 90.22; acc: 0.11
Batch: 460; loss: 78.6; acc: 0.22
Batch: 480; loss: 164.81; acc: 0.22
Batch: 500; loss: 98.17; acc: 0.17
Batch: 520; loss: 74.86; acc: 0.25
Batch: 540; loss: 140.9; acc: 0.16
Batch: 560; loss: 59.84; acc: 0.36
Batch: 580; loss: 214.27; acc: 0.08
Batch: 600; loss: 92.27; acc: 0.12
Batch: 620; loss: 190.01; acc: 0.23
Train Epoch over. train_loss: 125.34; train_accuracy: 0.14 

Batch: 0; loss: 101.09; acc: 0.17
Batch: 20; loss: 123.39; acc: 0.06
Batch: 40; loss: 105.97; acc: 0.2
Batch: 60; loss: 101.3; acc: 0.23
Batch: 80; loss: 120.42; acc: 0.14
Batch: 100; loss: 111.35; acc: 0.19
Batch: 120; loss: 105.18; acc: 0.17
Batch: 140; loss: 84.24; acc: 0.16
Val Epoch over. val_loss: 105.19555319038926; val_accuracy: 0.16968550955414013 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: 105.6; acc: 0.14
Batch: 20; loss: 106.05; acc: 0.12
Batch: 40; loss: 191.58; acc: 0.16
Batch: 60; loss: 152.86; acc: 0.09
Batch: 80; loss: 121.96; acc: 0.19
Batch: 100; loss: 116.41; acc: 0.11
Batch: 120; loss: 182.01; acc: 0.12
Batch: 140; loss: 97.96; acc: 0.08
Batch: 160; loss: 60.33; acc: 0.25
Batch: 180; loss: 105.23; acc: 0.09
Batch: 200; loss: 163.84; acc: 0.12
Batch: 220; loss: 103.99; acc: 0.17
Batch: 240; loss: 165.35; acc: 0.09
Batch: 260; loss: 97.31; acc: 0.08
Batch: 280; loss: 120.16; acc: 0.09
Batch: 300; loss: 148.93; acc: 0.14
Batch: 320; loss: 84.57; acc: 0.17
Batch: 340; loss: 112.23; acc: 0.14
Batch: 360; loss: 124.7; acc: 0.09
Batch: 380; loss: 62.71; acc: 0.22
Batch: 400; loss: 81.32; acc: 0.14
Batch: 420; loss: 98.84; acc: 0.08
Batch: 440; loss: 110.53; acc: 0.06
Batch: 460; loss: 116.27; acc: 0.11
Batch: 480; loss: 139.43; acc: 0.12
Batch: 500; loss: 66.75; acc: 0.19
Batch: 520; loss: 107.66; acc: 0.12
Batch: 540; loss: 178.81; acc: 0.12
Batch: 560; loss: 72.54; acc: 0.12
Batch: 580; loss: 169.17; acc: 0.14
Batch: 600; loss: 94.12; acc: 0.12
Batch: 620; loss: 143.0; acc: 0.22
Train Epoch over. train_loss: 125.11; train_accuracy: 0.14 

Batch: 0; loss: 113.54; acc: 0.17
Batch: 20; loss: 118.34; acc: 0.08
Batch: 40; loss: 123.06; acc: 0.16
Batch: 60; loss: 116.65; acc: 0.14
Batch: 80; loss: 129.3; acc: 0.11
Batch: 100; loss: 113.17; acc: 0.19
Batch: 120; loss: 111.11; acc: 0.2
Batch: 140; loss: 101.9; acc: 0.14
Val Epoch over. val_loss: 113.23164421130137; val_accuracy: 0.1490843949044586 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: 121.93; acc: 0.14
Batch: 20; loss: 126.89; acc: 0.08
Batch: 40; loss: 106.51; acc: 0.09
Batch: 60; loss: 83.99; acc: 0.16
Batch: 80; loss: 112.91; acc: 0.05
Batch: 100; loss: 157.41; acc: 0.08
Batch: 120; loss: 63.54; acc: 0.34
Batch: 140; loss: 126.43; acc: 0.22
Batch: 160; loss: 100.29; acc: 0.19
Batch: 180; loss: 92.78; acc: 0.12
Batch: 200; loss: 96.15; acc: 0.14
Batch: 220; loss: 130.19; acc: 0.12
Batch: 240; loss: 171.64; acc: 0.09
Batch: 260; loss: 93.41; acc: 0.06
Batch: 280; loss: 711.19; acc: 0.08
Batch: 300; loss: 95.35; acc: 0.14
Batch: 320; loss: 153.4; acc: 0.16
Batch: 340; loss: 306.72; acc: 0.09
Batch: 360; loss: 92.17; acc: 0.16
Batch: 380; loss: 257.57; acc: 0.11
Batch: 400; loss: 143.0; acc: 0.22
Batch: 420; loss: 85.15; acc: 0.12
Batch: 440; loss: 121.47; acc: 0.14
Batch: 460; loss: 72.13; acc: 0.11
Batch: 480; loss: 223.99; acc: 0.12
Batch: 500; loss: 140.39; acc: 0.11
Batch: 520; loss: 83.12; acc: 0.09
Batch: 540; loss: 217.79; acc: 0.03
Batch: 560; loss: 149.89; acc: 0.09
Batch: 580; loss: 203.26; acc: 0.11
Batch: 600; loss: 97.16; acc: 0.17
Batch: 620; loss: 152.66; acc: 0.17
Train Epoch over. train_loss: 131.59; train_accuracy: 0.14 

Batch: 0; loss: 159.39; acc: 0.14
Batch: 20; loss: 154.78; acc: 0.17
Batch: 40; loss: 147.56; acc: 0.14
Batch: 60; loss: 176.12; acc: 0.14
Batch: 80; loss: 175.45; acc: 0.06
Batch: 100; loss: 156.69; acc: 0.11
Batch: 120; loss: 126.87; acc: 0.09
Batch: 140; loss: 167.42; acc: 0.11
Val Epoch over. val_loss: 159.54589625073086; val_accuracy: 0.10469745222929937 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: 161.22; acc: 0.14
Batch: 20; loss: 136.9; acc: 0.14
Batch: 40; loss: 161.43; acc: 0.16
Batch: 60; loss: 98.93; acc: 0.11
Batch: 80; loss: 220.75; acc: 0.09
Batch: 100; loss: 99.1; acc: 0.12
Batch: 120; loss: 85.59; acc: 0.09
Batch: 140; loss: 139.43; acc: 0.09
Batch: 160; loss: 67.18; acc: 0.16
Batch: 180; loss: 113.92; acc: 0.09
Batch: 200; loss: 101.47; acc: 0.16
Batch: 220; loss: 64.86; acc: 0.2
Batch: 240; loss: 99.48; acc: 0.08
Batch: 260; loss: 142.55; acc: 0.05
Batch: 280; loss: 65.77; acc: 0.08
Batch: 300; loss: 159.44; acc: 0.12
Batch: 320; loss: 245.06; acc: 0.08
Batch: 340; loss: 85.75; acc: 0.16
Batch: 360; loss: 140.32; acc: 0.09
Batch: 380; loss: 147.69; acc: 0.14
Batch: 400; loss: 188.46; acc: 0.05
Batch: 420; loss: 139.9; acc: 0.12
Batch: 440; loss: 58.92; acc: 0.06
Batch: 460; loss: 81.44; acc: 0.11
Batch: 480; loss: 81.71; acc: 0.23
Batch: 500; loss: 288.05; acc: 0.06
Batch: 520; loss: 95.27; acc: 0.14
Batch: 540; loss: 87.97; acc: 0.09
Batch: 560; loss: 91.54; acc: 0.12
Batch: 580; loss: 116.66; acc: 0.11
Batch: 600; loss: 149.46; acc: 0.03
Batch: 620; loss: 148.24; acc: 0.2
Train Epoch over. train_loss: 134.84; train_accuracy: 0.13 

Batch: 0; loss: 53.82; acc: 0.25
Batch: 20; loss: 63.03; acc: 0.19
Batch: 40; loss: 49.91; acc: 0.23
Batch: 60; loss: 60.09; acc: 0.19
Batch: 80; loss: 67.44; acc: 0.17
Batch: 100; loss: 59.06; acc: 0.28
Batch: 120; loss: 42.3; acc: 0.28
Batch: 140; loss: 53.4; acc: 0.22
Val Epoch over. val_loss: 58.587660382507714; val_accuracy: 0.21218152866242038 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: 51.57; acc: 0.31
Batch: 20; loss: 207.71; acc: 0.12
Batch: 40; loss: 84.59; acc: 0.16
Batch: 60; loss: 112.18; acc: 0.16
Batch: 80; loss: 116.58; acc: 0.17
Batch: 100; loss: 69.62; acc: 0.16
Batch: 120; loss: 138.11; acc: 0.19
Batch: 140; loss: 126.64; acc: 0.12
Batch: 160; loss: 126.11; acc: 0.16
Batch: 180; loss: 93.65; acc: 0.19
Batch: 200; loss: 106.75; acc: 0.05
Batch: 220; loss: 108.02; acc: 0.05
Batch: 240; loss: 68.34; acc: 0.16
Batch: 260; loss: 86.33; acc: 0.2
Batch: 280; loss: 70.04; acc: 0.14
Batch: 300; loss: 246.88; acc: 0.12
Batch: 320; loss: 194.47; acc: 0.08
Batch: 340; loss: 140.73; acc: 0.19
Batch: 360; loss: 83.88; acc: 0.12
Batch: 380; loss: 89.31; acc: 0.12
Batch: 400; loss: 188.02; acc: 0.03
Batch: 420; loss: 105.14; acc: 0.12
Batch: 440; loss: 138.46; acc: 0.05
Batch: 460; loss: 98.3; acc: 0.23
Batch: 480; loss: 100.87; acc: 0.11
Batch: 500; loss: 119.27; acc: 0.19
Batch: 520; loss: 184.78; acc: 0.14
Batch: 540; loss: 96.81; acc: 0.39
Batch: 560; loss: 193.45; acc: 0.09
Batch: 580; loss: 203.5; acc: 0.06
Batch: 600; loss: 148.0; acc: 0.09
Batch: 620; loss: 236.8; acc: 0.09
Train Epoch over. train_loss: 135.19; train_accuracy: 0.13 

Batch: 0; loss: 179.74; acc: 0.23
Batch: 20; loss: 202.76; acc: 0.12
Batch: 40; loss: 184.72; acc: 0.19
Batch: 60; loss: 188.86; acc: 0.16
Batch: 80; loss: 207.65; acc: 0.17
Batch: 100; loss: 155.3; acc: 0.2
Batch: 120; loss: 156.21; acc: 0.3
Batch: 140; loss: 162.51; acc: 0.19
Val Epoch over. val_loss: 176.62970767658987; val_accuracy: 0.19814888535031847 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 175.68; acc: 0.22
Batch: 20; loss: 58.95; acc: 0.14
Batch: 40; loss: 47.14; acc: 0.17
Batch: 60; loss: 42.31; acc: 0.22
Batch: 80; loss: 37.76; acc: 0.16
Batch: 100; loss: 36.88; acc: 0.22
Batch: 120; loss: 42.4; acc: 0.2
Batch: 140; loss: 31.87; acc: 0.3
Batch: 160; loss: 32.2; acc: 0.27
Batch: 180; loss: 30.31; acc: 0.31
Batch: 200; loss: 32.89; acc: 0.22
Batch: 220; loss: 28.77; acc: 0.23
Batch: 240; loss: 31.86; acc: 0.28
Batch: 260; loss: 31.88; acc: 0.23
Batch: 280; loss: 23.82; acc: 0.31
Batch: 300; loss: 35.24; acc: 0.22
Batch: 320; loss: 20.32; acc: 0.38
Batch: 340; loss: 28.42; acc: 0.25
Batch: 360; loss: 31.24; acc: 0.25
Batch: 380; loss: 26.15; acc: 0.22
Batch: 400; loss: 26.71; acc: 0.27
Batch: 420; loss: 29.05; acc: 0.25
Batch: 440; loss: 30.28; acc: 0.27
Batch: 460; loss: 24.85; acc: 0.22
Batch: 480; loss: 33.93; acc: 0.17
Batch: 500; loss: 25.08; acc: 0.3
Batch: 520; loss: 26.54; acc: 0.3
Batch: 540; loss: 30.25; acc: 0.2
Batch: 560; loss: 21.56; acc: 0.39
Batch: 580; loss: 20.7; acc: 0.31
Batch: 600; loss: 21.8; acc: 0.33
Batch: 620; loss: 20.15; acc: 0.28
Train Epoch over. train_loss: 32.72; train_accuracy: 0.26 

Batch: 0; loss: 25.02; acc: 0.23
Batch: 20; loss: 27.86; acc: 0.19
Batch: 40; loss: 23.08; acc: 0.28
Batch: 60; loss: 26.13; acc: 0.25
Batch: 80; loss: 30.53; acc: 0.23
Batch: 100; loss: 24.21; acc: 0.23
Batch: 120; loss: 19.68; acc: 0.25
Batch: 140; loss: 27.76; acc: 0.2
Val Epoch over. val_loss: 27.211439557895538; val_accuracy: 0.2448248407643312 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 23.4; acc: 0.27
Batch: 20; loss: 29.01; acc: 0.27
Batch: 40; loss: 27.56; acc: 0.23
Batch: 60; loss: 23.41; acc: 0.27
Batch: 80; loss: 29.5; acc: 0.2
Batch: 100; loss: 24.02; acc: 0.27
Batch: 120; loss: 28.19; acc: 0.19
Batch: 140; loss: 24.77; acc: 0.3
Batch: 160; loss: 26.4; acc: 0.28
Batch: 180; loss: 25.79; acc: 0.3
Batch: 200; loss: 26.5; acc: 0.22
Batch: 220; loss: 26.34; acc: 0.3
Batch: 240; loss: 25.24; acc: 0.2
Batch: 260; loss: 28.46; acc: 0.22
Batch: 280; loss: 22.86; acc: 0.3
Batch: 300; loss: 18.4; acc: 0.38
Batch: 320; loss: 28.85; acc: 0.11
Batch: 340; loss: 26.72; acc: 0.25
Batch: 360; loss: 21.67; acc: 0.22
Batch: 380; loss: 25.29; acc: 0.27
Batch: 400; loss: 32.91; acc: 0.27
Batch: 420; loss: 29.52; acc: 0.27
Batch: 440; loss: 28.55; acc: 0.23
Batch: 460; loss: 21.3; acc: 0.28
Batch: 480; loss: 22.79; acc: 0.23
Batch: 500; loss: 23.29; acc: 0.22
Batch: 520; loss: 26.65; acc: 0.3
Batch: 540; loss: 27.29; acc: 0.27
Batch: 560; loss: 23.68; acc: 0.2
Batch: 580; loss: 29.85; acc: 0.25
Batch: 600; loss: 24.9; acc: 0.3
Batch: 620; loss: 31.29; acc: 0.19
Train Epoch over. train_loss: 25.06; train_accuracy: 0.27 

Batch: 0; loss: 25.05; acc: 0.27
Batch: 20; loss: 24.88; acc: 0.17
Batch: 40; loss: 22.51; acc: 0.28
Batch: 60; loss: 20.89; acc: 0.27
Batch: 80; loss: 27.11; acc: 0.25
Batch: 100; loss: 20.54; acc: 0.27
Batch: 120; loss: 20.17; acc: 0.3
Batch: 140; loss: 26.69; acc: 0.17
Val Epoch over. val_loss: 25.199833037746938; val_accuracy: 0.2411425159235669 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 17.31; acc: 0.34
Batch: 20; loss: 24.11; acc: 0.28
Batch: 40; loss: 22.07; acc: 0.23
Batch: 60; loss: 22.43; acc: 0.22
Batch: 80; loss: 25.07; acc: 0.28
Batch: 100; loss: 17.69; acc: 0.33
Batch: 120; loss: 24.06; acc: 0.25
Batch: 140; loss: 25.08; acc: 0.31
Batch: 160; loss: 19.55; acc: 0.34
Batch: 180; loss: 20.57; acc: 0.28
Batch: 200; loss: 24.71; acc: 0.28
Batch: 220; loss: 20.3; acc: 0.28
Batch: 240; loss: 20.38; acc: 0.28
Batch: 260; loss: 19.76; acc: 0.25
Batch: 280; loss: 18.78; acc: 0.36
Batch: 300; loss: 21.7; acc: 0.2
Batch: 320; loss: 25.36; acc: 0.2
Batch: 340; loss: 21.62; acc: 0.27
Batch: 360; loss: 22.23; acc: 0.23
Batch: 380; loss: 23.71; acc: 0.28
Batch: 400; loss: 26.64; acc: 0.14
Batch: 420; loss: 18.22; acc: 0.23
Batch: 440; loss: 29.33; acc: 0.2
Batch: 460; loss: 21.52; acc: 0.34
Batch: 480; loss: 20.48; acc: 0.39
Batch: 500; loss: 23.73; acc: 0.2
Batch: 520; loss: 25.33; acc: 0.19
Batch: 540; loss: 21.86; acc: 0.27
Batch: 560; loss: 22.21; acc: 0.33
Batch: 580; loss: 26.22; acc: 0.22
Batch: 600; loss: 24.92; acc: 0.23
Batch: 620; loss: 19.22; acc: 0.27
Train Epoch over. train_loss: 23.65; train_accuracy: 0.26 

Batch: 0; loss: 24.13; acc: 0.22
Batch: 20; loss: 26.1; acc: 0.14
Batch: 40; loss: 22.38; acc: 0.3
Batch: 60; loss: 20.79; acc: 0.28
Batch: 80; loss: 25.78; acc: 0.22
Batch: 100; loss: 19.65; acc: 0.3
Batch: 120; loss: 18.69; acc: 0.3
Batch: 140; loss: 24.87; acc: 0.12
Val Epoch over. val_loss: 23.801729882598682; val_accuracy: 0.2369625796178344 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 26.13; acc: 0.23
Batch: 20; loss: 23.19; acc: 0.23
Batch: 40; loss: 23.07; acc: 0.3
Batch: 60; loss: 20.98; acc: 0.23
Batch: 80; loss: 19.5; acc: 0.36
Batch: 100; loss: 20.85; acc: 0.23
Batch: 120; loss: 28.09; acc: 0.16
Batch: 140; loss: 26.06; acc: 0.25
Batch: 160; loss: 21.8; acc: 0.2
Batch: 180; loss: 22.43; acc: 0.23
Batch: 200; loss: 23.26; acc: 0.22
Batch: 220; loss: 17.09; acc: 0.33
Batch: 240; loss: 19.33; acc: 0.39
Batch: 260; loss: 22.17; acc: 0.27
Batch: 280; loss: 21.54; acc: 0.25
Batch: 300; loss: 18.06; acc: 0.33
Batch: 320; loss: 20.77; acc: 0.28
Batch: 340; loss: 24.26; acc: 0.23
Batch: 360; loss: 26.11; acc: 0.17
Batch: 380; loss: 21.87; acc: 0.27
Batch: 400; loss: 19.85; acc: 0.28
Batch: 420; loss: 25.48; acc: 0.23
Batch: 440; loss: 24.37; acc: 0.19
Batch: 460; loss: 20.01; acc: 0.3
Batch: 480; loss: 22.22; acc: 0.27
Batch: 500; loss: 24.33; acc: 0.28
Batch: 520; loss: 19.86; acc: 0.34
Batch: 540; loss: 20.71; acc: 0.2
Batch: 560; loss: 17.9; acc: 0.28
Batch: 580; loss: 21.22; acc: 0.31
Batch: 600; loss: 23.62; acc: 0.28
Batch: 620; loss: 17.27; acc: 0.23
Train Epoch over. train_loss: 22.7; train_accuracy: 0.26 

Batch: 0; loss: 24.32; acc: 0.2
Batch: 20; loss: 22.39; acc: 0.23
Batch: 40; loss: 22.89; acc: 0.34
Batch: 60; loss: 18.93; acc: 0.28
Batch: 80; loss: 23.14; acc: 0.23
Batch: 100; loss: 20.23; acc: 0.33
Batch: 120; loss: 19.19; acc: 0.25
Batch: 140; loss: 24.11; acc: 0.16
Val Epoch over. val_loss: 22.336289053509947; val_accuracy: 0.2441281847133758 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 23.23; acc: 0.23
Batch: 20; loss: 15.05; acc: 0.39
Batch: 40; loss: 24.59; acc: 0.23
Batch: 60; loss: 28.09; acc: 0.23
Batch: 80; loss: 23.8; acc: 0.2
Batch: 100; loss: 28.13; acc: 0.2
Batch: 120; loss: 22.87; acc: 0.33
Batch: 140; loss: 26.8; acc: 0.12
Batch: 160; loss: 23.48; acc: 0.25
Batch: 180; loss: 18.71; acc: 0.19
Batch: 200; loss: 21.1; acc: 0.42
Batch: 220; loss: 17.25; acc: 0.27
Batch: 240; loss: 26.25; acc: 0.19
Batch: 260; loss: 27.07; acc: 0.27
Batch: 280; loss: 23.98; acc: 0.25
Batch: 300; loss: 21.1; acc: 0.28
Batch: 320; loss: 19.94; acc: 0.31
Batch: 340; loss: 19.55; acc: 0.34
Batch: 360; loss: 23.77; acc: 0.28
Batch: 380; loss: 23.77; acc: 0.19
Batch: 400; loss: 21.8; acc: 0.28
Batch: 420; loss: 18.79; acc: 0.34
Batch: 440; loss: 19.14; acc: 0.23
Batch: 460; loss: 22.74; acc: 0.2
Batch: 480; loss: 22.87; acc: 0.17
Batch: 500; loss: 23.23; acc: 0.27
Batch: 520; loss: 19.32; acc: 0.27
Batch: 540; loss: 20.42; acc: 0.25
Batch: 560; loss: 21.05; acc: 0.3
Batch: 580; loss: 19.24; acc: 0.25
Batch: 600; loss: 23.74; acc: 0.31
Batch: 620; loss: 18.88; acc: 0.33
Train Epoch over. train_loss: 21.72; train_accuracy: 0.26 

Batch: 0; loss: 22.28; acc: 0.16
Batch: 20; loss: 21.91; acc: 0.23
Batch: 40; loss: 23.42; acc: 0.28
Batch: 60; loss: 18.08; acc: 0.25
Batch: 80; loss: 22.33; acc: 0.17
Batch: 100; loss: 22.53; acc: 0.23
Batch: 120; loss: 19.9; acc: 0.22
Batch: 140; loss: 25.16; acc: 0.11
Val Epoch over. val_loss: 22.096201696213644; val_accuracy: 0.22024283439490447 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 25.15; acc: 0.2
Batch: 20; loss: 21.93; acc: 0.25
Batch: 40; loss: 21.28; acc: 0.27
Batch: 60; loss: 16.42; acc: 0.31
Batch: 80; loss: 22.31; acc: 0.28
Batch: 100; loss: 21.68; acc: 0.22
Batch: 120; loss: 22.1; acc: 0.23
Batch: 140; loss: 28.0; acc: 0.17
Batch: 160; loss: 24.86; acc: 0.22
Batch: 180; loss: 19.77; acc: 0.31
Batch: 200; loss: 20.05; acc: 0.27
Batch: 220; loss: 22.4; acc: 0.22
Batch: 240; loss: 16.98; acc: 0.27
Batch: 260; loss: 18.29; acc: 0.22
Batch: 280; loss: 22.28; acc: 0.27
Batch: 300; loss: 20.48; acc: 0.27
Batch: 320; loss: 19.19; acc: 0.23
Batch: 340; loss: 21.9; acc: 0.3
Batch: 360; loss: 28.54; acc: 0.31
Batch: 380; loss: 28.27; acc: 0.2
Batch: 400; loss: 17.91; acc: 0.2
Batch: 420; loss: 21.7; acc: 0.19
Batch: 440; loss: 20.31; acc: 0.33
Batch: 460; loss: 19.45; acc: 0.22
Batch: 480; loss: 24.07; acc: 0.17
Batch: 500; loss: 19.69; acc: 0.28
Batch: 520; loss: 20.95; acc: 0.33
Batch: 540; loss: 15.37; acc: 0.38
Batch: 560; loss: 19.69; acc: 0.36
Batch: 580; loss: 18.77; acc: 0.28
Batch: 600; loss: 22.63; acc: 0.25
Batch: 620; loss: 24.81; acc: 0.11
Train Epoch over. train_loss: 21.52; train_accuracy: 0.26 

Batch: 0; loss: 21.86; acc: 0.23
Batch: 20; loss: 23.23; acc: 0.25
Batch: 40; loss: 23.03; acc: 0.28
Batch: 60; loss: 18.41; acc: 0.27
Batch: 80; loss: 21.67; acc: 0.22
Batch: 100; loss: 24.25; acc: 0.25
Batch: 120; loss: 19.23; acc: 0.28
Batch: 140; loss: 27.02; acc: 0.14
Val Epoch over. val_loss: 22.29058085277582; val_accuracy: 0.2474124203821656 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 17.78; acc: 0.36
Batch: 20; loss: 16.53; acc: 0.27
Batch: 40; loss: 20.97; acc: 0.27
Batch: 60; loss: 19.51; acc: 0.28
Batch: 80; loss: 24.55; acc: 0.2
Batch: 100; loss: 23.89; acc: 0.22
Batch: 120; loss: 23.97; acc: 0.28
Batch: 140; loss: 18.54; acc: 0.25
Batch: 160; loss: 17.87; acc: 0.2
Batch: 180; loss: 24.03; acc: 0.2
Batch: 200; loss: 19.61; acc: 0.3
Batch: 220; loss: 22.61; acc: 0.3
Batch: 240; loss: 22.28; acc: 0.28
Batch: 260; loss: 23.25; acc: 0.31
Batch: 280; loss: 24.98; acc: 0.17
Batch: 300; loss: 25.26; acc: 0.25
Batch: 320; loss: 16.99; acc: 0.38
Batch: 340; loss: 19.35; acc: 0.33
Batch: 360; loss: 18.93; acc: 0.36
Batch: 380; loss: 25.09; acc: 0.28
Batch: 400; loss: 16.92; acc: 0.25
Batch: 420; loss: 22.39; acc: 0.22
Batch: 440; loss: 19.65; acc: 0.25
Batch: 460; loss: 24.18; acc: 0.27
Batch: 480; loss: 18.58; acc: 0.33
Batch: 500; loss: 25.54; acc: 0.25
Batch: 520; loss: 22.96; acc: 0.23
Batch: 540; loss: 24.5; acc: 0.2
Batch: 560; loss: 18.19; acc: 0.3
Batch: 580; loss: 18.78; acc: 0.27
Batch: 600; loss: 20.81; acc: 0.25
Batch: 620; loss: 20.3; acc: 0.31
Train Epoch over. train_loss: 21.28; train_accuracy: 0.27 

Batch: 0; loss: 20.15; acc: 0.19
Batch: 20; loss: 22.11; acc: 0.28
Batch: 40; loss: 21.17; acc: 0.34
Batch: 60; loss: 19.75; acc: 0.28
Batch: 80; loss: 21.02; acc: 0.25
Batch: 100; loss: 21.47; acc: 0.27
Batch: 120; loss: 18.39; acc: 0.28
Batch: 140; loss: 22.73; acc: 0.25
Val Epoch over. val_loss: 21.20110466222095; val_accuracy: 0.26343550955414013 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 20.38; acc: 0.23
Batch: 20; loss: 16.45; acc: 0.3
Batch: 40; loss: 21.23; acc: 0.22
Batch: 60; loss: 18.81; acc: 0.28
Batch: 80; loss: 22.13; acc: 0.25
Batch: 100; loss: 19.02; acc: 0.38
Batch: 120; loss: 24.23; acc: 0.23
Batch: 140; loss: 18.92; acc: 0.31
Batch: 160; loss: 18.06; acc: 0.25
Batch: 180; loss: 24.66; acc: 0.2
Batch: 200; loss: 19.41; acc: 0.3
Batch: 220; loss: 18.77; acc: 0.38
Batch: 240; loss: 20.4; acc: 0.27
Batch: 260; loss: 17.19; acc: 0.2
Batch: 280; loss: 21.6; acc: 0.23
Batch: 300; loss: 23.09; acc: 0.2
Batch: 320; loss: 20.02; acc: 0.27
Batch: 340; loss: 22.09; acc: 0.3
Batch: 360; loss: 20.87; acc: 0.2
Batch: 380; loss: 21.88; acc: 0.16
Batch: 400; loss: 21.83; acc: 0.3
Batch: 420; loss: 25.14; acc: 0.31
Batch: 440; loss: 16.61; acc: 0.3
Batch: 460; loss: 25.95; acc: 0.27
Batch: 480; loss: 23.25; acc: 0.22
Batch: 500; loss: 23.08; acc: 0.28
Batch: 520; loss: 24.0; acc: 0.22
Batch: 540; loss: 24.94; acc: 0.23
Batch: 560; loss: 17.45; acc: 0.23
Batch: 580; loss: 25.02; acc: 0.2
Batch: 600; loss: 20.86; acc: 0.25
Batch: 620; loss: 25.23; acc: 0.2
Train Epoch over. train_loss: 20.83; train_accuracy: 0.27 

Batch: 0; loss: 19.86; acc: 0.27
Batch: 20; loss: 22.82; acc: 0.22
Batch: 40; loss: 18.43; acc: 0.31
Batch: 60; loss: 21.29; acc: 0.28
Batch: 80; loss: 20.73; acc: 0.25
Batch: 100; loss: 20.65; acc: 0.25
Batch: 120; loss: 18.01; acc: 0.33
Batch: 140; loss: 22.29; acc: 0.23
Val Epoch over. val_loss: 21.25571211432196; val_accuracy: 0.26423168789808915 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 23.62; acc: 0.31
Batch: 20; loss: 20.24; acc: 0.28
Batch: 40; loss: 21.08; acc: 0.23
Batch: 60; loss: 23.82; acc: 0.25
Batch: 80; loss: 20.32; acc: 0.33
Batch: 100; loss: 16.92; acc: 0.27
Batch: 120; loss: 21.92; acc: 0.25
Batch: 140; loss: 25.18; acc: 0.3
Batch: 160; loss: 23.18; acc: 0.33
Batch: 180; loss: 21.83; acc: 0.2
Batch: 200; loss: 19.45; acc: 0.38
Batch: 220; loss: 18.42; acc: 0.25
Batch: 240; loss: 23.56; acc: 0.16
Batch: 260; loss: 22.59; acc: 0.34
Batch: 280; loss: 17.08; acc: 0.42
Batch: 300; loss: 21.91; acc: 0.23
Batch: 320; loss: 19.46; acc: 0.19
Batch: 340; loss: 23.57; acc: 0.2
Batch: 360; loss: 24.06; acc: 0.23
Batch: 380; loss: 25.51; acc: 0.16
Batch: 400; loss: 22.01; acc: 0.14
Batch: 420; loss: 20.78; acc: 0.28
Batch: 440; loss: 21.22; acc: 0.23
Batch: 460; loss: 18.72; acc: 0.3
Batch: 480; loss: 24.58; acc: 0.27
Batch: 500; loss: 20.47; acc: 0.31
Batch: 520; loss: 18.7; acc: 0.23
Batch: 540; loss: 18.28; acc: 0.31
Batch: 560; loss: 27.6; acc: 0.25
Batch: 580; loss: 14.73; acc: 0.27
Batch: 600; loss: 21.08; acc: 0.28
Batch: 620; loss: 16.2; acc: 0.28
Train Epoch over. train_loss: 20.4; train_accuracy: 0.28 

Batch: 0; loss: 17.84; acc: 0.38
Batch: 20; loss: 21.82; acc: 0.27
Batch: 40; loss: 19.71; acc: 0.33
Batch: 60; loss: 19.41; acc: 0.36
Batch: 80; loss: 18.72; acc: 0.3
Batch: 100; loss: 21.6; acc: 0.28
Batch: 120; loss: 18.38; acc: 0.25
Batch: 140; loss: 21.94; acc: 0.16
Val Epoch over. val_loss: 20.986511455220022; val_accuracy: 0.2757762738853503 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: 25.11; acc: 0.22
Batch: 20; loss: 22.5; acc: 0.23
Batch: 40; loss: 12.42; acc: 0.44
Batch: 60; loss: 24.0; acc: 0.22
Batch: 80; loss: 19.52; acc: 0.2
Batch: 100; loss: 18.38; acc: 0.3
Batch: 120; loss: 19.87; acc: 0.23
Batch: 140; loss: 23.04; acc: 0.17
Batch: 160; loss: 21.12; acc: 0.28
Batch: 180; loss: 22.6; acc: 0.28
Batch: 200; loss: 19.72; acc: 0.31
Batch: 220; loss: 18.93; acc: 0.38
Batch: 240; loss: 17.79; acc: 0.44
Batch: 260; loss: 20.45; acc: 0.31
Batch: 280; loss: 21.15; acc: 0.25
Batch: 300; loss: 20.22; acc: 0.34
Batch: 320; loss: 27.42; acc: 0.22
Batch: 340; loss: 19.14; acc: 0.27
Batch: 360; loss: 18.14; acc: 0.27
Batch: 380; loss: 19.4; acc: 0.23
Batch: 400; loss: 16.52; acc: 0.31
Batch: 420; loss: 22.71; acc: 0.25
Batch: 440; loss: 19.76; acc: 0.34
Batch: 460; loss: 22.21; acc: 0.25
Batch: 480; loss: 17.77; acc: 0.3
Batch: 500; loss: 21.36; acc: 0.3
Batch: 520; loss: 18.91; acc: 0.27
Batch: 540; loss: 18.69; acc: 0.22
Batch: 560; loss: 20.41; acc: 0.28
Batch: 580; loss: 17.6; acc: 0.33
Batch: 600; loss: 24.47; acc: 0.17
Batch: 620; loss: 20.22; acc: 0.31
Train Epoch over. train_loss: 19.93; train_accuracy: 0.28 

Batch: 0; loss: 18.04; acc: 0.27
Batch: 20; loss: 23.51; acc: 0.25
Batch: 40; loss: 16.98; acc: 0.33
Batch: 60; loss: 20.08; acc: 0.27
Batch: 80; loss: 20.8; acc: 0.25
Batch: 100; loss: 20.19; acc: 0.28
Batch: 120; loss: 16.49; acc: 0.27
Batch: 140; loss: 22.86; acc: 0.17
Val Epoch over. val_loss: 20.78541321967058; val_accuracy: 0.26184315286624205 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 21.17; acc: 0.27
Batch: 20; loss: 22.34; acc: 0.22
Batch: 40; loss: 16.84; acc: 0.34
Batch: 60; loss: 16.3; acc: 0.25
Batch: 80; loss: 21.07; acc: 0.3
Batch: 100; loss: 21.99; acc: 0.23
Batch: 120; loss: 17.24; acc: 0.31
Batch: 140; loss: 19.37; acc: 0.23
Batch: 160; loss: 15.24; acc: 0.23
Batch: 180; loss: 19.31; acc: 0.25
Batch: 200; loss: 16.85; acc: 0.31
Batch: 220; loss: 15.77; acc: 0.39
Batch: 240; loss: 20.03; acc: 0.2
Batch: 260; loss: 16.07; acc: 0.28
Batch: 280; loss: 22.33; acc: 0.27
Batch: 300; loss: 19.76; acc: 0.33
Batch: 320; loss: 18.3; acc: 0.33
Batch: 340; loss: 14.45; acc: 0.38
Batch: 360; loss: 18.08; acc: 0.31
Batch: 380; loss: 18.26; acc: 0.28
Batch: 400; loss: 18.46; acc: 0.28
Batch: 420; loss: 16.13; acc: 0.33
Batch: 440; loss: 21.71; acc: 0.23
Batch: 460; loss: 25.78; acc: 0.17
Batch: 480; loss: 15.76; acc: 0.31
Batch: 500; loss: 19.57; acc: 0.27
Batch: 520; loss: 16.57; acc: 0.36
Batch: 540; loss: 17.54; acc: 0.28
Batch: 560; loss: 15.53; acc: 0.38
Batch: 580; loss: 15.53; acc: 0.39
Batch: 600; loss: 17.81; acc: 0.27
Batch: 620; loss: 19.24; acc: 0.3
Train Epoch over. train_loss: 18.73; train_accuracy: 0.29 

Batch: 0; loss: 16.53; acc: 0.3
Batch: 20; loss: 21.01; acc: 0.27
Batch: 40; loss: 15.61; acc: 0.33
Batch: 60; loss: 19.19; acc: 0.25
Batch: 80; loss: 17.8; acc: 0.3
Batch: 100; loss: 18.67; acc: 0.28
Batch: 120; loss: 15.32; acc: 0.31
Batch: 140; loss: 20.66; acc: 0.17
Val Epoch over. val_loss: 19.183131381964227; val_accuracy: 0.26781449044585987 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 20.03; acc: 0.28
Batch: 20; loss: 22.32; acc: 0.27
Batch: 40; loss: 21.81; acc: 0.22
Batch: 60; loss: 20.36; acc: 0.25
Batch: 80; loss: 20.55; acc: 0.2
Batch: 100; loss: 15.98; acc: 0.33
Batch: 120; loss: 24.38; acc: 0.25
Batch: 140; loss: 17.36; acc: 0.38
Batch: 160; loss: 23.93; acc: 0.27
Batch: 180; loss: 21.55; acc: 0.31
Batch: 200; loss: 19.2; acc: 0.39
Batch: 220; loss: 20.71; acc: 0.19
Batch: 240; loss: 21.13; acc: 0.25
Batch: 260; loss: 19.84; acc: 0.16
Batch: 280; loss: 18.12; acc: 0.31
Batch: 300; loss: 17.61; acc: 0.33
Batch: 320; loss: 16.47; acc: 0.34
Batch: 340; loss: 18.16; acc: 0.25
Batch: 360; loss: 20.15; acc: 0.25
Batch: 380; loss: 19.37; acc: 0.23
Batch: 400; loss: 14.97; acc: 0.31
Batch: 420; loss: 20.11; acc: 0.23
Batch: 440; loss: 18.21; acc: 0.28
Batch: 460; loss: 21.81; acc: 0.23
Batch: 480; loss: 17.37; acc: 0.28
Batch: 500; loss: 22.11; acc: 0.27
Batch: 520; loss: 22.13; acc: 0.22
Batch: 540; loss: 18.54; acc: 0.38
Batch: 560; loss: 23.02; acc: 0.25
Batch: 580; loss: 15.44; acc: 0.31
Batch: 600; loss: 16.42; acc: 0.28
Batch: 620; loss: 18.07; acc: 0.2
Train Epoch over. train_loss: 18.69; train_accuracy: 0.29 

Batch: 0; loss: 16.65; acc: 0.33
Batch: 20; loss: 21.02; acc: 0.27
Batch: 40; loss: 15.19; acc: 0.36
Batch: 60; loss: 19.05; acc: 0.28
Batch: 80; loss: 17.67; acc: 0.25
Batch: 100; loss: 18.75; acc: 0.27
Batch: 120; loss: 15.02; acc: 0.33
Batch: 140; loss: 20.36; acc: 0.19
Val Epoch over. val_loss: 19.114125403628986; val_accuracy: 0.27129777070063693 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 20.66; acc: 0.25
Batch: 20; loss: 16.52; acc: 0.39
Batch: 40; loss: 17.24; acc: 0.25
Batch: 60; loss: 20.99; acc: 0.28
Batch: 80; loss: 21.93; acc: 0.25
Batch: 100; loss: 15.66; acc: 0.34
Batch: 120; loss: 15.82; acc: 0.31
Batch: 140; loss: 18.21; acc: 0.27
Batch: 160; loss: 16.27; acc: 0.33
Batch: 180; loss: 17.02; acc: 0.31
Batch: 200; loss: 16.99; acc: 0.39
Batch: 220; loss: 18.28; acc: 0.22
Batch: 240; loss: 18.09; acc: 0.27
Batch: 260; loss: 16.97; acc: 0.3
Batch: 280; loss: 15.7; acc: 0.34
Batch: 300; loss: 21.04; acc: 0.22
Batch: 320; loss: 20.12; acc: 0.3
Batch: 340; loss: 16.59; acc: 0.31
Batch: 360; loss: 18.47; acc: 0.36
Batch: 380; loss: 18.14; acc: 0.34
Batch: 400; loss: 17.75; acc: 0.31
Batch: 420; loss: 17.76; acc: 0.34
Batch: 440; loss: 18.16; acc: 0.28
Batch: 460; loss: 19.43; acc: 0.25
Batch: 480; loss: 18.64; acc: 0.25
Batch: 500; loss: 18.9; acc: 0.39
Batch: 520; loss: 19.35; acc: 0.31
Batch: 540; loss: 19.06; acc: 0.27
Batch: 560; loss: 19.49; acc: 0.22
Batch: 580; loss: 18.77; acc: 0.31
Batch: 600; loss: 16.7; acc: 0.27
Batch: 620; loss: 16.62; acc: 0.42
Train Epoch over. train_loss: 18.66; train_accuracy: 0.29 

Batch: 0; loss: 16.6; acc: 0.33
Batch: 20; loss: 21.43; acc: 0.25
Batch: 40; loss: 15.01; acc: 0.36
Batch: 60; loss: 19.21; acc: 0.27
Batch: 80; loss: 17.59; acc: 0.31
Batch: 100; loss: 18.8; acc: 0.27
Batch: 120; loss: 15.06; acc: 0.33
Batch: 140; loss: 20.41; acc: 0.19
Val Epoch over. val_loss: 19.11704833036775; val_accuracy: 0.2706011146496815 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 20.35; acc: 0.33
Batch: 20; loss: 18.59; acc: 0.2
Batch: 40; loss: 18.85; acc: 0.23
Batch: 60; loss: 18.96; acc: 0.31
Batch: 80; loss: 17.91; acc: 0.39
Batch: 100; loss: 17.09; acc: 0.28
Batch: 120; loss: 18.27; acc: 0.25
Batch: 140; loss: 20.22; acc: 0.28
Batch: 160; loss: 15.39; acc: 0.25
Batch: 180; loss: 18.18; acc: 0.25
Batch: 200; loss: 19.13; acc: 0.28
Batch: 220; loss: 22.38; acc: 0.3
Batch: 240; loss: 17.73; acc: 0.27
Batch: 260; loss: 20.79; acc: 0.28
Batch: 280; loss: 15.72; acc: 0.38
Batch: 300; loss: 21.26; acc: 0.27
Batch: 320; loss: 16.27; acc: 0.38
Batch: 340; loss: 17.36; acc: 0.36
Batch: 360; loss: 17.66; acc: 0.27
Batch: 380; loss: 18.3; acc: 0.34
Batch: 400; loss: 19.35; acc: 0.38
Batch: 420; loss: 17.64; acc: 0.28
Batch: 440; loss: 22.02; acc: 0.28
Batch: 460; loss: 20.07; acc: 0.27
Batch: 480; loss: 17.59; acc: 0.31
Batch: 500; loss: 19.01; acc: 0.22
Batch: 520; loss: 18.01; acc: 0.27
Batch: 540; loss: 17.33; acc: 0.36
Batch: 560; loss: 16.26; acc: 0.31
Batch: 580; loss: 15.93; acc: 0.25
Batch: 600; loss: 19.69; acc: 0.25
Batch: 620; loss: 16.56; acc: 0.38
Train Epoch over. train_loss: 18.65; train_accuracy: 0.29 

Batch: 0; loss: 16.98; acc: 0.3
Batch: 20; loss: 21.07; acc: 0.23
Batch: 40; loss: 14.98; acc: 0.36
Batch: 60; loss: 19.01; acc: 0.31
Batch: 80; loss: 17.28; acc: 0.31
Batch: 100; loss: 18.69; acc: 0.27
Batch: 120; loss: 14.64; acc: 0.3
Batch: 140; loss: 20.1; acc: 0.17
Val Epoch over. val_loss: 19.1062379885631; val_accuracy: 0.27169585987261147 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 16.21; acc: 0.3
Batch: 20; loss: 17.83; acc: 0.28
Batch: 40; loss: 18.85; acc: 0.25
Batch: 60; loss: 19.52; acc: 0.3
Batch: 80; loss: 16.94; acc: 0.23
Batch: 100; loss: 22.36; acc: 0.25
Batch: 120; loss: 22.11; acc: 0.27
Batch: 140; loss: 17.26; acc: 0.28
Batch: 160; loss: 19.31; acc: 0.23
Batch: 180; loss: 15.54; acc: 0.45
Batch: 200; loss: 19.33; acc: 0.27
Batch: 220; loss: 19.26; acc: 0.17
Batch: 240; loss: 16.48; acc: 0.28
Batch: 260; loss: 21.39; acc: 0.17
Batch: 280; loss: 25.69; acc: 0.2
Batch: 300; loss: 16.62; acc: 0.34
Batch: 320; loss: 19.16; acc: 0.28
Batch: 340; loss: 19.81; acc: 0.25
Batch: 360; loss: 16.52; acc: 0.31
Batch: 380; loss: 23.8; acc: 0.2
Batch: 400; loss: 16.57; acc: 0.38
Batch: 420; loss: 24.4; acc: 0.22
Batch: 440; loss: 18.23; acc: 0.28
Batch: 460; loss: 23.54; acc: 0.11
Batch: 480; loss: 16.8; acc: 0.34
Batch: 500; loss: 16.19; acc: 0.19
Batch: 520; loss: 16.11; acc: 0.38
Batch: 540; loss: 18.66; acc: 0.3
Batch: 560; loss: 15.92; acc: 0.3
Batch: 580; loss: 14.0; acc: 0.28
Batch: 600; loss: 20.65; acc: 0.31
Batch: 620; loss: 16.47; acc: 0.27
Train Epoch over. train_loss: 18.63; train_accuracy: 0.29 

Batch: 0; loss: 16.6; acc: 0.3
Batch: 20; loss: 21.34; acc: 0.23
Batch: 40; loss: 14.74; acc: 0.36
Batch: 60; loss: 18.65; acc: 0.3
Batch: 80; loss: 17.48; acc: 0.27
Batch: 100; loss: 18.68; acc: 0.27
Batch: 120; loss: 14.64; acc: 0.28
Batch: 140; loss: 20.46; acc: 0.19
Val Epoch over. val_loss: 19.085271555906647; val_accuracy: 0.2679140127388535 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 17.01; acc: 0.23
Batch: 20; loss: 22.15; acc: 0.23
Batch: 40; loss: 17.18; acc: 0.34
Batch: 60; loss: 19.23; acc: 0.23
Batch: 80; loss: 23.46; acc: 0.22
Batch: 100; loss: 17.98; acc: 0.25
Batch: 120; loss: 18.27; acc: 0.31
Batch: 140; loss: 20.52; acc: 0.2
Batch: 160; loss: 18.03; acc: 0.31
Batch: 180; loss: 20.05; acc: 0.25
Batch: 200; loss: 20.91; acc: 0.25
Batch: 220; loss: 23.52; acc: 0.16
Batch: 240; loss: 20.39; acc: 0.14
Batch: 260; loss: 22.71; acc: 0.14
Batch: 280; loss: 18.38; acc: 0.25
Batch: 300; loss: 18.83; acc: 0.27
Batch: 320; loss: 18.86; acc: 0.23
Batch: 340; loss: 18.8; acc: 0.23
Batch: 360; loss: 17.57; acc: 0.23
Batch: 380; loss: 17.38; acc: 0.3
Batch: 400; loss: 18.43; acc: 0.3
Batch: 420; loss: 16.04; acc: 0.22
Batch: 440; loss: 17.58; acc: 0.45
Batch: 460; loss: 18.98; acc: 0.3
Batch: 480; loss: 16.35; acc: 0.27
Batch: 500; loss: 19.27; acc: 0.25
Batch: 520; loss: 17.55; acc: 0.42
Batch: 540; loss: 18.81; acc: 0.2
Batch: 560; loss: 17.16; acc: 0.36
Batch: 580; loss: 14.29; acc: 0.33
Batch: 600; loss: 17.51; acc: 0.2
Batch: 620; loss: 22.63; acc: 0.25
Train Epoch over. train_loss: 18.62; train_accuracy: 0.28 

Batch: 0; loss: 16.84; acc: 0.25
Batch: 20; loss: 21.24; acc: 0.27
Batch: 40; loss: 14.77; acc: 0.36
Batch: 60; loss: 19.13; acc: 0.27
Batch: 80; loss: 17.44; acc: 0.28
Batch: 100; loss: 18.58; acc: 0.25
Batch: 120; loss: 14.85; acc: 0.3
Batch: 140; loss: 20.22; acc: 0.2
Val Epoch over. val_loss: 19.09221615153513; val_accuracy: 0.2713972929936306 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 25.72; acc: 0.22
Batch: 20; loss: 21.53; acc: 0.2
Batch: 40; loss: 18.82; acc: 0.28
Batch: 60; loss: 18.17; acc: 0.31
Batch: 80; loss: 18.17; acc: 0.31
Batch: 100; loss: 20.85; acc: 0.23
Batch: 120; loss: 14.2; acc: 0.36
Batch: 140; loss: 18.93; acc: 0.31
Batch: 160; loss: 18.87; acc: 0.33
Batch: 180; loss: 20.32; acc: 0.34
Batch: 200; loss: 13.46; acc: 0.36
Batch: 220; loss: 12.44; acc: 0.34
Batch: 240; loss: 14.79; acc: 0.28
Batch: 260; loss: 16.93; acc: 0.41
Batch: 280; loss: 18.2; acc: 0.22
Batch: 300; loss: 18.95; acc: 0.3
Batch: 320; loss: 19.5; acc: 0.31
Batch: 340; loss: 15.51; acc: 0.3
Batch: 360; loss: 16.58; acc: 0.33
Batch: 380; loss: 18.45; acc: 0.27
Batch: 400; loss: 21.16; acc: 0.38
Batch: 420; loss: 14.83; acc: 0.34
Batch: 440; loss: 18.26; acc: 0.33
Batch: 460; loss: 20.01; acc: 0.34
Batch: 480; loss: 20.09; acc: 0.25
Batch: 500; loss: 17.88; acc: 0.23
Batch: 520; loss: 15.1; acc: 0.38
Batch: 540; loss: 16.88; acc: 0.31
Batch: 560; loss: 18.29; acc: 0.3
Batch: 580; loss: 18.0; acc: 0.3
Batch: 600; loss: 16.08; acc: 0.3
Batch: 620; loss: 17.88; acc: 0.34
Train Epoch over. train_loss: 18.61; train_accuracy: 0.29 

Batch: 0; loss: 16.6; acc: 0.28
Batch: 20; loss: 21.73; acc: 0.25
Batch: 40; loss: 14.75; acc: 0.34
Batch: 60; loss: 18.76; acc: 0.28
Batch: 80; loss: 17.54; acc: 0.33
Batch: 100; loss: 18.82; acc: 0.25
Batch: 120; loss: 14.81; acc: 0.3
Batch: 140; loss: 20.58; acc: 0.17
Val Epoch over. val_loss: 19.154557386021704; val_accuracy: 0.26512738853503187 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 21.53; acc: 0.28
Batch: 20; loss: 21.41; acc: 0.3
Batch: 40; loss: 18.21; acc: 0.33
Batch: 60; loss: 20.71; acc: 0.31
Batch: 80; loss: 19.2; acc: 0.17
Batch: 100; loss: 18.79; acc: 0.27
Batch: 120; loss: 14.6; acc: 0.33
Batch: 140; loss: 15.4; acc: 0.3
Batch: 160; loss: 19.17; acc: 0.25
Batch: 180; loss: 16.19; acc: 0.28
Batch: 200; loss: 19.04; acc: 0.28
Batch: 220; loss: 19.72; acc: 0.28
Batch: 240; loss: 20.58; acc: 0.28
Batch: 260; loss: 19.26; acc: 0.3
Batch: 280; loss: 14.85; acc: 0.3
Batch: 300; loss: 17.47; acc: 0.28
Batch: 320; loss: 21.06; acc: 0.27
Batch: 340; loss: 22.64; acc: 0.28
Batch: 360; loss: 21.06; acc: 0.2
Batch: 380; loss: 17.25; acc: 0.3
Batch: 400; loss: 19.97; acc: 0.19
Batch: 420; loss: 21.92; acc: 0.22
Batch: 440; loss: 19.29; acc: 0.25
Batch: 460; loss: 23.37; acc: 0.23
Batch: 480; loss: 14.03; acc: 0.39
Batch: 500; loss: 24.73; acc: 0.17
Batch: 520; loss: 19.47; acc: 0.31
Batch: 540; loss: 18.99; acc: 0.28
Batch: 560; loss: 16.3; acc: 0.33
Batch: 580; loss: 20.52; acc: 0.33
Batch: 600; loss: 17.91; acc: 0.34
Batch: 620; loss: 18.36; acc: 0.38
Train Epoch over. train_loss: 18.61; train_accuracy: 0.29 

Batch: 0; loss: 16.88; acc: 0.31
Batch: 20; loss: 21.17; acc: 0.25
Batch: 40; loss: 14.77; acc: 0.36
Batch: 60; loss: 18.92; acc: 0.28
Batch: 80; loss: 17.06; acc: 0.3
Batch: 100; loss: 18.36; acc: 0.27
Batch: 120; loss: 14.44; acc: 0.33
Batch: 140; loss: 20.24; acc: 0.19
Val Epoch over. val_loss: 19.06616181780578; val_accuracy: 0.27129777070063693 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 15.06; acc: 0.33
Batch: 20; loss: 24.23; acc: 0.16
Batch: 40; loss: 14.45; acc: 0.38
Batch: 60; loss: 18.94; acc: 0.28
Batch: 80; loss: 25.13; acc: 0.19
Batch: 100; loss: 22.3; acc: 0.22
Batch: 120; loss: 18.69; acc: 0.3
Batch: 140; loss: 22.59; acc: 0.33
Batch: 160; loss: 20.38; acc: 0.33
Batch: 180; loss: 17.1; acc: 0.31
Batch: 200; loss: 16.65; acc: 0.39
Batch: 220; loss: 17.23; acc: 0.31
Batch: 240; loss: 19.43; acc: 0.36
Batch: 260; loss: 17.29; acc: 0.27
Batch: 280; loss: 20.46; acc: 0.27
Batch: 300; loss: 18.44; acc: 0.27
Batch: 320; loss: 22.96; acc: 0.28
Batch: 340; loss: 19.61; acc: 0.25
Batch: 360; loss: 13.3; acc: 0.39
Batch: 380; loss: 19.42; acc: 0.17
Batch: 400; loss: 17.34; acc: 0.42
Batch: 420; loss: 16.97; acc: 0.25
Batch: 440; loss: 14.78; acc: 0.28
Batch: 460; loss: 17.7; acc: 0.34
Batch: 480; loss: 23.09; acc: 0.28
Batch: 500; loss: 15.99; acc: 0.31
Batch: 520; loss: 20.09; acc: 0.22
Batch: 540; loss: 16.51; acc: 0.34
Batch: 560; loss: 20.51; acc: 0.2
Batch: 580; loss: 18.03; acc: 0.25
Batch: 600; loss: 18.92; acc: 0.28
Batch: 620; loss: 21.02; acc: 0.25
Train Epoch over. train_loss: 18.6; train_accuracy: 0.28 

Batch: 0; loss: 16.72; acc: 0.27
Batch: 20; loss: 21.23; acc: 0.25
Batch: 40; loss: 14.58; acc: 0.38
Batch: 60; loss: 19.05; acc: 0.28
Batch: 80; loss: 17.22; acc: 0.33
Batch: 100; loss: 18.41; acc: 0.28
Batch: 120; loss: 14.54; acc: 0.3
Batch: 140; loss: 20.13; acc: 0.17
Val Epoch over. val_loss: 19.065159670106926; val_accuracy: 0.26970541401273884 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: 19.38; acc: 0.27
Batch: 20; loss: 20.68; acc: 0.33
Batch: 40; loss: 18.37; acc: 0.2
Batch: 60; loss: 20.96; acc: 0.22
Batch: 80; loss: 18.15; acc: 0.27
Batch: 100; loss: 24.5; acc: 0.27
Batch: 120; loss: 20.15; acc: 0.27
Batch: 140; loss: 19.44; acc: 0.28
Batch: 160; loss: 13.98; acc: 0.42
Batch: 180; loss: 20.44; acc: 0.27
Batch: 200; loss: 19.47; acc: 0.23
Batch: 220; loss: 15.27; acc: 0.31
Batch: 240; loss: 15.19; acc: 0.33
Batch: 260; loss: 21.97; acc: 0.2
Batch: 280; loss: 22.76; acc: 0.19
Batch: 300; loss: 20.84; acc: 0.25
Batch: 320; loss: 17.83; acc: 0.36
Batch: 340; loss: 19.73; acc: 0.28
Batch: 360; loss: 17.64; acc: 0.27
Batch: 380; loss: 19.86; acc: 0.22
Batch: 400; loss: 18.89; acc: 0.28
Batch: 420; loss: 22.04; acc: 0.22
Batch: 440; loss: 16.47; acc: 0.38
Batch: 460; loss: 20.89; acc: 0.23
Batch: 480; loss: 18.73; acc: 0.27
Batch: 500; loss: 17.84; acc: 0.25
Batch: 520; loss: 17.31; acc: 0.38
Batch: 540; loss: 16.31; acc: 0.39
Batch: 560; loss: 20.22; acc: 0.23
Batch: 580; loss: 19.35; acc: 0.36
Batch: 600; loss: 18.52; acc: 0.34
Batch: 620; loss: 18.98; acc: 0.22
Train Epoch over. train_loss: 18.6; train_accuracy: 0.29 

Batch: 0; loss: 17.01; acc: 0.25
Batch: 20; loss: 21.42; acc: 0.23
Batch: 40; loss: 14.47; acc: 0.36
Batch: 60; loss: 19.04; acc: 0.28
Batch: 80; loss: 17.12; acc: 0.3
Batch: 100; loss: 18.47; acc: 0.28
Batch: 120; loss: 14.49; acc: 0.3
Batch: 140; loss: 20.23; acc: 0.19
Val Epoch over. val_loss: 19.076180233317576; val_accuracy: 0.26940684713375795 

plots/subspace_training/reg_lenet/2020-01-16 13:47:42/d_dim_50_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 31325
elements in E: 9647000
fraction nonzero: 0.003247123458069866
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 278.61; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.02
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.12
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.12
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.17
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.17
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.2
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.02
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.23
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.14
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.12
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.16
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.05
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.02
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.06
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.14
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.02
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.14
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.05
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.14
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.17
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.02
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.02
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.16
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.03
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.2
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.05
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.16
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.05
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.03
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.17
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.02
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.03
Batch: 440; loss: nan; acc: 0.19
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.17
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.16
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.2
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.17
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.05
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.16
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.16
Batch: 40; loss: nan; acc: 0.17
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.02
Batch: 580; loss: nan; acc: 0.16
Batch: 600; loss: nan; acc: 0.2
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.03
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.16
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.16
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.17
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.03
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.03
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.16
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.19
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.05
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.17
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.06
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.03
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.03
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.19
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.02
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.0
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.06
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.05
Batch: 280; loss: nan; acc: 0.16
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.14
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.05
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.17
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.19
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.03
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.2
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.16
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

plots/subspace_training/reg_lenet/2020-01-16 13:47:42/d_dim_100_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 61844
elements in E: 19294000
fraction nonzero: 0.003205348813102519
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 278.61; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.02
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.12
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.12
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.17
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.17
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.2
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.02
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.23
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.14
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.12
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.16
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.05
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.02
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.06
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.14
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.02
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.14
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.05
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.14
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.17
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.02
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.02
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.16
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.03
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.2
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.05
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.16
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.05
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.03
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.17
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.02
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.03
Batch: 440; loss: nan; acc: 0.19
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.17
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.16
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.2
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.17
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.05
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.16
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.16
Batch: 40; loss: nan; acc: 0.17
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.02
Batch: 580; loss: nan; acc: 0.16
Batch: 600; loss: nan; acc: 0.2
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.03
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.16
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.16
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.17
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.03
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.03
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.16
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.19
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.05
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.17
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.06
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.03
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.03
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.19
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.02
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.0
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.06
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.05
Batch: 280; loss: nan; acc: 0.16
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.14
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.05
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.17
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.19
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.03
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.2
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.16
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

plots/subspace_training/reg_lenet/2020-01-16 13:47:42/d_dim_200_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 92884
elements in E: 28941000
fraction nonzero: 0.003209426073736222
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 278.61; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.02
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.12
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.12
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.17
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.17
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.2
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.02
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.23
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.14
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.12
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.16
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.05
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.02
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.06
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.14
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.02
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.14
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.05
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.14
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.17
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.02
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.02
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.16
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.03
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.2
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.05
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.16
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.05
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.03
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.17
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.02
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.03
Batch: 440; loss: nan; acc: 0.19
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.17
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.16
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.2
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.17
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.05
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.16
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.16
Batch: 40; loss: nan; acc: 0.17
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.02
Batch: 580; loss: nan; acc: 0.16
Batch: 600; loss: nan; acc: 0.2
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.03
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.16
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.16
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.17
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.03
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.03
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.16
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.19
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.05
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.17
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.06
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.03
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.03
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.19
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.02
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.0
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.06
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.05
Batch: 280; loss: nan; acc: 0.16
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.14
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.05
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.17
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.19
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.03
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.2
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.16
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

plots/subspace_training/reg_lenet/2020-01-16 13:47:42/d_dim_300_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 123495
elements in E: 38588000
fraction nonzero: 0.003200347258214989
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 278.61; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.02
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.12
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.12
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.17
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.17
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.2
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.02
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.23
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.14
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.12
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.16
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.05
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.02
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.06
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.14
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.02
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.14
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.05
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.14
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.17
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.02
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.02
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.16
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.03
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.2
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.05
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.16
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.05
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.03
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.17
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.02
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.03
Batch: 440; loss: nan; acc: 0.19
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.17
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.16
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.2
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.17
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.05
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.16
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.16
Batch: 40; loss: nan; acc: 0.17
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.02
Batch: 580; loss: nan; acc: 0.16
Batch: 600; loss: nan; acc: 0.2
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.03
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.16
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.16
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.17
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.03
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.03
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.16
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.19
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.05
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.17
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.06
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.03
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.03
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.19
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.02
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.0
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.06
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.05
Batch: 280; loss: nan; acc: 0.16
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.14
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.05
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.17
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.19
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.03
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.2
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.16
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

plots/subspace_training/reg_lenet/2020-01-16 13:47:42/d_dim_400_lr_0.1_seed_1_epochs_30_batchsize_64
nonzero elements in E: 155305
elements in E: 48235000
fraction nonzero: 0.003219757437545351
Epoch 1 start
The current lr is: 0.1
Batch: 0; loss: 278.61; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 2 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.02
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.12
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.12
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 3 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.17
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.17
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.2
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 4 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.02
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.23
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 5 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.14
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.12
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.16
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.09
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 6 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.05
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.02
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.14
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 7 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.12
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.14
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 8 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.11
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.16
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 9 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.06
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.14
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.02
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.14
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.05
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.14
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 10 start
The current lr is: 0.1
Batch: 0; loss: nan; acc: 0.14
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.17
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 11 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.02
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.02
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.16
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.14
Batch: 360; loss: nan; acc: 0.05
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 12 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.03
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.05
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 13 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.2
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.14
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.12
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.05
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.16
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 14 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.11
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.16
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.05
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.16
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 15 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.12
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.05
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.03
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.17
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 16 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.05
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.09
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.06
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.06
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.09
Batch: 540; loss: nan; acc: 0.17
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.06
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 17 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.02
Batch: 60; loss: nan; acc: 0.12
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.02
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.05
Batch: 300; loss: nan; acc: 0.06
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.03
Batch: 440; loss: nan; acc: 0.19
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.17
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.09
Batch: 560; loss: nan; acc: 0.12
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 18 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.05
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.16
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.2
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.17
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.09
Batch: 400; loss: nan; acc: 0.06
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.09
Batch: 500; loss: nan; acc: 0.12
Batch: 520; loss: nan; acc: 0.05
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.16
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 19 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.16
Batch: 40; loss: nan; acc: 0.17
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.14
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.14
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.14
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.05
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.02
Batch: 580; loss: nan; acc: 0.16
Batch: 600; loss: nan; acc: 0.2
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 20 start
The current lr is: 0.010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.08
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.03
Batch: 140; loss: nan; acc: 0.06
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.11
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.16
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.12
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.05
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 21 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.12
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.16
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.08
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.17
Batch: 240; loss: nan; acc: 0.06
Batch: 260; loss: nan; acc: 0.11
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.09
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.17
Batch: 460; loss: nan; acc: 0.14
Batch: 480; loss: nan; acc: 0.03
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.03
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.08
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 22 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.06
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.11
Batch: 200; loss: nan; acc: 0.09
Batch: 220; loss: nan; acc: 0.08
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.08
Batch: 280; loss: nan; acc: 0.11
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.09
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.03
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.16
Batch: 520; loss: nan; acc: 0.11
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.05
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.19
Batch: 620; loss: nan; acc: 0.14
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 23 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.05
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.05
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.14
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.05
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.17
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.08
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.11
Batch: 380; loss: nan; acc: 0.12
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.06
Batch: 500; loss: nan; acc: 0.08
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.06
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 24 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.14
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.08
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.03
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.19
Batch: 240; loss: nan; acc: 0.08
Batch: 260; loss: nan; acc: 0.09
Batch: 280; loss: nan; acc: 0.06
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.11
Batch: 340; loss: nan; acc: 0.09
Batch: 360; loss: nan; acc: 0.06
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.11
Batch: 420; loss: nan; acc: 0.14
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.11
Batch: 580; loss: nan; acc: 0.05
Batch: 600; loss: nan; acc: 0.03
Batch: 620; loss: nan; acc: 0.12
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 25 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.09
Batch: 40; loss: nan; acc: 0.03
Batch: 60; loss: nan; acc: 0.11
Batch: 80; loss: nan; acc: 0.14
Batch: 100; loss: nan; acc: 0.17
Batch: 120; loss: nan; acc: 0.11
Batch: 140; loss: nan; acc: 0.12
Batch: 160; loss: nan; acc: 0.06
Batch: 180; loss: nan; acc: 0.19
Batch: 200; loss: nan; acc: 0.06
Batch: 220; loss: nan; acc: 0.09
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.03
Batch: 280; loss: nan; acc: 0.12
Batch: 300; loss: nan; acc: 0.08
Batch: 320; loss: nan; acc: 0.16
Batch: 340; loss: nan; acc: 0.06
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.02
Batch: 460; loss: nan; acc: 0.09
Batch: 480; loss: nan; acc: 0.0
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.06
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.09
Batch: 620; loss: nan; acc: 0.03
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 26 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.12
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.09
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.09
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.11
Batch: 160; loss: nan; acc: 0.17
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.03
Batch: 220; loss: nan; acc: 0.06
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.05
Batch: 280; loss: nan; acc: 0.16
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.14
Batch: 340; loss: nan; acc: 0.11
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.05
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.06
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.11
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.12
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.08
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 27 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.08
Batch: 60; loss: nan; acc: 0.06
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.08
Batch: 120; loss: nan; acc: 0.06
Batch: 140; loss: nan; acc: 0.14
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.05
Batch: 200; loss: nan; acc: 0.12
Batch: 220; loss: nan; acc: 0.05
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.12
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.09
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.03
Batch: 360; loss: nan; acc: 0.09
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.08
Batch: 420; loss: nan; acc: 0.11
Batch: 440; loss: nan; acc: 0.12
Batch: 460; loss: nan; acc: 0.08
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.12
Batch: 540; loss: nan; acc: 0.08
Batch: 560; loss: nan; acc: 0.09
Batch: 580; loss: nan; acc: 0.11
Batch: 600; loss: nan; acc: 0.17
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 28 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.09
Batch: 20; loss: nan; acc: 0.19
Batch: 40; loss: nan; acc: 0.06
Batch: 60; loss: nan; acc: 0.19
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.11
Batch: 120; loss: nan; acc: 0.09
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.11
Batch: 180; loss: nan; acc: 0.06
Batch: 200; loss: nan; acc: 0.11
Batch: 220; loss: nan; acc: 0.12
Batch: 240; loss: nan; acc: 0.11
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.03
Batch: 300; loss: nan; acc: 0.12
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.12
Batch: 380; loss: nan; acc: 0.14
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.08
Batch: 440; loss: nan; acc: 0.11
Batch: 460; loss: nan; acc: 0.05
Batch: 480; loss: nan; acc: 0.08
Batch: 500; loss: nan; acc: 0.09
Batch: 520; loss: nan; acc: 0.08
Batch: 540; loss: nan; acc: 0.11
Batch: 560; loss: nan; acc: 0.08
Batch: 580; loss: nan; acc: 0.08
Batch: 600; loss: nan; acc: 0.12
Batch: 620; loss: nan; acc: 0.11
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 29 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.16
Batch: 20; loss: nan; acc: 0.03
Batch: 40; loss: nan; acc: 0.12
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.11
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.08
Batch: 140; loss: nan; acc: 0.09
Batch: 160; loss: nan; acc: 0.09
Batch: 180; loss: nan; acc: 0.12
Batch: 200; loss: nan; acc: 0.2
Batch: 220; loss: nan; acc: 0.16
Batch: 240; loss: nan; acc: 0.09
Batch: 260; loss: nan; acc: 0.16
Batch: 280; loss: nan; acc: 0.09
Batch: 300; loss: nan; acc: 0.05
Batch: 320; loss: nan; acc: 0.05
Batch: 340; loss: nan; acc: 0.17
Batch: 360; loss: nan; acc: 0.08
Batch: 380; loss: nan; acc: 0.16
Batch: 400; loss: nan; acc: 0.12
Batch: 420; loss: nan; acc: 0.09
Batch: 440; loss: nan; acc: 0.09
Batch: 460; loss: nan; acc: 0.11
Batch: 480; loss: nan; acc: 0.11
Batch: 500; loss: nan; acc: 0.14
Batch: 520; loss: nan; acc: 0.03
Batch: 540; loss: nan; acc: 0.16
Batch: 560; loss: nan; acc: 0.06
Batch: 580; loss: nan; acc: 0.06
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.09
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

Epoch 30 start
The current lr is: 0.0010000000000000002
Batch: 0; loss: nan; acc: 0.08
Batch: 20; loss: nan; acc: 0.08
Batch: 40; loss: nan; acc: 0.05
Batch: 60; loss: nan; acc: 0.09
Batch: 80; loss: nan; acc: 0.06
Batch: 100; loss: nan; acc: 0.09
Batch: 120; loss: nan; acc: 0.12
Batch: 140; loss: nan; acc: 0.16
Batch: 160; loss: nan; acc: 0.08
Batch: 180; loss: nan; acc: 0.08
Batch: 200; loss: nan; acc: 0.08
Batch: 220; loss: nan; acc: 0.14
Batch: 240; loss: nan; acc: 0.05
Batch: 260; loss: nan; acc: 0.06
Batch: 280; loss: nan; acc: 0.08
Batch: 300; loss: nan; acc: 0.11
Batch: 320; loss: nan; acc: 0.12
Batch: 340; loss: nan; acc: 0.08
Batch: 360; loss: nan; acc: 0.14
Batch: 380; loss: nan; acc: 0.08
Batch: 400; loss: nan; acc: 0.14
Batch: 420; loss: nan; acc: 0.06
Batch: 440; loss: nan; acc: 0.08
Batch: 460; loss: nan; acc: 0.16
Batch: 480; loss: nan; acc: 0.16
Batch: 500; loss: nan; acc: 0.06
Batch: 520; loss: nan; acc: 0.14
Batch: 540; loss: nan; acc: 0.12
Batch: 560; loss: nan; acc: 0.14
Batch: 580; loss: nan; acc: 0.14
Batch: 600; loss: nan; acc: 0.11
Batch: 620; loss: nan; acc: 0.05
Train Epoch over. train_loss: nan; train_accuracy: 0.1 

Batch: 0; loss: nan; acc: 0.11
Batch: 20; loss: nan; acc: 0.17
Batch: 40; loss: nan; acc: 0.11
Batch: 60; loss: nan; acc: 0.03
Batch: 80; loss: nan; acc: 0.12
Batch: 100; loss: nan; acc: 0.14
Batch: 120; loss: nan; acc: 0.16
Batch: 140; loss: nan; acc: 0.08
Val Epoch over. val_loss: nan; val_accuracy: 0.10121417197452229 

plots/subspace_training/reg_lenet/2020-01-16 13:47:42/d_dim_500_lr_0.1_seed_1_epochs_30_batchsize_64
plots/subspace_training/reg_lenet/2020-01-16 13:47:42/d_dim_XXXXX_lr_0.1_seed_1_epochs_30_batchsize_64
